[
    "@inproceedings{10.1145/3613904.3641954,\nauthor = {Lupetti, Maria Luce and Murray-Rust, Dave},\ntitle = {(Un)making AI Magic: A Design Taxonomy},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641954},\ndoi = {10.1145/3613904.3641954},\nabstract = {This paper examines the role that enchantment plays in the design of AI things by constructing a taxonomy of design approaches that increase or decrease the perception of magic and enchantment. We start from the design discourse surrounding recent developments in AI technologies, highlighting specific interaction qualities such as algorithmic uncertainties and errors and articulating relations to the rhetoric of magic and supernatural thinking. Through analyzing and reflecting upon 52 students’ design projects from two editions of a Masters course in design and AI, we identify seven design principles and unpack the effects of each in terms of enchantment and disenchantment. We conclude by articulating ways in which this taxonomy can be approached and appropriated by design/HCI practitioners, especially to support exploration and reflexivity.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1},\nnumpages = {21},\nkeywords = {artificial intelligence, critical computing, critical design, magic, research through design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642179,\nauthor = {Zhong, Ruican and Shin, Donghoon and Meza, Rosemary and Klasnja, Predrag and Colusso, Lucas and Hsieh, Gary},\ntitle = {AI-Assisted Causal Pathway Diagram for Human-Centered Design},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642179},\ndoi = {10.1145/3613904.3642179},\nabstract = {This paper explores the integration of causal pathway diagrams (CPD) into human-centered design (HCD), investigating how these diagrams can enhance the early stages of the design process. A dedicated CPD plugin for the online collaborative whiteboard platform Miro was developed to streamline diagram creation and offer real-time AI-driven guidance. Through a user study with designers (N = 20), we found that CPD’s branching and its emphasis on causal connections supported both divergent and convergent processes during design. CPD can also facilitate communication among stakeholders. Additionally, we found our plugin significantly reduces designers’ cognitive workload and increases their creativity during brainstorming, highlighting the implications of AI-assisted tools in supporting creative work and evidence-based designs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {2},\nnumpages = {19},\nkeywords = {LLM, causal pathway diagram, generative AI, human-centered design, implementation science},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642168,\nauthor = {Kuang, Emily and Li, Minghao and Fan, Mingming and Shinohara, Kristen},\ntitle = {Enhancing UX Evaluation Through Collaboration with Conversational AI Assistants: Effects of Proactive Dialogue and Timing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642168},\ndoi = {10.1145/3613904.3642168},\nabstract = {Usability testing is vital for enhancing the user experience (UX) of interactive systems. However, analyzing test videos is complex and resource-intensive. Recent AI advancements have spurred exploration into human-AI collaboration for UX analysis, particularly through natural language. Unlike user-initiated dialogue, our study investigated the potential of proactive conversational assistants to aid UX evaluators through automatic suggestions at three distinct times: before, in sync with, and after potential usability problems. We conducted a hybrid Wizard-of-Oz study involving 24 UX evaluators, using ChatGPT to generate automatic problem suggestions and a human actor to respond to impromptu questions. While timing did not significantly impact analytic performance, suggestions appearing after potential problems were preferred, enhancing trust and efficiency. Participants found the automatic suggestions useful, but they collectively identified more than twice as many problems, underscoring the irreplaceable role of human expertise. Our findings also offer insights into future human-AI collaborative tools for UX evaluation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {3},\nnumpages = {16},\nkeywords = {Human-AI collaboration, Proactive conversational assistants, Usability testing, User experience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641920,\nauthor = {Lin, David Chuan-En and Martelaro, Nikolas},\ntitle = {Jigsaw: Supporting Designers to Prototype Multimodal Applications by Chaining AI Foundation Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641920},\ndoi = {10.1145/3613904.3641920},\nabstract = {Recent advancements in AI foundation models have made it possible for them to be utilized off-the-shelf for creative tasks, including ideating design concepts or generating visual prototypes. However, integrating these models into the creative process can be challenging as they often exist as standalone applications tailored to specific tasks. To address this challenge, we introduce Jigsaw, a prototype system that employs puzzle pieces as metaphors to represent foundation models. Jigsaw allows designers to combine different foundation model capabilities across various modalities by assembling compatible puzzle pieces. To inform the design of Jigsaw, we interviewed ten designers and distilled design goals. In a user study, we showed that Jigsaw enhanced designers’ understanding of available foundation model capabilities, provided guidance on combining capabilities across different modalities and tasks, and served as a canvas to support design exploration, prototyping, and documentation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {4},\nnumpages = {15},\nkeywords = {foundation models, machine learning, multimodal, prototyping, visual programming interface},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641915,\nauthor = {Lawley, Lane and Maclellan, Christopher},\ntitle = {VAL: Interactive Task Learning with GPT Dialog Parsing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641915},\ndoi = {10.1145/3613904.3641915},\nabstract = {Machine learning often requires millions of examples to produce static, black-box models. In contrast, interactive task learning (ITL) emphasizes incremental knowledge acquisition from limited instruction provided by humans in modalities such as natural language. However, ITL systems often suffer from brittle, error-prone language parsing, which limits their usability. Large language models (LLMs) are resistant to brittleness but are not interpretable and cannot learn incrementally. We present VAL, an ITL system with a new philosophy for LLM/symbolic integration. By using LLMs only for specific tasks—such as predicate and argument selection—within an algorithmic framework, VAL reaps the benefits of LLMs to support interactive learning of hierarchical task knowledge from natural language. Acquired knowledge is human interpretable and generalizes to support execution of novel tasks without additional training. We studied users’ interactions with VAL in a video game setting, finding that most users could successfully teach VAL using language they felt was natural.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {5},\nnumpages = {18},\nkeywords = {GPT, hierarchical task networks, hybrid AI, large language models (LLMs), neuro-symbolic AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642782,\nauthor = {Duan, Peitong and Warner, Jeremy and Li, Yang and Hartmann, Bjoern},\ntitle = {Generating Automatic Feedback on UI Mockups with Large Language Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642782},\ndoi = {10.1145/3613904.3642782},\nabstract = {Feedback on user interface (UI) mockups is crucial in design. However, human feedback is not always readily available. We explore the potential of using large language models for automatic feedback. Specifically, we focus on applying GPT-4 to automate heuristic evaluation, which currently entails a human expert assessing a UI’s compliance with a set of design guidelines. We implemented a Figma plugin that takes in a UI design and a set of written heuristics, and renders automatically-generated feedback as constructive suggestions. We assessed performance on 51 UIs using three sets of guidelines, compared GPT-4-generated design suggestions with those from human experts, and conducted a study with 12 expert designers to understand fit with existing practice. We found that GPT-4-based feedback is useful for catching subtle errors, improving text, and considering UI semantics, but feedback also decreased in utility over iterations. Participants described several uses for this plugin despite its imperfect suggestions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {6},\nnumpages = {20},\nkeywords = {Computational UI Design Tools, Large Language Models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642350,\nauthor = {Feng, Sidong and Ma, Suyu and Wang, Han and Kong, David and Chen, Chunyang},\ntitle = {MUD: Towards a Large-Scale and Noise-Filtered UI Dataset for Modern Style UI Modeling},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642350},\ndoi = {10.1145/3613904.3642350},\nabstract = {The importance of computational modeling of mobile user interfaces (UIs) is undeniable. However, these require a high-quality UI dataset. Existing datasets are often outdated, collected years ago, and are frequently noisy with mismatches in their visual representation. This presents challenges in modeling UI understanding in the wild. This paper introduces a novel approach to automatically mine UI data from Android apps, leveraging Large Language Models (LLMs) to mimic human-like exploration. To ensure dataset quality, we employ the best practices in UI noise filtering and incorporate human annotation as a final validation step. Our results demonstrate the effectiveness of LLMs-enhanced app exploration in mining more meaningful UIs, resulting in a large dataset MUD of 18k human-annotated UIs from 3.3k apps. We highlight the usefulness of MUD in two common UI modeling tasks: element detection and UI retrieval, showcasing its potential to establish a foundation for future research into high-quality, modern UIs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {7},\nnumpages = {14},\nkeywords = {UI modeling, datasets, large language models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642068,\nauthor = {Li, Jiahao Nick and Xu, Yan and Grossman, Tovi and Santosa, Stephanie and Li, Michelle},\ntitle = {OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642068},\ndoi = {10.1145/3613904.3642068},\nabstract = {The progression to “Pervasive Augmented Reality” envisions easy access to multimodal information continuously. However, in many everyday scenarios, users are occupied physically, cognitively or socially. This may increase the friction to act upon the multimodal information that users encounter in the world. To reduce such friction, future interactive interfaces should intelligently provide quick access to digital actions based on users’ context. To explore the range of possible digital actions, we conducted a diary study that required participants to capture and share the media that they intended to perform actions on (e.g., images or audio), along with their desired actions and other contextual information. Using this data, we generated a holistic design space of digital follow-up actions that could be performed in response to different types of multimodal sensory inputs. We then designed OmniActions, a pipeline powered by large language models (LLMs) that processes multimodal sensory inputs and predicts follow-up actions on the target information grounded in the derived design space. Using the empirical data collected in the diary study, we performed quantitative evaluations on three variations of LLM techniques (intent classification, in-context learning and finetuning) and identified the most effective technique for our task. Additionally, as an instantiation of the pipeline, we developed an interactive prototype and reported preliminary user feedback about how people perceive and react to the action predictions and its errors.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {8},\nnumpages = {22},\nkeywords = {dataset, diary study, digital follow-up actions, large language models, pervasive augmented reality, predictive interface},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642481,\nauthor = {Xiang, Wei and Zhu, Hanfei and Lou, Suqi and Chen, Xinli and Pan, Zhenghua and Jin, Yuping and Chen, Shi and Sun, Lingyun},\ntitle = {SimUser: Generating Usability Feedback by Simulating Various Users Interacting with Mobile Applications},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642481},\ndoi = {10.1145/3613904.3642481},\nabstract = {The conflict between the rapid iteration demand of prototyping and the time-consuming nature of user tests has led researchers to adopt AI methods to identify usability issues. However, these AI-driven methods concentrate on evaluating the feasibility of a system, while often overlooking the influence of specified user characteristics and usage contexts. Our work proposes a tool named SimUser based on large language models (LLMs) with the Chain-of-Thought structure and user modeling method. It generates usability feedback by simulating the interaction between users and applications, which is influenced by user characteristics and contextual factors. The empirical study (48 human users and 21 designers) validated that in the context of a simple smartwatch interface, SimUser could generate heuristic usability feedback with the similarity varying from 35.7\\% to 100\\% according to the user groups and usability category. Our work provides insights into simulating users by LLM to improve future design activities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {9},\nnumpages = {17},\nkeywords = {Large language models, Usability feedback, User Simulation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642615,\nauthor = {Nair, Vishnu and Zhu, Hanxiu 'Hazel' and Song, Peize and Wang, Jizhong and Smith, Brian A.},\ntitle = {Surveyor: Facilitating Discovery Within Video Games for Blind and Low Vision Players},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642615},\ndoi = {10.1145/3613904.3642615},\nabstract = {Video games are increasingly accessible to blind and low vision (BLV) players, yet many aspects remain inaccessible. One aspect is the joy players feel when they explore environments and make new discoveries, which is integral to many games. Sighted players experience discovery by surveying environments and identifying unexplored areas. Current accessibility tools, however, guide BLV players directly to items and places, robbing them of that experience. Thus, a crucial challenge is to develop navigation assistance tools that also foster exploration and discovery. To address this challenge, we propose the concept of exploration assistance in games and design Surveyor, an in-game exploration assistance tool that enhances discovery by tracking where BLV players look and highlighting unexplored areas. We designed Surveyor using insights from a formative study and compared Surveyor’s effectiveness to approaches found in existing accessible games. Our findings reveal implications for facilitating richer play experiences for BLV users within games.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {10},\nnumpages = {15},\nkeywords = {blind-accessible video games, blindness and low vision, exploration within virtual environments},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642002,\nauthor = {Gao, Jie and Guo, Yuchen and Lim, Gionnieve and Zhang, Tianqin and Zhang, Zheng and Li, Toby Jia-Jun and Perrault, Simon Tangi},\ntitle = {CollabCoder: A Lower-barrier, Rigorous Workflow for Inductive Collaborative Qualitative Analysis with Large Language Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642002},\ndoi = {10.1145/3613904.3642002},\nabstract = {Collaborative Qualitative Analysis (CQA) can enhance qualitative analysis rigor and depth by incorporating varied viewpoints. Nevertheless, ensuring a rigorous CQA procedure itself can be both complex and costly. To lower this bar, we take a theoretical perspective to design a one-stop, end-to-end workflow, CollabCoder, that integrates Large Language Models (LLMs) into key inductive CQA stages. In the independent open coding phase, CollabCoder offers AI-generated code suggestions and records decision-making data. During the iterative discussion phase, it promotes mutual understanding by sharing this data within the coding team and using quantitative metrics to identify coding (dis)agreements, aiding in consensus-building. In the codebook development phase, CollabCoder provides primary code group suggestions, lightening the workload of developing a codebook from scratch. A 16-user evaluation confirmed the effectiveness of CollabCoder, demonstrating its advantages over the existing CQA platform. All related materials of CollabCoder, including code and further extensions, will be included in: https://gaojie058.github.io/CollabCoder/.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {11},\nnumpages = {29},\nkeywords = {Collaborative Qualitative Analysis, Grounded Theory, Inductive Qualitative Coding, Large Language Models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641917,\nauthor = {Wang, Jiyao and Hu, Haolong and Wang, Zuyuan and Yan, Song and Sheng, Youyu and He, Dengbo},\ntitle = {Evaluating Large Language Models on Academic Literature Understanding and Review: An Empirical Study among Early-stage Scholars},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641917},\ndoi = {10.1145/3613904.3641917},\nabstract = {The rapid advancement of large language models (LLMs) such as ChatGPT makes LLM-based academic tools possible. However, little research has empirically evaluated how scholars perform different types of academic tasks with LLMs. Through an empirical study followed by a semi-structured interview, we assessed 48 early-stage scholars’ performance in conducting core academic activities (i.e., paper reading and literature reviews) under different levels of time pressure. Before conducting the tasks, participants received different training programs regarding the limitations and capabilities of the LLMs. After completing the tasks, participants completed an interview. Quantitative data regarding the influence of time pressure, task type, and training program on participants’ performance in academic tasks was analyzed. Semi-structured interviews provided additional information on the influential factors of task performance, participants’ perceptions of LLMs, and concerns about integrating LLMs into academic workflows. The findings can guide more appropriate usage and design of LLM-based tools in assisting academic work.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {12},\nnumpages = {18},\nkeywords = {academic tasks, human-AI collaboration, large language model, user perception},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642266,\nauthor = {Shin, Donghoon and Wang, Lucy Lu and Hsieh, Gary},\ntitle = {From Paper to Card: Transforming Design Implications with Generative AI},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642266},\ndoi = {10.1145/3613904.3642266},\nabstract = {Communicating design implications is common within the HCI community when publishing academic papers, yet these papers are rarely read and used by designers. One solution is to use design cards as a form of translational resource that communicates valuable insights from papers in a more digestible and accessible format to assist in design processes. However, creating design cards can be time-consuming, and authors may lack the resources/know-how to produce cards. Through an iterative design process, we built a system that helps create design cards from academic papers using an LLM and text-to-image model. Our evaluation with designers (N = 21) and authors of selected papers (N = 12) revealed that designers perceived the design implications from our design cards as more inspiring and generative, compared to reading original paper texts, and the authors viewed our system as an effective way of communicating their design implications. We also propose future enhancements for AI-generated design cards.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {13},\nnumpages = {15},\nkeywords = {design card, generative AI, large language model, text-to-image model, translational science},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642289,\nauthor = {August, Tal and Lo, Kyle and Smith, Noah A. and Reinecke, Katharina},\ntitle = {Know Your Audience: The benefits and pitfalls of generating plain language summaries beyond the \"general\" audience},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642289},\ndoi = {10.1145/3613904.3642289},\nabstract = {Language models (LMs) show promise as tools for communicating science to the general public by simplifying and summarizing complex language. Because models can be prompted to generate text for a specific audience (e.g., college-educated adults), LMs might be used to create multiple versions of plain language summaries for people with different familiarities of scientific topics. However, it is not clear what the benefits and pitfalls of adaptive plain language are. When is simplifying necessary, what are the costs in doing so, and do these costs differ for readers with different background knowledge? Through three within-subjects studies in which we surface summaries for different envisioned audiences to participants of different backgrounds, we found that while simpler text led to the best reading experience for readers with little to no familiarity in a topic, high familiarity readers tended to ignore certain details in overly plain summaries (e.g., study limitations). Our work provides methods and guidance on ways of adapting plain language summaries beyond the single “general” audience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {14},\nnumpages = {26},\nkeywords = {LLMs, Language complexity, science communication},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642081,\nauthor = {Bhattacharjee, Ananya and Zeng, Yuchen and Xu, Sarah Yi and Kulzhabayeva, Dana and Ma, Minyi and Kornfield, Rachel and Ahmed, Syed Ishtiaque and Mariakakis, Alex and Czerwinski, Mary P and Kuzminykh, Anastasia and Liut, Michael and Williams, Joseph Jay},\ntitle = {Understanding the Role of Large Language Models in Personalizing and Scaffolding Strategies to Combat Academic Procrastination},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642081},\ndoi = {10.1145/3613904.3642081},\nabstract = {Traditional interventions for academic procrastination often fail to capture the nuanced, individual-specific factors that underlie them. Large language models (LLMs) hold immense potential for addressing this gap by permitting open-ended inputs, including the ability to customize interventions to individuals’ unique needs. However, user expectations and potential limitations of LLMs in this context remain underexplored. To address this, we conducted interviews and focus group discussions with 15 university students and 6 experts, during which a technology probe for generating personalized advice for managing procrastination was presented. Our results highlight the necessity for LLMs to provide structured, deadline-oriented steps and enhanced user support mechanisms. Additionally, our results surface the need for an adaptive approach to questioning based on factors like busyness. These findings offer crucial design implications for the development of LLM-based tools for managing procrastination while cautioning the use of LLMs for therapeutic guidance.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {15},\nnumpages = {18},\nkeywords = {ChatGPT, Education, GPT-4, Large Language Models, Personalized Reflections, Procrastination},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642451,\nauthor = {McConvey, Kelly and Guha, Shion},\ntitle = {\"This is not a data problem\": Algorithms and Power in Public Higher Education in Canada},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642451},\ndoi = {10.1145/3613904.3642451},\nabstract = {Algorithmic decision-making is increasingly being adopted across public higher education. The expansion of data-driven practices by post-secondary institutions has occurred in parallel with the adoption of New Public Management approaches by neoliberal administrations. In this study, we conduct a qualitative analysis of an in-depth ethnographic case study of data and algorithms in use at a public college in Ontario, Canada. We identify the data, algorithms, and outcomes in use at the college. We assess how the college’s processes and relationships support those outcomes and the different stakeholders’ perceptions of the college’s data-driven systems. In addition, we find that the growing reliance on algorithmic decisions leads to increased student surveillance, exacerbation of existing inequities, and the automation of the faculty-student relationship. Finally, we identify a cycle of increased institutional power perpetuated by algorithmic decision-making, and driven by a push towards financial sustainability.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {16},\nnumpages = {14},\nkeywords = {Artificial Intelligence, Higher Education, Human-Centered Machine Learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642698,\nauthor = {Liu, Yiren and Chen, Si and Cheng, Haocong and Yu, Mengxia and Ran, Xiao and Mo, Andrew and Tang, Yiliu and Huang, Yun},\ntitle = {How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642698},\ndoi = {10.1145/3613904.3642698},\nabstract = {Developing novel research questions (RQs) often requires extensive literature reviews, especially in interdisciplinary fields. To support RQ development through human-AI co-creation, we leveraged Large Language Models (LLMs) to build an LLM-based agent system named CoQuest. We conducted an experiment with 20 HCI researchers to examine the impact of two interaction designs: breadth-first and depth-first RQ generation. The findings revealed that participants perceived the breadth-first approach as more creative and trustworthy upon task completion. Conversely, during the task, participants considered the depth-first generated RQs as more creative. Additionally, we discovered that AI processing delays allowed users to reflect on multiple RQs simultaneously, leading to a higher quantity of generated RQs and an enhanced sense of control. Our work makes both theoretical and practical contributions by proposing and evaluating a mental model for human-AI co-creation of RQs. We also address potential ethical issues, such as biases and over-reliance on AI, advocating for using the system to improve human research creativity rather than automating scientific inquiry. The system’s source is available at: https://github.com/yiren-liu/coquest.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {17},\nnumpages = {25},\nkeywords = {Co-creation Systems, Large Language Models, Mixed-initiative Design, Scientifc Discovery},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3643043,\nauthor = {Wu, Chuhao and Chakravorti, Tatiana and Carroll, John M. and Rajtmajer, Sarah},\ntitle = {Integrating measures of replicability into scholarly search: Challenges and opportunities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3643043},\ndoi = {10.1145/3613904.3643043},\nabstract = {Challenges to reproducibility and replicability have gained widespread attention, driven by large replication projects with lukewarm success rates. A nascent work has emerged developing algorithms to estimate the replicability of published findings. The current study explores ways in which AI-enabled signals of confidence in research might be integrated into the literature search. We interview 17 PhD researchers about their current processes for literature search and ask them to provide feedback on a replicability estimation tool. Our findings suggest that participants tend to confuse replicability with generalizability and related concepts. Information about replicability can support researchers throughout the research design processes. However, the use of AI estimation is debatable due to the lack of explainability and transparency. The ethical implications of AI-enabled confidence assessment must be further studied before such tools could be widely accepted. We discuss implications for the design of technological tools to support scholarly activities and advance replicability.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {18},\nnumpages = {18},\nkeywords = {explainable artificial intelligence, literature search, replicability, reproducibility},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642196,\nauthor = {Lee, Yoonjoo and Kang, Hyeonsu B and Latzke, Matt and Kim, Juho and Bragg, Jonathan and Chang, Joseph Chee and Siangliulue, Pao},\ntitle = {PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642196},\ndoi = {10.1145/3613904.3642196},\nabstract = {With the rapid growth of scholarly archives, researchers subscribe to “paper alert’’ systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users’ research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to better understand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {19},\nnumpages = {19},\nkeywords = {Contextualized Descriptions, Large Language Models, Scientific Paper},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642948,\nauthor = {Li, Xiaoxuan and Ren, Xiangshi and Suzuki, Xin and Yamaji, Naoaki and Fung, Kin Wa and Gondo, Yasuyuki},\ntitle = {Designing a Multisensory VR Game Prototype for Older Adults - the Acceptability and Design Implications},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642948},\ndoi = {10.1145/3613904.3642948},\nabstract = {Simultaneous declines in visual function (e.g., dynamic visual acuity), cognitive ability (e.g., cognitive control/multitasking), and physical function (e.g., balance) are major symptoms of aging. Integrating stimulation for those sensory channels into a game could be a suitable way for older adults to engage in long-term health interventions. However, existing game design has not considered the relationship and synergistic impact of multisensory channels of dynamic visual acuity, cognitive ability, and physical function for older adults. We therefore developed the first multisensory VR game system prototype based on cognitive psychology paradigms (e.g., multitasking and Go/No-Go tasks), full-body movement (limb movement), and dynamic visual acuity exercises (horizontal, vertical and forward-backward eye movements) in the VR system environment. We then conducted an experiment to measure the acceptability (in terms of e.g., cybersickness, mental workload, etc.) of our VR game for older adults. The young adults and a PC task were included for comparisons. Qualitative and quantitative results showed that older adults did not experience cybersickness in either sitting or standing postures during the VR gameplay; they well-accepted the workload of the VR game compared to the PC task. Our findings revealed that the design combination of three sensory channels shows synergistic benefits for older adults. Our game encourages older adults to engage in extensive body movement in sitting and standing postures, this is particularly important to people with disabilities who cannot stand. Design implications are provided for the future development and implementation of VR game design for older adults. Our work provides empirical support for the acceptability of multisensory VR systems in older adults, and contributes to the future design of VR games for older adults.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {20},\nnumpages = {18},\nkeywords = {Acceptability, Cognitive ability, Cybersickness, Dynamic visual acuity, Older adults, Physical ability, VR Game, Workload},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641901,\nauthor = {Jin, Xiaofu and Tong, Wai and Wei, Xiaoying and Wang, Xian and Kuang, Emily and Mo, Xiaoyu and Qu, Huamin and Fan, Mingming},\ntitle = {Exploring the Opportunity of Augmented Reality (AR) in Supporting Older Adults to Explore and Learn Smartphone Applications},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641901},\ndoi = {10.1145/3613904.3641901},\nabstract = {The global aging trend compels older adults to navigate the evolving digital landscape, presenting a substantial challenge in mastering smartphone applications. While Augmented Reality (AR) holds promise for enhancing learning and user experience, its role in aiding older adults’ smartphone app exploration remains insufficiently explored. Therefore, we conducted a two-phase study: (1) a workshop with 18 older adults to identify app exploration challenges and potential AR interventions, and (2) tech-probe participatory design sessions with 15 participants to co-create AR support tools. Our research highlights AR’s effectiveness in reducing physical and cognitive strain among older adults during app exploration, especially during multi-app usage and the trial-and-error learning process. We also examined their interactional experiences with AR, yielding design considerations on tailoring AR tools for smartphone app exploration. Ultimately, our study unveils the prospective landscape of AR in supporting the older demographic, both presently and in future scenarios.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {21},\nnumpages = {18},\nkeywords = {augmented reality, independent learning, older adults, smartphone exploration},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642938,\nauthor = {Tanprasert, Teerapaun and Dai, Jiamin and McGrenere, Joanna},\ntitle = {HelpCall: Designing Informal Technology Assistance for Older Adults via Videoconferencing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642938},\ndoi = {10.1145/3613904.3642938},\nabstract = {Older adults commonly rely on younger family members for remote tech support, but the current general-purpose video-conferencing platforms fall short of effectively catering to their needs. We introduce the design concept and prototypes for HelpCall, an augmentation of these platforms that provides aids for learning computer tasks, including a step-by-step visual guide automatically generated from synchronous human instruction. Through observations and interviews with older adults (N=14), we assessed the potential of the HelpCall concept and compared its two design candidates: Tooltip with numbered location markers and List of written steps. All participants acknowledged HelpCall’s potential to improve the comfort and efficiency of synchronous tech support. Tooltip emerged as more promising and could be enhanced by incorporating the well-received features from List. Our findings provide clear directions for advancing HelpCall design and new insights into designing synchronous software help for older adults, taking a step towards universal accessibility of digital technology.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {22},\nnumpages = {23},\nkeywords = {informal support, older adults, remote, seniors, software learning, synchronous},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642796,\nauthor = {Yu, Ja Eun and Chattopadhyay, Debaleena},\ntitle = {Reducing the Search Space on demand helps Older Adults find Mobile UI Features quickly, on par with Younger Adults},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642796},\ndoi = {10.1145/3613904.3642796},\nabstract = {As mobile user interfaces (UI) become feature-rich, navigation gets more complex. Finding features quickly starts demanding information-intensive strategies for decision-making—which can be challenging for older adults. Older adults examine fewer details, requiring fewer cognitive resources, when searching for information with a large number of alternatives. In this paper, we first systematically examine various ways to convey a reduced feature space. Visually emphasizing three relevant options helped older adults find a specific feature more quickly—on par with younger adults. Older users were more efficient when options were highlighted along with their context or with a weighted zoom than when just highlighted, and they also preferred these two the most. We then present Nav Nudge, an interaction technique that uses voice input and large language models to visually reduce the feature search space on demand—and discuss how older adults use it within a mobile map application.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {23},\nnumpages = {22},\nkeywords = {accessibility, interaction technique, mobile interface, older adults, tech support, visual feedback},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642558,\nauthor = {Wu, Zhiqing and Wang, Duotun and Zhang, Shumeng and Huang, Yuru and Wang, Zeyu and Fan, Mingming},\ntitle = {Toward Making Virtual Reality (VR) More Inclusive for Older Adults: Investigating Aging Effect on Target Selection and Manipulation Tasks in VR},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642558},\ndoi = {10.1145/3613904.3642558},\nabstract = {Recent studies show the promise of VR in improving physical, cognitive, and emotional health of older adults. However, prior work on optimizing object selection and manipulation performance in VR was mostly conducted among younger adults. It remains unclear how older adults would perform such tasks compared to younger adults and the challenges they might face. To fill in this gap, we conducted two studies with both older and younger adults to understand their performances and user experiences of object selection and manipulation in VR respectively. Based on the results, we delineated interaction difficulties that older adults exhibited in VR and identified multiple factors, such as headset-related neck fatigue, extra head movements from out-of-view interactions, and slow spatial perceptions, that significantly decreased the motor performance of older adults. We further proposed design recommendations for improving the accessibility of direct interaction experiences in VR for older adults.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {24},\nnumpages = {17},\nkeywords = {Empirical study that tells us about people, Lab Study, Older Adults, Virtual/Augmented Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641905,\nauthor = {Salimzadeh, Sara and He, Gaole and Gadiraju, Ujwal},\ntitle = {Dealing with Uncertainty: Understanding the Impact of Prognostic Versus Diagnostic Tasks on Trust and Reliance in Human-AI Decision Making},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641905},\ndoi = {10.1145/3613904.3641905},\nabstract = {While existing literature has explored and revealed several insights pertaining to the role of human factors (e.g., prior experience, domain knowledge) and attributes of AI systems (e.g., accuracy, trustworthiness), there is a limited understanding around how the important task characteristics of complexity and uncertainty shape human decision-making and human-AI team performance. In this work, we aim to address this research and empirical gap by systematically exploring how task complexity and uncertainty influence human-AI decision-making. Task complexity refers to the load of information associated with a task, while task uncertainty refers to the level of unpredictability associated with the outcome of a task. We conducted a between-subjects user study (N = 258) in the context of a trip-planning task to investigate the impact of task complexity and uncertainty on human trust and reliance on AI systems. Our results revealed that task complexity and uncertainty have a significant impact on user reliance on AI systems. When presented with complex and uncertain tasks, users tended to rely more on AI systems while demonstrating lower levels of appropriate reliance compared to tasks that were less complex and uncertain. In contrast, we found that user trust in the AI systems was not influenced by task complexity and uncertainty. Our findings can help inform the future design of empirical studies exploring human-AI decision-making. Insights from this work can inform the design of AI systems and interventions that are better aligned with the challenges posed by complex and uncertain tasks. Finally, the lens of diagnostic versus prognostic tasks can inspire the operationalization of uncertainty in human-AI decision-making studies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {25},\nnumpages = {17},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642586,\nauthor = {Riccio, Piera and Hofmann, Thomas and Oliver, Nuria},\ntitle = {Exposed or Erased: Algorithmic Censorship of Nudity in Art},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642586},\ndoi = {10.1145/3613904.3642586},\nabstract = {The intersection between art and technology poses new challenges for creative expression in the digital space. This paper investigates the algorithmic censorship of artistic nudity in social platforms by means of a qualitative study via semi-structured interviews with 14 visual artists who have experienced censorship online. We explore the professional, emotional, financial and artistic consequences of content removal or shadow-banning. Focusing on the concept of artistic nudity, our findings emphasize the significant impact on artists of the algorithmic censorship of art, the need to consider art as a special case to safeguard the freedom of expression, the importance of education, the limitations of today’s content moderation algorithms and the pressing need for transparency and recourse mechanisms. We advocate for a multi-stakeholder governance model conducive to a more supportive, safer and inclusive online environment that respects and nurtures human creativity.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {26},\nnumpages = {17},\nkeywords = {algorithmic censorship, artistic nudity, online content moderation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642780,\nauthor = {Ahn, Daehwan and Almaatouq, Abdullah and Gulabani, Monisha and Hosanagar, Kartik},\ntitle = {Impact of Model Interpretability and Outcome Feedback on Trust in AI},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642780},\ndoi = {10.1145/3613904.3642780},\nabstract = {This paper bridges the gap in Human-Computer Interaction (HCI) research by comparatively assessing the effects of interpretability and outcome feedback on user trust and collaborative performance with AI. Through novel pre-registered experiments (N=1,511 total participants) using an interactive prediction task, we analyzed how interpretability and outcome feedback influence users’ task performance and trust in AI. The results counter the widespread belief that interpretability drives trust, showing that interpretability led to no robust improvements in trust and that outcome feedback had a significantly greater and more reliable effect. However, both factors had modest effects on participants’ task performance. These findings suggest that (1) interpretability may be less effective at increasing trust than factors like outcome feedback, and (2) augmenting human performance via AI systems may not be a simple matter of increasing trust in AI, as increased trust is not always associated with equally sizable performance improvements. Our exploratory analyses further delve into the mechanisms underlying this trust-performance paradox. These findings present an opportunity for research to focus not only on methods for generating interpretations but also on techniques that ensure interpretations impact trust and performance in practice.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {27},\nnumpages = {25},\nkeywords = {human-computer systems, hybrid intelligence, machine learning, trust in AI, human-subject experiments, AI-assisted human decision-making, explainable AI, interpretability, outcome feedback},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642018,\nauthor = {Vereschak, Oleksandra and Alizadeh, Fatemeh and Bailly, Gilles and Caramiaux, Baptiste},\ntitle = {Trust in AI-assisted Decision Making: Perspectives from Those Behind the System and Those for Whom the Decision is Made},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642018},\ndoi = {10.1145/3613904.3642018},\nabstract = {Trust between humans and AI in the context of decision-making has acquired an important role in public policy, research and industry. In this context, Human-AI Trust has often been tackled from the lens of cognitive science and psychology, but lacks insights from the stakeholders involved. In this paper, we conducted semi-structured interviews with 7 AI practitioners and 7 decision subjects from various decision domains. We found that 1) interviewees identified the prerequisites for the existence of trust and distinguish trust from trustworthiness, reliance, and compliance; 2) trust in AI-integrated systems is strongly influenced by other human actors, more than the system’s features; 3) the role of Human-AI trust factors is stakeholder-dependent. These results provide clues for the design of Human-AI interactions in which trust plays a major role, as well as outline new research directions in Human-AI Trust.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {28},\nnumpages = {14},\nkeywords = {AI practitioners, artificial intelligence, decision making, decision subjects, qualitative study, trust},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642157,\nauthor = {Meyer, Louie and Aaen, Johanne Engel and Tranberg, Anitamalina Regitse and Kun, Peter and Freiberger, Matthias and Risi, Sebastian and L\\o{}vlie, Anders Sundnes},\ntitle = {Algorithmic Ways of Seeing: Using Object Detection to Facilitate Art Exploration},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642157},\ndoi = {10.1145/3613904.3642157},\nabstract = {This Research through Design paper explores how object detection may be applied to a large digital art museum collection to facilitate new ways of encountering and experiencing art. We present the design and evaluation of an interactive application called SMKExplore, which allows users to explore a museum’s digital collection of paintings by browsing through objects detected in the images, as a novel form of open-ended exploration. We provide three contributions. First, we show how an object detection pipeline can be integrated into a design process for visual exploration. Second, we present the design and development of an app that enables exploration of an art museum’s collection. Third, we offer reflections on future possibilities for museums and HCI researchers to incorporate object detection techniques into the digitalization of museums.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {29},\nnumpages = {18},\nkeywords = {Art, Computer Vision, Experience Design, Exploratory Search, Object Detection},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642731,\nauthor = {Chakrabarty, Tuhin and Laban, Philippe and Agarwal, Divyansh and Muresan, Smaranda and Wu, Chien-Sheng},\ntitle = {Art or Artifice? Large Language Models and the False Promise of Creativity},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642731},\ndoi = {10.1145/3613904.3642731},\nabstract = {Researchers have argued that large language models (LLMs) exhibit high-quality writing capabilities from blogs to stories. However, evaluating objectively the creativity of a piece of writing is challenging. Inspired by the Torrance Test of Creative Thinking (TTCT) [64], which measures creativity as a process, we use the Consensual Assessment Technique [3] and propose Torrance Test of Creative Writing (TTCW) to evaluate creativity as product. TTCW consists of 14 binary tests organized into the original dimensions of Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative writers and implement a human assessment of 48 stories written either by professional authors or LLMs using TTCW. Our analysis shows that LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals. In addition, we explore the use of LLMs as assessors to automate the TTCW evaluation, revealing that none of the LLMs positively correlate with the expert assessments.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {30},\nnumpages = {34},\nkeywords = {Creativity, Design Methods, Evaluation, Human-AI collaboration, Large Language Models, Natural Language Generation, StoryTelling},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642529,\nauthor = {Kim, Taewook and Han, Hyomin and Adar, Eytan and Kay, Matthew and Chung, John Joon Young},\ntitle = {Authors' Values and Attitudes Towards AI-bridged Scalable Personalization of Creative Language Arts},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642529},\ndoi = {10.1145/3613904.3642529},\nabstract = {Generative AI has the potential to create a new form of interactive media: AI-bridged creative language arts (CLA), which bridge the author and audience by personalizing the author’s vision to the audience’s context and taste at scale. However, it is unclear what the authors’ values and attitudes would be regarding AI-bridged CLA. To identify these values and attitudes, we conducted an interview study with 18 authors across eight genres (e.g., poetry, comics) by presenting speculative but realistic AI-bridged CLA scenarios. We identified three benefits derived from the dynamics between author, artifact, and audience: those that 1) authors get from the process, 2) audiences get from the artifact, and 3) authors get from the audience. We found how AI-bridged CLA would either promote or reduce these benefits, along with authors’ concerns. We hope our investigation hints at how AI can provide intriguing experiences to CLA audiences while promoting authors’ values.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {31},\nnumpages = {16},\nkeywords = {Authorial control, Creative language arts, Creative writing, Generative AI, Large language models, Scalable personalization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642461,\nauthor = {Shelby, Renee and Rismani, Shalaleh and Rostamzadeh, Negar},\ntitle = {Generative AI in Creative Practice: ML-Artist Folk Theories of T2I Use, Harm, and Harm-Reduction},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642461},\ndoi = {10.1145/3613904.3642461},\nabstract = {Understanding how communities experience algorithms is necessary to mitigate potential harmful impacts. This paper presents folk theories of text-to-image (T2I) models to enrich understanding of how artist communities experience creative machine learning systems. This research draws on data collected from a workshop with 15 artists from 10 countries who incorporate T2I models in their creative practice. Through reflexive thematic analysis of workshop data, we highlight artist folk theories of T2I use, harm, and harm reduction. Folk theories of use envision T2I models as an artistic medium, a mundane tool, and locate true creativity as rising above model affordances. Theories of harm articulate T2I models as harmed by engineering efforts to eliminate glitches and product policy efforts to limit functionality. Theories of harm-reduction orient towards protecting T2I models for creative practice through transparency and distributed governance. We examine how these theories relate, and conclude by discussing how folk theorization informs responsible AI efforts.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {32},\nnumpages = {17},\nkeywords = {Art \\& Technology, Creativity, Folk Theory, Generative AI, T2I},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642548,\nauthor = {Vear, Craig and Hazzard, Adrian and Moroz, Solomiya and Benerradi, Johann},\ntitle = {Jess+: AI and robotics with inclusive music-making},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642548},\ndoi = {10.1145/3613904.3642548},\nabstract = {This paper discusses the findings from a cross-sector research project investigating how a digital score created using AI and robotics might stimulate new creative opportunities and relationships within the practices of an inclusive music ensemble. Through the concept of a digital score [65], AI and a robotic arm were introduced into an ensemble’s musical practice to evaluate the impact and benefits of using autonomous systems to challenge barriers around a disabled musician’s access to creative music-making. Throughout the development process we placed an emphasis on involvement and togetherness of not only the AI and robots’ contribution to shared creativity amongst the ensemble, but also to the social aspects of the creative process across the team of musicians, developers, researchers and supporting organisations. The findings were surprising with many aspects of the project exceeding the expectations of the original aims. In short, all the musicians benefited from the introduction of these unfamiliar technologies with practices enhanced and relationships transformed.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {33},\nnumpages = {17},\nkeywords = {Embodied AI, creativity, digital score, music-making, neural networks, robotics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642413,\nauthor = {Cao, Jiaxun and Peng, Xuening and Liang, Fan and Tong, Xin},\ntitle = {\"Voices Help Correlate Signs and Words\": Analyzing Deaf and Hard-of-Hearing (DHH) TikTokers’ Content, Practices, and Pitfalls},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642413},\ndoi = {10.1145/3613904.3642413},\nabstract = {Video-sharing platforms such as TikTok have offered new opportunities for d/Deaf and hard-of-hearing (DHH) people to create public-facing content using sign language – an integral part of DHH culture. Besides sign language, DHH creators deal with a variety of modalities when creating videos, such as captions and audio. However, hardly any work has comprehensively addressed DHH creators’ multimodal practices with the lay public’s reactions taken into account. In this paper, we systematically analyzed 308 DHH-authored TikTok videos using a mixed-methods approach, focusing on DHH TikTokers’ content, practices, pitfalls, and viewer engagement. Our findings highlight that while voice features such as synchronous voices are scant and challenging for DHH TikTokers, they may help promote viewer engagement. Other empirical findings, including the distributions of topics, practices, pitfalls, and their correlations with viewer engagement, further lead to actionable suggestions for DHH TikTokers and video-sharing platforms.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {34},\nnumpages = {18},\nkeywords = {Content creation, Deaf and hard-of-hearing, TikTok},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642454,\nauthor = {Ohshiro, Keita and Cartwright, Mark},\ntitle = {Audio Engineering by People Who Are deaf and Hard of Hearing: Balancing Confidence and Limitations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642454},\ndoi = {10.1145/3613904.3642454},\nabstract = {With technological advancements, audio engineering has evolved from a domain exclusive to professionals to one open to amateurs. However, research is limited on the accessibility of audio engineering, particularly for deaf, Deaf, and hard of hearing (DHH) individuals. To bridge this gap, we interviewed eight deaf and hard of hearing (dHH) audio engineers in music to understand accessibility in audio engineering. We found that their hearing magnified challenges in audio engineering: insecurities in sound perception undermined their confidence, and the required extra “hearing work” added complexity. As workarounds, participants employed various technologies and techniques, relied on the support of hearing peers, and developed strategies for learning and growth. Through these practices, they navigate audio engineering while balancing confidence and limitations. For future directions, we recommend exploring technologies that reduce insecurities and “hearing work” to empower DHH audio engineers and working toward a DHH-community-driven approach to accessible audio engineering.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {35},\nnumpages = {13},\nkeywords = {Deaf, accessibility, audio engineering, deaf, hard of hearing, music},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642953,\nauthor = {Luna, Sanzida Mojib and Xu, Jiangnan and Papangelis, Konstantinos and Tigwell, Garreth W. and Lalone, Nicolas and Saker, Michael and Chamberlain, Alan and Laato, Samuli and Dunham, John and Wang, Yihong},\ntitle = {Communication, Collaboration, and Coordination in a Co-located Shared Augmented Reality Game: Perspectives From Deaf and Hard of Hearing People},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642953},\ndoi = {10.1145/3613904.3642953},\nabstract = {Co-located collaborative shared augmented reality (CS-AR) environments have gained considerable research attention, mainly focusing on design, implementation, accuracy, and usability. Yet, a gap persists in our understanding regarding the accessibility and inclusivity of such environments for diverse user groups, such as deaf and Hard of Hearing (DHH) people. To investigate this domain, we used Urban Legends, a multiplayer game in a co-located CS-AR setting. We conducted a user study followed by one-on-one interviews with 17 DHH participants. Our findings revealed the usage of multimodal communication (verbal and non-verbal) before and during the game, impacting the amount of collaboration among participants and how their coordination with AR components, their surroundings, and other participants improved throughout the rounds. We utilize our data to propose design enhancements, including onscreen visuals and speech-to-text transcription, centered on participant perspectives and our analysis.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {36},\nnumpages = {14},\nkeywords = {Co-located AR, Collaborative AR, Deaf and Hard of Hearing, Shared AR},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642057,\nauthor = {Veluri, Bandhav and Itani, Malek and Chen, Tuochao and Yoshioka, Takuya and Gollakota, Shyamnath},\ntitle = {Look Once to Hear: Target Speech Hearing with Noisy Examples},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642057},\ndoi = {10.1145/3613904.3642057},\nabstract = {In crowded settings, the human brain can focus on speech from a target speaker, given prior knowledge of how they sound. We introduce a novel intelligent hearable system that achieves this capability, enabling target speech hearing to ignore all interfering speech and noise, but the target speaker. A na\\\"{\\i}ve approach is to require a clean speech example to enroll the target speaker. This is however not well aligned with the hearable application domain since obtaining a clean example is challenging in real world scenarios, creating a unique user interface problem. We present the first enrollment interface where the wearer looks at the target speaker for a few seconds to capture a single, short, highly noisy, binaural example of the target speaker. This noisy example is used for enrollment and subsequent speech extraction in the presence of interfering speakers and noise. Our system achieves a signal quality improvement of 7.01 dB using less than 5 seconds of noisy enrollment audio and can process 8 ms of audio chunks in 6.24 ms on an embedded CPU. Our user studies demonstrate generalization to real-world static and mobile speakers in previously unseen indoor and outdoor multipath environments. Finally, our enrollment interface for noisy examples does not cause performance degradation compared to clean examples, while being convenient and user-friendly. Taking a step back, this paper takes an important step towards enhancing the human auditory perception with artificial intelligence.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {37},\nnumpages = {16},\nkeywords = {Augmented hearing, auditory perception, spatial computing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642233,\nauthor = {Li, Franklin Mingzhe and Liu, Michael Xieyang and Kane, Shaun K. and Carrington, Patrick},\ntitle = {A Contextual Inquiry of People with Vision Impairments in Cooking},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642233},\ndoi = {10.1145/3613904.3642233},\nabstract = {Individuals with vision impairments employ a variety of strategies for object identification, such as pans or soy sauce, in the culinary process. In addition, they often rely on contextual details about objects, such as location, orientation, and current status, to autonomously execute cooking activities. To understand how people with vision impairments collect and use the contextual information of objects while cooking, we conducted a contextual inquiry study with 12 participants in their own kitchens. This research aims to analyze object interaction dynamics in culinary practices to enhance assistive vision technologies for visually impaired cooks. We outline eight different types of contextual information and the strategies that blind cooks currently use to access the information while preparing meals. Further, we discuss preferences for communicating contextual information about kitchen objects as well as considerations for the deployment of AI-powered assistive technologies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {38},\nnumpages = {14},\nkeywords = {Accessibility, Assistive technology, Blind, Contextual Inquiry, Cooking, People with Vision Impairments},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642213,\nauthor = {Guan, Zhitong and Xiong, Zeyu and Fan, Mingming},\ntitle = {FetchAid: Making Parcel Lockers More Accessible to Blind and Low Vision People With Deep-learning Enhanced Touchscreen Guidance, Error-Recovery Mechanism, and AR-based Search Support},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642213},\ndoi = {10.1145/3613904.3642213},\nabstract = {Parcel lockers have become an increasingly prevalent last-mile delivery method. Yet, a recent study revealed its accessibility challenges to blind and low-vision people (BLV). Informed by the study, we designed FetchAid, a standalone intelligent mobile app assisting BLV in using a parcel locker in real-time by integrating computer vision and augmented reality (AR) technologies. FetchAid first uses a deep network to detect the user’s fingertip and relevant buttons on the touch screen of the parcel locker to guide the user to reveal and scan the QR code to open the target compartment door and then guide the user to reach the door safely with AR-based context-aware audio feedback. Moreover, FetchAid provides an error-recovery mechanism and real-time feedback to keep the user on track. We show that FetchAid substantially improved task accomplishment and efficiency, and reduced frustration and overall effort in a study with 12 BLV participants, regardless of their vision conditions and previous experience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {39},\nnumpages = {15},\nkeywords = {Accessibility, Assistive technology, Augmented reality, Blind and low vision, Computer vision, KuaiDiGui, Mobile devices, Object detection, Package delivery, People with vision impairments},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642816,\nauthor = {Teng, Yuanyang and Courtien, Connor and Rios, David Angel and Tseng, Yves M and Gibson, Jacqueline and Aziz, Maryam and Reyna, Avery and Vaish, Rajan and Smith, Brian A.},\ntitle = {Help Supporters: Exploring the Design Space of Assistive Technologies to Support Face-to-Face Help Between Blind and Sighted Strangers},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642816},\ndoi = {10.1145/3613904.3642816},\nabstract = {Blind and low-vision (BLV) people face many challenges when venturing into public environments, often wishing it were easier to get help from people nearby. Ironically, while many sighted individuals are willing to help, such interactions are infrequent. Asking for help is socially awkward for BLV people, and sighted people lack experience in helping BLV people. Through a mixed-ability research-through-design process, we explore four diverse approaches toward how assistive technology can serve as help supporters that collaborate with both BLV and sighted parties throughout the help process. These approaches span two phases: the connection phase (finding someone to help) and the collaboration phase (facilitating help after finding someone). Our findings from a 20-participant mixed-ability study reveal how help supporters can best facilitate connection, which types of information they should present during both phases, and more. We discuss design implications for future approaches to support face-to-face help.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {40},\nnumpages = {24},\nkeywords = {community-based intervention, mixed-ability, social collaboration},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642512,\nauthor = {Pandey, Maulishree and Oney, Steve and Begel, Andrew},\ntitle = {Towards Inclusive Source Code Readability Based on the Preferences of Programmers with Visual Impairments},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642512},\ndoi = {10.1145/3613904.3642512},\nabstract = {Code readability is crucial for program comprehension, maintenance, and collaboration. However, many of the standards for writing readable code are derived from sighted developers’ readability needs. We conducted a qualitative study with 16 blind and visually impaired (BVI) developers to better understand their readability preferences for common code formatting rules such as identifier naming conventions, line length, and the use of indentation. Our findings reveal how BVI developers’ preferences contrast with those of sighted developers and how we can expand the existing rules to improve code readability on screen readers. Based on the findings, we contribute an inclusive understanding of code readability and derive implications for programming languages, development environments, and style guides. Our work helps broaden the meaning of readable code in software engineering and accessibility research.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {41},\nnumpages = {18},\nkeywords = {accessibility, blind or visually impaired, code readability, software developers},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642753,\nauthor = {Perera, Minoli and Lee, Bongshin and Choe, Eun Kyoung and Marriott, Kim},\ntitle = {Visual Cues for Data Analysis Features Amplify Challenges for Blind Spreadsheet Users},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642753},\ndoi = {10.1145/3613904.3642753},\nabstract = {Spreadsheets are widely used for storing, manipulating, analyzing, and visualizing data. Features such as conditional formatting, formulas, sorting, and filtering play an important role when understanding and analyzing data in spreadsheets. They employ visual cues, but we have little understanding of the experiences of blind screen reader (SR) users with such features. We conducted a study with 12 blind SR users to gain insights into their challenges, workarounds, and strategies in understanding and extracting information from a spreadsheet consisting of multiple tables that incorporated data analysis features. We identified five factors that impact blind SR users’ experiences: cognitive overload, time-information trade-off, lack of awareness and expertise, inadequate system feedback, and delayed and absent SR responses. Drawn from these findings, we discuss design suggestions and future research agenda to improve SR users’ spreadsheet experiences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {42},\nnumpages = {16},\nkeywords = {accessibility, assistive technology, blind, data analysis, screen readers., spreadsheets, tables},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642188,\nauthor = {Wang, Yanan and Zhao, Yuhang and Kim, Yea-Seul},\ntitle = {How Do Low-Vision Individuals Experience Information Visualization?},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642188},\ndoi = {10.1145/3613904.3642188},\nabstract = {In recent years, there has been a growing interest in enhancing the accessibility of visualizations for people with visual impairments. While much of the research has focused on improving accessibility for screen reader users, the specific needs of people with remaining vision (i.e., low-vision individuals) have been largely unaddressed. To bridge this gap, we conducted a qualitative study that provides insights into how low-vision individuals experience visualizations. We found that participants utilized various strategies to examine visualizations using the screen magnifiers and also observed that the default zoom level participants use for general purposes may not be optimal for reading visualizations. We identified that participants relied on their prior knowledge and memory to minimize the traversing cost when examining visualization. Based on the findings, we motivate a personalized tool to accommodate varying visual conditions of low-vision individuals and derive the design goals and features of the tool.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {43},\nnumpages = {15},\nkeywords = {data accessibility, low-vision, people with visual impairments, visualization accessibility},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642227,\nauthor = {Cai, Shaojun and Ram, Ashwin and Gou, Zhengtai and Shaikh, Mohd Alqama Wasim and Chen, Yu-An and Wan, Yingjia and Hara, Kotaro and Zhao, Shengdong and Hsu, David},\ntitle = {Navigating Real-World Challenges: A Quadruped Robot Guiding System for Visually Impaired People in Diverse Environments},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642227},\ndoi = {10.1145/3613904.3642227},\nabstract = {Blind and Visually Impaired (BVI) people find challenges in navigating unfamiliar environments, even using assistive tools such as white canes or smart devices. Increasingly affordable quadruped robots offer us opportunities to design autonomous guides that could improve how BVI people find ways around unfamiliar environments and maneuver therein. In this work, we designed RDog, a quadruped robot guiding system that supports BVI individuals’ navigation and obstacle avoidance in indoor and outdoor environments. RDog combines an advanced mapping and navigation system to guide users with force feedback and preemptive voice feedback. Using this robot as an evaluation apparatus, we conducted experiments to investigate the difference in BVI people’s ambulatory behaviors using a white cane, a smart cane, and RDog. Results illustrated the benefits of RDog-based ambulation, including faster and smoother navigation with fewer collisions and limitations, and reduced cognitive load. We discuss the implications of our work for multi-terrain assistive guidance systems.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {44},\nnumpages = {18},\nkeywords = {assistive technology, navigation, orientation and mobility, robot guide dog, visual impairment},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642222,\nauthor = {Zhao, Yichun and Nacenta, Miguel A and Sukhai, Mahadeo A. and Somanath, Sowmya},\ntitle = {TADA: Making Node-link Diagrams Accessible to Blind and Low-Vision People},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642222},\ndoi = {10.1145/3613904.3642222},\nabstract = {Diagrams often appear as node-link representations in contexts such as taxonomies, mind maps and networks in textbooks. Despite their pervasiveness, they present accessibility challenges for blind and low-vision people. To address this challenge, we introduce Touch-and-Audio-based Diagram Access (TADA), a tablet-based interactive system that makes diagram exploration accessible through musical tones and speech. We designed TADA informed by an interview study with 15 participants who shared their challenges and strategies with diagrams. TADA enables people to access a diagram by: i) engaging in open-ended touch-based explorations, ii) searching for nodes, iii) navigating between nodes and iv) filtering information. We evaluated TADA with 25 participants and found it useful for gaining different perspectives on diagrammatic information.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {45},\nnumpages = {20},\nkeywords = {Accessibility, Artifact or System, Assistive Technologies, Gestures, Haptics, Individuals with Disabilities, Pointing, Touch},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641996,\nauthor = {Zong, Jonathan and Pedraza Pineros, Isabella and Chen, Mengzhu (Katie) and Hajas, Daniel and Satyanarayan, Arvind},\ntitle = {Umwelt: Accessible Structured Editing of Multi-Modal Data Representations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641996},\ndoi = {10.1145/3613904.3641996},\nabstract = {We present Umwelt, an authoring environment for interactive multimodal data representations. In contrast to prior approaches, which center the visual modality, Umwelt treats visualization, sonification, and textual description as coequal representations: they are all derived from a shared abstract data model, such that no modality is prioritized over the others. To simplify specification, Umwelt evaluates a set of heuristics to generate default multimodal representations that express a dataset’s functional relationships. To support smoothly moving between representations, Umwelt maintains a shared query predicated that is reified across all modalities — for instance, navigating the textual description also highlights the visualization and filters the sonification. In a study with 5 blind / low-vision expert users, we found that Umwelt’s multimodal representations afforded complementary overview and detailed perspectives on a dataset, allowing participants to fluidly shift between task- and representation-oriented ways of thinking.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {46},\nnumpages = {20},\nkeywords = {accessibility, multimodal data representation, sonification, textual description, visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641970,\nauthor = {Jones, Shuli and Pedraza Pineros, Isabella and Hajas, Daniel and Zong, Jonathan and Satyanarayan, Arvind},\ntitle = {“Customization is Key”: Reconfigurable Textual Tokens for Accessible Data Visualizations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641970},\ndoi = {10.1145/3613904.3641970},\nabstract = {Customization is crucial for making visualizations accessible to blind and low-vision (BLV) people with widely-varying needs. But what makes for usable or useful customization? We identify four design goals for how BLV people should be able to customize screen-reader-accessible visualizations: presence, or what content is included; verbosity, or how concisely content is presented; ordering, or how content is sequenced; and, duration, or how long customizations are active. To meet these goals, we model a customization as a sequence of content tokens, each with a set of adjustable properties. We instantiate our model by extending Olli, an open-source accessible visualization toolkit, with a settings menu and command box for persistent and ephemeral customization respectively. Through a study with 13 BLV participants, we find that customization increases the ease of identifying and remembering information. However, customization also introduces additional complexity, making it more helpful for users familiar with similar tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {47},\nnumpages = {14},\nkeywords = {accessible visualization, customization, hierarchical text structures, screen reader, text description},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642030,\nauthor = {Xie, Jingyi and Yu, Rui and Zhang, He and Lee, Sooyeon and Billah, Syed Masum and Carroll, John M.},\ntitle = {BubbleCam: Engaging Privacy in Remote Sighted Assistance},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642030},\ndoi = {10.1145/3613904.3642030},\nabstract = {Remote sighted assistance (RSA) offers prosthetic support to people with visual impairments (PVI) through image- or video-based conversations with remote sighted assistants. While useful, RSA services introduce privacy concerns, as PVI may reveal private visual content inadvertently. Solutions have emerged to address these concerns on image-based asynchronous RSA, but exploration into solutions for video-based synchronous RSA remains limited. In this study, we developed BubbleCam, a high-fidelity prototype allowing PVI to conceal objects beyond a certain distance during RSA, granting them privacy control. Through an exploratory field study with 24 participants, we found that 22 appreciated the privacy enhancements offered by BubbleCam. The users gained autonomy, reducing embarrassment by concealing private items, messy areas, or bystanders, while assistants could avoid irrelevant content. Importantly, BubbleCam maintained RSA’s primary function without compromising privacy. Our study highlighted a cooperative approach to privacy preservation, transitioning the traditionally individual task of maintaining privacy into an interactive, engaging privacy preserving experience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {48},\nnumpages = {16},\nkeywords = {People with visual impairments, computer vision, privacy, remote sighted assistance},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642525,\nauthor = {Neto, Isabel and Hu, Yuhan and Correia, Filipa and Rocha, Filipa and Hoffman, Guy and Nicolau, Hugo and Paiva, Ana},\ntitle = {Conveying Emotions through Shape-changing to Children with and without Visual Impairment},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642525},\ndoi = {10.1145/3613904.3642525},\nabstract = {Shape-changing skin is an exciting modality due to its accessible and engaging nature. Its softness and flexibility make it adaptable to different interactive devices that children with and without visual impairments can share. Although their potential as an emotionally expressive medium has been shown for sighted adults, their potential as an inclusive modality remains unexplored. This work explores the shape-emotional mappings in children with and without visual impairment. We conducted a user study with 50 children (26 with visual impairment) to investigate their emotional associations with five skin shapes and two movement conditions. Results show that shape-emotional mappings are dependent on visual abilities. Our study raises awareness of the influence of visual experiences on tactile vocabulary and emotional mapping among sighted, low-vision, and blind children. We finish discussing the causal associations between tactile stimuli and emotions and suggest inclusive design recommendations for shape-changing devices.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {49},\nnumpages = {16},\nkeywords = {Soft robotics, emotion expression, empirical study, human-robot interaction, nonverbal behavior, shape-changing, tactile interaction, texture-change, visually impaired},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642203,\nauthor = {Shinde, Pranali Uttam and Martin-Hammond, Aqueasha},\ntitle = {Designing to Support Blind and Visually Impaired Older Adults in Managing the Invisible Labor of Social Participation: Opportunities and Challenges},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642203},\ndoi = {10.1145/3613904.3642203},\nabstract = {Continued social participation is a key determinant of healthy aging and lowers the risks of isolation and loneliness. While online technologies can provide a convenient way for older adults to connect socially, some prefer connecting offline with others in their community, which can pose different challenges, especially for those with disabilities. Yet, we still know little about how older adults with visual disabilities might leverage technology to address their needs for engaging in social events in their communities. We interviewed 16 blind or visually impaired (BVI) adults 60 years or older to understand their experiences engaging in community social activities and the role of technology in the process. We describe the challenges participants faced connecting with others in their community and their use of technology to overcome them. Based on our findings, we discuss design opportunities for technology to help BVI older adults manage the hidden labor social participation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {50},\nnumpages = {14},\nkeywords = {blind and visually impaired, older adults, social participation, social wellness},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642939,\nauthor = {Liu, Zhe and Chen, Chunyang and Wang, Junjie and Chen, Mengzhuo and Wu, Boyu and Huang, Yuekai and Hu, Jun and Wang, Qing},\ntitle = {Unblind Text Inputs: Predicting Hint-text of Text Input in Mobile Apps via LLM},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642939},\ndoi = {10.1145/3613904.3642939},\nabstract = {Mobile apps have become indispensable for accessing and participating in various environments, especially for low-vision users. Users with visual impairments can use screen readers to read the content of each screen and understand the content that needs to be operated. Screen readers need to read the hint-text attribute in the text input component to remind visually impaired users what to fill in. Unfortunately, based on our analysis of 4,501 Android apps with text inputs, over 76\\% of them are missing hint-text. These issues are mostly caused by developers’ lack of awareness when considering visually impaired individuals. To overcome these challenges, we developed an LLM-based hint-text generation model called HintDroid, which analyzes the GUI information of input components and uses in-context learning to generate the hint-text. To ensure the quality of hint-text generation, we further designed a feedback-based inspection mechanism to further adjust hint-text. The automated experiments demonstrate the high BLEU and a user study further confirms its usefulness. HintDroid can not only help visually impaired individuals, but also help ordinary people understand the requirements of input components. HintDroid demo video: https://youtu.be/FWgfcctRbfI.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {51},\nnumpages = {20},\nkeywords = {App Accessibility, Large Language Model, Mobile App Design, User Interface},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642940,\nauthor = {Jones, Katherine Mary and Leonards, Ute and Metatla, Oussama},\ntitle = {“I Don't Really Get Involved In That Way”: Investigating Blind and Visually Impaired Individuals' Experiences of Joint Attention with Sighted People},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642940},\ndoi = {10.1145/3613904.3642940},\nabstract = {Joint attention (JA) is a crucial component of social interaction, relying heavily on visual cues like eye gaze and pointing. This creates barriers for blind and visually impaired people (BVI) to engage in JA with sighted peers. Yet, little research has characterised these barriers or the strategies BVI people employ to overcome them. We interviewed ten BVI adults to understand JA experiences and analysed videos of four BVI children with eight sighted partners engaging in activities conducive to JA. Interviews revealed that lack of JA feedback is perceived as voids that block engagement, exacerbated in group settings, with an emphasis on oneself to fill those voids. Video analysis anchored the absence of the person element within typical JA triads, suggesting a potential for technology to foster alternative dynamics between BVI and sighted people. We argue these findings could inform technology design that supports more inclusive JA interactions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {52},\nnumpages = {16},\nkeywords = {Adults, Blind, Children, Joint attention, Joint engagement, Vision impaired},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642094,\nauthor = {Tran, Nina and DeVries, Paige S and Seita, Matthew and Kushalnagar, Raja and Glasser, Abraham and Vogler, Christian},\ntitle = {Assessment of Sign Language-Based versus Touch-Based Input for Deaf Users Interacting with Intelligent Personal Assistants},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642094},\ndoi = {10.1145/3613904.3642094},\nabstract = {With the recent advancements in intelligent personal assistants (IPAs), their popularity is rapidly increasing when it comes to utilizing Automatic Speech Recognition within households. In this study, we used a Wizard-of-Oz methodology to evaluate and compare the usability of American Sign Language (ASL), Tap to Alexa, and smart home apps among 23 deaf participants within a limited-domain smart home environment. Results indicate a slight usability preference for ASL. Linguistic analysis of the participants' signing reveals a diverse range of expressions and vocabulary as they interacted with IPAs in the context of a restricted-domain application. On average, deaf participants exhibited a vocabulary of 47 +/- 17 signs with an additional 10 +/- 7 fingerspelled words, for a total of 246 different signs and 93 different fingerspelled words across all participants. We discuss the implications for the design of limited-vocabulary applications as a stepping-stone toward general-purpose ASL recognition in the future.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {53},\nnumpages = {15},\nkeywords = {Accessibility, Deaf and Hard of hearing, Empirical Studies, Intelligent Personal Assistants, Usability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641988,\nauthor = {Arroyo Chavez, Mariana and Feanny, Molly and Seita, Matthew and Thompson, Bernard and Delk, Keith and Officer, Skyler and Glasser, Abraham and Kushalnagar, Raja and Vogler, Christian},\ntitle = {How Users Experience Closed Captions on Live Television: Quality Metrics Remain a Challenge},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641988},\ndoi = {10.1145/3613904.3641988},\nabstract = {This paper presents a mixed methods study on how deaf, hard of hearing and hearing viewers perceive live TV caption quality with captioned video stimuli designed to mirror TV captioning experiences. To assess caption quality, we used four commonly-used quality metrics focusing on accuracy: word error rate, weighted word error rate, automated caption evaluation (ACE), and its successor ACE2. We calculated the correlation between the four quality metrics and viewer ratings for subjective quality and found that the correlation was weak, revealing that other factors besides accuracy affect user ratings. Additionally, even high-quality captions are perceived to have problems, despite controlling for confounding factors. Qualitative analysis of viewer comments revealed three major factors affecting their experience: Errors within captions, difficulty in following captions, and caption appearance. The findings raise questions as to how objective caption quality metrics can be reconciled with the user experience across a diverse spectrum of viewers.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {54},\nnumpages = {16},\nkeywords = {caption usability, closed captioning, quality metrics, subtitles},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642017,\nauthor = {Chen, Si and Waller, James and Seita, Matthew and Vogler, Christian and Kushalnagar, Raja and Wang, Qi},\ntitle = {Towards Co-Creating Access and Inclusion: A Group Autoethnography on a Hearing Individual's Journey Towards Effective Communication in Mixed-Hearing Ability Higher Education Settings},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642017},\ndoi = {10.1145/3613904.3642017},\nabstract = {We present a group autoethnography detailing a hearing student’s journey in adopting communication technologies at a mixed-hearing ability summer research camp. Our study focuses on how this student, a research assistant with emerging American Sign Language (ASL) skills, (in)effectively communicates with deaf and hard-of-hearing (DHH) peers and faculty during the ten-week program. The DHH members also reflected on their communication with the hearing student. We depict scenarios and analyze the (in)effectiveness of how emerging technologies like live automatic speech recognition (ASR) and typing are utilized to facilitate communication. We outline communication strategies to engage everyone with diverse signing skills in conversations - directing visual attention, pause-for-attention-and-proceed, and back-channeling via expressive body. These strategies promote inclusive collaboration and leverage technology advancements. Furthermore, we delve into the factors that have motivated individuals to embrace more inclusive communication practices and provide design implications for accessible communication technologies within the mixed-hearing ability context.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {55},\nnumpages = {14},\nkeywords = {American Sign Language, DHH, Higher Education, Mixed-Ability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642287,\nauthor = {Chen, Si and Cheng, Haocong and Situ, Jason and Kirst, Desir\\'{e}e and Su, Suzy and Malhotra, Saumya and Angrave, Lawrence and Wang, Qi and Huang, Yun},\ntitle = {Towards Inclusive Video Commenting: Introducing Signmaku for the Deaf and Hard-of-Hearing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642287},\ndoi = {10.1145/3613904.3642287},\nabstract = {Previous research underscored the potential of danmaku–a text-based commenting feature on videos–in engaging hearing audiences. Yet, for many Deaf and hard-of-hearing (DHH) individuals, American Sign Language (ASL) takes precedence over English. To improve inclusivity, we introduce “Signmaku,” a new commenting mechanism that uses ASL, serving as a sign language counterpart to danmaku. Through a need-finding study (N=12) and a within-subject experiment (N=20), we evaluated three design styles: real human faces, cartoon-like figures, and robotic representations. The results showed that cartoon-like signmaku not only entertained but also encouraged participants to create and share ASL comments, with fewer privacy concerns compared to the other designs. Conversely, the robotic representations faced challenges in accurately depicting hand movements and facial expressions, resulting in higher cognitive demands on users. Signmaku featuring real human faces elicited the lowest cognitive load and was the most comprehensible among all three types. Our findings offered novel design implications for leveraging generative AI to create signmaku comments, enriching co-learning experiences for DHH individuals.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {56},\nnumpages = {18},\nkeywords = {DHH, Danmaku, Signmaku, Social Interactions},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642162,\nauthor = {May, Lloyd and Ohshiro, Keita and Dang, Khang and Sridhar, Sripathi and Pai, Jhanvi and Fuentes, Magdalena and Lee, Sooyeon and Cartwright, Mark},\ntitle = {Unspoken Sound: Identifying Trends in Non-Speech Audio Captioning on YouTube},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642162},\ndoi = {10.1145/3613904.3642162},\nabstract = {High-quality closed captioning of both speech and non-speech elements (e.g., music, sound effects, manner of speaking, and speaker identification) is essential for the accessibility of video content, especially for d/Deaf and hard-of-hearing individuals. While many regions have regulations mandating captioning for television and movies, a regulatory gap remains for the vast amount of web-based video content, including the staggering 500+ hours uploaded to YouTube every minute. Advances in automatic speech recognition have bolstered the presence of captions on YouTube. However, the technology has notable limitations, including the omission of many non-speech elements, which are often crucial for understanding content narratives. This paper examines the contemporary and historical state of non-speech information (NSI) captioning on YouTube through the creation and exploratory analysis of a dataset of over 715k videos. We identify factors that influence NSI caption practices and suggest avenues for future research to enhance the accessibility of online video content.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {57},\nnumpages = {19},\nkeywords = {closed captioning, datasets, extra-speech information, non-speech information, subtitles},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642635,\nauthor = {South, Laura and Yildirim, Caglar and Pavel, Amy and Borkin, Michelle A.},\ntitle = {Barriers to Photosensitive Accessibility in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642635},\ndoi = {10.1145/3613904.3642635},\nabstract = {Virtual reality (VR) systems have grown in popularity as an immersive modality for daily activities such as gaming, socializing, and working. However, this technology is not always accessible for people with photosensitive epilepsy (PSE) who may experience seizures or other adverse symptoms when exposed to certain light stimuli (e.g., flashes or strobes). How can VR be made more inclusive and safer for people with PSE? In this paper, we report on a series of semi-structured interviews about current perceptions of accessibility in VR among people with PSE. We identify 12 barriers to accessibility that fall into four categories: physical VR equipment, VR interfaces and content, specific VR applications, and individual differences in sensitivity. Our findings allow researchers and practitioners to better understand the meaning of photosensitive accessibility in the context of VR, and provide a step towards enabling people with PSE to enjoy the benefits offered by immersive technology.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {58},\nnumpages = {13},\nkeywords = {accessibility, photosensitive epilepsy, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642274,\nauthor = {Curtis, Humphrey and Neate, Timothy},\ntitle = {Beyond Repairing with Electronic Speech: Towards Embodied Communication and Assistive Technology},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642274},\ndoi = {10.1145/3613904.3642274},\nabstract = {Traditionally, Western philosophies have strongly favoured a dualist interpretation of consciousness – emphasising the importance of the ‘mind’ over the ‘body’. However, we argue that adopted assistive technologies become embodied and extend intentionality within environments. In this paper, we restore an embodied view of the mind to theoretically enhance: understandings of assistive technology and human-human communication. Initially, we explore literature on: phenomenological theories of human experience, post-phenomenological accounts of technology, embodied accounts of assistive technology and participatory design. We then present a case study demonstrating the generative and disruptive effects of the embodied framework for co-designing AAC with people living with aphasia. Our findings show that the embodied framework supports a more multidimensional account of experience and suggests a shift from AAC devices that seek to ‘repair’ users’ speech. Reflecting on our case study, we then outline concerns with nascent technologies that could disembody and limit accessibility.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {59},\nnumpages = {12},\nkeywords = {Assistive technology, embodiment., participatory design, phenomenology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642884,\nauthor = {Tian, Jingze and Wang, Yingna and Yu, Keye and Xu, Liyi and Xie, Junan and Li, Franklin Mingzhe and Niu, Yafeng and Fan, Mingming},\ntitle = {Designing Upper-Body Gesture Interaction with and for People with Spinal Muscular Atrophy in VR},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642884},\ndoi = {10.1145/3613904.3642884},\nabstract = {Recent research proposed gaze-assisted gestures to enhance interaction within virtual reality (VR), providing opportunities for people with motor impairments to experience VR. Compared to people with other motor impairments, those with Spinal Muscular Atrophy (SMA) exhibit enhanced distal limb mobility, providing them with more design space. However, it remains unknown what gaze-assisted upper-body gestures people with SMA would want and be able to perform. We conducted an elicitation study in which 12 VR-experienced people with SMA designed upper-body gestures for 26 VR commands, and collected 312 user-defined gestures. Participants predominantly favored creating gestures with their hands. The type of tasks and participants’ abilities influence their choice of body parts for gesture design. Participants tended to enhance their body involvement and preferred gestures that required minimal physical effort, and were aesthetically pleasing. Our research will contribute to creating better gesture-based input methods for people with motor impairments to interact with VR.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {60},\nnumpages = {19},\nkeywords = {people with spinal muscular atrophy, upper-body gestures, user-defined gestures, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642189,\nauthor = {Hatada, Yuji and Barbareschi, Giulia and Takeuchi, Kazuaki and Kato, Hiroaki and Yoshifuji, Kentaro and Minamizawa, Kouta and Narumi, Takuji},\ntitle = {People with Disabilities Redefining Identity through Robotic and Virtual Avatars: A Case Study in Avatar Robot Cafe},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642189},\ndoi = {10.1145/3613904.3642189},\nabstract = {Robotic avatars and telepresence technology enable people with disabilities to engage in physical work. Despite the recent popularity of the metaverse, few studies have explored the use of virtual avatars and environments by people with disabilities. In this study, seven disabled participants working in a cafe where remote customer service is provided via robotic avatars, were engaged in the development and use of personalized virtual avatars displayed on a large screen in-situ in combination with existing physical robots, creating a hybrid cyber-physical space. We conducted longitudinal semi-structured interviews to investigate the psychological changes experienced by the participants. The results revealed that mass-produced robotic avatars allowed participants to not disclose their disability if they did not want to, but also backgrounded their identities; by contrast, customized virtual avatars shaped without physical constraints, highlighted their personalities. The combined use of robotic and virtual avatars complemented each other and can support pilots in redefining their identity.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {61},\nnumpages = {13},\nkeywords = {Avatar, people with disabilities, remote collaboration, remote customer survice},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642378,\nauthor = {Sun, Qin and Hu, Yunqi and Fan, Mingming and Li, Jingting and Wang, Su-Jing},\ntitle = {“Can It Be Customized According to My Motor Abilities?”: Toward Designing User-Defined Head Gestures for People with Dystonia},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642378},\ndoi = {10.1145/3613904.3642378},\nabstract = {Recent studies proposed above-the-neck gestures for people with upper-body motor impairments interacting with mobile devices without finger touch, resulting in an appropriate user-defined gesture set. However, many gestures involve sustaining eyelids in closed or open states for a period. This is challenging for people with dystonia, who have difficulty sustaining and intermitting muscle contractions. Meanwhile, other facial parts, such as the tongue and nose, can also be used to alleviate the sustained use of eyes in the interaction. Consequently, we conducted a user study inviting 16 individuals with dystonia to design gestures based on facial muscle movements for 26 common smartphone commands. We collected 416 user-defined head gestures involving facial features and shoulders. Finally, we obtained the preferred gestures set for individuals with dystonia. Participants preferred to make the gestures with their heads and use unnoticeable gestures. Our findings provide valuable references for the universal design of natural interaction technology.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {62},\nnumpages = {11},\nkeywords = {Dystonia, Gesture interaction, Human-computer interaction, Interaction preferences, Interaction technology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642593,\nauthor = {Park, Doeun and Choo, Myounglee and Cho, Minseo and Kim, Jinwoo and Shin, Yee-Jin},\ntitle = {Collaborative School Mental Health System: Leveraging a Conversational Agent for Enhancing Children's Executive Function},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642593},\ndoi = {10.1145/3613904.3642593},\nabstract = {Attention deficit hyperactivity disorder (ADHD) is a common childhood psychiatric disorder. Schools can play a vital role in the early detection and treatment of mental health issues. However, stigma and fear regarding mental health often prevent schools from engaging in active interventions. ADHD is characterized by deficits in executive function, a critical contributor to children's self-directed behavior. We developed a conversational agent to assist children in planning and accomplishing daily tasks, with the aim of enhancing their executive function. We also designed supportive systems for both parents and teachers, proposing a collaborative school mental health system that incorporates various stakeholders. Through practical implementation with first-graders, this study confirmed the system's potential to improve structured living and symptoms among children with ADHD. Surveys involving parents and teachers confirmed that the application improved executive function and reduced inattention. Therefore, we suggest an enhanced mental health support system.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {63},\nnumpages = {17},\nkeywords = {Attention Deficit Hyperactivity Disorder (ADHD), Conversational Agent, Executive Function, School Mental System},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642112,\nauthor = {Tran, Tien and Lee, Hae-Na and Park, Ji Hwan},\ntitle = {Discovering Accessible Data Visualizations for People with ADHD},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642112},\ndoi = {10.1145/3613904.3642112},\nabstract = {There have been many studies on understanding data visualizations regarding general users. However, we have a limited understanding of how people with ADHD comprehend data visualizations and how it might be different from the general users. To understand accessible data visualization for people with ADHD, we conducted a crowd-sourced survey involving 70 participants with ADHD and 77 participants without ADHD. Specifically, we tested the chart components of color, text amount, and use of visual embellishments/pictographs, finding that some of these components and ADHD affected participants’ response times and accuracy. We outlined the neurological traits of ADHD and discussed specific findings on accessible data visualizations for people with ADHD. We found that various chart embellishment types affected accuracy and response times for those with ADHD differently depending on the types of questions. Based on these results, we suggest visual design recommendations to make accessible data visualizations for people with ADHD.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {64},\nnumpages = {19},\nkeywords = {ADHD, accessibility, color, data visualizations, pictographs, text amount},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3643021,\nauthor = {Cuber, Isabelle and Goncalves De Souza, Juliana G and Jacobs, Irene and Lowman, Caroline and Shepherd, David and Fritz, Thomas and Langberg, Joshua M},\ntitle = {Examining the Use of VR as a Study Aid for University Students with ADHD},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3643021},\ndoi = {10.1145/3613904.3643021},\nabstract = {Attention-deficit/hyperactivity disorder (ADHD) is a neurodevelopmental condition characterized by patterns of inattention and impulsivity, which lead to difficulties maintaining concentration and motivation while completing academic tasks. University settings, characterized by a high student-to-staff ratio, make treatments relying on human monitoring challenging. One potential replacement is Virtual Reality (VR) technology, which has shown potential to enhance learning outcomes and promote flow experience. In this study, we investigate the usage of VR with 27 university students with ADHD in an effort to improve their performance in completing homework, including an exploration of automated feedback via a technology probe. Quantitative results show significant increases in concentration, motivation, and effort levels during these VR sessions and qualitative data offers insight into considerations like comfort and deployment. Together, the results suggest that VR can be a valuable tool in leveling the playing field for university students with ADHD.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {65},\nnumpages = {16},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642357,\nauthor = {Kim, Bogoan and Jeong, Dayoung and Hong, Hwajung and Han, Kyungsik},\ntitle = {Narrating Routines through Game Dynamics: Impact of a Gamified Routine Management App for Autistic Individuals},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642357},\ndoi = {10.1145/3613904.3642357},\nabstract = {Maintaining a daily routine has profound implications for physical, emotional, and social well-being. Autistic individuals may experience various challenges in establishing and maintaining a healthy daily routine due to their tendency to be inactive in daily life combined with their characteristics and preferences. Previous studies employing mobile technology to support autistic individuals have primarily focused on self-help functions, with limited exploration into the detailed needs of these individuals to develop and maintain personalized routines. In this study, we conducted a nine-week field study with 18 autistic individuals using RoutineAid, a gamified app designed to support key routines of autistic individuals (i.e., physical activity, diet, mindfulness, and sleep). Our analysis incorporated five measures of self-evaluation on daily life, app usage logs, Fitbit physical activity data, and interviews. Our findings demonstrate the effectiveness of RoutineAid and highlight its two primary affordances for autistic individuals: (1) promoting self-efficacy and embedding health behavior and (2) refining daily routines for healthier outcomes. We discuss salient design insights for developing daily routine management systems for autistic individuals.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {66},\nnumpages = {15},\nkeywords = {Autism, Daily Routine Management, Field Study, Gamified App},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642829,\nauthor = {Bei, Rongqi and Liu, Yajie and Wang, Yihe and Huang, Yuxuan and Li, Ming and Zhao, Yuhang and Tong, Xin},\ntitle = {StarRescue: the Design and Evaluation of A Turn-Taking Collaborative Game for Facilitating Autistic Children's Social Skills},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642829},\ndoi = {10.1145/3613904.3642829},\nabstract = {Autism Spectrum Disorder (ASD) presents challenges in social interaction skill development, particularly in turn-taking. Digital interventions offer potential solutions for improving autistic children’s social skills but often lack addressing specific collaboration techniques. Therefore, we designed a prototype of a turn-taking collaborative tablet game, StarRescue, which encourages children’s distinct collaborative roles and interdependence while progressively enhancing sharing and mutual planning skills. We further conducted a controlled study with 32 autistic children to evaluate StarRescue’s usability and potential effectiveness in improving their social skills. Findings indicated that StarRescue has great potential to foster turn-taking skills and social communication skills (e.g., prompting, negotiation, task allocation) within the game and also extend beyond the game. Additionally, we discussed implications for future work, such as including parents as game spectators and understanding autistic children’s territory awareness in collaboration. Our study contributes a promising digital intervention for autistic children’s turn-taking social skill development via a scaffolding approach and valuable design implications for future research.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {67},\nnumpages = {19},\nkeywords = {Autism Spectrum Disorder, Games, Social Skills, Turn-taking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642210,\nauthor = {Zolyomi, Annuska and Snyder, Jaime},\ntitle = {An Emotion Translator: Speculative Design By Neurodiverse Dyads},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642210},\ndoi = {10.1145/3613904.3642210},\nabstract = {For autistic individuals, navigating social and emotional interactions can be complex, often involving disproportionately high cognitive labor in contrast to neurotypical conversation partners. Through a novel approach to speculative co-design, autistic adults explored affective imaginaries — imagined futuristic technology interventions — to probe a provocative question: What if technology could translate emotions like it can translate spoken language? The resulting speculative prototype for an image-enabled emotion translator chat application included: (1) a visual system for representing personalized emotion taxonomies, and (2) a Wizard of Oz implementation of these taxonomies in a low-fidelity chat application. Although wary of technology that purports to understand emotions, autistic participants saw value in being able to deploy visual emotion taxonomies during chats with neurotypical conversation partners. This work shows that affective technology should enable users to: (1) curate encodings of emotions used in system artifacts, (2) enhance interactive emotional understanding, and (3) have agency over how and when to use emotion features.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {68},\nnumpages = {18},\nkeywords = {accessibility, affective computing, autism, speculative design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642798,\nauthor = {Rizvi, Naba and Wu, William and Bolds, Mya and Mondal, Raunak and Begel, Andrew and Munyaka, Imani N. S.},\ntitle = {Are Robots Ready to Deliver Autism Inclusion?: A Critical Review},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642798},\ndoi = {10.1145/3613904.3642798},\nabstract = {The marginalization of autistic people in our society today is multi-faceted as it includes violence that is both physical and ideological in nature. It is rooted in the dehumanization, infantilization, and masculinization of autistic people and pervasive even in contemporary research studies that continue to echo ableist ideologies from the past. In this work, we identify how HRI research reproduces systemic social inequalities and explain how they align with historical misrepresentations, and other systemic barriers. We analyzed 142 papers focusing on HRI and autism published between 2016 and 2022. We critique these studies through a mixed-methods analysis of their definition of autism, study designs, participant recruitment, and results. Our findings indicate that HRI research stigmatizes autism in three dimensions - 1) the pathologization of autism, 2) gender and age-based essentialism, and 3) power imbalances. Our work uncovered that about 90\\% of HRI research during the timeline explored excluded the perspectives of autistic people, particularly those from understudied groups. We recommend broadening the inclusion of autistic people, considering research objectives beyond clinical use, and diversifying collaborations, foundational works considered, \\& participant demographics for more inclusive future work.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {69},\nnumpages = {18},\nkeywords = {autism, human-computer interaction, robotics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642960,\nauthor = {Hu, Jiaxiong and Li, Junze and Zeng, Yuhang and Yang, Dongjie and Liang, Danxuan and Meng, Helen and Ma, Xiaojuan},\ntitle = {Designing Scaffolding Strategies for Conversational Agents in Dialog Task of Neurocognitive Disorders Screening},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642960},\ndoi = {10.1145/3613904.3642960},\nabstract = {Regular screening is critical for individuals at risk of neurocognitive disorders (NCDs) to receive early intervention. Conversational agents (CAs) have been adopted to administer dialog-based NCD screening tests for their scalability compared to human-administered tests. However, unique communication skills are required for CAs during NCD screening, e.g., clinicians often apply scaffolding to ensure subjects’ understanding of and engagement in screening tests. Based on scaffolding theories and analysis of clinicians’ practices from human-administered test recordings, we designed a scaffolding framework for the CA. In an exploratory wizard-of-Oz study, the CA empowered by ChatGPT administered tasks in the Grocery Shopping Dialog Task with 15 participants (10 diagnosed with NCDs). Clinical experts verified the quality of the CA’s scaffolding and we explored its effects on task understanding of the participants. Moreover, we proposed implications for the future design of CAs that enable scaffolding for scalable NCD screening.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {70},\nnumpages = {21},\nkeywords = {Aging, Conversational Agent, Health, Neurocognitive Disorder Screening, Scaffolding},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642626,\nauthor = {Alabood, Lorans and Dow, Travis and Feeley, Kaylyn B and Jaswal, Vikram K. and Krishnamurthy, Diwakar},\ntitle = {From Letterboards to Holograms: Advancing Assistive Technology for Nonspeaking Autistic Individuals with the HoloBoard},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642626},\ndoi = {10.1145/3613904.3642626},\nabstract = {About one-third of autistic individuals are nonspeaking, i.e., they cannot use speech to convey their thoughts reliably. Many in this population communicate via spelling, a process in which they point to letters on a letterboard held upright in their field of view by a trained Communication and Regulation Partner (CRP). This paper focuses on transitioning such individuals to more independent, digital spelling that requires less support from the CRP, a goal most nonspeakers we consulted with desire. To enable this transition, we followed an approach that mimics an environment familiar to the nonspeaker and that harnesses the skills they already possess from physical letterboard training. Using this approach, we developed HoloBoard, a system that allows a nonspeaker, their CRP, and others, e.g., researchers, to share a common Augmented Reality (AR) environment containing a virtual letterboard. We configured the system to offer a brief (less than 10 minutes, on average) training module with graduated spelling tasks on the virtual letterboard. In a study involving 23 participants, 16 completed the entire module. These participants were able to spell words on the virtual letterboard without the CRP holding that board, an outcome we had not expected. When offered the opportunity to continue interacting with the virtual letterboard after the training module, 14 performed more complicated tasks than we had anticipated, spelling full sentences, or even offering feedback on the HoloBoard using solely the virtual board. Furthermore, five of these participants used the system solo, i.e., with the CRP and researchers absent from the virtual environment. These results suggest that training with the HoloBoard can lay the foundation for more independent communication, providing new social and educational opportunities for this marginalized population.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {71},\nnumpages = {18},\nkeywords = {Cross-reality, accessibility, assistive technology, extended reality, nonspeaking autistic people},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641989,\nauthor = {Choi, Dasom and Lee, Sunok and Kim, Sung-In and Lee, Kyungah and Yoo, Hee Jeong and Lee, Sangsu and Hong, Hwajung},\ntitle = {Unlock Life with a Chat(GPT): Integrating Conversational AI with Large Language Models into Everyday Lives of Autistic Individuals},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641989},\ndoi = {10.1145/3613904.3641989},\nabstract = {Autistic individuals often draw on insights from their supportive networks to develop self-help life strategies ranging from everyday chores to social activities. However, human resources may not always be immediately available. Recently emerging conversational agents (CAs) that leverage large language models (LLMs) have the potential to serve as powerful information-seeking tools, facilitating autistic individuals to tackle daily concerns independently. This study explored the opportunities and challenges of LLM-driven CAs in empowering autistic individuals through focus group interviews and workshops (N=14). We found that autistic individuals expected LLM-driven CAs to offer a non-judgmental space, encouraging them to approach day-to-day issues proactively. However, they raised issues regarding critically digesting the CA responses and disclosing their autistic characteristics. Based on these findings, we propose approaches that place autistic individuals at the center of shaping the meaning and role of LLM-driven CAs in their lives, while preserving their unique needs and characteristics.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {72},\nnumpages = {17},\nkeywords = {autism, conversational agent, large language model, participatory design workshop},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642197,\nauthor = {Ara, Zinat and Ganguly, Amrita and Peppard, Donna and Chung, Dongjun and Vucetic, Slobodan and Genaro Motti, Vivian and Hong, Sungsoo Ray},\ntitle = {Collaborative Job Seeking for People with Autism: Challenges and Design Opportunities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642197},\ndoi = {10.1145/3613904.3642197},\nabstract = {Successful job search results from job seekers’ well-shaped social communication. While well-known differences in communication exist between people with autism and neurotypicals, little is known about how people with autism collaborate with their social surroundings to strive in the job market. To better understand the practices and challenges of collaborative job seeking for people with autism, we interviewed 20 participants including applicants with autism, their social surroundings, and career experts. Through the interviews, we identified social challenges that people with autism face during their job seeking; the social support they leverage to be successful; and the technological limitations that hinder their collaboration. We designed four probes that represent major collaborative features found from the interviews–executive planning, communication, stage-wise preparation, and neurodivergent community formation–and discussed their potential usefulness and impact through three focus groups. We provide implications regarding how our findings can enhance collaborative job seeking experiences for people with autism through new designs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {73},\nnumpages = {17},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642424,\nauthor = {Hall, Kaely and Arora, Parth and Lowy, Rachel and Kim, Jennifer G},\ntitle = {Designing for Strengths: Opportunities to Support Neurodiversity in the Workplace},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642424},\ndoi = {10.1145/3613904.3642424},\nabstract = {Supported employment programs have demonstrated the ability to enhance employment outcomes for neurodivergent individuals by offering personalized job coaching that aligns with the strengths of each individual. While various technological interventions have been designed to support these programs, technologies that hyperfocus on users’ assumed challenges through deficit-based design have been criticized due to their potential to undermine the agency of neurodivergent individuals. Therefore, we use strengths-based co-design to explore the opportunities for a technology that supports neurodivergent employees using their strengths. The co-design activities uncovered our participants’ current strategies to address workplace challenges, the strengths they employ, and the technology designs that our participants developed to operationalize those strengths in a supportive technology. We find that incorporating strengths-based strategies for emotional regulation, interpersonal problem solving, and learning job-related skills can provide a supportive technology experience that bolsters neurodiverse employees’ agency and independence in the workplace. In response, we suggest design implications for using neurodiverse strengths as design requirements and how to design for independence in workplace.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {74},\nnumpages = {14},\nkeywords = {Neurodiversity, Strengths-based design, Workplace support},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642567,\nauthor = {Cullen, Spring and Johnson, Elizabeth S and Wisniewski, Pamela J. and Page, Xinru},\ntitle = {Towards Digital Independence: Identifying the Tensions between Autistic Young Adults and Their Support Network When Mediating Social Media},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642567},\ndoi = {10.1145/3613904.3642567},\nabstract = {We conducted an ethnographically-informed study with 28 participants (9 autistic Young Adults or \"YAs\" in need of substantial daily support, 6 parents, 13 support staff) to understand how autistic YAs self-regulate and receive mediation on social media. We found that autistic YAs relied on blanket boundary rules and struggled with impulse control; therefore, they coped by asking their support network to help them deal with negative social experiences. Their support networks responded by providing informal advice, in-the-moment instruction, and formal education, but often resorted to monitoring and restrictive mediation when more proactive approaches were ineffective. Overall, we saw boundary tensions arise between Autistic YAs and their support networks as they struggled to find the right balance between providing oversight versus promoting autonomy. This work contributes to the critical disability literature by revealing the benefits and tensions of allyship in the context of helping young autistic adults navigate social media.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {75},\nnumpages = {21},\nkeywords = {autism, boundary turbulence, mediation, social media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642578,\nauthor = {Kong, Ha-Kyung and Yadav, Saloni and Lowy, Rachel and Ruzinov, Daniella Rose and Kim, Jennifer G},\ntitle = {Understanding Online Job and Housing Search Practices of Neurodiverse Young Adults to Support Their Independence},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642578},\ndoi = {10.1145/3613904.3642578},\nabstract = {Securing employment and housing are key aspects of pursuing independent living. As these activities are increasingly practiced online, web accessibility of related services becomes critical for a successful major life transition. Support for this transition is especially important for people with autism or intellectual disability, who often face issues of underemployment and social isolation. In this study, we conducted semi-structured interviews and contextual inquiries with neurotypical adults and adults with autism or intellectual disability to understand common and unique goals, strategies, and challenges of neurodiverse adults when searching for employment and housing resources online. Our findings revealed that current interfaces adequately support practical (e.g., finance) goals but lack information on social (e.g., inclusivity) goals. Furthermore, unexpected search results and inaccessible social and contextual information diminished search experiences for neurodivergent users, which suggests the need for predictability and structured guidance in searching online. We conclude with design suggestions to make neurodivergent users’ online search experience an opportunity to demonstrate their independence.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {76},\nnumpages = {14},\nkeywords = {neurodiverse users, web accessibility, web search},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642894,\nauthor = {Jang, JiWoong and Moharana, Sanika and Carrington, Patrick and Begel, Andrew},\ntitle = {“It’s the only thing I can trust”: Envisioning Large Language Model Use by Autistic Workers for Communication Assistance},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642894},\ndoi = {10.1145/3613904.3642894},\nabstract = {Autistic adults often experience stigma and discrimination at work, leading them to seek social communication support from coworkers, friends, and family despite emotional risks. Large language models (LLMs) are increasingly considered an alternative. In this work, we investigate the phenomenon of LLM use by autistic adults at work and explore opportunities and risks of LLMs as a source of social communication advice. We asked 11 autistic participants to present questions about their own workplace-related social difficulties to (1) a GPT-4-based chatbot and (2) a disguised human confederate. Our evaluation shows that participants strongly preferred LLM over confederate interactions. However, a coach specializing in supporting autistic job-seekers raised concerns that the LLM was dispensing questionable advice. We highlight how this divergence in participant and practitioner attitudes reflects existing schisms in HCI on the relative privileging of end-user wants versus normative good and propose design considerations for LLMs to center autistic experiences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {77},\nnumpages = {18},\nkeywords = {Artificial Intelligence, Autism, Neurodiversity, large language models, social communication, workplace},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642666,\nauthor = {Bahnsen, Kilian L and Tiemann, Lucas and Plabst, Lucas and Grundgeiger, Tobias},\ntitle = {Augmented Reality Cues Facilitate Task Resumption after Interruptions in Computer-Based and Physical Tasks},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642666},\ndoi = {10.1145/3613904.3642666},\nabstract = {Many work domains include numerous interruptions, which can contribute to errors. We investigated the potential of augmented reality (AR) cues to facilitate primary task resumption after interruptions of varying lengths. Experiment 1 (N = 83) involved a computer-based primary task with a red AR arrow at the to-be-resumed task step which was placed via a gesture by the participants or automatically. Compared to no cue, both cues significantly reduced the resumption lag (i.e., the time between the end of the interruption and the resumption of the primary task) following long but not short interruptions. Experiment 2 (N = 38) involved a tangible sorting task, utilizing only the automatic cue. The AR cue facilitated task resumption compared to not cue after both short and long interruptions. We demonstrated the potential of AR cues in mitigating the negative effects of interruptions and make suggestions for integrating AR technologies for task resumption.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {78},\nnumpages = {16},\nkeywords = {Augmented Reality, Human Error, Interruption, Resumption Lag, Task Resumption},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642540,\nauthor = {Bai, Yunpeng and Ikkala, Aleksi and Oulasvirta, Antti and Zhao, Shengdong and Wang, Lucia J and Yang, Pengzhi and Xu, Peisen},\ntitle = {Heads-Up Multitasker: Simulating Attention Switching On Optical Head-Mounted Displays},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642540},\ndoi = {10.1145/3613904.3642540},\nabstract = {Optical Head-Mounted Displays (OHMDs) allow users to read digital content while walking. A better understanding of how users allocate attention between these two tasks is crucial for improving OHMD interfaces. This paper introduces a computational model for simulating users’ attention switches between reading and walking. We model users’ decision to deploy visual attention as a hierarchical reinforcement learning problem, wherein a supervisory controller optimizes attention allocation while considering both reading activity and walking safety. Our model simulates the control of eye movements and locomotion as an adaptation to the given task priority, design of digital content, and walking speed. The model replicates key multitasking behaviors during OHMD reading while walking, including attention switches, changes in reading and walking speeds, and reading resumptions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {79},\nnumpages = {18},\nkeywords = {bounded optimal control, computational rationality, deep reinforcement learning, heads-up computing, multitasking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642256,\nauthor = {Whitmore, Nathan W and Chan, Samantha and Zhang, Jingru and Chwalek, Patrick and Chin, Sam and Maes, Pattie},\ntitle = {Improving Attention Using Wearables via Haptic and Multimodal Rhythmic Stimuli},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642256},\ndoi = {10.1145/3613904.3642256},\nabstract = {Rhythmic light, sound and haptic stimuli can improve cognition through neural entrainment and by modifying autonomic nervous system function. However, the effects and user experience of using wearables for inducing such rhythmic stimuli have been under-investigated. We conducted a study with 20 participants to understand the effects of rhythmic stimulation wearables on attention. We found that combined sound and light stimuli from a glasses device provided the strongest improvement to attention but were the least usable and socially acceptable. Haptic vibration stimuli from a wristband also improved attention and were the most usable and socially acceptable. Our field study (N=12) with haptic stimuli from a smartwatch showed that such systems can be easy to use and were used frequently in a range of contexts but more exploration is needed to improve the comfort. Our work contributes to developing future wearables to support attention and cognition.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {80},\nnumpages = {14},\nkeywords = {attention, attention enhancement, audio-visual entrainment, brainwave entrainment, cognitive enhancement, cognitive psychology, entrainment, field study, haptics, neuroscience, smartwatch, sustained attention to response task, user study},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642629,\nauthor = {Nith, Romain and Ho, Yun and Lopes, Pedro},\ntitle = {SplitBody: Reducing Mental Workload while Multitasking via Muscle Stimulation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642629},\ndoi = {10.1145/3613904.3642629},\nabstract = {Techniques like electrical muscle stimulation (EMS) offer promise in assisting physical tasks by automating movements, e.g., shaking a spray-can or tapping a button. However, existing actuation systems improve the performance of a task that users are already focusing on (e.g., users are already focused on using the spray-can). Instead, we investigate whether these interactive-actuation systems (e.g., EMS) offer any benefits if they automate a task that happens in the background of the user's focus. Thus, we explored whether automating a repetitive movement via EMS would reduce mental workload while users perform parallel tasks (e.g., focusing on writing an essay while EMS stirs a pot of soup). In our study, participants performed a cognitively-demanding multitask aided by EMS (SplitBody condition) or performed by themselves (baseline). We found that with SplitBody performance increased (35\\% on both tasks, 18\\% on the non-EMS-automated task), physical-demand decreased (31\\%), and mental-workload decreased (26\\%).},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {81},\nnumpages = {11},\nkeywords = {Agency, Cognitive Load, Electrical Muscle Stimulation, Haptics, Mental Workload},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642063,\nauthor = {Lingler, Alexander and Talypova, Dinara and Jokinen, Jussi P. P. and Oulasvirta, Antti and Wintersberger, Philipp},\ntitle = {Supporting Task Switching with Reinforcement Learning},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642063},\ndoi = {10.1145/3613904.3642063},\nabstract = {Attention management systems aim to mitigate the negative effects of multitasking. However, sophisticated real-time attention management is yet to be developed. We present a novel concept for attention management with reinforcement learning that automatically switches tasks. The system was trained with a user model based on principles of computational rationality. Due to this user model, the system derives a policy that schedules task switches by considering human constraints such as visual limitations and reaction times. We evaluated its capabilities in a challenging dual-task balancing game. Our results confirm our main hypothesis that an attention management system based on reinforcement learning can significantly improve human performance, compared to humans’ self-determined interruption strategy. The system raised the frequency and difficulty of task switches compared to the users while still yielding a lower subjective workload. We conclude by arguing that the concept can be applied to a great variety of multitasking settings.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {82},\nnumpages = {18},\nkeywords = {Artifact or System, Interruption, Lab Study, Machine Learning, Notification, Quantitative Methods, Task Switching},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642514,\nauthor = {Tan, Felicia Fang-Yi and Xu, Peisen and Ram, Ashwin and Suen, Wei Zhen and Zhao, Shengdong and Huang, Yun and Hurter, Christophe},\ntitle = {AudioXtend: Assisted Reality Visual Accompaniments for Audiobook Storytelling During Everyday Routine Tasks},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642514},\ndoi = {10.1145/3613904.3642514},\nabstract = {The rise of multitasking in contemporary lifestyles has positioned audio-first content as an essential medium for information consumption. We present AudioXtend, an approach to augment audiobook experiences during daily tasks by integrating glanceable, AI-generated visuals through optical see-through head-mounted displays (OHMDs). Our initial study showed that these visual augmentations not only preserved users’ primary task efficiency but also dramatically enhanced immediate auditory content recall by 33.3\\% and 7-day recall by 32.7\\%, alongside a marked improvement in narrative engagement. Through participatory design workshops involving digital arts designers, we crafted a set of design principles for visual augmentations that are attuned to the requirements of multitaskers. Finally, a 3-day take-home field study further revealed new insights for everyday use, underscoring the potential of assisted reality (aR) to enhance heads-up listening and incidental learning experiences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {83},\nnumpages = {22},\nkeywords = {Assisted Reality, Audiobook Augmentation, Heads-Up Computing, Incidental learning, Optical See-Through Head-Mounted Displays, Recall Enhancement, Smart-glasses, Visual Storytelling},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642015,\nauthor = {Syiem, Brandon Victor and Webber, Sarah and Kelly, Ryan M. and Zhou, Qiushi and Goncalves, Jorge and Velloso, Eduardo},\ntitle = {Augmented Reality at Zoo Exhibits: A Design Framework for Enhancing the Zoo Experience},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642015},\ndoi = {10.1145/3613904.3642015},\nabstract = {Augmented Reality (AR) offers unique opportunities for contributing to zoos’ objectives of public engagement and education about animal and conservation issues. However, the diversity of animal exhibits pose challenges in designing AR applications that are not encountered in more controlled environments, such as museums. To support the design of AR applications that meaningfully engage the public with zoo objectives, we first conducted two scoping reviews to interrogate previous work on AR and broader technology use at zoos. We then conducted a workshop with zoo representatives to understand the challenges and opportunities in using AR to achieve zoo objectives. Additionally, we conducted a field trip to a public zoo to identify exhibit characteristics that impacts AR application design. We synthesise the findings from these studies into a framework that enables the design of diverse AR experiences. We illustrate the utility of the framework by presenting two concepts for feasible AR applications.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {84},\nnumpages = {18},\nkeywords = {augmented reality, design framework, ethnography, field study},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642431,\nauthor = {Park, Hyerim and Min, Aram and Lee, Hyunjin and Shakeri, Maryam and Jeon, Ikbeom and Woo, Woontack},\ntitle = {Comfortable Mobility vs. Attractive Scenery: The Key to Augmenting Narrative Worlds in Outdoor Locative Augmented Reality Storytelling},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642431},\ndoi = {10.1145/3613904.3642431},\nabstract = {We investigate how path context, encompassing both comfort and attractiveness, shapes user experiences in outdoor locative storytelling using Augmented Reality (AR). Addressing a research gap that predominantly concentrates on indoor settings or narrative backdrops, our user-focused research delves into the interplay between perceived path context and locative AR storytelling on routes with diverse walkability levels. We examine the correlation and causation between narrative engagement, spatial presence, perceived workload, and perceived path context. Our findings show that on paths with reasonable path walkability, attractive elements positively influence the narrative experience. However, even in environments with assured narrative walkability, inappropriate safety elements can divert user attention to mobility, hindering the integration of real-world features into the narrative. These results carry significant implications for path creation in outdoor locative AR storytelling, underscoring the importance of ensuring comfort and maintaining a balance between comfort and attractiveness to enrich the outdoor AR storytelling experience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {85},\nnumpages = {19},\nkeywords = {Augmented Reality, Locative Storytelling, Mixed Reality, Walkability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642819,\nauthor = {Shin, Jae-Eun and Kim, Hayun and Park, Hyerim and Woo, Woontack},\ntitle = {Investigating the Design of Augmented Narrative Spaces Through Virtual-Real Connections: A Systematic Literature Review},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642819},\ndoi = {10.1145/3613904.3642819},\nabstract = {Augmented Reality (AR) is regarded as an innovative storytelling medium that presents novel experiences by layering a virtual narrative space over a real 3D space. However, understanding of how the virtual narrative space and the real space are connected with one another in the design of augmented narrative spaces has been limited. For this, we conducted a systematic literature review of 64 articles featuring AR storytelling applications and systems in HCI, AR, and MR research. We investigated how virtual narrative spaces have been paired, functionalized, placed, and registered in relation to the real spaces they target. Based on these connections, we identified eight dominant types of augmented narrative spaces that are primarily categorized by whether they virtually narrativize reality or realize the virtual narrative. We discuss our findings to propose design recommendations on how virtual-real connections can be incorporated into a more structured approach to AR storytelling.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {86},\nnumpages = {18},\nkeywords = {Augmented Reality, Mixed Reality, augmented narrative space, storytelling},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642744,\nauthor = {Zhang, Lei and Kim, Daekun and Cho, Youjean and Robinson, Ava and Tham, Yu Jiang and Vaish, Rajan and Monroy-Hern\\'{a}ndez, Andr\\'{e}s},\ntitle = {Jigsaw: Authoring Immersive Storytelling Experiences with Augmented Reality and Internet of Things},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642744},\ndoi = {10.1145/3613904.3642744},\nabstract = {Augmented Reality (AR) presents new opportunities for immersive storytelling. However, this immersiveness faces two main hurdles. First, AR’s immersive quality is often confined to visual elements, such as pixels on a screen. Second, crafting immersive narratives is complex and generally beyond the reach of amateurs due to the need for advanced technical skills. We introduce Jigsaw, a system that empowers beginners to both experience and craft immersive stories, blending virtual and physical elements. Jigsaw uniquely combines mobile AR with readily available Internet-of-things (IoT) devices. We conducted a qualitative study with 20 participants to assess Jigsaw’s effectiveness in both consuming and creating immersive narratives. The results were promising: participants not only successfully created their own immersive stories but also found the playback of three such stories deeply engaging. However, sensory overload emerged as a significant challenge in these experiences. We discuss design trade-offs and considerations for future endeavors in immersive storytelling involving AR and IoT.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {87},\nnumpages = {14},\nkeywords = {augmented reality, authoring tool, internet-of-things, storytelling},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642029,\nauthor = {Chang, Xiang and Chen, Zihe and Dong, Xiaoyan and Cai, Yuxin and Yan, Tingmin and Cai, Haolin and Zhou, Zherui and Zhou, Guyue and Gong, Jiangtao},\ntitle = {\"It Must Be Gesturing Towards Me\": Gesture-Based Interaction between Autonomous Vehicles and Pedestrians},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642029},\ndoi = {10.1145/3613904.3642029},\nabstract = {Interacting with pedestrians understandably and efficiently is one of the toughest challenges faced by autonomous vehicles (AVs) due to the limitations of current algorithms and external human-machine interfaces (eHMIs). In this paper, we design eHMIs based on gestures inspired by the most popular method of interaction between pedestrians and human drivers. Eight common gestures were selected to convey AVs’ yielding or non-yielding intentions at uncontrolled crosswalks from previous literature. Through a VR experiment (N1 = 31) and a following online survey (N2 = 394), we discovered significant differences in the usability of gesture-based eHMIs compared to current eHMIs. Good gesture-based eHMIs increase the efficiency of pedestrian-AV interaction while ensuring safety. Poor gestures, however, cause misinterpretation. The underlying reasons were explored: ambiguity regarding the recipient of the signal and whether the gestures are precise, polite, and familiar to pedestrians. Based on this empirical evidence, we discuss potential opportunities and provide valuable insights into developing comprehensible gesture-based eHMIs in the future to support better interaction between AVs and other road users.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {88},\nnumpages = {25},\nkeywords = {Autonomous Driving, Autonomous Vehicles and Pedestrian Interaction, Gesture-based Interaction, eHMI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642118,\nauthor = {Tran, Tram Thi Minh and Parker, Callum and Hoggenm\\\"{u}ller, Marius and Wang, Yiyuan and Tomitsch, Martin},\ntitle = {Exploring the Impact of Interconnected External Interfaces in Autonomous Vehicles on Pedestrian Safety and Experience},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642118},\ndoi = {10.1145/3613904.3642118},\nabstract = {Policymakers advocate for the use of external Human-Machine Interfaces (eHMIs) to allow autonomous vehicles (AVs) to communicate their intentions or status. Nonetheless, scalability concerns in complex traffic scenarios arise, such as potentially increasing pedestrian cognitive load or conveying contradictory signals. Building upon precursory works, our study explores ‘interconnected eHMIs,’ where multiple AV interfaces are interconnected to provide pedestrians with clear and unified information. In a virtual reality study (N=32), we assessed the effectiveness of this concept in improving pedestrian safety and their crossing experience. We compared these results against two conditions: no eHMIs and unconnected eHMIs. Results indicated interconnected eHMIs enhanced safety feelings and encouraged cautious crossings. However, certain design elements, such as the use of the colour red, led to confusion and discomfort. Prior knowledge slightly influenced perceptions of interconnected eHMIs, underscoring the need for refined user education. We conclude with practical implications and future eHMI design research directions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {89},\nnumpages = {17},\nkeywords = {autonomous vehicles, eHMIs, external communication, scalability, vehicle-pedestrian interaction, vulnerable road users},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642019,\nauthor = {Al-Taie, Ammar and Wilson, Graham and Freeman, Euan and Pollick, Frank and Brewster, Stephen Anthony},\ntitle = {Light it Up: Evaluating Versatile Autonomous Vehicle-Cyclist External Human-Machine Interfaces},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642019},\ndoi = {10.1145/3613904.3642019},\nabstract = {The social cues drivers exchange with cyclists to negotiate space-sharing will disappear as autonomous vehicles (AVs) join our roads, leading to safety concerns. External Human-Machine Interfaces (eHMIs) on vehicles can replace driver social signals, but how these should be designed to communicate with cyclists is unknown. We evaluated three eHMIs across multiple traffic scenarios in two stages. First, we compared eHMI versatility, acceptability and usability in a VR cycling simulator. Cyclists preferred colour-coded signals communicating AV intent, easily seen through quick glances. Second, we refined the interfaces based on our findings and compared them outdoors. Participants cycled around a moving car with real eHMIs. They preferred eHMIs using large surfaces on the vehicle and animations reinforcing colour changes. We conclude with novel design guidelines for versatile eHMIs based on first-hand interaction feedback. Our findings establish the factors that enable AVs to operate safely around cyclists across different traffic scenarios.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {90},\nnumpages = {20},\nkeywords = {Autonomous Vehicle-Cyclist Interaction, eHMI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642031,\nauthor = {Dey, Debargha and Senan, Toros Ufuk and Hengeveld, Bart and Colley, Mark and Habibovic, Azra and Ju, Wendy},\ntitle = {Multi-Modal eHMIs: The Relative Impact of Light and Sound in AV-Pedestrian Interaction},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642031},\ndoi = {10.1145/3613904.3642031},\nabstract = {External Human-Machine Interfaces (eHMIs) have been evaluated to facilitate interactions between Automated Vehicles (AVs) and pedestrians. Most eHMIs are, however, visual/ light-based solutions, and multi-modal eHMIs have received little attention to date. We ran an experimental video study (<Formula format=\"inline\"><TexMath><?TeX $N~=~29$?></TexMath><AltText>Math 1</AltText><File name=\"chi24-142-inline1\" type=\"svg\"/></Formula>) to systematically understand the effect on pedestrian’s willingness to cross the road and user preferences of a light-based eHMI (light bar on the bumper) and two sound-based eHMIs (bell sound and droning sound), and combinations thereof. We found no objective change in pedestrians’ willingness to cross the road based on the nature of eHMI, although people expressed different subjective preferences for the different ways an eHMI may communicate, and sometimes even strong dislike for multi-modal eHMIs. This shows that the modality of the evaluated eHMI concepts had relatively little impact on their effectiveness. Consequently, this lays an important groundwork for accessibility considerations of future eHMIs, and points towards the insight that provisions can be made for taking user preferences into account without compromising effectiveness.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {91},\nnumpages = {16},\nkeywords = {Automated vehicle, VRU, eHMI, multimodal interface, pedestrian, vehicle-pedestrian interaction, vulnerable road user},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642648,\nauthor = {Ding, Yaohan and Jia, Lesong and Du, Na},\ntitle = {One Size Does Not Fit All: Designing and Evaluating Criticality-Adaptive Displays in Highly Automated Vehicles},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642648},\ndoi = {10.1145/3613904.3642648},\nabstract = {To promote drivers’ overall experiences in highly automated vehicles, we designed three objective criticality-adaptive displays: IO display highlighting Influential Objects, CO display highlighting Critical Objects, and ICO display highlighting Influential and Critical Objects differently. We conducted an online video-based survey study with 295 participants to evaluate them in varying traffic conditions. Results showed that low-trust propensity participants found ICO display more useful while high-trust propensity participants found CO displays more useful. When interacting with vulnerable road users (VRUs), participants had higher situational awareness (SA) but worse non-driving related task (NDRT) performance. Aging and CO displays also led to slower NDRT reactions. Nonetheless, older participants found displays more useful. We recommend providing different criticality-adaptive displays based on drivers’ trust propensity, age, and NDRT choice to enhance driving and NDRT performance and suggest carefully treating objects of different categories in traffic.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {92},\nnumpages = {15},\nkeywords = {adaptive display, automated vehicles, criticality, individual differences, situational awareness, traffic density, trust, usability.},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642651,\nauthor = {D'Adamo, Amar and Roel Lesur, Marte and Turmo Vidal, Laia and Dehshibi, Mohammad Mahdi and De La Prida, Daniel and Diaz-Dur\\'{a}n, Joaqu\\'{\\i}n R. and Azpicueta-Ruiz, Luis Antonio and V\\\"{a}ljam\\\"{a}e, Aleksander and Tajadura-Jim\\'{e}nez, Ana},\ntitle = {SoniWeight Shoes: Investigating Effects and Personalization of a Wearable Sound Device for Altering Body Perception and Behavior},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642651},\ndoi = {10.1145/3613904.3642651},\nabstract = {Changes in body perception influence behavior and emotion and can be induced through multisensory feedback. Auditory feedback to one’s actions can trigger such alterations; however, it is unclear which individual factors modulate these effects. We employ and evaluate SoniWeight Shoes, a wearable device based on literature for altering one’s weight perception through manipulated footstep sounds. In a healthy population sample across a spectrum of individuals (n=84) with varying degrees of eating disorder symptomatology, physical activity levels, body concerns, and mental imagery capacities, we explore the effects of three sound conditions (low-frequency, high-frequency and control) on extensive body perception measures (demographic, behavioral, physiological, psychological, and subjective). Analyses revealed an impact of individual differences in each of these dimensions. Besides replicating previous findings, we reveal and highlight the role of individual differences in body perception, offering avenues for personalized sonification strategies. Datasets, technical refinements, and novel body map quantification tools are provided.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {93},\nnumpages = {20},\nkeywords = {Auditory Body Perception, Embodied Interaction, Emotion, Evaluation Method, Interaction Styles, Multimodal Interfaces, Sonification, Wearable Computers},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642807,\nauthor = {Zheng, Chengbo and Yuan, Kangyu and Guo, Bingcan and Hadi Mogavi, Reza and Peng, Zhenhui and Ma, Shuai and Ma, Xiaojuan},\ntitle = {Charting the Future of AI in Project-Based Learning: A Co-Design Exploration with Students},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642807},\ndoi = {10.1145/3613904.3642807},\nabstract = {Students’ increasing use of Artificial Intelligence (AI) presents new challenges for assessing their mastery of knowledge and skills in project-based learning (PBL). This paper introduces a co-design study to explore the potential of students’ AI usage data as a novel material for PBL assessment. We conducted workshops with 18 college students, encouraging them to speculate an alternative world where they could freely employ AI in PBL while needing to report this process to assess their skills and contributions. Our workshops yielded various scenarios of students’ use of AI in PBL and ways of analyzing such usage grounded by students’ vision of how educational goals may transform. We also found that students with different attitudes toward AI exhibited distinct preferences in how to analyze and understand their use of AI. Based on these findings, we discuss future research opportunities on student-AI interactions and understanding AI-enhanced learning.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {94},\nnumpages = {19},\nkeywords = {AI for education, co-design, generative AI, project-based learning, qualitative study},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642660,\nauthor = {Ge, Xiao and Xu, Chunchen and Misaki, Daigo and Markus, Hazel Rose and Tsai, Jeanne L},\ntitle = {How Culture Shapes What People Want From AI},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642660},\ndoi = {10.1145/3613904.3642660},\nabstract = {There is an urgent need to incorporate the perspectives of culturally diverse groups into AI developments. We present a novel conceptual framework for research that aims to expand, reimagine, and reground mainstream visions of AI using independent and interdependent cultural models of the self and the environment. Two survey studies support this framework and provide preliminary evidence that people apply their cultural models when imagining their ideal AI. Compared with European American respondents, Chinese respondents viewed it as less important to control AI and more important to connect with AI, and were more likely to prefer AI with capacities to influence. Reflecting both cultural models, findings from African American respondents resembled both European American and Chinese respondents. We discuss study limitations and future directions and highlight the need to develop culturally responsive and relevant AI to serve a broader segment of the world population.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {95},\nnumpages = {15},\nkeywords = {culture, diversity, equity, human-centered AI, independence/interdependence, models of agency, survey study, theory},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642538,\nauthor = {Xygkou, Anna and Ang, Chee Siang and Siriaraya, Panote and Kopecki, Jonasz Piotr and Covaci, Alexandra and Kanjo, Eiman and She, Wan-Jou},\ntitle = {MindTalker: Navigating the Complexities of AI-Enhanced Social Engagement for People with Early-Stage Dementia},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642538},\ndoi = {10.1145/3613904.3642538},\nabstract = {People living with dementia are at risk of social isolation, and conversational AI agents can potentially support such individuals by reducing their loneliness. In our study, a conversational AI agent, called MindTalker, co-designed with therapists and utilizing the GPT-4 Large Language Model (LLM), was developed to support people with early-stage dementia, allowing them to experience a new type of “social relationship” that could be extended to real life. Eight PwD engaged with MindTalker for one month or even longer, and data was collected from interviews. Our findings emphasized that participants valued the novelty of AI, but sought more consistent, deeper interactions. They desired a personal touch from AI, while stressing the irreplaceable value of human interactions. The findings underscore the complexities of AI engagement dynamics, where participants commented on the artificial nature of AI, highlighting important insights into the future design of conversational AI for this population.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {96},\nnumpages = {15},\nkeywords = {Chatbots, Conversational AI, Dementia, Reminiscence Therapy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641887,\nauthor = {Zhao, Wei and Kelly, Ryan M. and Rogerson, Melissa J. and Waycott, Jenny},\ntitle = {Older Adults Imagining Future Technologies in Participatory Design Workshops: Supporting Continuity in the Pursuit of Meaningful Activities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641887},\ndoi = {10.1145/3613904.3641887},\nabstract = {Recent innovations in digital technology offer significant opportunities for older adults to engage in meaningful activities. To investigate older adults’ perceptions of using existing and emerging technologies for meaningful activities, we conducted three participatory design workshops and follow-up interviews with adults aged over 65. The workshops encompassed discussions on existing technologies for meaningful activities, demonstrations of emerging technologies such as VR, AR, and AI, and design activities including prototyping and storyboarding. Our findings show that while participants had diverse interpretations of meaningful activities, they sought to use technologies to support continuity in the pursuit of these activities. Specifically, participants highlighted the importance of safe aging at home, which provides a pathway for meaningful activities in later life. We further discuss participants’ discerning attitudes when assessing the use of different technologies for meaningful activities and several values and attributes they desire when envisioning future technologies, including simplicity, positivity, proactivity, and integration.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {97},\nnumpages = {18},\nkeywords = {meaningful activity, older adult, older people, participatory design, technology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642619,\nauthor = {Petsolari, Melina and Ibrahim, Seray B and Slovak, Petr},\ntitle = {Socio-technical Imaginaries: Envisioning and Understanding AI Parenting Supports through Design Fiction},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642619},\ndoi = {10.1145/3613904.3642619},\nabstract = {How might emerging modalities (e.g., NLP) be leveraged to transform the provision of parenting support? To explore the role of AI technologies in supporting parenting behaviour—and child-well-being—we surveyed 92 parents to gather their perspectives on nine future-oriented scenarios. We used Design Fiction and Speed Dating to understand parents needs and preferences around the design of agent-based supports. We explore the perceived benefits of AI assistants (i.e., receiving objective feedback, managing emotions and personalised guidance) and the most voiced concerns (i.e., AI undermining parental authority, replacing human interactions, and promoting lazy parenting). Finally, we highlight a number of plausible design directions based on the scenarios that parents were positive about.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {98},\nnumpages = {27},\nkeywords = {agent-based supports, artificial intelligence, design fiction, parenting, socio-technical design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642741,\nauthor = {Wales, Michaelah and Wheeler, Michael and Cimolino, Gabriele and Levin, Laura and Mees, Jayna and Graham, T.C. Nicholas},\ntitle = {Process, Roles, Tools, and Team: Understanding the Emerging Medium of Virtual Reality Theatre},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642741},\ndoi = {10.1145/3613904.3642741},\nabstract = {Virtual reality (VR) theatre artists are combining theatre production and game development practices to create live performances in VR. To date, little is known about VR theatre creators’ experiences of this process or how staging a play in VR might affect the audience’s experience. To capture the experience of developing a VR theatre production we interviewed the production team behind the VR play You Should Have Stayed Home. Members of this team felt the process was a learning experience and shared the lessons they plan to incorporate into their future work. We report on the team’s efforts to understand the VR theatre medium, how this team was constructed, and challenges that they encountered. In this paper we present the opportunities that the production team members identified for creating novel experiences for VR audiences, and their own needs as creators.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {99},\nnumpages = {14},\nkeywords = {Design Process, Drama, Intermedial Theatre, Virtual Reality Theatre},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642425,\nauthor = {Venkatraj, Karthikeya Puttur and Meijer, Wo and Perusquia-Hernandez, Monica and Huisman, Gijs and El Ali, Abdallah},\ntitle = {ShareYourReality: Investigating Haptic Feedback and Agency in Virtual Avatar Co-embodiment},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642425},\ndoi = {10.1145/3613904.3642425},\nabstract = {Virtual co-embodiment enables two users to share a single avatar in Virtual Reality (VR). During such experiences, the illusion of shared motion control can break during joint-action activities, highlighting the need for position-aware feedback mechanisms. Drawing on the perceptual crossing paradigm, we explore how haptics can enable non-verbal coordination between co-embodied participants. In a within-subjects study (20 participant pairs), we examined the effects of vibrotactile haptic feedback (None, Present) and avatar control distribution (25-75\\%, 50-50\\%, 75-25\\%) across two VR reaching tasks (Targeted, Free-choice) on participants’ Sense of Agency (SoA), co-presence, body ownership, and motion synchrony. We found (a) lower SoA in the free-choice with haptics than without, (b) higher SoA during the shared targeted task, (c) co-presence and body ownership were significantly higher in the free-choice task, (d) players’ hand motions synchronized more in the targeted task. We provide cautionary considerations when including haptic feedback mechanisms for avatar co-embodiment experiences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {100},\nnumpages = {15},\nkeywords = {Virtual reality, avatar co-embodiment, body ownership, co-presence, haptics, perceptual crossing, sense of agency},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641927,\nauthor = {Zhou, Qian and Ledo, David and Fitzmaurice, George and Anderson, Fraser},\ntitle = {TimeTunnel: Integrating Spatial and Temporal Motion Editing for Character Animation in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641927},\ndoi = {10.1145/3613904.3641927},\nabstract = {Editing character motion in Virtual Reality is challenging as it requires working with both spatial and temporal data using controls with multiple degrees-of-freedom. The spatial and temporal controls are separated, making it difficult to adjust poses over time and predict the effects across adjacent frames. To address this challenge, we propose TimeTunnel, an immersive motion editing interface that integrates spatial and temporal control for 3D character animation in VR. TimeTunnel provides an approachable editing experience via KeyPoses and Trajectories. KeyPoses are a set of representative poses automatically computed to concisely depict motion. Trajectories are 3D animation curves that pass through the joints of KeyPoses to represent in-betweens. TimeTunnel integrates spatial and temporal control by superimposing Trajectories and KeyPoses onto a 3D character. We conducted two studies to evaluate TimeTunnel. In our quantitative study, TimeTunnel reduced the amount of time required for editing motion, and saved effort in locating target poses. Our qualitative study with domain experts demonstrated how TimeTunnel is an approachable interface that can simplify motion editing, while still preserving a direct representation of motion.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {101},\nnumpages = {17},\nkeywords = {3D interface, immersive animation authoring, keypose, motion editing, motion path},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642691,\nauthor = {Park, Joo Young and Hsueh, Stacy and Campo Woytuk, Nadia and Huang, Xuni and Ciolfi Felice, Marianela and Balaam, Madeline},\ntitle = {Critiquing Menstrual Pain Technologies through the Lens of Feminist Disability Studies},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642691},\ndoi = {10.1145/3613904.3642691},\nabstract = {Menstrual pain or dysmenorrhea refers to abdominal cramping or pain before and during menstruation, causing a spectrum of discomfort among people who menstruate. Menstrual pain is often regarded as ‘female trouble’, as a nuisance that gets dismissed or as a symptom requiring medical intervention. While there are FemTech products that explicitly attend to menstrual pain, they predominantly seek to hide it without accounting for the lived experience of this pain. In this paper we use feminist disability studies (FDS) as a critical analytical lens to reframe the understanding of menstrual pain. Using this lens, we conduct an interaction critique of FemTech market exemplars for alleviating menstrual pain. We then offer three design provocations to better design menstrual pain technology and call for designers to attend to menstrual pain as a cyclical, chronic lived experience with the potential of spurring leaky contagious coalitions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {102},\nnumpages = {15},\nkeywords = {FemTech, Feminist HCI, crip theory, design provocations, dysmenorrhea, feminist disability studies, interaction criticism, menstrual pain},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642021,\nauthor = {Park, Kieun and Lim, Hajin and Lee, Joonhwan and Suh, Bongwon},\ntitle = {Enhancing Auto-Generated Baseball Highlights via Win Probability and Bias Injection Method},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642021},\ndoi = {10.1145/3613904.3642021},\nabstract = {The automatic generation of sports highlight videos is emerging in both the sports entertainment domain and research community. Earlier methods for generating highlights rely on visual-audio cues or contextual cues, so they may not capture the overall flow of the game well. In this paper, we propose a technique based on Win Probability Added (WPA), an empirical sabermetric baseball statistic, to generate baseball highlights that can better reflect in-game dynamics. Additionally, we introduce methods for generating “biased” highlights toward one team by systematically manipulating WPAs. Through a mixed-method user study with 43 baseball enthusiasts, we found that participants evaluated WPA-based highlights more favorably than existing AI highlights. For (un)favorably biased highlights, the game result (win/loss) was the most dominating factor in user perception, but bias directions and strengths also had nuanced effects on them. Our work contributes to the development of automated tools for generating customized sports highlights.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {103},\nnumpages = {18},\nkeywords = {Automatic Generation of Sports Highlights, Baseball Highlights, Biased Highlights, Personalized Highlights, WPA, Win Probability, Win Probability Added},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642716,\nauthor = {Das Swain, Vedant and Gao, Lan and Mondal, Abhirup and Abowd, Gregory D. and De Choudhury, Munmun},\ntitle = {Sensible and Sensitive AI for Worker Wellbeing: Factors that Inform Adoption and Resistance for Information Workers},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642716},\ndoi = {10.1145/3613904.3642716},\nabstract = {Algorithmic estimations of worker behavior are gaining popularity. Passive Sensing–enabled AI (PSAI) systems leverage behavioral traces from workers’ digital tools to infer their experience. Despite their conceptual promise, the practical designs of these systems elicit tensions that lead to workers resisting adoption. This paper teases apart the monolithic representation of PSAI by investigating system components that maximize value and mitigate concerns. We conducted an interactive online survey using the Experimental Vignette Method. Using Linear Mixed-effects Models we found that PSAI systems were more acceptable when sensing digital time use or physical activity, instead of visual modes. Inferences using language were only acceptable in work-restricted contexts. Compared to insights into performance, workers preferred insights into mental wellbeing. However, they resisted systems that automatically forwarded these insights to others. Our findings provide a template to reflect on existing systems and plan future implementations of PSAI to be more worker-centered.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {104},\nnumpages = {30},\nkeywords = {digital phenotyping, future of work, harms, human resource management, impacts, information work, mental health, passive sensing, technology adoption, worker wellbeing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642724,\nauthor = {Biggs, Heidi and Bardzell, Shaowen},\ntitle = {Thrown from Normative Ground: Exploring the Potential of Disorientation as a Critical Methodological Strategy in HCI},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642724},\ndoi = {10.1145/3613904.3642724},\nabstract = {We introduce the concept of disorientation as an emerging critical methodological strategy for design research in HCI. Disorientation is a phenomenological concept developed by queer feminist theorist Sarah Ahmed that acknowledges the spatio-embodied ‘orientations’ of societal and cultural norms and the queering potential of ‘disorientations’. We use humanistic close reading to analyze three examples from queer, feminist, and more-than-human work in HCI. Our interpretation focuses on how HCI researchers utilize disorientation as a methodological strategy for questioning norms of technologies as well as generatively, toward alternatives. We discuss the tenets of disorientation and several tactics we saw emerge in practice for other practitioners to build upon. Finally, we reflect on implications for the field, as disorientation requires vulnerability and willingness to undergo change, acknowledges embodied knowledge that emerges before interpretation, and suggests the possibility of generative and alternative orientations stemming from those epistemological commitments.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {105},\nnumpages = {11},\nkeywords = {Design Methodologies, Disorientation, Phenomenology, Queer Theory},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641937,\nauthor = {Xu, Kefan and Yan, Xinghui (Erica) and Ryu, Myeonghan and Newman, Mark W and Arriaga, Rosa I.},\ntitle = {Understanding the Effect of Reflective Iteration on Individuals’ Physical Activity Planning},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641937},\ndoi = {10.1145/3613904.3641937},\nabstract = {Many people do not get enough physical activity. Establishing routines to incorporate physical activity into people’s daily lives is known to be effective, but many people struggle to establish and maintain routines when facing disruptions. In this paper, we build on prior self-experimentation work to assist people in establishing or improving physical activity routines using a framework we call “reflective iteration.” This framework encourages individuals to articulate, reflect upon, and iterate on high-level “strategies” that inform their day-to-day physical activity plans. We designed and deployed a mobile application, Planneregy, that implements this framework. Sixteen U.S. college students used the Planneregy app for 42 days to reflectively iterate on their weekly physical exercise routines. Based on an analysis of usage data and interviews, we found that the reflective iteration approach has the potential to help people find and maintain effective physical activity routines, even in the face of life changes and temporary disruptions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {106},\nnumpages = {17},\nkeywords = {Mobile Health, Personal Informatics, Physical Activity, Planning, Qualitative Research, Self-Tracking, Self-experimentation, Self-refection},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642707,\nauthor = {Young, Jacob and Ferreira, Jennifer and Pantidi, Nadia},\ntitle = {\"I Shot the Interviewer!\": The Effects of In-VR Interviews on Participant Feedback and Rapport},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642707},\ndoi = {10.1145/3613904.3642707},\nabstract = {The integration of questionnaires into virtual reality experiences has recently been proposed as a way to reduce the potential biases introduced through the negative effects of leaving VR, however there has been little attention paid to how qualitative interviews could similarly be integrated into the virtual world for the purposes of user evaluation. In this paper we explore how conducting interviews within the virtual environment may affect the outcome of the evaluation and the relationship between participant and interviewer, and how this may differ with and without visual representation of the interviewer through use of an avatar. We conclude that in-VR interviews are a valid and promising method of data collection for user evaluation with similar data quality to in-person interviews, but that the interviewer should have a visual presence in the environment to maintain their relationship with the participant and the perceived realism of the environment.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {107},\nnumpages = {17},\nkeywords = {Interview, Qualitative Methods, Virtual/Augmented Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642511,\nauthor = {Chen, Junjian and Wang, Yuqian and Luximon, Yan},\ntitle = {CamTroller: An Auxiliary Tool for Controlling Your Avatar in PC Games Using Natural Motion Mapping},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642511},\ndoi = {10.1145/3613904.3642511},\nabstract = {Natural motion mapping enhances the gaming experience by reducing the cognitive burden and increasing immersion. However, many players still use the keyboard and mouse in recent commercial PC games. To solve the conflict between complex avatar motion and the limited interaction system, we introduced CamTroller, an auxiliary tool for commercial one-to-one avatar mapping PC games following the concept of a NUI (natural user interface). To validate this concept, we selected PUBG as the application scenario and developed a proof-of-concept system to help players achieve a better experience by naturally mapping selected human motions to the avatars in games through an RGB webcam. A within-subject study with 18 non-professional players practiced common operation (Basic), professional player’s operation (Pro), and CamTroller. Results showed that the performance of CamTroller was as good as the Pro and significantly higher than Basic. Also, the subjective evaluation showed that CamTroller achieved significantly higher intuitiveness than Basic and Pro.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {108},\nnumpages = {17},\nkeywords = {NUI, PC game, intuitive interaction, motion tracking, natural mapping},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642268,\nauthor = {Ponton, Jose Luis and Keshavarz, Reza and Beacco, Alejandro and Pelechano, Nuria},\ntitle = {Stretch your reach: Studying Self-Avatar and Controller Misalignment in Virtual Reality Interaction},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642268},\ndoi = {10.1145/3613904.3642268},\nabstract = {Immersive Virtual Reality typically requires a head-mounted display (HMD) to visualize the environment and hand-held controllers to interact with the virtual objects. Recently, many applications display full-body avatars to represent the user and animate the arms to follow the controllers. Embodiment is higher when the self-avatar movements align correctly with the user. However, having a full-body self-avatar following the user’s movements can be challenging due to the disparities between the virtual body and the user’s body. This can lead to misalignments in the hand position that can be noticeable when interacting with virtual objects. In this work, we propose five different interaction modes to allow the user to interact with virtual objects despite the self-avatar and controller misalignment and study their influence on embodiment, proprioception, preference, and task performance. We modify aspects such as whether the virtual controllers are rendered, whether controllers are rendered in their real physical location or attached to the user’s hand, and whether stretching the avatar arms to always reach the real controllers. We evaluate the interaction modes both quantitatively (performance metrics) and qualitatively (embodiment, proprioception, and user preference questionnaires). Our results show that the stretching arms solution, which provides body continuity and guarantees that the virtual hands or controllers are in the correct location, offers the best results in embodiment, user preference, proprioception, and performance. Also, rendering the controller does not have an effect on either embodiment or user preference.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {109},\nnumpages = {15},\nkeywords = {3D interaction, avatars, embodiment, perception, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642328,\nauthor = {D\\\"{o}llinger, Nina and Mal, David and Keppler, Sebastian and Wolf, Erik and Botsch, Mario and Israel, Johann Habakuk and Latoschik, Marc Erich and Wienrich, Carolin},\ntitle = {Virtual Body Swapping: A VR-Based Approach to Embodied Third-Person Self-Processing in Mind-Body Therapy},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642328},\ndoi = {10.1145/3613904.3642328},\nabstract = {Virtual reality (VR) offers various opportunities for innovative therapeutic approaches, especially regarding self-related mind-body interventions. We introduce a VR body swap system enabling multiple users to swap their perspectives and appearances and evaluate its effects on virtual sense of embodiment (SoE) and perception- and cognition-based self-related processes. In a self-compassion-framed scenario, twenty participants embodied their personalized, photorealistic avatar, swapped bodies with an unfamiliar peer, and reported their SoE, interoceptive awareness (perception), and self-compassion (cognition). Participants’ experiences differed between bottom-up and top-down processes. Regarding SoE, their agency and self-location shifted to the swap avatar, while their top-down self-identification remained with their personalized avatar. Further, the experience positively affected interoceptive awareness but not self-compassion. Our outcomes offer novel insights into the SoE in a multiple-embodiment scenario and highlight the need to differentiate between the different processes in intervention design. They raise concerns and requirements for future research on avatar-based mind-body interventions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {110},\nnumpages = {18},\nkeywords = {Virtual reality, body awareness, body swap, embodiment, perspective taking., self-compassion},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642568,\nauthor = {Jang, Yeonju and Kim, Taenyun and Kwon, Huisung and Park, Hyemin and Kim, Ki Joon},\ntitle = {Your Avatar Seems Hesitant to Share About Yourself: How People Perceive Others' Avatars in the Transparent System},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642568},\ndoi = {10.1145/3613904.3642568},\nabstract = {In avatar-mediated communications, users often cannot identify how others’ avatars are created, which is one of the important information they need to evaluate others. Thus, we tested a social virtual world that is transparent about others’ avatar-creation methods and investigated how knowing about others’ avatar-creation methods shapes users’ perceptions of others and their self-disclosure. We conducted a 2x2 mixed-design experiment with system design (nontransparent vs. transparent system) as a between-subjects and avatar-creation method (customized vs. personalized avatar) as a within-subjects variable with 60 participants. The results revealed that personalized avatars in the transparent system were viewed less positively than customized avatars in the transparent system or avatars in the nontransparent system. These avatars appeared less comfortable and honest in their self-disclosure and less competent. Interestingly, avatars in the nontransparent system attracted more followers. Our results suggest being cautious when creating a social virtual world that discloses the avatar-creation process.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {111},\nnumpages = {14},\nkeywords = {avatar-mediated communication, receiver, self-disclosure, sender, transparent system},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642885,\nauthor = {Smith, Garrett and Carson, Sarah and Vengurlekar, Rhea G and Morales, Stephanie and Tsai, Yun-Chieh and George, Rachel and Bedwell, Josh and Jones, Trevor and Mondal, Mainack and Smith, Brian and Su, Norman Makoto and Knijnenburg, Bart and Page, Xinru},\ntitle = {\"I Know I'm Being Observed:\" Video Interventions to Educate Users about Targeted Advertising on Facebook},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642885},\ndoi = {10.1145/3613904.3642885},\nabstract = {Recent work explores how to educate and encourage users to protect their online privacy. We tested the efficacy of short videos for educating users about targeted advertising on Facebook. We designed a video that utilized an emotional appeal to explain risks associated with targeted advertising (fear appeal), and which demonstrated how to use the associated ad privacy settings (digital literacy). We also designed a version of this video which additionally showed the viewer their personal Facebook ad profile, facilitating personal reflection on how they are currently being profiled (reflective learning). We conducted an experiment (n = 127) in which participants watched a randomly assigned video and measured the impact over the following 10 weeks. We found that these videos significantly increased user engagement with Facebook advertising preferences, especially for those who viewed the reflective learning content. However, those who only watched the fear appeal content were more likely to disengage with Facebook as a whole.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {112},\nnumpages = {27},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642144,\nauthor = {Fiani, Cristina and Bretin, Robin and Macdonald, Shaun Alexander and Khamis, Mohamed and Mcgill, Mark},\ntitle = {\"Pikachu would electrocute people who are misbehaving\": Expert, Guardian and Child Perspectives on Automated Embodied Moderators for Safeguarding Children in Social Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642144},\ndoi = {10.1145/3613904.3642144},\nabstract = {Automated embodied moderation has the potential to create safer spaces for children in social VR, providing a protective figure that takes action to mitigate harmful interactions. However, little is known about how such moderation should be employed in practice. Through interviews with 16 experts in online child safety and psychology, and workshops with 8 guardians and 13 children, we contribute a comprehensive overview of how Automated Embodied Moderators (AEMs) can safeguard children in social VR. We explore perceived concerns, benefits and preferences across the stakeholder groups and gather first-of-their-kind recommendations and reflections around AEM design. The results stress the need to adapt AEMs to children, whether victims or harassers, based on age and development, emphasising empowerment, psychological impact and humans/guardians-in-the-loop. Our work provokes new participatory design-led directions to consider in the development of AEMs for children in social VR taking child, guardian, and expert insights into account.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {113},\nnumpages = {23},\nkeywords = {child online safety, children, design workshops, experts, grandparent, guardian, interviews, metaverse, parent, social virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642447,\nauthor = {Stephenson, Sophie and Page, Christopher Nathaniel and Wei, Miranda and Kapadia, Apu and Roesner, Franziska},\ntitle = {Sharenting on TikTok: Exploring Parental Sharing Behaviors and the Discourse Around Children’s Online Privacy},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642447},\ndoi = {10.1145/3613904.3642447},\nabstract = {Since the inception of social media, parents have been sharing information about their children online. Unfortunately, this “sharenting” can expose children to several online and offline risks. Although researchers have studied sharenting on multiple platforms, sharenting on short-form video platforms like TikTok—where posts can contain detailed information, spread quickly, and spark considerable engagement—is understudied. Thus, we provide a targeted exploration of sharenting on TikTok. We analyzed 328 TikTok videos that demonstrate sharenting and 438 videos where TikTok creators discuss sharenting norms. Our results indicate that sharenting on TikTok indeed creates several risks for children, not only within individual posts but also in broader patterns of sharenting that arise when parents repeatedly use children to generate viral content. At the same time, creators voiced sharenting concerns and boundaries that reflect what has been observed on other platforms, indicating the presence of cross-platform norms. Promisingly, we observed that TikTok users are engaging in thoughtful conversations around sharenting and beginning to shift norms toward safer sharenting. We offer concrete suggestions for designers and platforms based on our findings.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {114},\nnumpages = {17},\nkeywords = {TikTok, children, parents, privacy, sharenting, social media, youth},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642315,\nauthor = {Alsoubai, Ashwaq and Park, Jinkyung and Qadir, Sarvech and Stringhini, Gianluca and Razi, Afsaneh and Wisniewski, Pamela J.},\ntitle = {Systemization of Knowledge (SoK): Creating a Research Agenda for Human-Centered Real-Time Risk Detection on Social Media Platforms},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642315},\ndoi = {10.1145/3613904.3642315},\nabstract = {Accurate real-time risk identification is vital to protecting social media users from online harm, which has driven research towards advancements in machine learning (ML). While strides have been made regarding the computational facets of algorithms for “real-time” risk detection, such research has not yet evaluated these advancements through a human-centered lens. To this end, we conducted a systematic literature review of 53 peer-reviewed articles on real-time risk detection on social media. Real-time detection was mainly operationalized as “early” detection after-the-fact based on pre-defined chunks of data and evaluated based on standard performance metrics, such as timeliness. We identified several human-centered opportunities for advancing current algorithms, such as integrating human insight in feature selection, algorithms’ improvement considering human behavior, and utilizing human evaluations. This work serves as a critical call-to-action for the HCI and ML communities to work together to protect social media users before, during, and after exposure to risks.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {115},\nnumpages = {21},\nkeywords = {Human-Centered Machine Learning, Literature Review, Online Risk, Real-Time Risk Detection, Social Media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642313,\nauthor = {Agha, Zainab and Park, Jinkyung and Wan, Ruyuan and Ali, Naima Samreen and Wang, Yiwei and Difranzo, Dominic and Badillo-Urquiola, Karla and Wisniewski, Pamela J.},\ntitle = {Tricky vs. Transparent: Towards an Ecologically Valid and Safe Approach for Evaluating Online Safety Nudges for Teens},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642313},\ndoi = {10.1145/3613904.3642313},\nabstract = {HCI research has been at the forefront of designing interventions for protecting teens online; yet, how can we test and evaluate these solutions without endangering the youth we aim to protect? Towards this goal, we conducted focus groups with 20 teens to inform the design of a social media simulation platform and study for evaluating online safety nudges co-designed with teens. Participants evaluated risk scenarios, personas, platform features, and our research design to provide insight regarding the ecological validity of these artifacts. Teens expected risk scenarios to be subtle and tricky, while also higher in risk to be believable. The teens iterated on the nudges to prioritize risk prevention without reducing autonomy, risk coping, and community accountability. For the simulation, teens recommended using transparency with some deceit to balance realism and respect for participants. Our meta-level research provides a teen-centered action plan to evaluate online safety interventions safely and effectively.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {116},\nnumpages = {20},\nkeywords = {Adolescent Online Safety, Behavior Change, Ecological Validity, Evaluations, Interventions, Nudges, Simulations, User Personas},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642492,\nauthor = {Newman, Michele and Sun, Kaiwen and Dalla Gasperina, Ilena B and Shin, Grace Y. and Pedraja, Matthew Kyle and Kanchi, Ritesh and Song, Maia B. and Li, Rannie and Lee, Jin Ha and Yip, Jason},\ntitle = {\"I want it to talk like Darth Vader\": Helping Children Construct Creative Self-Efficacy with Generative AI},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642492},\ndoi = {10.1145/3613904.3642492},\nabstract = {The emergence of generative artificial intelligence (GenAI) has ignited discussions surrounding its potential to enhance creative pursuits. However, distinctions between children’s and adult’s creative needs exist, which is important when considering the possibility of GenAI for children’s creative usage. Building upon work in Human-Computer Interaction (HCI), fostering children’s computational thinking skills, this study explores interactions between children (aged 7-13) and GenAI tools through methods of participatory design. We seek to answer two questions: (1) How do children in co-design workshops perceive GenAI tools and their usage for creative works? and (2) How do children navigate the creative process while using GenAI tools? How might these interactions support their confidence in their ability to create? Our findings contribute a model that describes the potential contexts underpinning child-GenAI creative interactions and explores implications of this model for theories of creativity, design, and use of GenAI as a constructionist tool for creative self-efficacy.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {117},\nnumpages = {18},\nkeywords = {Artificial Intelligence, Children, Co-Design, Constructionism, Creativity, Participatory Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642852,\nauthor = {Liu, Di and Zhou, Hanqing and An, Pengcheng},\ntitle = {\"When He Feels Cold, He Goes to the Seahorse\"—Blending Generative AI into Multimaterial Storymaking for Family Expressive Arts Therapy},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642852},\ndoi = {10.1145/3613904.3642852},\nabstract = {Storymaking, as an integrative form of expressive arts therapy, is an effective means to foster family communication. Yet, the integration of generative AI as expressive materials in therapeutic storymaking remains underexplored. And there is a lack of HCI implications on how to support families and therapists in this context. Addressing this, our study involved five weeks of storymaking sessions with seven families guided by a professional therapist. In these sessions, the families used both traditional art-making materials and image-based generative AI to create and evolve their family stories. Via the rich empirical data and commentaries from four expert therapists, we contextualize how families creatively melded AI and traditional expressive materials to externalize their ideas and feelings. Through the lens of Expressive Therapies Continuum (ETC), we characterize the therapeutic implications of AI as expressive materials. Desirable interaction qualities to support children, parents, and therapists are distilled for future HCI research.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {118},\nnumpages = {21},\nkeywords = {Expressive arts therapy, children, family, generative AI, human-AI interaction., storymaking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641923,\nauthor = {Shen, Chenxinran and Mcgrenere, Joanna and Yoon, Dongwook},\ntitle = {LegacySphere: Facilitating Intergenerational Communication Through Perspective-Taking and Storytelling in Embodied VR},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641923},\ndoi = {10.1145/3613904.3641923},\nabstract = {Intergenerational communication can enhance well-being and family cohesion, but stereotypes and low empathy can be barriers to achieving effective communication. VR perspective-taking is a potential approach that is known to enhance understanding and empathy toward others by allowing a user to take another’s viewpoint. In this study, we introduce LegacySphere, a novel VR perspective-taking experience leveraging the combination of embodiment, role-play, and storytelling. To explore LegacySphere’s design and impact, we conducted an observational study involving five dyads with a one-generation gap. We found that LegacySphere promotes empathetic and reflexive intergenerational dialogue. Specifically, avatar embodiment encourages what we term “relationship cushioning,” fostering a trustful, open environment for genuine communications. The blending of real and embodied identities prompts insightful questions, merging both perspectives. The experience also nurtures a sense of unity and stimulates reflections on aging. Our work highlights the potential of immersive technologies for enhancing empathetic intergenerational relationships.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {119},\nnumpages = {16},\nkeywords = {Empathy, Intergenerational communication, Perspective-taking, Proteus effect, Role-play, Storytelling, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642580,\nauthor = {Lee, Jungeun and Yoon, Suwon and Lee, Kyoosik and Jeong, Eunae and Cho, Jae-Eun and Park, Wonjeong and Yim, Dongsun and Hwang, Inseok},\ntitle = {Open Sesame? Open Salami! Personalizing Vocabulary Assessment-Intervention for Children via Pervasive Profiling and Bespoke Storybook Generation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642580},\ndoi = {10.1145/3613904.3642580},\nabstract = {Children acquire language by interacting with their surroundings. Due to the different language environments each child is exposed to, the words they encounter and need in their life vary. Despite the standard tools for assessment and intervention as per predefined vocabulary sets, speech-language pathologists and parents struggle with the absence of systematic tools for child-specific custom vocabulary, i.e., out-of-standard but personally more important. We propose “Open Sesame? Open Salami! (OSOS)”, a personalized vocabulary assessment and intervention system with pervasive language profiling and targeted storybook generation, collaboratively developed with speech-language pathologists. Melded into a child’s daily life and powered by large language models (LLM), OSOS profiles the child’s language environment, extracts priority words therein, and generates bespoke storybooks naturally incorporating those words. We evaluated OSOS through 4-week-long deployments to 9 families. We report their experiences with OSOS, and its implications in supporting personalization outside standards.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {120},\nnumpages = {32},\nkeywords = {generative AI, language assessment and intervention, large language model, storybook generation, vocabulary learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642307,\nauthor = {Yu, Junnan and Qi, Xiang and Yang, Siqi},\ntitle = {Parent-Child Joint Media Engagement Within HCI: A Scoping Analysis of the Research Landscape},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642307},\ndoi = {10.1145/3613904.3642307},\nabstract = {Parents play essential roles in children’s play and learning with various media, often leading to positive and productive engagement outcomes for both parties. As such, an increasing number of HCI research has focused on understanding parent-child joint media engagement (JME) and designing new technologies to foster productive joint media experiences for children and parents. However, we currently lack a systematic view of this emerging field, which hinders the research and design of new joint media experiences and technologies for families. In this work, we conduct a scoping review of parent-child JME research within HCI (N = 89) and analyze the included papers from three lenses: publication features, methodological features, and JME features. Based on these findings, we identify gaps and opportunities in parent-child JME research and further expand the theoretical framing of JME by developing a framework that captures different JME dimensions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {121},\nnumpages = {21},\nkeywords = {Child, Framework Development, Joint Media Engagement, Parent, Scoping Review},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642806,\nauthor = {Ho, Hui-Ru and Hubbard, Edward M. and Mutlu, Bilge},\ntitle = {\"It's Not a Replacement:\" Enabling Parent-Robot Collaboration to Support In-Home Learning Experiences of Young Children},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642806},\ndoi = {10.1145/3613904.3642806},\nabstract = {Learning companion robots for young children are increasingly adopted in informal learning environments. Although parents play a pivotal role in their children’s learning, very little is known about how parents prefer to incorporate robots into their children’s learning activities. We developed prototype capabilities for a learning companion robot to deliver educational prompts and responses to parent-child pairs during reading sessions and conducted in-home user studies involving 10 families with children aged 3–5. Our data indicates that parents want to work with robots as collaborators to augment parental activities to foster children’s learning, introducing the notion of parent-robot collaboration. Our findings offer an empirical understanding of the needs and challenges of parent-child interaction in informal learning scenarios and design opportunities for integrating a companion robot into these interactions. We offer insights into how robots might be designed to facilitate parent-robot collaboration, including parenting policies, collaboration patterns, and interaction paradigms.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {122},\nnumpages = {18},\nkeywords = {Human-robot interaction, field study, home, informal learning, parent-child dyads, parent-robot collaboration, young children},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642294,\nauthor = {Wang, Ge and Zhao, Jun and Johnston, Samantha-Kaye and Zhang, Zhilin and Van Kleek, Max and Shadbolt, Nigel},\ntitle = {CHAITok: A Proof-of-Concept System Supporting Children's Sense of Data Autonomy on Social Media},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642294},\ndoi = {10.1145/3613904.3642294},\nabstract = {Social media has become a primary source of entertainment and education for children globally. While much attention has been given to children’s online well-being, a pressing concern often goes unnoticed: the pervasive data harvesting underlying social media and its manipulative impact on undermining children’s autonomy. In this paper, we present CHAITok, an Android mobile application designed to enhance children’s sense of autonomy over their data on social media. Through 27 user study sessions with 109 children aged 10–13, we offer insights into the current lack of data autonomy among children regarding their online information, and how we can foster children’s sense of data autonomy through a socio-technical journey. Our findings inspire design recommendations to respect children’s values, support children’s evolving autonomy, and design for children’s digital rights. We emphasize data autonomy as a fundamental right for children, call for further research, design innovation, and policy changes on this critical issue.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {123},\nnumpages = {19},\nkeywords = {Children, Data Autonomy, Social Media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642745,\nauthor = {Silva, Lucas M. and Cibrian, Franceli L. and Bonang, Clarisse and Bhattacharya, Arpita and Min, Aehong and Monteiro, Elissa M and Beltran, Jesus Armando and Schuck, Sabrina and Lakes, Kimberley D and Hayes, Gillian R. and Epstein, Daniel A.},\ntitle = {Co-Designing Situated Displays for Family Co-Regulation with ADHD Children},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642745},\ndoi = {10.1145/3613904.3642745},\nabstract = {Family informatics often uses shared data dashboards to promote awareness of each other’s health-related behaviors. However, these interfaces often stop short of providing families with needed guidance around how to improve family functioning and health behaviors. We consider the needs of family co-regulation with ADHD children to understand how in-home displays can support family well-being. We conducted three co-design sessions with each of eight families with ADHD children who had used a smartwatch for self-tracking. Results indicate that situated displays could nudge families to jointly use their data for learning and skill-building. Accommodating individual needs and preferences when family members are alone is also important, particularly to support parents exploring their co-regulation role, and assisting children with data interpretation and guidance on self and co-regulation. We discuss opportunities for displays to nurture multiple intents of use, such as joint or independent use, while potentially connecting with external expertise.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {124},\nnumpages = {19},\nkeywords = {ADHD, Co-design, Family Informatics, Health tracking, Situated display, Smartwatches},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642763,\nauthor = {Cassidy, Cameron Tyler and Figueira, Isabela and Park, Sohyeon and Kim, Jin Seo and Edwards, Emory James and Branham, Stacy Marie},\ntitle = {Cuddling Up With a Print-Braille Book: How Intimacy and Access Shape Parents' Reading Practices with Children},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642763},\ndoi = {10.1145/3613904.3642763},\nabstract = {Like many parents, visually impaired parents (VIPs) read books with their children. However, research on accessible reading technologies predominantly focuses on blind adults reading alone or sighted adults reading with blind children, such that the motivations, strategies, and needs of blind parents reading with their sighted children are still largely undocumented. To address this gap, we interviewed 13 VIPs with young children. We found that VIPs (1) sought familial intimacy through reading with their child, often prioritizing intimacy over their own access needs, (2) took on many types of access labor to read with their children, and (3) desired novel assistive technologies (ATs) for reading that prioritize intimacy while reducing access labor. We contribute the notion of Intimate AT, along with a demonstrative design space, which together constitute a new design paradigm that draws attention to intimacy as a facet of both independently and collaboratively accessible ATs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {125},\nnumpages = {15},\nkeywords = {accessibility, blind, books, co-reading, eBooks, low vision, voice assistants},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642334,\nauthor = {Su, Zhaoyuan and Kamath, Sunil P. and Tirakitsoontorn, Pornchai and Chen, Yunan},\ntitle = {Creating Safe Places: Understanding the Lived Experiences of Families Managing Cystic Fibrosis in Young Children},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642334},\ndoi = {10.1145/3613904.3642334},\nabstract = {While previous HCI research has examined chronic care management for children, less is known about supporting families with young children facing serious illnesses. We interviewed 12 families affected by cystic fibrosis (CF) to understand their experiences and explore opportunities to support CF management. We identified three stages of CF management in young children: diagnosis at birth, parental navigation of CF management in the early years, and gradual involvement of children in their CF care. We underscore child development milestones as a key macro-temporal structure in children’s health management, the multifaceted and evolving parental values in crafting a safe place for children, and the balancing acts parents conduct to recreate this safe place as their children grow. We provide design implications to inform the future of child-centered and family-oriented health technologies that can evolve with parents’ and children’s values to assist in creating a safe environment for managing children’s health.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {126},\nnumpages = {18},\nkeywords = {Caregiving, Child Safety, Child-centered Design, Chronic Illness Management, Cystic Fibrosis, Family Informatics, Health and Wellbeing, Parent-child Interaction, Parenting, Personal Value},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642435,\nauthor = {Lakhdhir, Sabrina and Nayar, Chehak and Anderson, Fraser and Fournier, Helene and Holsti, Liisa and Kondratova, Irina and Perin, Charles and Somanath, Sowmya},\ntitle = {GlucoMaker: Enabling Collaborative Customization of Glucose Monitors},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642435},\ndoi = {10.1145/3613904.3642435},\nabstract = {Millions of individuals with diabetes use glucose monitors to track blood sugar levels. Research shows that such individuals seek to customize different aspects of their interactions with these devices, including how they engage with, decorate, and wear them. However, it remains challenging to tailor both device form and function to accommodate individual needs. To address this challenge, we introduce GlucoMaker, a system for collaboratively customizing physical design aspects of glucose monitors. Prior to designing GlucoMaker, we conducted a prototyping and focus group study to understand customization preferences and collaboration benefits. GlucoMaker provides individuals with the ability to a) select monitor form and function preferences, b) alter predefined and downloadable digital model files, c) receive feedback on monitor designs from stakeholders, and d) learn technical design aspects. We further demonstrate the versatility and design space of GlucoMaker with three examples of different form and function use cases.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {127},\nnumpages = {21},\nkeywords = {collaboration, customization, design, design for health, fabrication, glucose monitors},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642553,\nauthor = {Paymal, L\\'{e}a and Homewood, Sarah},\ntitle = {Good Days, Bad Days: Understanding the Trajectories of Technology Use During Chronic Fatigue Syndrome},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642553},\ndoi = {10.1145/3613904.3642553},\nabstract = {People with chronic illness often fluctuate between “good days” and “bad days” where symptoms are more or less severe depending on a range of factors and triggers. Our research contributes preliminary empirical knowledge on technology use during chronic illness depending on fluctuations in symptoms over time. We conducted a scoping study with people with myalgic encephalomyelitis/chronic fatigue syndrome (ME/CFS) to understand how their illness shapes how they use technologies in their everyday lives. This research contributes a timely HCI lens on the under-researched illness of ME/CFS, proposes the “trajectories of technology use” model that can be used to articulate how technologies are used during chronic illness, and points to design openings for technologies that are more accessible for people who experience chronic fatigue, sensory sensitivities and cognitive limitations. These design openings include non-screen-based technologies, and designing technologies that acknowledge and adapt to the changing body during fluctuations in symptoms.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {128},\nnumpages = {10},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642075,\nauthor = {Sefidgar, Yasaman S. and Castillo, Carla L. and Chopra, Shaan and Jiang, Liwei and Jones, Tae and Mittal, Anant and Ryu, Hyeyoung and Schroeder, Jessica and Cole, Allison and Murinova, Natalia and Munson, Sean A. and Fogarty, James},\ntitle = {MigraineTracker: Examining Patient Experiences with Goal-Directed Self-Tracking for a Chronic Health Condition},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642075},\ndoi = {10.1145/3613904.3642075},\nabstract = {Self-tracking and personal informatics offer important potential in chronic condition management, but such potential is often undermined by difficulty in aligning self-tracking tools to an individual’s goals. Informed by prior proposals of goal-directed tracking, we designed and developed MigraineTracker, a prototype app that emphasizes explicit expression of goals for migraine-related self-tracking. We then examined migraine patient experiences in a deployment study for an average of 12+ months, including a total of 50 interview sessions with 10 patients working with 3 different clinicians. Patients were able to express multiple types of goals, evolve their goals over time, align tracking to their goals, personalize their tracking, reflect in the context of their goals, and gain insights that enabled understanding, communication, and action. We discuss how these results highlight the importance of accounting for distinct and concurrent goals in personal informatics together with implications for the design of future goal-directed personal informatics tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {129},\nnumpages = {19},\nkeywords = {Chronic Conditions, Goal Evolution, Goal-Directed Tracking, Long-Term Tracking, Migraine, Personal Informatics, Self-Tracking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642215,\nauthor = {Kandel, Jade and Duppen, Chelsea and Zhang, Qian and Jiang, Howard and Angelopoulos, Angelos and Neall, Ashley Paula-Ann and Wagh, Pranav and Szafir, Daniel and Fuchs, Henry and Lewek, Michael and Szafir, Danielle Albers},\ntitle = {PD-Insighter: A Visual Analytics System to Monitor Daily Actions for Parkinson's Disease Treatment},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642215},\ndoi = {10.1145/3613904.3642215},\nabstract = {People with Parkinson’s Disease (PD) can slow the progression of their symptoms with physical therapy. However, clinicians lack insight into patients’ motor function during daily life, preventing them from tailoring treatment protocols to patient needs. This paper introduces PD-Insighter, a system for comprehensive analysis of a person’s daily movements for clinical review and decision-making. PD-Insighter provides an overview dashboard for discovering motor patterns and identifying critical deficits during activities of daily living and an immersive replay for closely studying the patient’s body movements with environmental context. Developed using an iterative design study methodology in consultation with clinicians, we found that PD-Insighter’s ability to aggregate and display data with respect to time, actions, and local environment enabled clinicians to assess a person’s overall functioning during daily life outside the clinic. PD-Insighter’s design offers future guidance for generalized multiperspective body motion analytics, which may significantly improve clinical decision-making and slow the functional decline of PD and other medical conditions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {130},\nnumpages = {18},\nkeywords = {Health, Immersive Analytics, Visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642827,\nauthor = {Pater, Jessica and Chopra, Shaan and Carroll, Jeanne and Zaccour, Juliette and Nova, Fayika Farhat and Toscos, Tammy and Guha, Shion and Chang, Fen Lei},\ntitle = {Charting the COVID Long Haul Experience - A Longitudinal Exploration of Symptoms, Activity, and Clinical Adherence},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642827},\ndoi = {10.1145/3613904.3642827},\nabstract = {COVID Long Haul (CLH) is an emerging chronic illness with varied patient experiences. Our understanding of CLH is often limited to data from electronic health records (EHRs), such as diagnoses or problem lists, which do not capture the volatility and severity of symptoms or their impact. To better understand the unique presentation of CLH, we conducted a 3-month long cohort study with 14 CLH patients, collecting objective (EHR, daily Fitbit logs) and subjective (weekly surveys, interviews) data. Our findings reveal a complex presentation of symptoms, associated uncertainty, and the ensuing impact CLH has on patients’ personal and professional lives. We identify patient needs, practices, and challenges around adhering to clinical recommendations, engaging with health data, and establishing \"new normals\" post COVID. We reflect on the potential found at the intersection of these various data streams and the persuasive heuristics possible when designing for this new population and their specific needs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {131},\nnumpages = {21},\nkeywords = {COVID Long Haul, COVID-19, Electronic Health Record, Fitbit, Interviews, Long COVID, PASC, Post-COVID, Qualitative Methods, Surveys},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642618,\nauthor = {Branco, Diogo and M\\'{o}teiro, Margarida and Bou\\c{c}a-Machado, Raquel and Miranda, Rita and Reis, Tiago and Decoroso, \\'{E}lia and Cardoso, Rita and Ramalho, Joana and Rato, Filipa and Malheiro, Joana and Miranda, Diana and Cani\\c{c}a, Ver\\'{o}nica and Pona-Ferreira, Filipa and Guerreiro, Daniela and Leit\\~{a}o, Mariana and Braz, Alexandra Sa\\'{u}de and J Ferreira, Joaquim and Guerreiro, Tiago},\ntitle = {Co-designing Customizable Clinical Dashboards with Multidisciplinary Teams: Bridging the Gap in Chronic Disease Care},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642618},\ndoi = {10.1145/3613904.3642618},\nabstract = {Providing care to individuals with chronic diseases benefits from a multidisciplinary approach and longitudinal symptom, event, and disease monitoring, in and out of clinical facilities. Technological advancements, including the ubiquitous presence of sensors and devices, present opportunities to collect large amounts of data and extract evidence-based insights about the patient and disease. Nevertheless, practical examples of clinical utility of those technologies remain sparse, and in specific focus areas (e.g, insights from a single device). This paper explores the challenges and opportunities of multidisciplinary clinical dashboards to support clinicians caring for people with chronic diseases. We report on a focus group and co-design workshops with a multidisciplinary team of clinicians and HCI researchers. We offer insights into how technological outcomes and visualizations can enhance clinical practice and the intricacies of information-sharing dynamics. We discuss the potential of dashboards to trigger actions in clinical settings and emphasize the benefits of customizable dashboards.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {132},\nnumpages = {18},\nkeywords = {chronic diseases, co-design, customization, dashboards, multidisciplinary, sensors},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642184,\nauthor = {Wu, Ling and Seguin, Joshua Paolo and Chandrasekara, Dharshani and Cardamone-Breen, Mairead Claire and Xie, Jue and Mcnaney, Roisin and Bartindale, Tom and Olivier, Patrick and Yap, Marie B H},\ntitle = {Designing online peer support for parents of adolescents at risk of mental health challenges},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642184},\ndoi = {10.1145/3613904.3642184},\nabstract = {While online parenting interventions have been shown to improve youth mental health, parents find it challenging to engage with and implement strategies from self-directed interventions. Our study purposefully designed a parent peer-support community for parents seeking support. Our two-phased qualitative study included parent interviews and design workshops. Our findings show that while parents need others’ lived experiences to learn about parenting, perceived judgment and self-doubt can stop them from actively contributing to the peer support group. To address this design challenge, we operationalised parents’ needs and challenges gained in the interviews and workshops into design implications. We demonstrate a parent-centered design approach where we formulate design implications that integrate parents’ needs and expectations with multidisciplinary theoretical and empirical evidence to deepen and concretise the design for an online parent peer-support community that cultivates empathy, encourages confidence and self-efficacy, and motivates change and growth.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {133},\nnumpages = {14},\nkeywords = {parent, peer support, service design, youth mental health},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642882,\nauthor = {Jain, Taru and Mudliar, Preeti},\ntitle = {Platforming PCOS Treatment Online: FemTech Logics of Care},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642882},\ndoi = {10.1145/3613904.3642882},\nabstract = {This paper explores how FemTech platforms appropriate the term ‘care’ to create a collaborative, non-judgmental, and data-driven approach for a sexual and reproductive health condition like Polycystic Ovary Syndrome (PCOS). In contrast, offline healthcare for PCOS is insufficient owing to disruptions in treatment, gynaecological indifference, and a lack of time and attention to patient concerns. We share findings from an ethnographic study conducted in India, involving interviews and observations with FemTech platform founders, gynaecologists, and people with PCOS. Our study highlights how FemTech start-ups, led by engineering and management professionals, establish a unified digital care approach by capitalizing on the shortcomings of traditional offline gynaecological healthcare infrastructures. We identify the logics of care surrounding FemTech platforms and assess their sustainability as digital alternatives to offline gynaecological care. We offer recommendations to FemTech founders and policymakers to build sustainable and inclusive offline and online health infrastructures.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {134},\nnumpages = {18},\nkeywords = {Care, FemTech, Gynaecology, India, PCOS, Women’s Health},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642234,\nauthor = {Barth, Clara-Maria and Bernard, J\\\"{u}rgen and Huang, Elaine M.},\ntitle = {\"It's like a glimpse into the future\": Exploring the Role of Blood Glucose Prediction Technologies for Type 1 Diabetes Self-Management},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642234},\ndoi = {10.1145/3613904.3642234},\nabstract = {Self-management of type 1 diabetes (T1D) involves multiple factors, frequent anticipation of changes in blood glucose, and complex decision-making. ML-based blood glucose predictions (BGP) may be valuable in supporting T1D management. However, it may be difficult for people with T1D to integrate BGP into their decision-making due to prediction uncertainty and interpretation. In this study, we investigate the lived experience of people with T1D focusing on their needs and expectations in using apps that provide BGP. We designed MOON-T1D, an app that shows simulated BGP and conducted a five-day study using the Experience Sampling Method coupled with semi-structured interviews with 15 individuals with T1D who used MOON-T1D. A reflexive thematic analysis of our data revealed implications for the design and use of BGP, including the complex role of emotions and trust surrounding predictions, and ways in which BGP may ease or complicate T1D management.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {135},\nnumpages = {21},\nkeywords = {Artificial Intelligence, Health, Mobile Health, Qualitative Study, Type 1 Diabetes},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642245,\nauthor = {Orii, Lisa and Feldacker, Caryl and Huwa, Jacqueline Madalitso and Thawani, Agness and Viola, Evelyn and Kiruthu-Kamamia, Christine and Sande, Odala and Tweya, Hannock and Anderson, Richard},\ntitle = {HIV Client Perspectives on Digital Health in Malawi},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642245},\ndoi = {10.1145/3613904.3642245},\nabstract = {eHealth has strong potential to advance HIV care in low- and middle-income countries. Given the sensitivity of HIV-related information and the risks associated with unintended HIV status disclosure, clients’ privacy perceptions towards eHealth applications should be examined to develop client-centered technologies. Through focus group discussions with antiretroviral therapy (ART) clients from Lighthouse Trust, Malawi’s public HIV care program, we explored perceptions of data security and privacy, including their understanding of data flow and their concerns about data confidentiality across several layers of data use. Our findings highlight the broad privacy concerns that affect ART clients’ day-to-day choices, clients’ trust in Malawi’s health system, and their acceptance of, and familiarity with, point-of-care technologies used in HIV care. Based on our findings, we provide recommendations for building robust digital health systems in low- and middle-income countries with limited resources, nascent privacy regulations, and political will to take action to protect client data.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {136},\nnumpages = {13},\nkeywords = {HCI4D, HIV, Mobile Devices, Privacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642344,\nauthor = {Cha, Yoon Jeong and Gunal, Yasemin and Wou, Alice and Lee, Joyce and Newman, Mark W and Park, Sun Young},\ntitle = {Shared Responsibility in Collaborative Tracking for Children with Type 1 Diabetes and their Parents},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642344},\ndoi = {10.1145/3613904.3642344},\nabstract = {Efficient Type 1 Diabetes (T1D) management necessitates comprehensive tracking of various factors that influence blood sugar levels. However, tracking health data for children with T1D poses unique challenges, as it requires the active involvement of both children and their parents. This study aims to uncover the benefits, challenges, and strategies associated with collaborative tracking for children (ages 6-12) with T1D and their parents. Over a three-week data collection probe study with 22 child-parent pairs, we found that collaborative tracking, characterized by the shared responsibility of tracking management and data provision, yielded positive outcomes for both children and their parents. Drawing from these findings, we delineate four distinct tracking approaches: child-independent, child-led, parent-led, and parent-independent. Our study offers insights for designing health technologies that empower both children and parents in learning and encourage the sharing of different perspectives through collaborative tracking.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {137},\nnumpages = {20},\nkeywords = {T1D, child, child-parent collaboration, chronic illness management, collaborative healthcare technology, collaborative tracking, health tracking, pediatrics, probe study, type 1 diabetes},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642701,\nauthor = {Keys, Rachel and Marshall, Paul and Stuart, Graham and O'Kane, Aisling Ann},\ntitle = {“I think it saved me. I think it saved my heart”: The Complex Journey From Self-Tracking With Wearables To Diagnosis},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642701},\ndoi = {10.1145/3613904.3642701},\nabstract = {Despite their nonclinical origins, wearables are emerging as valuable tools in supporting the diagnosis of cardiovascular disease, one of the leading causes of death worldwide. Diagnostic data once only available via a cardiologist is now available to consumers simply by wearing a smartwatch, so understanding how smartwatches currently support diagnosis is important for healthcare providers and for the designers of increasingly sophisticated personal informatics technology. We conducted a qualitative study comprising interviews and analysis of posts on an online community of accounts of smartwatch assisted cardiac diagnosis. Our findings reveal how smartwatches bridge a current gap in clinical diagnostic modalities, facilitating a diagnostic journey instigated and shaped by the interplay of self-collected data, bodily self-awareness, and increasing clinical acceptance. These insights focus attention on the consequences of the democratisation of health data, with ethical and design implications for health providers, consumer electronic companies, and third-party application designers.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {138},\nnumpages = {15},\nkeywords = {Cardiovascular Disease, Self-Tracking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642612,\nauthor = {Xu, Tian and Jost, Emily and Messer, Laurel H. and Cook, Paul F. and Forlenza, Gregory P and Sankaranarayanan, Sriram and Fiesler, Casey and Voida, Stephen},\ntitle = {“Obviously, Nothing's Gonna Happen in Five Minutes”: How Adolescents and Young Adults Infrastructure Resources to Learn Type 1 Diabetes Management},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642612},\ndoi = {10.1145/3613904.3642612},\nabstract = {Learning personalized self-management routines is pivotal for people with type 1 diabetes (T1D), particularly early in diagnosis. Context-aware technologies, such as hybrid closed-loop (HCL) insulin pumps, are important tools for diabetes self-management. However, clinicians have observed that practices using these technologies involve significant individual differences. We conducted interviews with 20 adolescents and young adults who use HCL insulin pump systems for managing T1D, and we found that these individuals leverage both technological and non-technological means to maintain situational awareness about their condition. We discuss how these practices serve to infrastructure their self-management routines, including medical treatment, diet, and glucose measurement-monitoring routines. Our study provides insights into adolescents’ and young adults’ lived experiences of using HCL systems and related technology to manage diabetes, and contributes to a more nuanced understanding of how the HCI community can support the contextualized management of diabetes through technology design.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {139},\nnumpages = {16},\nkeywords = {context-aware health technology, diabetes management, hybrid-closed loop systems, infrastructuring health practice},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642239,\nauthor = {Yan, Litao and Hwang, Alyssa and Wu, Zhiyuan and Head, Andrew},\ntitle = {Ivie: Lightweight Anchored Explanations of Just-Generated Code},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642239},\ndoi = {10.1145/3613904.3642239},\nabstract = {Programming assistants have reshaped the experience of programming into one where programmers spend less time writing and more time critically examining code. In this paper, we explore how programming assistants can be extended to accelerate the inspection of generated code. We introduce an extension to the programming assistant called Ivie, or instantly visible in-situ explanations. When using Ivie, a programmer’s generated code is instantly accompanied by explanations positioned just adjacent to the code. Our design was optimized for low-cost invocation and dismissal. Explanations are compact and informative. They describe meaningful expressions, from individual variables to entire blocks of code. We present an implementation of Ivie that forks VS Code, applying a modern LLM for timely segmentation and explanation of generated code. In a lab study, we compared Ivie to a contemporary baseline tool for code understanding. Ivie improved understanding of generated code, and was received by programmers as a highly useful, low distraction complement to the programming assistant.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {140},\nnumpages = {15},\nkeywords = {Programming assistants, anchored explanations, brevity, comprehension support, easy dismissal, easy invocation, instructive copilots, label overlays, variable levels of detail},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642377,\nauthor = {Chen, John and Lu, Xi and Du, Yuzhou and Rejtig, Michael and Bagley, Ruth and Horn, Mike and Wilensky, Uri},\ntitle = {Learning Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT \\& NetLogo Chat},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642377},\ndoi = {10.1145/3613904.3642377},\nabstract = {Large Language Models (LLMs) have the potential to fundamentally change the way people engage in computer programming. Agent-based modeling (ABM) has become ubiquitous in natural and social sciences and education, yet no prior studies have explored the potential of LLMs to assist it. We designed NetLogo Chat to support the learning and practice of NetLogo, a programming language for ABM. To understand how users perceive, use, and need LLM-based interfaces, we interviewed 30 participants from global academia, industry, and graduate schools. Experts reported more perceived benefits than novices and were more inclined to adopt LLMs in their workflow. We found significant differences between experts and novices in their perceptions, behaviors, and needs for human-AI collaboration. We surfaced a knowledge gap between experts and novices as a possible reason for the benefit gap. We identified guidance, personalization, and integration as major needs for LLM-based interfaces to support the programming of ABM.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {141},\nnumpages = {18},\nkeywords = {Agent-based Modeling, ChatGPT, LLM Companion, Learning with LLMs, NetLogo Chat, Programming Assistant},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641936,\nauthor = {Mozannar, Hussein and Bansal, Gagan and Fourney, Adam and Horvitz, Eric},\ntitle = {Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641936},\ndoi = {10.1145/3613904.3641936},\nabstract = {Code-recommendation systems, such as Copilot and CodeWhisperer, have the potential to improve programmer productivity by suggesting and auto-completing code. However, to fully realize their potential, we must understand how programmers interact with these systems and identify ways to improve that interaction. To seek insights about human-AI collaboration with code recommendations systems, we studied GitHub Copilot, a code-recommendation system used by millions of programmers daily. We developed CUPS, a taxonomy of common programmer activities when interacting with Copilot. Our study of 21 programmers, who completed coding tasks and retrospectively labeled their sessions with CUPS, showed that CUPS can help us understand how programmers interact with code-recommendation systems, revealing inefficiencies and time costs. Our insights reveal how programmers interact with Copilot and motivate new interface designs and metrics.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {142},\nnumpages = {16},\nkeywords = {AI-assisted Programming, Copilot, User State Model},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642495,\nauthor = {Ferdowsi, Kasra and Huang, Ruanqianqian (Lisa) and James, Michael B. and Polikarpova, Nadia and Lerner, Sorin},\ntitle = {Validating AI-Generated Code with Live Programming},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642495},\ndoi = {10.1145/3613904.3642495},\nabstract = {AI-powered programming assistants are increasingly gaining popularity, with GitHub Copilot alone used by over a million developers worldwide. These tools are far from perfect, however, producing code suggestions that may be incorrect in subtle ways. As a result, developers face a new challenge: validating AI’s suggestions. This paper explores whether Live Programming (LP), a continuous display of a program’s runtime values, can help address this challenge. To answer this question, we built a Python editor that combines an AI-powered programming assistant with an existing LP environment. Using this environment in a between-subjects study (N = 17), we found that by lowering the cost of validation by execution, LP can mitigate over- and under-reliance on AI-generated programs and reduce the cognitive load of validation for certain types of tasks.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {143},\nnumpages = {8},\nkeywords = {AI Assistants, Live Programming},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642903,\nauthor = {Hong, Matt-Heun and Sunberg, Zachary Nolan and Szafir, Danielle Albers},\ntitle = {Cieran: Designing Sequential Colormaps via In-Situ Active Preference Learning},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642903},\ndoi = {10.1145/3613904.3642903},\nabstract = {Quality colormaps can help communicate important data patterns. However, finding an aesthetically pleasing colormap that looks “just right” for a given scenario requires significant design and technical expertise. We introduce Cieran, a tool that allows any data analyst to rapidly find quality colormaps while designing charts within Jupyter Notebooks. Our system employs an active preference learning paradigm to rank expert-designed colormaps and create new ones from pairwise comparisons, allowing analysts who are novices in color design to tailor colormaps to their data context. We accomplish this by treating colormap design as a path planning problem through the CIELAB colorspace with a context-specific reward model. In an evaluation with twelve scientists, we found that Cieran effectively modeled user preferences to rank colormaps and leveraged this model to create new quality designs. Our work shows the potential of active preference learning for supporting efficient visualization design optimization.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {144},\nnumpages = {15},\nkeywords = {colormaps, design optimization, preference learning, visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642265,\nauthor = {Salvi, Amey and Lu, Kecheng and Papka, Michael E. and Wang, Yunhai and Reda, Khairi},\ntitle = {Color Maker: a Mixed-Initiative Approach to Creating Accessible Color Maps},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642265},\ndoi = {10.1145/3613904.3642265},\nabstract = {Quantitative data is frequently represented using color, yet designing effective color mappings is a challenging task, requiring one to balance perceptual standards with personal color preference. Current design tools either overwhelm novices with complexity or offer limited customization options. We present ColorMaker, a mixed-initiative approach for creating colormaps. ColorMaker combines fluid user interaction with real-time optimization to generate smooth, continuous color ramps. Users specify their loose color preferences while leaving the algorithm to generate precise color sequences, meeting both designer needs and established guidelines. ColorMaker can create new colormaps, including designs accessible for people with color-vision deficiencies, starting from scratch or with only partial input, thus supporting ideation and iterative refinement. We show that our approach can generate designs with similar or superior perceptual characteristics to standard colormaps. A user study demonstrates how designers of varying skill levels can use this tool to create custom, high-quality colormaps. ColorMaker is available at: colormaker.org},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {145},\nnumpages = {17},\nkeywords = {Mixed-initiative systems, color design, colormaps, simulated annealing.},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642223,\nauthor = {Shi, Xinyu and Liu, Mingyu and Zhou, Ziqi and Neshati, Ali and Rossi, Ryan and Zhao, Jian},\ntitle = {Exploring Interactive Color Palettes for Abstraction-Driven Exploratory Image Colorization},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642223},\ndoi = {10.1145/3613904.3642223},\nabstract = {Color design is essential in areas such as product, graphic, and fashion design. However, current tools like Photoshop, with their concrete-driven color manipulation approach, often stumble during early ideation, favoring polished end results over initial exploration. We introduced Mondrian as a test-bed for abstraction-driven approach using interactive color palettes for image colorization. Through a formative study with six design experts, we selected three design options for visual abstractions in color design and developed Mondrian where humans work with abstractions and AI manages the concrete aspects. We carried out a user study to understand the benefits and challenges of each abstraction format and compare the Mondrian with Photoshop. A survey involving 100 participants further examined the influence of each abstraction format on color composition perceptions. Findings suggest that interactive visual abstractions encourage a non-linear exploration workflow and an open mindset during ideation, thus providing better creative affordance.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {146},\nnumpages = {16},\nkeywords = {Color palettes, abstraction., creative design, image colorization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641976,\nauthor = {Hegemann, Lena and Oulasvirta, Antti},\ntitle = {Palette, Purpose, Prototype: The Three Ps of Color Design and How Designers Navigate Them},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641976},\ndoi = {10.1145/3613904.3641976},\nabstract = {This paper contributes to understanding of a fundamental process in design: choosing colors. While much has been written on color theory and about general design processes, understanding of designers’ actual color-design practice and experiences remains patchy. To address this gap, this paper presents qualitative findings from an interview-based study with 12 designers and, on their basis, a conceptual framework of three interlinked color design spaces: purpose, palette, and prototype. Respectively, these represent a meaning the colors should deliver, a proposed set of colors fitting this purpose, and a possible allocation of these colors to a candidate design. Through a detailed report on how designers iteratively navigate these spaces, the findings offer a rich account of color-design practice and point to possible design benefits from computational toolsthat integrate considerations of all three.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {147},\nnumpages = {19},\nkeywords = {Color design, color choice, design practice, interview studies, qualitative methods},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642711,\nauthor = {Shi, Xinyu and Wang, Yinghou and Wang, Yun and Zhao, Jian},\ntitle = {Piet: Facilitating Color Authoring for Motion Graphics Video},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642711},\ndoi = {10.1145/3613904.3642711},\nabstract = {Motion graphic (MG) videos are effective and compelling for presenting complex concepts through animated visuals; and colors are important to convey desired emotions, maintain visual continuity, and signal narrative transitions. However, current video color authoring workflows are fragmented, lacking contextual previews, hindering rapid theme adjustments, and not aligning with designers’ progressive authoring flows. To bridge this gap, we introduce Piet, the first tool tailored for MG video color authoring. Piet features an interactive palette to visually represent color distributions, support controllable focus levels, and enable quick theme probing via grouped color shifts. We interviewed 6 domain experts to identify the frustrations in current tools and inform the design of Piet. An in-lab user study with 13 expert designers showed that Piet effectively simplified the MG video color authoring and reduced the friction in creative color theme exploration.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {148},\nnumpages = {17},\nkeywords = {Motion graphics, color authoring, creative design., interactive color palettes},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642917,\nauthor = {Kim, Hanseob and Han, Bin and Kim, Jieun and Lubis, Muhammad Firdaus Syawaludin and Kim, Gerard Jounghyun and Hwang, Jae-In},\ntitle = {Engaged and Affective Virtual Agents: Their Impact on Social Presence, Trustworthiness, and Decision-Making in the Group Discussion},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642917},\ndoi = {10.1145/3613904.3642917},\nabstract = {This study investigates how different virtual agent (VA) behaviors influence subjects’ perceptions and group decision-making. Participants carried out experimental group discussions with a VA exhibiting varying levels of engagement and affective behavior. Engagement refers to the VA’s focus on the group task, whereas affective behavior reflects the VA’s emotional state. The findings revealed that VA’s engagements effectively captured participants’ attention even in the group setting and enhanced group synergy, thereby facilitating more in-depth discussion and producing better consensus. On the other hand, VA’s affective behavior negatively affected the perceived social presence and trustworthiness. Consequently, in the context of group discussion, participants preferred the engaged and non-affective VA to the non-engaged and affective VA. The study provides valuable insights for improving the VA’s behavioral design as a team member for collaborative tasks.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {149},\nnumpages = {17},\nkeywords = {Affective computing, Emotional suppression, Engagement, Group decision-making., Virtual agent},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642544,\nauthor = {Kwak, Daehyun and Park, Soobin and Cha, Inha and Kim, Hankyung and Lim, Youn-Kyung},\ntitle = {Investigating the Potential of Group Recommendation Systems As a Medium of Social Interactions: A Case of Spotify Blend Experiences between Two Users},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642544},\ndoi = {10.1145/3613904.3642544},\nabstract = {Designing user experiences for group recommendation systems (GRS) is challenging, requiring a nuanced understanding of the influence of social interactions between users. Using Spotify Blend as a real-world case of music GRS, we conducted empirical studies to investigate intricate social interactions among South Korean users in GRS. Through a preliminary survey about Blend experiences in general, we narrowed the focus for the main study to relationships between two users who are acquainted or close. Building on this, we conducted a 21-day diary study and interviews with 30 participants (15 pairs) to probe more in-depth interpersonal dynamics within Blend. Our findings reveal that users engaged in implicit social interactions, including tacit understanding of their companions and indirect communication. We conclude by discussing the newly discovered value of GRS as a social catalyst, along with design attributes and challenges for the social experiences it mediates.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {150},\nnumpages = {15},\nkeywords = {group recommendation systems (GRS), social dynamics, technology-mediated social interaction, user experience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642241,\nauthor = {Soliman, Nouran and Kang, Hyeonsu B and Latzke, Matthew and Bragg, Jonathan and Chang, Joseph Chee and Zhang, Amy Xian and Karger, David R},\ntitle = {Mitigating Barriers to Public Social Interaction with Meronymous Communication},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642241},\ndoi = {10.1145/3613904.3642241},\nabstract = {In communities with social hierarchies, fear of judgment can discourage communication. While anonymity may alleviate some social pressure, fully anonymous spaces enable toxic behavior and hide the social context that motivates people to participate and helps them tailor their communication. We explore a design space of meronymous communication, where people can reveal carefully chosen aspects of their identity and also leverage trusted endorsers to gain credibility. We implemented these ideas in a system for scholars to meronymously seek and receive paper recommendations on Twitter and Mastodon. A formative study with 20 scholars confirmed that scholars see benefits to participating but are deterred due to social anxiety. From a month-long public deployment, we found that with meronymity, junior scholars could comfortably ask “newbie” questions and get responses from senior scholars who they normally found intimidating. Responses were also tailored to the aspects about themselves that junior scholars chose to reveal.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {151},\nnumpages = {26},\nkeywords = {Identity, Online Communities, Online Safety, Partial Anonymity, Q Social Recommendation, Self-Disclosure, Self-Presentation, Social Media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642427,\nauthor = {Schneiders, Eike and Fourie, Christopher and Celestin, Stanley and Shah, Julie and Jung, Malte},\ntitle = {Understanding Entrainment in Human Groups: Optimising Human-Robot Collaboration from Lessons Learned during Human-Human Collaboration},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642427},\ndoi = {10.1145/3613904.3642427},\nabstract = {Successful entrainment during collaboration positively affects trust, willingness to collaborate, and likeability towards collaborators. In this paper, we present a mixed-method study to investigate characteristics of successful entrainment leading to pair and group-based synchronisation. Drawing inspiration from industrial settings, we designed a fast-paced, short-cycle repetitive task. Using motion tracking, we investigated entrainment in both dyadic and triadic task completion. Furthermore, we utilise audio-video recordings and semi-structured interviews to contextualise participants’ experiences. This paper contributes to the Human-Computer/Robot Interaction (HCI/HRI) literature using a human-centred approach to identify characteristics of entrainment during pair- and group-based collaboration. We present five characteristics related to successful entrainment. These are related to the occurrence of entrainment, leader-follower patterns, interpersonal communication, the importance of the point-of-assembly, and the value of acoustic feedback. Finally, we present three design considerations for future research and design on collaboration with robots.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {152},\nnumpages = {13},\nkeywords = {collaboration in groups, entrainment in dyads and triads, non-dyadic human-robot interaction, temporal synchronisation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642739,\nauthor = {Newendorp, Amanda K. and Sanaei, Mohammadamin and Perron, Arthur J and Sabouni, Hila and Javadpour, Nikoo and Sells, Maddie and Nelson, Katherine and Dorneich, Michael and Gilbert, Stephen B.},\ntitle = {Apple's Knowledge Navigator: Why Doesn't that Conversational Agent Exist Yet?},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642739},\ndoi = {10.1145/3613904.3642739},\nabstract = {Apple's 1987 Knowledge Navigator video contains a vision of a sophisticated digital personal assistant, but the natural human-agent conversational dialog shown does not currently exist. To investigate why, the authors analyzed the video using three theoretical frameworks: the DiCoT framework, the HAT Game Analysis framework, and the Flows of Power framework. These were used to codify the human-agent interactions and classify the agent's capabilities. While some barriers to creating such agents are technological, other barriers arise from privacy, social and situational factors, trust, and the financial business case. The social roles and asymmetric interactions of the human and agent are discussed in the broader context of HAT research, along with the need for a new term for these agents that does not rely on a human social relationship metaphor. This research offers designers of conversational agents a research roadmap to build more highly capable and trusted non-human teammates.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {153},\nnumpages = {14},\nkeywords = {conversational agent, human-agent teaming (HAT), natural language interface, shared context},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642007,\nauthor = {Hoque, Md Naimul and Mahfuz, Ayman A and Kindi, Mayukha Sridhatri and Hassan, Naeemul},\ntitle = {Towards Designing a Question-Answering Chatbot for Online News: Understanding Questions and Perspectives},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642007},\ndoi = {10.1145/3613904.3642007},\nabstract = {Large Language Models (LLMs) have created opportunities for designing chatbots that can support complex question-answering (QA) scenarios and improve news audience engagement. However, we still lack an understanding of what roles journalists and readers deem fit for such a chatbot in newsrooms. To address this gap, we first interviewed six journalists to understand how they answer questions from readers currently and how they want to use a QA chatbot for this purpose. To understand how readers want to interact with a QA chatbot, we then conducted an online experiment (N=124) where we asked each participant to read three news articles and ask questions to either the author(s) of the articles or a chatbot. By combining results from the studies, we present alignments and discrepancies between how journalists and readers want to use QA chatbots and propose a framework for designing effective QA chatbots in newsrooms.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {154},\nnumpages = {17},\nkeywords = {LLMs, Online news, chatbots, question-answering},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642840,\nauthor = {Liu, Zihan and Li, Han and Chen, Anfan and Zhang, Renwen and Lee, Yi-Chieh},\ntitle = {Understanding Public Perceptions of AI Conversational Agents: A Cross-Cultural Analysis},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642840},\ndoi = {10.1145/3613904.3642840},\nabstract = {Conversational Agents (CAs) have increasingly been integrated into everyday life, sparking significant discussions on social media. While previous research has examined public perceptions of AI in general, there is a notable lack in research focused on CAs, with fewer investigations into cultural variations in CA perceptions. To address this gap, this study used computational methods to analyze about one million social media discussions surrounding CAs and compared people’s discourses and perceptions of CAs in the US and China. We find Chinese participants tended to view CAs hedonically, perceived voice-based and physically embodied CAs as warmer and more competent, and generally expressed positive emotions. In contrat, US participants saw CAs more functionally, with an ambivalent attitude. Warm perception was a key driver of positive emotions toward CAs in both countries. We discussed practical implications for designing contextually sensitive and user-centric CAs to resonate with various users’ preferences and needs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {155},\nnumpages = {17},\nkeywords = {Conversational agents, Cultural differences, Public perceptions, Topic modeling, Twitter, Weibo, Word embedding},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642385,\nauthor = {Zhang, Zhiping and Jia, Michelle and Lee, Hao-Ping (Hank) and Yao, Bingsheng and Das, Sauvik and Lerner, Ada and Wang, Dakuo and Li, Tianshi},\ntitle = {“It's a Fair Game”, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642385},\ndoi = {10.1145/3613904.3642385},\nabstract = {The widespread use of Large Language Model (LLM)-based conversational agents (CAs), especially in high-stakes domains, raises many privacy concerns. Building ethical LLM-based CAs that respect user privacy requires an in-depth understanding of the privacy risks that concern users the most. However, existing research, primarily model-centered, does not provide insight into users’ perspectives. To bridge this gap, we analyzed sensitive disclosures in real-world ChatGPT conversations and conducted semi-structured interviews with 19 LLM-based CA users. We found that users are constantly faced with trade-offs between privacy, utility, and convenience when using LLM-based CAs. However, users’ erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks. Additionally, the human-like interactions encouraged more sensitive disclosures, which complicated users’ ability to navigate the trade-offs. We discuss practical design guidelines and the needs for paradigm shifts to protect the privacy of LLM-based CA users.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {156},\nnumpages = {26},\nkeywords = {Artificial general intelligence (AGI), Chatbots, Contextual integrity, Conversational agents, Empirical studies, Interviews, Large language models (LLM), Privacy, Privacy risks, Privacy-enhancing technologies},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642129,\nauthor = {Fan, Xianzhe and Wu, Zihan and Yu, Chun and Rao, Fenggui and Shi, Weinan and Tu, Teng},\ntitle = {ContextCam: Bridging Context Awareness with Creative Human-AI Image Co-Creation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642129},\ndoi = {10.1145/3613904.3642129},\nabstract = {The rapid advancement of AI-generated content (AIGC) promises to transform various aspects of human life significantly. This work particularly focuses on the potential of AIGC to revolutionize image creation, such as photography and self-expression. We introduce ContextCam, a novel human-AI image co-creation system that integrates context awareness with mainstream AIGC technologies like Stable Diffusion. ContextCam provides user’s image creation process with inspiration by extracting relevant contextual data, and leverages Large Language Model-based (LLM) multi-agents to co-create images with the user. A study with 16 participants and 136 scenarios revealed that ContextCam was well-received, showcasing personalized and diverse outputs as well as interesting user behavior patterns. Participants provided positive feedback on their engagement and enjoyment when using ContextCam, and acknowledged its ability to inspire creativity.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {157},\nnumpages = {17},\nkeywords = {Context-Aware Systems, Human-AI Co-Creation, Image Generation and Editing, LLM-Based Multi-Agent Systems},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642128,\nauthor = {Yao, Zhihao and Sun, Qirui and Liu, Beituo and Lu, Yao and Liu, Guanhong and Yang, Xing-Dong and Mi, Haipeng},\ntitle = {InkBrush: A Sketching Tool for 3D Ink Painting},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642128},\ndoi = {10.1145/3613904.3642128},\nabstract = {InkBrush is a new sketch-based 3D drawing tool for creating 3D ink paintings using free-form 3D ink strokes. It offers a digital calligraphy brush and various editing tools to generate realistic ink-like brush strokes with attributes like hairy edges, ink drips, and scattered dots. Users can adjust parameters such as moisture, color, darkness, dryness, and stroke style to customize the appearance of the brush strokes. The development of InkBrush was guided by a design study involving artists and designers. It was developed as a plugin for Blender, a popular 3D modeling tool, and its effectiveness and usability were evaluated through a user study involving 75 participants. Preliminary feedback from the participants was overwhelmingly positive, indicating that InkBrush was intuitive and easy to use. Following this, we also sought in-depth assessments from experts in ink painting and 3D design. Their evaluations further demonstrated the effectiveness of InkBrush.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {158},\nnumpages = {15},\nkeywords = {Ink painting, authoring, procedural modeling, sketching},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642865,\nauthor = {Wang, Fengjie and Lin, Yanna and Yang, Leni and Li, Haotian and Gu, Mingyang and Zhu, Min and Qu, Huamin},\ntitle = {OutlineSpark: Igniting AI-powered Presentation Slides Creation from Computational Notebooks through Outlines},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642865},\ndoi = {10.1145/3613904.3642865},\nabstract = {Computational notebooks are widely utilized for exploration and analysis. However, creating slides to communicate analysis results from these notebooks is quite tedious and time-consuming. Researchers have proposed automatic systems for generating slides from notebooks, which, however, often do not consider the process of users conceiving and organizing their messages from massive code cells. Those systems ask users to go directly into the slide creation process, which causes potentially ill-structured slides and burdens in further refinement. Inspired by the common and widely recommended slide creation practice: drafting outlines first and then adding concrete content, we introduce OutlineSpark, an AI-powered slide creation tool that generates slides from a slide outline written by the user. The tool automatically retrieves relevant notebook cells based on the outlines and converts them into slide content. We evaluated OutlineSpark with 12 users. Both the quantitative and qualitative feedback from the participants verify its effectiveness and usability.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {159},\nnumpages = {16},\nkeywords = {computational notebooks, data science, outlines, slides generation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642858,\nauthor = {Almeda, Shm Garanganao and Zamfirescu-Pereira, J.D. and Kim, Kyu Won and Mani Rathnam, Pradeep and Hartmann, Bjoern},\ntitle = {Prompting for Discovery: Flexible Sense-Making for AI Art-Making with Dreamsheets},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642858},\ndoi = {10.1145/3613904.3642858},\nabstract = {Design space exploration (DSE) for Text-to-Image (TTI) models entails navigating a vast, opaque space of possible image outputs, through a commensurately vast input space of hyperparameters and prompt text. Perceptually small movements in prompt-space can surface unexpectedly disparate images. How can interfaces support end-users in reliably steering prompt-space explorations towards interesting results? Our design probe, DreamSheets, supports user-composed exploration strategies with LLM-assisted prompt construction and large-scale simultaneous display of generated results, hosted in a spreadsheet interface. Two studies, a preliminary lab study and an extended two-week study where five expert artists developed custom TTI sheet-systems, reveal various strategies for targeted TTI design space exploration—such as using templated text generation to define and layer semantic “axes” for exploration. We identified patterns in exploratory structures across our participants’ sheet-systems: configurable exploration “units” that we distill into a UI mockup, and generalizable UI components to guide future interfaces.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {160},\nnumpages = {17},\nkeywords = {design space exploration, generative AI, text to image},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642443,\nauthor = {Chen, Yuexi and Morariu, Vlad I and Truong, Anh and Liu, Zhicheng},\ntitle = {TutoAI: a cross-domain framework for AI-assisted mixed-media tutorial creation on physical tasks},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642443},\ndoi = {10.1145/3613904.3642443},\nabstract = {Mixed-media tutorials, which integrate videos, images, text, and diagrams to teach procedural skills, offer more browsable alternatives than timeline-based videos. However, manually creating such tutorials is tedious, and existing automated solutions are often restricted to a particular domain. While AI models hold promise, it is unclear how to effectively harness their powers, given the multi-modal data involved and the vast landscape of models. We present TutoAI, a cross-domain framework for AI-assisted mixed-media tutorial creation on physical tasks. First, we distill common tutorial components by surveying existing work; then, we present an approach to identify, assemble, and evaluate AI models for component extraction; finally, we propose guidelines for designing user interfaces (UI) that support tutorial creation based on AI-generated components. We show that TutoAI has achieved higher or similar quality compared to a baseline model in preliminary user studies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {161},\nnumpages = {17},\nkeywords = {AI-assisted creation, Human-AI interaction, mixed-media tutorials},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642173,\nauthor = {De, Ankolika and Lu, Zhicong},\ntitle = {#PoetsOfInstagram: Navigating The Practices And Challenges Of Novice Poets On Instagram},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642173},\ndoi = {10.1145/3613904.3642173},\nabstract = {Commencing as a photo-sharing platform, Instagram has since become multifaceted, accommodating diverse art forms, with poetry emerging as a prominent one. However, the academic understanding of Instagram’s poetry community is limited, yet its significance emerges from its distinctive utilization of a primarily visual social media platform guided by recommendation algorithms for disseminating poetry, further characterized by a predominantly novice creative population. We employ qualitative analysis to explore motivations, experiences, and algorithmic influence within Instagram’s poetry community. We demonstrate that participants prioritize conforming to algorithmic constraints for visibility, yet maintain their community’s values of integrity and originality, illustrating the tension between algorithmic growth and participant authenticity. We introduce the concept of Algorithmically Mediated Creative Labor, a phenomenon specific to non-monetizing creative users who are impacted by the prioritization of professional creators and continually adapt their creative endeavours to align with platform logic, thereby affecting their motivation and creative outputs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {162},\nnumpages = {16},\nkeywords = {algorithmic perception and theory, creative work, creative work., online communities, social media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642758,\nauthor = {Rodriguez, Richard and Sullivan, Brian T. and Barrera Machuca, Mayra Donaji and Batmaz, Anil Ufuk and Tornatzky, Cyane and Ortega, Francisco R.},\ntitle = {An Artists' Perspectives on Natural Interactions for Virtual Reality 3D Sketching},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642758},\ndoi = {10.1145/3613904.3642758},\nabstract = {Virtual Reality (VR) applications like OpenBrush offer artists access to 3D sketching tools within the digital 3D virtual space. These 3D sketching tools allow users to “paint” using virtual digital strokes that emulate real-world mark-making. Yet, users paint these strokes through (unimodal) VR controllers. Given that sketching in VR is a relatively nascent field, this paper investigates ways to expand our understanding of sketching in virtual space, taking full advantage of what an immersive digital canvas offers. Through a study conducted with the participation of artists, we identify potential methods for natural multimodal and unimodal interaction techniques in 3D sketching. These methods demonstrate ways to incrementally improve existing interaction techniques and incorporate artistic feedback into the design.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {163},\nnumpages = {20},\nkeywords = {3D Sketching, Gestures, Multimodal Interaction, Speech, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642575,\nauthor = {Jo, Hye-Young and Suzuki, Ryo and Kim, Yoonji},\ntitle = {CollageVis: Rapid Previsualization Tool for Indie Filmmaking using Video Collages},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642575},\ndoi = {10.1145/3613904.3642575},\nabstract = {Previsualization, previs, is essential for film production, allowing cinematographic experiments and effective collaboration. However, traditional previs methods like 2D storyboarding and 3D animation require substantial time, cost, and technical expertise, posing challenges for indie filmmakers. We introduce CollageVis, a rapid previsualization tool using video collages. CollageVis enables filmmakers to create previs through two main user interfaces. First, it automatically segments actors from videos and assigns roles using name tags, color filters, and face swaps. Second, it positions video layers on a virtual stage and allows users to record shots using mobile as a proxy for a virtual camera. These features were developed based on formative interviews by reflecting indie filmmakers’ needs and working methods. We demonstrate the system’s capability by replicating seven film scenes and evaluate the system’s usability with six indie filmmakers. The findings indicate that CollageVis allows more flexible yet expressive previs creation for idea development and collaboration.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {164},\nnumpages = {16},\nkeywords = {indie filmmaking, previsualization, storyboard},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642855,\nauthor = {Sivertsen, Christian and Salimbeni, Guido and L\\o{}vlie, Anders Sundnes and Benford, Steven David and Zhu, Jichen},\ntitle = {Machine Learning Processes As Sources of Ambiguity: Insights from AI Art},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642855},\ndoi = {10.1145/3613904.3642855},\nabstract = {Ongoing efforts to turn Machine Learning (ML) into a design material have encountered limited success. This paper examines the burgeoning area of AI art to understand how artists incorporate ML in their creative work. Drawing upon related HCI theories, we investigate how artists create ambiguity by analyzing nine AI artworks that use computer vision and image synthesis. Our analysis shows that, in addition to the established types of ambiguity, artists worked closely with the ML process (dataset curation, model training, and application) and developed various techniques to evoke the ambiguity of processes. Our finding indicates that the current conceptualization of ML as a design material needs to reframe the ML process as design elements, instead of technical details. Finally, this paper offers reflections on commonly held assumptions in HCI about ML uncertainty, dependability, and explainability, and advocates to supplement the artifact-centered design perspective of ML with a process-centered one.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {165},\nnumpages = {14},\nkeywords = {ambiguity, art, artificial intelligence, computer vision, generative art, machine learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642410,\nauthor = {Wan, Qian and Feng, Xin and Bei, Yining and Gao, Zhiqi and Lu, Zhicong},\ntitle = {Metamorpheus: Interactive, Affective, and Creative Dream Narration Through Metaphorical Visual Storytelling},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642410},\ndoi = {10.1145/3613904.3642410},\nabstract = {Human emotions are essentially molded by lived experiences, from which we construct personalised meaning. The engagement in such meaning-making process has been practiced as an intervention in various psychotherapies to promote wellness. Nevertheless, to support recollecting and recounting lived experiences in everyday life remains under explored in HCI. It also remains unknown how technologies such as generative AI models can facilitate the meaning making process, and ultimately support affective mindfulness. In this paper we present Metamorpheus, an affective interface that engages users in a creative visual storytelling of emotional experiences during dreams. Metamorpheus arranges the storyline based on a dream’s emotional arc, and provokes self-reflection through the creation of metaphorical images and text depictions. The system provides metaphor suggestions, and generates visual metaphors and text depictions using generative AI models, while users can apply generations to recolour and re-arrange the interface to be visually affective. Our experience-centred evaluation manifests that, by interacting with Metamorpheus, users can recall their dreams in vivid detail, through which they relive and reflect upon their experiences in a meaningful way.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {166},\nnumpages = {16},\nkeywords = {Affective Computing, Creativity, Experience-centred Design, Human-AI Interaction, Meaning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642908,\nauthor = {Davis, Richard Lee and Wambsganss, Thiemo and Jiang, Wei and Kim, Kevin Gonyop and K\\\"{a}ser, Tanja and Dillenbourg, Pierre},\ntitle = {Fashioning Creative Expertise with Generative AI: Graphical Interfaces for Design Space Exploration Better Support Ideation Than Text Prompts},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642908},\ndoi = {10.1145/3613904.3642908},\nabstract = {This paper investigates the potential impact of deep generative models on the work of creative professionals. We argue that current generative modeling tools lack critical features that would make them useful creativity support tools, and introduce our own tool, generative.fashion1, which was designed with theoretical principles of design space exploration in mind. Through qualitative studies with fashion design apprentices, we demonstrate how generative.fashion supported both divergent and convergent thinking, and compare it with a state-of-the-art text-based interface using Stable Diffusion. In general, the apprentices preferred generative.fashion, citing the features explicitly designed to support ideation. In two follow-up studies, we provide quantitative results that support and expand on these insights. We conclude that text-only prompts in existing models restrict creative exploration, especially for novices. Our work demonstrates that interfaces which are theoretically aligned with principles of design space exploration are essential for unlocking the full creative potential of generative AI.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {167},\nnumpages = {26},\nkeywords = {Convergent Thinking, Creativity, Creativity Support Tools (CSTs), Deep Generative Models, Design Space Exploration, Divergent Thinking, Fashion Design, Generative AI, Ideation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642824,\nauthor = {Huang, Rong and Lin, Haichuan and Chen, Chuanzhang and Zhang, Kang and Zeng, Wei},\ntitle = {PlantoGraphy: Incorporating Iterative Design Process into Generative Artificial Intelligence for Landscape Rendering},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642824},\ndoi = {10.1145/3613904.3642824},\nabstract = {Landscape renderings are realistic images of landscape sites, allowing stakeholders to perceive better and evaluate design ideas. While recent advances in Generative Artificial Intelligence (GAI (generative artificial intelligence)) enable automated generation of landscape renderings, the End to End (endtoend) methods are not compatible with common design processes, leading to insufficient alignment with design idealizations and limited cohesion of iterative landscape design. Informed by a formative study for comprehending design requirements, we present PlantoGraphy, an iterative design system that allows for interactive configuration of generative artificial intelligence models to accommodate human-centered design practice. A two-stage pipeline is incorporated: first, the concretization module transforms conceptual ideas into concrete scene layouts with a domain-oriented large language model; and second, the illustration module converts scene layouts into realistic landscape renderings with a layout-guided diffusion model Fine-tune (finetune)ed through Low-Rank Adaptation (LoRA) (lora). PlantoGraphy has undergone a series of performance evaluations and user studies, demonstrating its effectiveness in landscape rendering generation and the high recognition of its interactive functionality.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {168},\nnumpages = {19},\nkeywords = {Landscape rendering, generative artificial intelligence, large language model, scene graph},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642868,\nauthor = {Wang, Sitong and Menon, Samia and Long, Tao and Henderson, Keren and Li, Dingzeyu and Crowston, Kevin and Hansen, Mark and Nickerson, Jeffrey V and Chilton, Lydia B},\ntitle = {ReelFramer: Human-AI Co-Creation for News-to-Video Translation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642868},\ndoi = {10.1145/3613904.3642868},\nabstract = {Short videos on social media are the dominant way young people consume content. News outlets aim to reach audiences through news reels—short videos conveying news—but struggle to translate traditional journalistic formats into short, entertaining videos. To translate news into social media reels, we support journalists in reframing the narrative. In literature, narrative framing is a high-level structure that shapes the overall presentation of a story. We identified three narrative framings for reels that adapt social media norms but preserve news value, each with a different balance of information and entertainment. We introduce ReelFramer, a human-AI co-creative system that helps journalists translate print articles into scripts and storyboards. ReelFramer supports exploring multiple narrative framings to find one appropriate to the story. AI suggests foundational narrative details, including characters, plot, setting, and key information. ReelFramer also supports visual framing; AI suggests character and visual detail designs before generating a full storyboard. Our studies show that narrative framing introduces the necessary diversity to translate various articles into reels, and establishing foundational details helps generate scripts that are more relevant and coherent. We also discuss the benefits of using narrative framing and foundational details in content retargeting.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {169},\nnumpages = {20},\nkeywords = {creativity support tools, generative AI, narratives, scriptwriting, short videos, storyboarding},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642812,\nauthor = {Zhou, Jiayi and Li, Renzhong and Tang, Junxiu and Tang, Tan and Li, Haotian and Cui, Weiwei and Wu, Yingcai},\ntitle = {Understanding Nonlinear Collaboration between Human and AI Agents: A Co-design Framework for Creative Design},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642812},\ndoi = {10.1145/3613904.3642812},\nabstract = {Creative design is a nonlinear process where designers generate diverse ideas in the pursuit of an open-ended goal and converge towards consensus through iterative remixing. In contrast, AI-powered design tools often employ a linear sequence of incremental and precise instructions to approximate design objectives. Such operations violate customary creative design practices and thus hinder AI agents’ ability to complete creative design tasks. To explore better human-AI co-design tools, we first summarize human designers’ practices through a formative study with 12 design experts. Taking graphic design as a representative scenario, we formulate a nonlinear human-AI co-design framework and develop a proof-of-concept prototype, OptiMuse. We evaluate OptiMuse and validate the nonlinear framework through a comparative study. We notice a subconscious change in people’s attitudes towards AI agents, shifting from perceiving them as mere executors to regarding them as opinionated colleagues. This shift effectively fostered the exploration and reflection processes of individual designers.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {170},\nnumpages = {16},\nkeywords = {Creative Design, Creativity Support Tool, Human-AI Co-creativity},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642476,\nauthor = {Kim, Jini and Kim, Hajun},\ntitle = {Unlocking Creator-AI Synergy: Challenges, Requirements, and Design Opportunities in AI-Powered Short-Form Video Production},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642476},\ndoi = {10.1145/3613904.3642476},\nabstract = {The emergence of AI-Powered Short-Form Video Generators (ASVG) has showcased the potential to streamline production time and foster creative ideas. Despite their widespread adoption, research has underexplored ASVG, especially from creators’ perspectives. To evaluate the role of ASVG as creator-centered collaborators, we conducted mixed-method research: (1) interviews (N = 17) and (2) a participatory design workshop (N = 12) with short-form video creators. In our interviews, we investigated creators’ production process and challenges in creating short-form videos. In participatory workshops, short-form video creators envisioned AI-powered video tools, addressing their requirements and AI collaboration perceptions. Our findings indicate ASVGs can provide various advantages including inspiration, swift access to video sources, and automated highlight generation. To put things in perspective, we also underscore concerns arising from AI collaboration, including potential creator identity dilution, reduced creative output, and information bubble. We also discuss design considerations when designing ASVG to retain their creative values.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {171},\nnumpages = {23},\nkeywords = {Algorithmic experience, Creative economy, Creators, Human-AI Collaboration, Participatory design, Short-form video generation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642224,\nauthor = {Hou, Yihan and Yang, Manling and Cui, Hao and Wang, Lei and Xu, Jie and Zeng, Wei},\ntitle = {C2Ideas: Supporting Creative Interior Color Design Ideation with a Large Language Model},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642224},\ndoi = {10.1145/3613904.3642224},\nabstract = {Interior color design is a creative process that endeavors to allocate colors to furniture and other elements within an interior space. While much research focuses on generating realistic interior designs, these automated approaches often misalign with user intention and disregard design rationales. Informed by a need-finding preliminary study, we develop C2Ideas, an innovative system for designers to creatively ideate color schemes enabled by an intent-aligned and domain-oriented large language model. C2Ideas integrates a three-stage process: Idea Prompting stage distills user intentions into color linguistic prompts; Word-Color Association stage transforms the prompts into semantically and stylistically coherent color schemes; and Interior Coloring stage assigns colors to interior elements complying with design principles. We also develop an interactive interface that enables flexible user refinement and interpretable reasoning. C2Ideas has undergone a series of indoor cases and user studies, demonstrating its effectiveness and high recognition of interactive functionality by designers.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {172},\nnumpages = {18},\nkeywords = {color deisgn, large language model},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641889,\nauthor = {Boucher, Josiah D and Smith, Gillian and Telliel, Yunus Do\\u{g}an},\ntitle = {Is Resistance Futile?: Early Career Game Developers, Generative AI, and Ethical Skepticism},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641889},\ndoi = {10.1145/3613904.3641889},\nabstract = {This paper presents a study that examines developer perceptions and usage of generative AI (GAI) in a summer professional development program for game development interns focused on mobile game design. GAI applications are in common usage worldwide, yet the impacts of this technology in game development remain relatively underexplored. Through a qualitative study using ethnographic interviews and participatory observation, this paper explores how GAI impacted the workflows, creative processes, and professional identities of early career game developers. We present a case of GAI integration that was not a straightforward adoption. Focusing on the interns’ resistance, negotiation, and reimagining, we show that the interns were actively developing a new professional culture both with and against generative AI. For the interns, their ethical commitments to fellow game developers and the future of their profession were as important as their practical concerns about usability, utility, and efficacy of GAI tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {173},\nnumpages = {13},\nkeywords = {Creativity Support, Future of GAI, Games/Play, Generative AI, Professional Communities, Programming/Development Support, Qualitative Methods},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642440,\nauthor = {Oh, Jeongseok and Kim, Seungju and Kim, Seungjun},\ntitle = {LumiMood: A Creativity Support Tool for Designing the Mood of a 3D Scene},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642440},\ndoi = {10.1145/3613904.3642440},\nabstract = {The aesthetic design of 3D scenes in game content enhances players’ experience by inducing desired emotions. Creating emotionally engaging scenes involves designing low-level features, such as color distribution, contrast, and brightness. This study presents LumiMood, an AI-driven creativity support tool (CST) that automatically adjusts lighting and post-processing to create moods for 3D scenes. LumiMood supports designers by synthesizing reference images, creating mood templates, and providing intermediate design steps. Our formative study with 10 designers identified distinct challenges in mood design based on the participants’ experience levels. A user study involving 40 designers revealed that using LumiMood benefits the designers by streamlining workflow, improving precision, and increasing mood intention accuracy. Results indicate that LumiMood supports clarifying mood concepts and improves interpretation of lighting and post-processing, thus resolving the challenges. We observe the effect of template based designing and discuss considerable factors for AI-driven CSTs for users with varying levels of experiences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {174},\nnumpages = {21},\nkeywords = {affective computing, artificial intelligence, creativity support tool, graphics design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642185,\nauthor = {Xiao, Shishi and Wang, Liangwei and Ma, Xiaojuan and Zeng, Wei},\ntitle = {TypeDance: Creating Semantic Typographic Logos from Image through Personalized Generation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642185},\ndoi = {10.1145/3613904.3642185},\nabstract = {Semantic typographic logos harmoniously blend typeface and imagery to represent semantic concepts while maintaining legibility. Conventional methods using spatial composition and shape substitution are hindered by the conflicting requirement for achieving seamless spatial fusion between geometrically dissimilar typefaces and semantics. While recent advances made AI generation of semantic typography possible, the end-to-end approaches exclude designer involvement and disregard personalized design. This paper presents TypeDance, an AI-assisted tool incorporating design rationales with the generative model for personalized semantic typographic logo design. It leverages combinable design priors extracted from uploaded image exemplars and supports type-imagery mapping at various structural granularity, achieving diverse aesthetic designs with flexible control. Additionally, we instantiate a comprehensive design workflow in TypeDance, including ideation, selection, generation, evaluation, and iteration. A two-task user evaluation, including imitation and creation, confirmed the usability of TypeDance in design across different usage scenarios.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {175},\nnumpages = {18},\nkeywords = {generative model, personalized design, semantic typography},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642133,\nauthor = {Han, Yuanning and Qiu, Ziyi and Cheng, Jiale and LC, RAY},\ntitle = {When Teams Embrace AI: Human Collaboration Strategies in Generative Prompting in a Creative Design Task},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642133},\ndoi = {10.1145/3613904.3642133},\nabstract = {Studies of Generative AI (GenAI)-assisted creative workflows have focused on individuals overcoming challenges of prompting to produce what they envisioned. When designers work in teams, how do collaboration and prompting influence each other, and how do users perceive generative AI and their collaborators during the co-prompting process? We engaged students with design or performance backgrounds, and little exposure to GenAI, to work in pairs with GenAI to create stage designs based on a creative theme. We found two patterns of collaborative prompting focused on generating story descriptions first, or visual imagery first. GenAI tools helped participants build consensus in the task, and allowed for discussion of the prompting strategies. Participants perceived GenAI as efficient tools rather than true collaborators, suggesting that human partners reduced the reliance on their use. This work highlights the importance of human-human collaboration when working with GenAI tools, suggesting systems that take advantage of shared human expertise in the prompting process.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {176},\nnumpages = {14},\nkeywords = {GenAI engineering, creative co-design, human-AI collaboration, team-work in prompting},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642792,\nauthor = {Fang, Cathy Mengying and Huang, Lingdong and Kuang, Quincy and Lieberman, Zach and Maes, Pattie and Ishii, Hiroshi},\ntitle = {An Accessible, Three-Axis Plotter for Enhancing Calligraphy Learning through Generated Motion},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642792},\ndoi = {10.1145/3613904.3642792},\nabstract = {Learning a motor skill is essential for many aspects of our lives. The complexity of some of these activities makes it hard for novices to understand through observation. Calligraphy writing is one such artistic practice where learners compare the visual differences between their writing and expert manuscripts and adjust until they have achieved similar results. We propose an accessible plotter-based system that guides the learner’s arm and hand in three directions with an actuated brush. It converts static Chinese calligraphy manuscripts to G-code that reproduces the calligrapher’s movement. Through a user study with twelve novice calligraphy learners, we validated the efficacy of our system as a learning tool that allows novices to gain an intuition of nuanced skills such as depth variation more effectively compared to watching a video recording of the same movement.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {177},\nnumpages = {11},\nkeywords = {Calligraphy, Haptics, Kinesthetic learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641947,\nauthor = {Turkmen, Rumeysa and Gelmez, Zeynep Ecem and Batmaz, Anil Ufuk and Stuerzlinger, Wolfgang and Asente, Paul and Sarac, Mine and Pfeuffer, Ken and Barrera Machuca, Mayra Donaji},\ntitle = {EyeGuide \\& EyeConGuide: Gaze-based Visual Guides to Improve 3D Sketching Systems},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641947},\ndoi = {10.1145/3613904.3641947},\nabstract = {Visual guides help to align strokes and raise accuracy in Virtual Reality (VR) sketching tools. Automatic guides that appear at relevant sketching areas are convenient to have for a seamless sketching with a guide. We explore guides that exploit eye-tracking to render them adaptive to the user’s visual attention. EyeGuide and EyeConGuide cause visual grid fragments to appear spatially close to the user’s intended sketches, based on the information of the user’s eye-gaze direction and the 3D position of the hand. Here we evaluated the techniques in two user studies across simple and complex sketching objectives in VR. The results show that gaze-based guides have a positive effect on sketching accuracy, perceived usability and preference over manual activation in the tested tasks. Our research contributes to integrating gaze-contingent techniques for assistive guides and presents important insights into multimodal design applications in VR.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {178},\nnumpages = {14},\nkeywords = {3D Sketching, 3D User Interface, Eye-Gaze, VR},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642653,\nauthor = {Chan, Joel and Ding, Zijian and Kamrah, Eesh and Fuge, Mark},\ntitle = {Formulating or Fixating: Effects of Examples on Problem Solving Vary as a Function of Example Presentation Interface Design},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642653},\ndoi = {10.1145/3613904.3642653},\nabstract = {Interactive systems that facilitate exposure to examples can augment problem solving performance. However designers of such systems are often faced with many practical design decisions about how users will interact with examples, with little clear theoretical guidance. To understand how example interaction design choices affect whether/how people benefit from examples, we conducted an experiment where 182 participants worked on a controlled analog to an exploratory creativity task, with access to examples of varying diversity and presentation interfaces. Task performance was worse when examples were presented in a list, compared to contextualized in the exploration space or shown in a dropdown list. Example lists were associated with more fixation, whereas contextualized examples were associated with using examples to formulate a model of the problem space to guide exploration. We discuss implications of these results for a theoretical framework that maps design choices to fundamental psychological mechanisms of creative inspiration from examples.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {179},\nnumpages = {16},\nkeywords = {Creativity, Examples, Interface, Problem Solving},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642847,\nauthor = {Son, Kihoon and Choi, DaEun and Kim, Tae Soo and Kim, Young-Ho and Kim, Juho},\ntitle = {GenQuery: Supporting Expressive Visual Search with Generative Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642847},\ndoi = {10.1145/3613904.3642847},\nabstract = {Designers rely on visual search to explore and develop ideas in early design stages. However, designers can struggle to identify suitable text queries to initiate a search or to discover images for similarity-based search that can adequately express their intent. We propose  GenQuery, a novel system that integrates generative models into the visual search process. GenQuery can automatically elaborate on users’ queries and surface concrete search directions when users only have abstract ideas. To support precise expression of search intents, the system enables users to generatively modify images and use these in similarity-based search. In a comparative user study (N=16), designers felt that they could more accurately express their intents and find more satisfactory outcomes with  GenQuery compared to a tool without generative features. Furthermore, the unpredictability of generations allowed participants to uncover more diverse outcomes. By supporting both convergence and divergence, GenQuery led to a more creative experience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {180},\nnumpages = {19},\nkeywords = {Creativity Support, Generative Model, Generative Search, Search Intent Expression, Visual Exploration, Visual Search},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642498,\nauthor = {Shi, Lei and Campbell, Rachel and Chi, Peggy and Cirimele, Maria and Cleron, Mike and Climer, Kirsten and Fleming, Chelsey Q and Ganti, Ashwin and Gervais, Philippe and Gonnet, Pedro and Karim, Tayeb A and Maksai, Andrii and Melancon, Chris and Mickle, Rob and Musat, Claudiu and Nandy, Palash and Qu, Xiaoyu Iris and Robishaw, David and Singh, Angad and Venkatesan, Mathangi},\ntitle = {Inkeraction: An Interaction Modality Powered by Ink Recognition and Synthesis},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642498},\ndoi = {10.1145/3613904.3642498},\nabstract = {Ink is a powerful medium for note-taking and creativity tasks. Multi-touch devices and stylus input have enabled digital ink to be editable and searchable. To extend the capabilities of digital ink, we introduce Inkeraction, an interaction modality powered by ink recognition and synthesis. Inkeraction segments and classifies digital ink objects (e.g., handwriting and sketches), identifies relationships between them, and generates strokes in different writing styles. Inkeraction reshapes the design space for digital ink by enabling features that include: (1) assisting users to manipulate ink objects, (2) providing word-processor features such as spell checking, (3) automating repetitive writing tasks such as transcribing, and (4) bridging with generative models’ features such as brainstorming. Feedback from two user studies with a total of 22 participants demonstrated that Inkeraction supported writing activities by enabling participants to write faster with fewer steps and achieve better writing quality.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {181},\nnumpages = {26},\nkeywords = {Digital pen, Ink, Stylus interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642165,\nauthor = {Zeng, Xingchen and Gao, Ziyao and Ye, Yilin and Zeng, Wei},\ntitle = {IntentTuner: An Interactive Framework for Integrating Human Intentions in Fine-tuning Text-to-Image Generative Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642165},\ndoi = {10.1145/3613904.3642165},\nabstract = {Fine-tuning facilitates the adaptation of text-to-image generative models to novel concepts (e.g., styles and portraits), empowering users to forge creatively customized content. Recent efforts on fine-tuning focus on reducing training data and lightening computation overload but neglect alignment with user intentions, particularly in manual curation of multi-modal training data and intent-oriented evaluation. Informed by a formative study with fine-tuning practitioners for comprehending user intentions, we propose IntentTuner, an interactive framework that intelligently incorporates human intentions throughout each phase of the fine-tuning workflow. IntentTuner enables users to articulate training intentions with imagery exemplars and textual descriptions, automatically converting them into effective data augmentation strategies. Furthermore, IntentTuner introduces novel metrics to measure user intent alignment, allowing intent-aware monitoring and evaluation of model training. Application exemplars and user studies demonstrate that IntentTuner streamlines fine-tuning, reducing cognitive effort and yielding superior models compared to the common baseline tool.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {182},\nnumpages = {18},\nkeywords = {and data augmentation, text-to-image generative model, user intent understanding},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642861,\nauthor = {Mahdavi Goloujeh, Atefeh and Sullivan, Anne and Magerko, Brian},\ntitle = {Is It AI or Is It Me? Understanding Users’ Prompt Journey with Text-to-Image Generative AI Tools},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642861},\ndoi = {10.1145/3613904.3642861},\nabstract = {Generative Artificial Intelligence (AI) has witnessed unprecedented growth in text-to-image AI tools. Yet, much remains unknown about users’ prompt journey with such tools in the wild. In this paper, we posit that designing human-centered text-to-image AI tools requires a clear understanding of how individuals intuitively approach crafting prompts, and what challenges they may encounter. To address this, we conducted semi-structured interviews with 19 existing users of a text-to-image AI tool. Our findings (1) offer insights into users’ prompt journey including structures and processes for writing, evaluating, and refining prompts in text-to-image AI tools and (2) indicate that users must overcome barriers to aligning AI to their intents, and mastering prompt crafting knowledge. From the findings, we discuss the prompt journey as an individual yet a social experience and highlight opportunities for aligning text-to-image AI tools and users’ intents.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {183},\nnumpages = {13},\nkeywords = {Prompt engineering, generative AI, text-to-image generation, user journey},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642391,\nauthor = {Shi, Yang and Peng, Yechun and Dang, Shengqi and Zhao, Nanxuan and Cao, Nan},\ntitle = {Personalizing Products with Stylized Head Portraits for Self-Expression},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642391},\ndoi = {10.1145/3613904.3642391},\nabstract = {Personalizing products aesthetically or functionally can help users increase personal relevance and support self-expression. However, using non-abstract personal data such as head portraits for product personalization has been understudied. While recent advances in Artificial Intelligence have enabled generating stylized head portraits, these images also raise concerns about lack of control, artificiality, and ethics, which potentially limit their broader use. In this work, we present PicMe, a design support tool that converts user face photos into stylized head portraits as vector graphics that can be used to personalize products. To enable style transfer, PicMe leverages a deep-learning-based algorithm trained on an extended open-source illustration dataset of characters in a cartoonish and minimalistic style. We evaluated PicMe through two experiments and a user study. The results of our evaluation showed that PicMe can help create personalized head portraits that support self-expression.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {184},\nnumpages = {18},\nkeywords = {Artificial Intelligence, Design Support Tools, Self-Expression, Stylized Head Portraits},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642803,\nauthor = {Wang, Zhijie and Huang, Yuheng and Song, Da and Ma, Lei and Zhang, Tianyi},\ntitle = {PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642803},\ndoi = {10.1145/3613904.3642803},\nabstract = {The recent advancements in Generative AI have significantly advanced the field of text-to-image generation. The state-of-the-art text-to-image model, Stable Diffusion, is now capable of synthesizing high-quality images with a strong sense of aesthetics. Crafting text prompts that align with the model’s interpretation and the user’s intent thus becomes crucial. However, prompting remains challenging for novice users due to the complexity of the stable diffusion model and the non-trivial efforts required for iteratively editing and refining the text prompts. To address these challenges, we propose PromptCharm, a mixed-initiative system that facilitates text-to-image creation through multi-modal prompt engineering and refinement. To assist novice users in prompting, PromptCharm first automatically refines and optimizes the user’s initial prompt. Furthermore, PromptCharm supports the user in exploring and selecting different image styles within a large database. To assist users in effectively refining their prompts and images, PromptCharm renders model explanations by visualizing the model’s attention values. If the user notices any unsatisfactory areas in the generated images, they can further refine the images through model attention adjustment or image inpainting within the rich feedback loop of PromptCharm. To evaluate the effectiveness and usability of PromptCharm, we conducted a controlled user study with 12 participants and an exploratory user study with another 12 participants. These two studies show that participants using PromptCharm were able to create images with higher quality and better aligned with the user’s expectations compared with using two variants of PromptCharm that lacked interaction or visualization support.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {185},\nnumpages = {21},\nkeywords = {Generative AI, Large Language Models, Prompt Engineering},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642415,\nauthor = {Huang, Yanwei and Yang, Yurun and Shu, Xinhuan and Chen, Ran and Weng, Di and Wu, Yingcai},\ntitle = {Table Illustrator: Puzzle-based interactive authoring of plain tables},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642415},\ndoi = {10.1145/3613904.3642415},\nabstract = {Plain tables excel at displaying data details and are widely used in data presentation, often polished to an elaborate appearance for readability in many scenarios. However, existing authoring tools fail to provide both flexible and efficient support for altering the table layout and styles, motivating us to develop an intuitive and swift tool for table prototyping. To this end, we contribute Table Illustrator, a table authoring system taking a novel visual metaphor, puzzle, as the primary interaction unit. Through combinations and configurations on puzzles, the system enables rapid table construction and supports a diverse range of table layouts and styles. The tool design is informed by practical challenges and requirements from interviews with 10 table practitioners and a structured design space based on an analysis of over 2,500 real-world tables. User studies showed that Table Illustrator achieved comparable performance to Microsoft Excel while reducing users’ completion time and perceived workload.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {186},\nnumpages = {18},\nkeywords = {Plain tables, data presentation, design study, interaction design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641912,\nauthor = {Uhl, Jakob Carl and Gutierrez, Rodrigo and Regal, Georg and Schrom-Feiertag, Helmut and Schuster, Benjamin and Tscheligi, Manfred},\ntitle = {Choosing the Right Reality: A Comparative Analysis of Tangibility in Immersive Trauma Simulations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641912},\ndoi = {10.1145/3613904.3641912},\nabstract = {In the field of medical first responder training, the choice of training modality is crucial for skill retention and real-world application. This study introduces the Green Manikin, an advanced Mixed Reality (MR) tool, conceptually combining the immersiveness of Virtual Reality (VR) with the tangibility of real-world training, and compares it against traditional real-world simulations and VR training. Our findings indicate that MR and real-world settings excel in Self and Social Presence, and in intention to use, offering heightened psychological presence suitable for complex training scenarios. Effort expectancy was highest in real-world environments, suggesting their ease of use for basic skill acquisition. This nuanced understanding allows for better tailoring of training modalities to specific educational objectives. Our research validates the utility of MR and offers a framework for selecting the most effective training environment for different learning outcomes in medical first responder training.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {187},\nnumpages = {17},\nkeywords = {chroma-key, comparative study, medical first responder, mixed reality, modality, training},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642646,\nauthor = {McGee, Fintan and McCall, Roderick and Baixauli, Joan},\ntitle = {Comparison of Spatial Visualization Techniques for Radiation in Augmented Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642646},\ndoi = {10.1145/3613904.3642646},\nabstract = {Augmented Reality (AR) provides a safe and low-cost option for hazardous safety training that allows for the visualization of aspects that may be invisible, such as radiation. Effectively visually communicating such threats in the environment around the user is not straightforward. This work describes visually encoding radiation using the spatial awareness mesh of an AR Head Mounted Display. We leverage the AR device’s GPUs to develop a real time solution that accumulates multiple dynamic sources and uses stencils to prevent an environment being over saturated with a visualization, as well as supporting the encoding of direction explicitly in the visualization. We perform a user study (25 participants) of different visualizations and obtain user feedback. Results show that there are complex interactions and while no visual representation was statistically superior or inferior, user opinions vary widely. We also discuss the evaluation approaches and provide recommendations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {188},\nnumpages = {15},\nkeywords = {Augmented Reality, CBRN Response Training, Spatial Awareness, Visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642195,\nauthor = {Zhang, Kexin and Cochran, Brianna R and Chen, Ruijia and Hartung, Lance and Sprecher, Bryce and Tredinnick, Ross and Ponto, Kevin and Banerjee, Suman and Zhao, Yuhang},\ntitle = {Exploring the Design Space of Optical See-through AR Head-Mounted Displays to Support First Responders in the Field},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642195},\ndoi = {10.1145/3613904.3642195},\nabstract = {First responders (FRs) navigate hazardous, unfamiliar environments in the field (e.g., mass-casualty incidents), making life-changing decisions in a split second. AR head-mounted displays (HMDs) have shown promise in supporting them due to its capability of recognizing and augmenting the challenging environments in a hands-free manner. However, the design space have not been thoroughly explored by involving various FRs who serve different roles (e.g., firefighters, law enforcement) but collaborate closely in the field. We interviewed 26 first responders in the field who experienced a state-of-the-art optical-see-through AR HMD, as well as its interaction techniques and four types of AR cues (i.e., overview cues, directional cues, highlighting cues, and labeling cues), soliciting their first-hand experiences, design ideas, and concerns. Our study revealed both generic and role-specific preferences and needs for AR hardware, interactions, and feedback, as well as identifying desired AR designs tailored to urgent, risky scenarios (e.g., affordance augmentation to facilitate fast and safe action). While acknowledging the value of AR HMDs, concerns were also raised around trust, privacy, and proper integration with other equipment. Finally, we derived comprehensive and actionable design guidelines to inform future AR systems for in-field FRs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {189},\nnumpages = {19},\nkeywords = {Augmented Reality, Design, First Responders, In-the-field Tasks},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642634,\nauthor = {Rezk, Anna Marie and Simkute, Auste and Luger, Ewa and Vines, John and Elsden, Chris and Evans, Michael and Jones, Rhianne},\ntitle = {Agency Aspirations: Understanding Users' Preferences And Perceptions Of Their Role In Personalised News Curation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642634},\ndoi = {10.1145/3613904.3642634},\nabstract = {Recommender systems are increasingly employed by journalistic outlets to deliver personalised news, transforming news curation into a reciprocal yet insufficiently defined process influenced by editors, recommender systems, and individual user actions. To understand the tension in this dynamic and users’ preferences and perceptions of their role in personalised news curation, we conducted a study with UK participants aged 16-34. Building on a preliminary survey and interview study, which revealed a strong desire from participants for increased agency in personalisation, we designed an interactive news recommender provotype (provocative design artefact) which probed the role of agency in news curation with participants (n=16). Findings highlighted a behaviour-intention gap, indicating participants desire for agency yet reluctance to intervene actively in personalisation. Our research offers valuable insights into how users perceive their agency in personalised news curation, underscoring the importance for systems to be designed to support individuals becoming active agents in news personalisation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {190},\nnumpages = {16},\nkeywords = {Interaction Design, Personalisation, User Experience Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642204,\nauthor = {Jhaver, Shagun and Rathi, Himanshu and Saha, Koustuv},\ntitle = {Bystanders of Online Moderation: Examining the Effects of Witnessing Post-Removal Explanations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642204},\ndoi = {10.1145/3613904.3642204},\nabstract = {Prior research on transparency in content moderation has demonstrated the benefits of offering post-removal explanations to sanctioned users. In this paper, we examine whether the influence of such explanations transcends those who are moderated to the bystanders who witness such explanations. We conduct a quasi-experimental study on two popular Reddit communities (r/AskReddit and r/science) by collecting their data spanning 13 months—a total of 85.5M posts made by 5.9M users. Our causal-inference analyses show that bystanders significantly increase their posting activity and interactivity levels as compared to their matched control set of users. In line with previous applications of Deterrence Theory on digital platforms, our findings highlight that understanding the rationales behind sanctions on other users significantly shapes observers’ behaviors. We discuss the theoretical implications and design recommendations of this research, focusing on how investing more efforts in post-removal explanations can help build thriving online communities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {191},\nnumpages = {9},\nkeywords = {causal-inference, content moderation, social media, transparency},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642675,\nauthor = {Kou, Yubo and Ma, Renkai and Zhang, Zinan and Zhou, Yingfan and Gui, Xinning},\ntitle = {Community Begins Where Moderation Ends: Peer Support and Its Implications for Community-Based Rehabilitation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642675},\ndoi = {10.1145/3613904.3642675},\nabstract = {Moderation systems of online games often follow a retributive model inspired by real-world criminal justice, expecting that punishments can help users to reform behavior. However, decades of criminological research show that punishments alone do not work and call for a rehabilitative approach, such as community-based rehabilitation (CBR), to help offenders transform their minds and behavioral patterns. Motivated by this call, we explore how moderated users view punishments in a community context and how other community members respond in League of Legends (LoL), one of the largest online games. Specifically, we focus on how peer support is sought and provided on the /r/LeagueOfLegends subreddit, the largest LoL-related online community. Our content analysis of player discussions characterized the communication between moderated users and peers as informative, constructive, and reflexive. We highlight the importance of involving community in moderation systems and discuss implications for designing CBR mechanisms that could enhance moderation systems.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {192},\nnumpages = {18},\nkeywords = {community-based rehabilitation, moderation experience, online moderation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642278,\nauthor = {Kuo, Tzu-Sheng and Halfaker, Aaron Lee and Cheng, Zirui and Kim, Jiwoo and Wu, Meng-Hsin and Wu, Tongshuang and Holstein, Kenneth and Zhu, Haiyi},\ntitle = {Wikibench: Community-Driven Data Curation for AI Evaluation on Wikipedia},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642278},\ndoi = {10.1145/3613904.3642278},\nabstract = {AI tools are increasingly deployed in community contexts. However, datasets used to evaluate AI are typically created by developers and annotators outside a given community, which can yield misleading conclusions about AI performance. How might we empower communities to drive the intentional design and curation of evaluation datasets for AI that impacts them? We investigate this question on Wikipedia, an online community with multiple AI-based content moderation tools deployed. We introduce Wikibench, a system that enables communities to collaboratively curate AI evaluation datasets, while navigating ambiguities and differences in perspective through discussion. A field study on Wikipedia shows that datasets curated using Wikibench can effectively capture community consensus, disagreement, and uncertainty. Furthermore, study participants used Wikibench to shape the overall data curation process, including refining label definitions, determining data inclusion criteria, and authoring data statements. Based on our findings, we propose future directions for systems that support community-driven data curation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {193},\nnumpages = {24},\nkeywords = {AI evaluation, Wikipedia, community-driven AI, data curation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642276,\nauthor = {Cheng, Peter C-H and Garcia Garcia, Grecia and Raggi, Daniel and Jamnik, Mateja},\ntitle = {A Human Information Processing Theory of the Interpretation of Visualizations: Demonstrating Its Utility},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642276},\ndoi = {10.1145/3613904.3642276},\nabstract = {Providing an approach to model the memory structures that humans build as they use visualizations could be useful for researchers, designers and educators in the field of information visualization. Cheng and colleagues formulated Representation Interpretive Structure Theory (RIST) for that purpose. RIST adopts a human information processing perspective in order to address the immediate, short timescale, cognitive load likely to be experienced by visualization users. RIST is operationalized in a graphical modeling notation and browser-based editor. This paper demonstrates the utility of RIST by showing that (a): RIST models are compatible with established empirical and computational cognitive findings about differences in human performance on alternative representations; (b) they can encompass existing explanations from the literature; and, (c) they provide new explanations about causes of those performance differences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {194},\nnumpages = {14},\nkeywords = {cognition, human memory structures, interpretation of visualizations},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3643022,\nauthor = {Shao, Hongbo and Martinez-Maldonado, Roberto and Echeverria, Vanessa and Yan, Lixiang and Gasevic, Dragan},\ntitle = {Data Storytelling in Data Visualisation: Does it Enhance the Efficiency and Effectiveness of Information Retrieval and Insights Comprehension?},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3643022},\ndoi = {10.1145/3613904.3643022},\nabstract = {Data storytelling (DS) is rapidly gaining attention as an approach that integrates data, visuals, and narratives to create data stories that can help a particular audience to comprehend the key messages underscored by the data with enhanced efficiency and effectiveness. It is been posited that DS can be especially advantageous for audiences with limited visualisation literacy, by presenting the data clearly and concisely. However, empirical studies confirming whether data stories indeed provide these benefits over conventional data visualisations are scarce. To bridge this gap, we conducted a study with 103 participants to determine whether DS indeed improve both efficiency and effectiveness in tasks related to information retrieval and insights comprehension. Our findings suggest that data stories do improve the efficiency of comprehension tasks, as well as the effectiveness of comprehension tasks that involve a single insight, compared with conventional visualisations. Interestingly, these benefits were not associated with participants’ visualisation literacy.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {195},\nnumpages = {21},\nkeywords = {Data Storytelling, Data comprehension, Data visualisation, Visualisation Literacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642049,\nauthor = {Zhu, Qian and Wang, Zhuo and Zeng, Wei and Tong, Wai and Lin, Weiyue and Ma, Xiaojuan},\ntitle = {Make Interaction Situated: Designing User Acceptable Interaction for Situated Visualization in Public Environments},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642049},\ndoi = {10.1145/3613904.3642049},\nabstract = {Situated visualization blends data into the real world to fulfill individuals’ contextual information needs. However, interacting with situated visualization in public environments faces challenges posed by users’ acceptance and contextual constraints. To explore appropriate interaction design, we first conduct a formative study to identify users’ needs for data and interaction. Informed by the findings, we summarize appropriate interaction modalities with eye-based, hand-based and spatially-aware object interaction for situated visualization in public environments. Then, through an iterative design process with six users, we explore and implement interactive techniques for activating and analyzing with situated visualization. To assess the effectiveness and acceptance of these interactions, we integrate them into an AR prototype and conduct a within-subjects study in public scenarios using conventional hand-only interactions as the baseline. The results show that participants preferred our prototype over the baseline, attributing their preference to the interactions being more acceptable, flexible, and practical in public.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {196},\nnumpages = {21},\nkeywords = {Interactive Techniques, Situated Visualization, Social Acceptability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642760,\nauthor = {Nobre, Carolina and Zhu, Kehang and M\\\"{o}rth, Eric and Pfister, Hanspeter and Beyer, Johanna},\ntitle = {Reading Between the Pixels: Investigating the Barriers to Visualization Literacy},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642760},\ndoi = {10.1145/3613904.3642760},\nabstract = {In our current visual-centric digital age, the capability to interpret, understand, and produce visual representations of data —termed visualization literacy— is paramount. However, not everyone is adept at navigating this visual terrain. This paper explores the barriers that individuals who misread a visualization encounter, aiming to understand their specific mental gaps. Utilizing a mixed-method approach, we administered the Visualization Literacy Assessment Test (VLAT) to a group of 120 participants drawn from diverse demographic backgrounds, which provided us with 1774 task completions. We augmented the standard VLAT test to capture quantitative and qualitative data on participants’ errors. We collected participant sketches and open-ended text about their analysis approach, providing insight into users’ mental models and rationale. Our findings reveal that individuals who incorrectly answer visualization literacy questions often misread visual channels, confound chart labels with data values, or struggle to translate data-driven questions into visual queries. Recognizing and bridging visualization literacy gaps not only ensures inclusivity but also enhances the overall effectiveness of visual communication in our society.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {197},\nnumpages = {17},\nkeywords = {Visualization, conceptual barriers, visualization literacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642237,\nauthor = {Ying, Lu and Wu, Aoyu and Li, Haotian and Deng, Zikun and Lan, Ji and Wu, Jiang and Wang, Yong and Qu, Huamin and Deng, Dazhen and Wu, Yingcai},\ntitle = {VAID: Indexing View Designs in Visual Analytics System},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642237},\ndoi = {10.1145/3613904.3642237},\nabstract = {Visual analytics (VA) systems have been widely used in various application domains. However, VA systems are complex in design, which imposes a serious problem: although the academic community constantly designs and implements new designs, the designs are difficult to query, understand, and refer to by subsequent designers. To mark a major step forward in tackling this problem, we index VA designs in an expressive and accessible way, transforming the designs into a structured format. We first conducted a workshop study with VA designers to learn user requirements for understanding and retrieving professional designs in VA systems. Thereafter, we came up with an index structure VAID to describe advanced and composited visualization designs with comprehensive labels about their analytical tasks and visual designs. The usefulness of VAID was validated through user studies. Our work opens new perspectives for enhancing the accessibility and reusability of professional visualization designs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {198},\nnumpages = {15},\nkeywords = {Visual Analytics, Visualization Design, Visualization Retrieval},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642448,\nauthor = {Lisnic, Maxim and Lex, Alexander and Kogan, Marina},\ntitle = {\"Yeah, this graph doesn't show that\": Analysis of Online Engagement with Misleading Data Visualizations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642448},\ndoi = {10.1145/3613904.3642448},\nabstract = {Attempting to make sense of a phenomenon or crisis, social media users often share data visualizations and interpretations that can be erroneous or misleading. Prior work has studied how data visualizations can mislead, but do misleading visualizations reach a broad social media audience? And if so, do users amplify or challenge misleading interpretations? To answer these questions, we conducted a mixed-methods analysis of the public’s engagement with data visualization posts about COVID-19 on Twitter. Compared to posts with accurate visual insights, our results show that posts with misleading visualizations garner more replies in which the audiences point out nuanced fallacies and caveats in data interpretations. Based on the results of our thematic analysis of engagement, we identify and discuss important opportunities and limitations to effectively leveraging crowdsourced assessments to address data-driven misinformation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {199},\nnumpages = {14},\nkeywords = {COVID-19, Visualization, misinformation, social media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642172,\nauthor = {Zhou, Tongyu and Huang, Jeff and Chan, Gromit Yeuk-Yin},\ntitle = {Epigraphics: Message-Driven Infographics Authoring},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642172},\ndoi = {10.1145/3613904.3642172},\nabstract = {The message a designer wants to convey plays a pivotal role in directing the design of an infographic, yet most authoring workflows start with creating the visualizations or graphics first without gauging whether they fit the message. To address this gap, we propose Epigraphics, a web-based authoring system that treats an “epigraph” as the first-class object, and uses it to guide infographic asset creation, editing, and syncing. The system uses the text-based message to recommend visualizations, graphics, data filters, color palettes, and animations. It further supports between-asset interactions and fine-tuning such as recoloring, highlighting, and animation syncing that enhance the aesthetic cohesiveness of the assets. A gallery and case studies show that our system can produce infographics inspired by existing popular ones, and a task-based usability study with 10 designers show that a text-sourced workflow can standardize content, empower users to think more about the big picture, and facilitate rapid prototyping.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {200},\nnumpages = {18},\nkeywords = {data visualization, infographics authoring, visual storytelling},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642248,\nauthor = {Morais, Luiz and Panagiotidou, Georgia and Hayes, Sarah and Losev, Tatiana and Noonan, Rebecca and Hinrichs, Uta},\ntitle = {From Exploration to End of Life: Unpacking Sustainability in Physicalization Practices},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642248},\ndoi = {10.1145/3613904.3642248},\nabstract = {Data physicalizations have gained prominence across domains, but their environmental impact has been largely overlooked. This work addresses this gap by investigating the interplay between sustainability and physicalization practices. We conducted interviews with experts from diverse backgrounds, followed by a survey to gather insights into how they approach physicalization projects and reflect on sustainability. Our thematic analysis revealed sustainability considerations throughout the entire physicalization life cycle—a framework that encompasses various stages in a physicalization’s existence. Notably, we found no single agreed-upon definition for sustainable physicalizations, highlighting the complexity of integrating sustainability into physicalization practices. We outline sustainability challenges and strategies based on participants’ experiences and propose the Sustainable Physicalization Practices (SuPPra) Matrix, providing a structured approach for designers to reflect on and enhance the environmental impact of their future physicalizations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {201},\nnumpages = {17},\nkeywords = {life cycle, physicalization, sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642286,\nauthor = {Gao, Lei and Christopoulos, Giorgos and Mittal, Prateek and Hirayama, Ryuji and Subramanian, Sriram},\ntitle = {StableLev: Data-Driven Stability Enhancement for Multi-Particle Acoustic Levitation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642286},\ndoi = {10.1145/3613904.3642286},\nabstract = {Acoustic levitation is an emerging technique that has found application in contactless assembly and dynamic displays. It uses precise phase control in an ultrasound transducer array to manage the positions and movements of multiple particles. Yet, maintaining stable mid-air particles is challenging, with unexpected drops disrupting the intended motion and position. Here, we present StableLev, a data-driven pipeline for the detection and amendment of instabilities in multi-particle levitation. We first curate a hybrid levitation dataset, blending optimized simulations with labels based on actual trajectory outcomes. We then design an AutoEncoder to detect anomalies in the simulated data, correlating closely with observed particle drops. Finally, we reconstruct the acoustic field at anomaly regions to improve particle stability and experimentally demonstrate successful dynamic levitation for trajectories within our dataset. Our work provides new insights into multi-particle levitation and enhances its robustness, which will be valuable in a wide range of applications.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {202},\nnumpages = {11},\nkeywords = {acoustic levitation, anomaly detection, levitation dataset, stability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641900,\nauthor = {Du, Xiaojiao and Satriadi, Kadek Ananta and Drogemuller, Adam and Matthews, Brandon J and Smith, Ross and Walsh, James A. and Cunningham, Andrew},\ntitle = {That's Rough! Encoding Data into Roughness for Physicalization},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641900},\ndoi = {10.1145/3613904.3641900},\nabstract = {While visual channels (e.g., color, shape, size) have been explored for visualizing data in data physicalizations, there is a lack of understanding regarding how to encode data into physical material properties (e.g., roughness, hardness). This understanding is critical for ensuring data is correctly communicated and for potentially extending the channels and bandwidth available for encoding that data. We present a method to encode ordinal data into roughness, validated through user studies. In the first study, we identified just noticeable differences in perceived roughness from this method. In the second study, we 3D-printed proof of concepts for five different multivariate physicalizations using the model. These physicalizations were qualitatively explored (N=10) to understand people’s comprehension and impressions of the roughness channel. Our findings suggest roughness may be used for certain types of data encoding, and the context of the data can impact how people interpret roughness mapping direction.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {203},\nnumpages = {16},\nkeywords = {data encoding, data physicalization, material properties, physical channel},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642813,\nauthor = {Quadri, Ghulam Jilani and Wang, Arran Zeyu and Wang, Zhehao and Adorno, Jennifer and Rosen, Paul and Szafir, Danielle Albers},\ntitle = {Do You See What I See? A Qualitative Study Eliciting High-Level Visualization Comprehension},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642813},\ndoi = {10.1145/3613904.3642813},\nabstract = {Designers often create visualizations to achieve specific high-level analytical or communication goals. These goals require people to naturally extract complex, contextualized, and interconnected patterns in data. While limited prior work has studied general high-level interpretation, prevailing perceptual studies of visualization effectiveness primarily focus on isolated, predefined, low-level tasks, such as estimating statistical quantities. This study more holistically explores visualization interpretation to examine the alignment between designers’ communicative goals and what their audience sees in a visualization, which we refer to as their comprehension. We found that statistics people effectively estimate from visualizations in classical graphical perception studies may differ from the patterns people intuitively comprehend in a visualization. We conducted a qualitative study on three types of visualizations—line graphs, bar graphs, and scatterplots—to investigate the high-level patterns people naturally draw from a visualization. Participants described a series of graphs using natural language and think-aloud protocols. We found that comprehension varies with a range of factors, including graph complexity and data distribution. Specifically, 1) a visualization’s stated objective often does not align with people’s comprehension, 2) results from traditional experiments may not predict the knowledge people build with a graph, and 3) chart type alone is insufficient to predict the information people extract from a graph. Our study confirms the importance of defining visualization effectiveness from multiple perspectives to assess and inform visualization practices.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {204},\nnumpages = {26},\nkeywords = {Communicative goals, High-level comprehension, Insight, Qualitative evaluation, Visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642127,\nauthor = {Strain, Gabriel and Stewart, Andrew J. and Warren, Paul A. and Jay, Caroline},\ntitle = {Effects of Point Size and Opacity Adjustments in Scatterplots},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642127},\ndoi = {10.1145/3613904.3642127},\nabstract = {Systematically changing the size and opacity of points on scatterplots can be used to induce more accurate perceptions of correlation by viewers. Evidence points to the mechanisms behind these effects being similar, so one may expect their combination to be additive regarding their effects on correlation estimation. We present a fully-reproducible study in which we combine techniques for influencing correlation perception to show that in reality, effects of changing point size and opacity interact in a non-additive fashion. We show that there is a great deal of scope for using visual features to change viewers’ perceptions of data visualizations. Additionally, we use our results to further interrogate the perceptual mechanisms at play when changing point size and opacity in scatterplots.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {205},\nnumpages = {13},\nkeywords = {correlation, crowdsourced, perception, scatterplot},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641959,\nauthor = {Robinson Moore, Wilfredo Joshua and Kalal, Medhani and Tennison, Jennifer L. and Giudice, Nicholas A and Gorlewicz, Jenna},\ntitle = {Spatial Audio-Enhanced Multimodal Graph Rendering for Efficient Data Trend Learning on Touchscreen Devices},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641959},\ndoi = {10.1145/3613904.3641959},\nabstract = {Touchscreen-based rendering of graphics using vibrations, sonification, and text-to-speech is a promising approach for nonvisual access to graphical information, but extracting trends from complex data representations nonvisually is challenging. This work presents the design of a multimodal feedback scheme with integrated spatial audio for the exploration of histograms and scatter plots on touchscreens. We detail the hardware employed and the algorithms used to control vibrations and sonification adjustments through the change of pitch and directional stereo output. We conducted formative testing with 5 blind or visually impaired participants, and results illustrate that spatial audio has the potential to increase the identification of trends in the data, at the expense of a skewed mental representation of the graph. This design work and pilot study are critical to the iterative, human-centered approach of rendering multimodal graphics on touchscreens and contribute a new scheme for efficiently capturing data trends in complex data representations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {206},\nnumpages = {12},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642102,\nauthor = {Long, Sheng and Kay, Matthew},\ntitle = {To Cut or Not To Cut? A Systematic Exploration of Y-Axis Truncation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642102},\ndoi = {10.1145/3613904.3642102},\nabstract = {Y-axis truncation is a well-known, much-debated visualization practice. Our work complements existing empirical work by providing a systematic analysis of y-axis truncation on grouped bar charts. Drawing upon theoretical frameworks such as Algebraic Visualization Design, we examine how structure-preserving modifications to visualization affect user performance by systematically dividing the space of possible truncations according to their monotonicity and the type of relations in the underlying data. Our results demonstrate that for comparing and estimating the difference between the lengths of two bars, truncating the y-axis does not affect task performance. For comparing or estimating the relative growth between two bars, truncating monotonically has similar performance to no truncation, while truncating non-monotonically is very likely to impair performance. We discuss possible extensions of our work and recommendations for y-axis truncation. All supplementary materials are available at https://osf.io/k4hjd/?view_only=008b087fc3d94be7ba0ce7aea95012a7.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {207},\nnumpages = {12},\nkeywords = {algebraic visualization design, deceptive visualization, structure-preserving transformations, y-axis truncation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642857,\nauthor = {Patnaik, Biswaksen and Peng, Huaishu and Elmqvist, Niklas},\ntitle = {VisTorch: Interacting with Situated Visualizations using Handheld Projectors},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642857},\ndoi = {10.1145/3613904.3642857},\nabstract = {Spatial data is best analyzed in situ, but existing mixed reality technologies can be bulky, expensive, or unsuitable for collaboration. We present VisTorch: a handheld device for projected situated analytics consisting of a pico-projector, a multi-spectrum camera, and a touch surface. VisTorch enables viewing charts situated in physical space by simply pointing the device at a surface to reveal visualizations in that location. We evaluated the approach using both a user study and an expert review. In the former, we asked 20 participants to first organize charts in space and then refer to these charts to answer questions. We observed three spatial and one temporal pattern in participant analyses. In the latter, four experts—a museum designer, a statistical software developer, a theater stage designer, and an environmental educator—utilized VisTorch to derive practical usage scenarios. Results from our study showcase the utility of situated visualizations for memory and recall.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {208},\nnumpages = {13},\nkeywords = {Ubiquitous analytics, augmented reality, immersive analytics, situated visualization.},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642740,\nauthor = {He, Shuqi and Yao, Haonan and Jiang, Luyan and Li, Kaiwen and Xiang, Nan and Li, Yue and Liang, Hai-Ning and Yu, Lingyun},\ntitle = {Data Cubes in Hand: A Design Space of Tangible Cubes for Visualizing 3D Spatio-Temporal Data in Mixed Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642740},\ndoi = {10.1145/3613904.3642740},\nabstract = {Tangible interfaces in mixed reality (MR) environments allow for intuitive data interactions. Tangible cubes, with their rich interaction affordances, high maneuverability, and stable structure, are particularly well-suited for exploring multi-dimensional data types. However, the design potential of these cubes is underexplored. This study introduces a design space for tangible cubes in MR, focusing on interaction space, visualization space, sizes, and multiplicity. Using spatio-temporal data, we explored the interaction affordances of these cubes in a workshop (N=24). We identified unique interactions like rotating, tapping, and stacking, which are linked to augmented reality (AR) visualization commands. Integrating user-identified interactions, we created a design space for tangible-cube interactions and visualization. A prototype visualizing global health spending with small cubes was developed and evaluated, supporting both individual and combined cube manipulation. This research enhances our grasp of tangible interaction in MR, offering insights for future design and application in diverse data contexts.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {209},\nnumpages = {21},\nkeywords = {mixed reality, spatio-temporal data, tangible interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642001,\nauthor = {Coscia, Adam and Sapers, Haley M. and Deutsch, Noah and Khurana, Malika and Magyar, John S. and Parra, Sergio A. and Utter, Daniel R. and Wipfler, Rebecca L. and Caress, David W. and Martin, Eric J. and Paduan, Jennifer B. and Hendrie, Maggie and Lombeyda, Santiago and Mushkin, Hillary and Endert, Alex and Davidoff, Scott and Orphan, Victoria J.},\ntitle = {DeepSee: Multidimensional Visualizations of Seabed Ecosystems},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642001},\ndoi = {10.1145/3613904.3642001},\nabstract = {Scientists studying deep ocean microbial ecosystems use limited numbers of sediment samples collected from the seafloor to characterize important life-sustaining biogeochemical cycles in the environment. Yet conducting fieldwork to sample these extreme remote environments is both expensive and time consuming, requiring tools that enable scientists to explore the sampling history of field sites and predict where taking new samples is likely to maximize scientific return. We conducted a collaborative, user-centered design study with a team of scientific researchers to develop DeepSee, an interactive data workspace that visualizes 2D and 3D interpolations of biogeochemical and microbial processes in context together with sediment sampling history overlaid on 2D seafloor maps. Based on a field deployment and qualitative interviews, we found that DeepSee increased the scientific return from limited sample sizes, catalyzed new research workflows, reduced long-term costs of sharing data, and supported teamwork and communication between team members with diverse research goals.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {210},\nnumpages = {16},\nkeywords = {Data visualization, deep ocean research., design study, scientific visualization, visual analytics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642730,\nauthor = {Seo, JooYoung and Xia, Yilin and Lee, Bongshin and Mccurry, Sean and Yam, Yu Jun},\ntitle = {MAIDR: Making Statistical Visualizations Accessible with Multimodal Data Representation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642730},\ndoi = {10.1145/3613904.3642730},\nabstract = {This paper investigates new data exploration experiences that enable blind users to interact with statistical data visualizations—bar plots, heat maps, box plots, and scatter plots—leveraging multimodal data representations. In addition to sonification and textual descriptions that are commonly employed by existing accessible visualizations, our MAIDR (multimodal access and interactive data representation) system incorporates two additional modalities (braille and review) that offer complementary benefits. It also provides blind users with the autonomy and control to interactively access and understand data visualizations. In a user study involving 11 blind participants, we found the MAIDR system facilitated the accurate interpretation of statistical visualizations. Participants exhibited a range of strategies in combining multiple modalities, influenced by their past interactions and experiences with data visualizations. This work accentuates the overlooked potential of combining refreshable tactile representation with other modalities and elevates the discussion on the importance of user autonomy when designing accessible data visualizations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {211},\nnumpages = {22},\nkeywords = {Accessibility, Blind, Braille Display, Multimodality, Screen Readers, Statistical Visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642944,\nauthor = {Chen, Juntong and Huang, Haiwen and Ye, Huayuan and Peng, Zhong and Li, Chenhui and Wang, Changbo},\ntitle = {SalienTime: User-driven Selection of Salient Time Steps for Large-Scale Geospatial Data Visualization},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642944},\ndoi = {10.1145/3613904.3642944},\nabstract = {The voluminous nature of geospatial temporal data from physical monitors and simulation models poses challenges to efficient data access, often resulting in cumbersome temporal selection experiences in web-based data portals. Thus, selecting a subset of time steps for prioritized visualization and pre-loading is highly desirable. Addressing this issue, this paper establishes a multifaceted definition of salient time steps via extensive need-finding studies with domain experts to understand their workflows. Building on this, we propose a novel approach that leverages autoencoders and dynamic programming to facilitate user-driven temporal selections. Structural features, statistical variations, and distance penalties are incorporated to make more flexible selections. User-specified priorities, spatial regions, and aggregations are used to combine different perspectives. We design and implement a web-based interface to enable efficient and context-aware selection of time steps and evaluate its efficacy and usability through case studies, quantitative evaluations, and expert interviews.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {212},\nnumpages = {19},\nkeywords = {Geospatial Data, Key Time Selection, Large-scale Data Visualization, Need-finding Study, Visualization Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642132,\nauthor = {Fan, Arlen and Lei, Fan and Mancenido, Michelle and Maceachren, Alan M. and Maciejewski, Ross},\ntitle = {Understanding Reader Takeaways in Thematic Maps Under Varying Text, Detail, and Spatial Autocorrelation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642132},\ndoi = {10.1145/3613904.3642132},\nabstract = {Maps are crucial in conveying geospatial data in diverse contexts such as news and scientific reports. This research, utilizing thematic maps, probes deeper into the underexplored intersection of text framing and map types in influencing map interpretation. In this work, we conducted experiments to evaluate how textual detail and semantic content variations affect the quality of insights derived from map examination. We also explored the influence of explanatory annotations across different map types (e.g., choropleth, hexbin, isarithmic), base map details, and changing levels of spatial autocorrelation in the data. From two online experiments with N = 103 participants, we found that annotations, their specific attributes, and map type used to present the data significantly shape the quality of takeaways. Notably, we found that the effectiveness of annotations hinges on their contextual integration. These findings offer valuable guidance to the visualization community for crafting impactful thematic geospatial representations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {213},\nnumpages = {17},\nkeywords = {Visualization, annotation, design, maps, text},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642556,\nauthor = {Houben, Maarten and Brankaert, Rens and Gosen, Maudy and van Overloop, Veerle and IJsselsteijn, Wijnand},\ntitle = {Design Opportunities for Care Transitions in Dementia: Understanding Informal Caregivers’ Experiences Through a Practice-Informed Approach},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642556},\ndoi = {10.1145/3613904.3642556},\nabstract = {The transition from home to formal residential care is described as stressful and emotionally difficult for people with dementia and their informal caregivers. While HCI research investigated how technology supports people with dementia at home or in formal care, there still is a need to understand how technology can support care transitions. This paper presents a practice-informed approach to gather insights collaboratively between care professionals and HCI researchers. We interviewed 42 informal caregivers of people with dementia to uncover their experiences before, during, and after care transitions. Our findings reveal how informal caregivers were: 1) navigating hurdles of information on care transitions, 2) caught up in the evolving challenges of informal caregiving, and 3) shifting from uncertainty in decision-making to acceptance of admission. Next, we formulate six design opportunities to support transitions in dementia care and encourage HCI researchers to pursue a practice-informed approach to address societal challenges in dementia.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {214},\nnumpages = {13},\nkeywords = {Dementia, Design, Informal caregiver, Interview, Transitions in care},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642190,\nauthor = {Ruitenburg, Yvon and Lee, Minha and Markopoulos, Panos and IJsselsteijn, Wijnand},\ntitle = {Evolving Presentation of Self: The Influence of Dementia Communication Challenges on Everyday Interactions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642190},\ndoi = {10.1145/3613904.3642190},\nabstract = {Communication can become challenging for people with dementia due to language, speech, discourse, and memory impairments. Although recent developments in Human-Computer Interaction have addressed some of these communication challenges, little is known about how they affect the self-presentation of people with dementia in everyday interactions. To understand this connection, we conducted interviews with sixteen people with dementia, six spouses, and fourteen formal caregivers. Our qualitative data revealed that people with dementia’s presentation of competence, politeness, engagement, and reality are altered by communication challenges, which can impact their self-esteem, interactions, and relationships. Our study highlights the need for developing technologies that can enhance mutual understanding and acceptance of people with dementia’s evolving presentation of self. Additionally, policy changes are required to reduce the stigma associated with communication challenges to foster social inclusion.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {215},\nnumpages = {16},\nkeywords = {Communication, Dementia, Presentation of Self, Stigma, Technology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642308,\nauthor = {Vidas, Dianna and Thompson, Zara and Kelly, Ryan M and Waycott, Jenny and Tamplin, Jeanette and Vieira Sousa, Tanara and Kulik, Lars and Lampit, Amit and Lautenschlager, Nicola T and Baker, Felicity A},\ntitle = {Family Caregiver Experiences of Using a Mobile App for Music-based Training to Support Dementia Care},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642308},\ndoi = {10.1145/3613904.3642308},\nabstract = {Family caregivers of people living with dementia need easy-to-access strategies to manage changing care needs. Music therapy is valuable for supporting dementia care, but not always accessible. Technologies could potentially facilitate accessible, home-based music therapy support, but need to be carefully evaluated. We conducted an 8-week field trial of a prototype mobile application, MATCH, with caregivers and people living with dementia. MATCH contains training videos and suggested playlists showing how to use music for specific care needs. MATCH, and music streaming broadly, enabled caregivers to add new strategies to their care repertoire, addressing a range of care needs and enhancing the care relationship. To make MATCH work, however, caregivers needed to fit it into complex care environments and existing technologies. We argue that digital therapeutic tools need to be adopted by caregivers to fit their individual contexts, and this can challenge assumptions about how therapeutic tools will work in practice.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {216},\nnumpages = {16},\nkeywords = {Caregiving, Dementia care, Family caregivers, Mobile app, Music},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642783,\nauthor = {Baumann, Andrea and Shaw, Peter and Trotter, Ludwig and Clinch, Sarah and Davies, Nigel},\ntitle = {Mnemosyne - Supporting Reminiscence for Individuals with Dementia in Residential Care Settings},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642783},\ndoi = {10.1145/3613904.3642783},\nabstract = {Reminiscence is known to play an important part in helping to mitigate the effects of dementia. Within the HCI community, work has typically focused on supporting reminiscence at an individual or social level but less attention has been given to supporting reminiscence in residential care settings. This lack of research became particularly apparent during the COVID pandemic when traditional forms of reminiscence involving physical artefacts and face-to-face interactions became especially challenging. In this paper we report on the design, development and evaluation of a reminiscence system, deployed in a residential care home over a two-year-period that included the pandemic. Mnemosyne comprises a pervasive display network and a browser-based application whose adoption and use we explored using a mixed methods approach. Our findings offer insights that will help shape the development and evaluation of future systems, particularly those that use pervasive displays to support unsupervised reminiscence.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {217},\nnumpages = {17},\nkeywords = {dementia, pervasive displays, reminiscence, residential care},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641977,\nauthor = {Sun, Yuling and Yi, Zhennan and Ma, Xiaojuan and Mao, Junyan and Tong, Xin},\ntitle = {Technology-Mediated Non-pharmacological Interventions for Dementia: Needs for and Challenges in Professional, Personalized and Multi-Stakeholder Collaborative Interventions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641977},\ndoi = {10.1145/3613904.3641977},\nabstract = {Designing and using technologies to support Non-Pharmacological Interventions (NPI) for People with Dementia (PwD) has drawn increasing attention in HCI, with the potential expectations of higher user engagement and positive outcomes. Yet, technologies for NPI can only be valuable if practitioners successfully incorporate them into their ongoing intervention practices beyond a limited research period. Currently, we know little about how practitioners experience and perceive these technologies in practical NPI for PwD. In this paper, we investigate this question through observations of five in-person NPI activities and interviews with 11 therapists and 5 caregivers. Our findings elaborate the practical NPI workflow process and characteristics, and practitioners’ attitudes, experiences, and perceptions to technology-mediated NPI in practice. Generally, our participants emphasized practical NPI is a complex and professional practice, needing fine-grained, personalized evaluation and planning, and the practical executing process is situated, and multi-stakeholder collaborative. Yet, existing technologies often fail to consider these specific characteristics, which leads to limitations in practical effectiveness or sustainable use. Drawing on our findings, we discuss the possible implications for designing more useful and practical NPI intervention technologies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {218},\nnumpages = {19},\nkeywords = {Adoption, Non-Pharmacological Interventions (NPI), Older adults, People with Dementia (PwD), Practitioners, Real-world Experience, Technology, Therapists},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642250,\nauthor = {Doyle, Dylan Thomas and Brubaker, Jed R.},\ntitle = {\"I Am So Overwhelmed I Don't Know Where to Begin!\" Towards Developing Relationship-Based and Values-Based End-of-Life Data Planning Approaches},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642250},\ndoi = {10.1145/3613904.3642250},\nabstract = {To support people at the end of life as they create management plans for their assets, planning approaches like estate planning are increasingly considering data. HCI scholarship has argued that developing more effective planning approaches to support end-of-life data planning is important. However, empirical research is needed to evaluate specific approaches and identify design considerations. To support end-of-life data planning, this paper presents a qualitative study evaluating two approaches to co-designing end-of-life data plans with participants. We find that asset-first inventory-centric approaches, common in material estate planning, may be ineffective when making plans for data. In contrast, heavily facilitated, mission-driven, relationship-centric approaches were more effective. This study expands previous research by validating the importance of starting end-of-life data planning with relationships and values, and highlights collaborative facilitation as a critical part of successful data planning approaches.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {219},\nnumpages = {14},\nkeywords = {data planning, death, digital legacy, end of life, heirlooms, identity, inheritance, legacy, memorial, memory, stewardship},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641953,\nauthor = {Pais, Pedro and Gon\\c{c}alves, David and Reis, Daniel and Godinho, Jo\\~{a}o Cadete Nunes and Morais, Jo\\~{a}o Filipe and Pi\\c{c}arra, Manuel and Trindade, Pedro and Alexandrovsky, Dmitry and Gerling, Kathrin and Guerreiro, Jo\\~{a}o and Rodrigues, Andr\\'{e}},\ntitle = {A Living Framework for Understanding Cooperative Games},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641953},\ndoi = {10.1145/3613904.3641953},\nabstract = {Playing cooperative games is recognised as a positive social activity. Yet, we have limited means to rigorously define or communicate the structures that govern these experiences, hindering attempts at consolidating knowledge and limiting the potential of design efforts. In this work, we introduce the Living Framework for Cooperative Games (LFCG), a framework derived from a multi-step systematic analysis of 129 cooperative games with contributions of eleven researchers. We describe how LFCG can be used as a tool for analyses and ideation, and as a shared language for describing a game’s design. LFCG is published as a web application to facilitate use and appropriation. It supports the creation, dissemination and aggregation of game reports and specifications; and enables stakeholders to extend and publish custom versions. Lastly, we discuss using a research-driven approach for formalising game structures and the advantages of community contributions for consolidation and reach.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {220},\nnumpages = {17},\nkeywords = {Cooperative Games, Formalisation, Framework, Game Design, Qualitative Analysis},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642886,\nauthor = {Son, Kihoon and Choi, DaEun and Kim, Tae Soo and Kim, Juho},\ntitle = {Demystifying Tacit Knowledge in Graphic Design: Characteristics, Instances, Approaches, and Guidelines},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642886},\ndoi = {10.1145/3613904.3642886},\nabstract = {Despite the growing demand for professional graphic design knowledge, the tacit nature of design inhibits knowledge sharing. However, there is a limited understanding on the characteristics and instances of tacit knowledge in graphic design. In this work, we build a comprehensive set of tacit knowledge characteristics through a literature review. Through interviews with 10 professional graphic designers, we collected 123 tacit knowledge instances and labeled their characteristics. By qualitatively coding the instances, we identified the prominent elements, actions, and purposes of tacit knowledge. To identify which instances have been addressed the least, we conducted a systematic literature review of prior system support to graphic design. By understanding the reasons for the lack of support on these instances based on their characteristics, we propose design guidelines for capturing and applying tacit knowledge in design tools. This work takes a step towards understanding tacit knowledge, and how this knowledge can be communicated.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {221},\nnumpages = {18},\nkeywords = {Design Guideline, Graphic Design, Knowledge Capturing, Knowledge Sharing, Tacit Knowledge},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642340,\nauthor = {Takashita, Shuto and Arai, Ken and Saito, Hiroto and Kitazaki, Michiteru and Inami, Masahiko},\ntitle = {Embodied Tentacle: Mapping Design to Control of Non-Analogous Body Parts with the Human Body},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642340},\ndoi = {10.1145/3613904.3642340},\nabstract = {Manipulating a non-humanoid body using a mapping approach that translates human body activity into different structural movements enables users to perform tasks that are difficult with their innate bodies. However, a key challenge is how to design an effective mapping to control non-analogous body parts with the human body. To address this challenge, we designed an articulated virtual arm and investigated the effect of mapping methods on a user’s manipulation experience. Specifically, we developed an unbranched 12-joint virtual arm with an octopus-like appearance. Using this arm, we conducted a user study to compare the effects of several mapping methods with different arrangements on task performance and subjective evaluations of embodiment and user preference. As a result, we identified three important factors in mapping: “Visual and Configurational Similarity”, “Kinematics Suitability for the User”, and “Correspondence with Everyday Actions.” Based on these findings, we discuss a mapping design for non-humanoid body manipulation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {222},\nnumpages = {19},\nkeywords = {Virtual Reality, body schema, embodiment, gestural interaction, human augmentation, non-anthropomorphic avatars, non-humanoid avatars},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642609,\nauthor = {Jensen, Victor Vadmand and Laursen, Kristina and Jensen, Rikke Hagensby and Smith, Rachel Charlotte},\ntitle = {Imagining Sustainable Energy Communities: Design Narratives of Future Digital Technologies, Sites, and Participation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642609},\ndoi = {10.1145/3613904.3642609},\nabstract = {Increasingly, research projects narrate visions of energy communities that portray hopes of more sustainable, democratic energy futures. However, it remains unarticulated how such research narratives are embedded in the design of digital technology for communal energy futures that are situated in everyday life. While sustainable HCI has identified relevant design narratives, little attention has been paid to those of communal energy projects. In this paper, we scope energy community literature at ACM to identify design narratives that tell stories about how energy communities are imagined and why they are relevant. Through a critical discourse analysis, we describe how design narratives currently shape energy community research on sites, participation, and digital technologies. We use these stories to discuss and suggest three trajectories of how future HCI researchers and practitioners may explore alternative and sustainable visions of energy community futures.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {223},\nnumpages = {17},\nkeywords = {design narratives, discourse, energy communities, energy community, energy technologies, future, participation, sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642560,\nauthor = {Oogjes, Doenja and Desjardins, Audrey},\ntitle = {A temporal vocabulary of Design Events for Research through Design},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642560},\ndoi = {10.1145/3613904.3642560},\nabstract = {Much reporting on research-through-design (RtD) is vague about markers of time and temporal qualities. This lack of temporal attunement risks obscuring important contextual knowledge, hidden labour, material agencies and potential knowledge contributions. We turn to the notion of the event to articulate the granularities and nuances of RtD processes with an expanded vocabulary. We draw on prior calls from RtD practitioners, the philosophical roots of events, and our previous work with the term in our own research. We describe seven terms to expand the temporal vocabulary of RtD, which can be used to build narratives that emphasize knowledge created along the way, and relieve pressure from the ‘final’ artifact. Our contributions are 1) design events as an ontological shift and analytical tool and 2) a vocabulary that scaffolds design events as a sensitizing tool. We end with a call for more experimentation of non-chronological narratives of RtD.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {224},\nnumpages = {12},\nkeywords = {design events, design process, design research, dissemination, research-through-design, storytelling, temporality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642052,\nauthor = {Mueller, Florian ‘Floyd’ and Montoya, Maria F. and Pell, Sarah Jane and Oppermann, Leif and Blythe, Mark and Dietz, Paul H and Marshall, Joe and Bateman, Scott and Smith, Ian and Ananthanarayan, Swamy and Mazalek, Ali and Verni, Alexander and Bakogeorge, Alexander and Simonnet, Mathieu and Ellis, Kirsten and Semertzidis, Nathan Arthur and Burleson, Winslow and Quarles, John and Mann, Steve and Hill, Chris and Clashing, Christal and Elvitigala, Don Samitha},\ntitle = {Grand challenges in WaterHCI},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642052},\ndoi = {10.1145/3613904.3642052},\nabstract = {Recent combinations of interactive technology, humans, and water have resulted in “WaterHCI”. WaterHCI design seeks to complement the many benefits of engagement with the aquatic domain, by offering, for example, augmented reality systems for snorkelers, virtual reality in floatation tanks, underwater musical instruments for artists, robotic systems for divers, and wearables for swimmers. We conducted a workshop in which WaterHCI experts articulated the field's grand challenges, aiming to contribute towards a systematic WaterHCI research agenda and ultimately advance the field.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {225},\nnumpages = {18},\nkeywords = {fluid user interfaces, grand challenges, human-water interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642283,\nauthor = {Wang, Ge and Zhao, Jun and Kollnig, Konrad and Zier, Adrien and Duron, Blanche and Zhang, Zhilin and Van Kleek, Max and Shadbolt, Nigel},\ntitle = {KOALA Hero Toolkit: A New Approach to Inform Families of Mobile Datafication Risks},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642283},\ndoi = {10.1145/3613904.3642283},\nabstract = {Children today are deeply immersed in the online world, where their activities are routinely tracked, analysed, and monetised. This exposes them to various datafication risks, including harmful profiling, micro-targeting and behavioural engineering. Most existing measures focus on immediate online threats, rather than informing children about these implicit risks. In this paper, we present The KOALA Hero Toolkit, a hybrid toolkit designed to help children and parents jointly understand the datafication risks posed by their mobile apps. Through user studies involving 17 families we evaluate how the toolkit influenced families’ thought processes, perceptions and decision-making regarding mobile datafication risks. Our findings show that KOALA Hero supports families’ critical thinking and promotes family engagement. We identify future design recommendations for family support, featuring ideas such as integrating triggering moments and bonding moments in toolkit designs. This work provides timely inputs on global efforts aimed at addressing datafication risks and underscores the importance of strengthening legislative and policy enforcement of ethical data governance.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {226},\nnumpages = {18},\nkeywords = {Children online privacy, children online safety, family, mobile apps},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642409,\nauthor = {Lev Ari, Eilat and Roichman, Maayan and Toch, Eran},\ntitle = {Strategies of Product Managers: Negotiating Social Values in Digital Product Design},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642409},\ndoi = {10.1145/3613904.3642409},\nabstract = {Product managers are central figures in digital product development, coordinating teams and prioritizing features. Despite their influence, little research explores how their decisions affect user experience, especially in integrating social values into product architecture. Employing a mixed-methods framework, we conducted semi-structured interviews with 20 product managers and an online survey with an additional 81, all based in Israel. Our study identifies four unique strategies product managers utilize to balance business goals, user satisfaction, and ethical considerations. The survey data further substantiates the prevalence of these strategies across diverse sectors, confirming they reflect industry-wide approaches in the Israeli tech sector rather than isolated practices. To conclude, we emphasize how “soft resistance” tactics, such as adjusting data interpretations based on personal values, impact digital product designs. Moreover, our findings highlight that maintaining an ethical reputation in the job market can be pivotal in shaping product design.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {227},\nnumpages = {17},\nkeywords = {Product managers, design, ethics, values},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642458,\nauthor = {Mackay, Wendy E. and Battut, Alexandre and Leiva, Germ\\`{a}n and Beaudouin-Lafon, Michel},\ntitle = {VideoClipper: Rapid Prototyping with the \"Editing-in-the-Camera\" Method},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642458},\ndoi = {10.1145/3613904.3642458},\nabstract = {Although video is extremely useful for expressing interaction, post-hoc editing makes it impractical for rapid prototyping. We present an early-stage video-based design method — “editing-in-the-camera” — where title cards guide video capture and label video clips. This method lets designers easily create video prototypes that can be discussed within the same design session, without further editing. We present VideoClipper, a mobile app that incorporates this method by transforming sequences of title cards into an interactive storyboard that designers can shoot into directly. VideoClipper offers simple special effects to better illustrate user interaction with paper prototypes, including ghosting and stop-motion animation. We also present Collaborative VideoClipper, which was created during the COVID-19 pandemic to support multi-user, multi-device rapid prototyping with remote participants. We describe evaluation results of both applications and describe our experiences in diverse educational and professional settings, including brainstorming, interviewing, video prototyping, user studies and participatory design workshops.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {228},\nnumpages = {14},\nkeywords = {Interaction design, Participatory design, Rapid prototyping, Shooting video, Storyboards, Video brainstorming, Video prototyping},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642090,\nauthor = {John, Kevin and Li, Yinan and Seifi, Hasti},\ntitle = {AdapTics: A Toolkit for Creative Design and Integration of Real-Time Adaptive Mid-Air Ultrasound Tactons},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642090},\ndoi = {10.1145/3613904.3642090},\nabstract = {Mid-air ultrasound haptic technology can enhance user interaction and immersion in extended reality (XR) applications through contactless touch feedback. Yet, existing design tools for mid-air haptics primarily support creating tactile sensations (i.e., tactons) which cannot change at runtime. These tactons lack expressiveness in interactive scenarios where a continuous closed-loop response to user movement or environmental states is desirable. This paper introduces AdapTics, a toolkit featuring a graphical interface for rapid prototyping of adaptive tactons—dynamic sensations that can adjust at runtime based on user interactions, environmental changes, or other inputs. A software library and a Unity package accompany the graphical interface to enable integration of adaptive tactons in existing applications. We present the design space offered by AdapTics for creating adaptive mid-air ultrasound tactons and show the design tool can improve Creativity Support Index ratings for Exploration and Expressiveness in a user study with 12 XR and haptic designers.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {229},\nnumpages = {15},\nkeywords = {design tool, haptic design, mid-air ultrasound haptics, real-time adaptation, tacton},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642673,\nauthor = {Alvarado Garcia, Adriana and Wong-Villacres, Marisol and Hern\\'{a}ndez, Benjam\\'{\\i}n and Le Dantec, Christopher A},\ntitle = {Bitacora: A Toolkit for Supporting NonProfits to Critically Reflect on Social Media Data Use},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642673},\ndoi = {10.1145/3613904.3642673},\nabstract = {In this paper, we describe the design and evaluation of the toolkit Bitacora, addressed to practitioners working in non-profit organizations interested in integrating Twitter data into their work. The toolkit responds to the call to maintain the locality of data by promoting a qualitative and contextualized approach to analyzing Twitter data. We assessed the toolkit’s effectiveness in guiding practitioners to search, collect, and be critical when analyzing data from Twitter. We evaluated the toolkit with ten practitioners from three non-profit organizations of different aims and sizes in Mexico. The assessment surfaced tensions between the assumptions embedded in the toolkit’s design and practitioners’ expectations, needs, and backgrounds. We show that practitioners navigated these tensions in some cases by developing strategies and, in others, questioning the appropriateness of using Twitter data to inform their work. We conclude with recommendations for researchers who developed tools for non-profit organizations to inform humanitarian action.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {230},\nnumpages = {29},\nkeywords = {NGO, data annotation, data experts, data work, design, ground truth, humanitarian context, non-profit, social media data, toolkits, user-generated content},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641916,\nauthor = {Hao, Shan and Wang, Zezhong and Bach, Benjamin and Pschetz, Larissa},\ntitle = {Design Patterns for Data-Driven News Articles},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641916},\ndoi = {10.1145/3613904.3641916},\nabstract = {Technological advancements have resulted in great shifts in the production and consumption of news articles. This, in turn, lead to the requirement of new educational and practical frameworks. In this paper, we present a classification of data-driven news articles and related design patterns defined to describe their visual and textual components. Through the analysis of 162 data-driven news articles collected from news media, we identified five types of articles based on the level of data involvement and narrative complexity: Quick Update, Briefing, Chart Description, Investigation, and In-depth Investigation. We then identified 72 design patterns to understand and construct data-driven news articles. To evaluate this approach, we conducted workshops with 23 students from journalism, design, and sociology who were newly introduced to the subject. Our findings suggest that our approach can be used as an out-of-box framework for the formulation of plans and consideration of details in the workflow of data-driven news creation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {231},\nnumpages = {16},\nkeywords = {Classification, Data Journalism, Data-Driven Storytelling, Design Patterns, Education},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642681,\nauthor = {Wilner, Tamar and Adavi, Krishna Akhil Kumar and Mandava, Sreehana and Bhimdiwala, Ayesha and Frluckaj, Hana and Turns, Jennifer and Arif, Ahmer},\ntitle = {From Concept to Community: Unpacking the Work of Designing Educational and Activist Toolkits},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642681},\ndoi = {10.1145/3613904.3642681},\nabstract = {Toolkits are an important means of sharing expertise and influencing practice. However, the work of making and sustaining toolkits is not well understood. We address this gap by conducting 20 semi-structured interviews with toolkit designers, focusing on toolkits intended to help practitioners such as librarians, teachers, and community workers. We analyze these interviews to surface key aspects of participants’ design journeys: (1) how their projects began; (2) how they conceptualized use; (3) how they collaborated with users; (4) and what happened once their toolkit was released. We illustrate these aspects through three narratives, and discuss our findings to provide considerations for designers and scholars. We highlight how designers co-construct communities alongside their toolkits, helping us form a more nuanced understanding of the social aspects underpinning toolkit projects. Collectively, these contributions can help us identify challenges and opportunities in this design space, laying the groundwork to increase toolkits’ social impact.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {232},\nnumpages = {15},\nkeywords = {activism, appropriation, design, education, evaluation, participatory design, toolkits},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642121,\nauthor = {Kato, Jun and Hara, Kenta and Hirasawa, Nao},\ntitle = {Griffith: A Storyboarding Tool Designed with Japanese Animation Professionals},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642121},\ndoi = {10.1145/3613904.3642121},\nabstract = {The “E-conte,” storyboard in English, is commonly referred to as the “blueprint” in Japanese animation (anime) production, consisting of scene illustrations, timing information, and textual descriptions. This paper introduces “Griffith,” a digital system for creating these storyboards. Due to its highly cultural and domain-specific nature, the tool design entailed an in-depth study of the E-conte process and a longitudinal collaboration with an experienced anime director and producers. The resulting system contributes not only domain knowledge, but also generalizable insights into a creativity support environment for visual storytelling, including the importance of vertical timelines and discrete yet integrated tools. To reflect on the interaction design, we presented Griffith to professionals with diverse roles in anime production. Our findings highlight the benefits of the Griffith user interface and the need for a socio-technical focus in designing creativity support tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {233},\nnumpages = {14},\nkeywords = {Creativity support, animation, storyboard, user interface},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642363,\nauthor = {Chen, Chaoran and Li, Weijun and Song, Wenxin and Ye, Yanfang and Yao, Yaxing and Li, Toby Jia-Jun},\ntitle = {An Empathy-Based Sandbox Approach to Bridge the Privacy Gap among Attitudes, Goals, Knowledge, and Behaviors},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642363},\ndoi = {10.1145/3613904.3642363},\nabstract = {Managing privacy to reach privacy goals is challenging, as evidenced by the privacy attitude-behavior gap. Mitigating this discrepancy requires solutions that account for both system opaqueness and users’ hesitations in testing different privacy settings due to fears of unintended data exposure. We introduce an empathy-based approach that allows users to experience how privacy attributes may alter system outcomes in a risk-free sandbox environment from the perspective of artificially generated personas. To generate realistic personas, we introduce a novel pipeline that augments the outputs of large language models (e.g., GPT-4) using few-shot learning, contextualization, and chain of thoughts. Our empirical studies demonstrated the adequate quality of generated personas and highlighted the changes in privacy-related applications (e.g., online advertising) caused by different personas. Furthermore, users demonstrated cognitive and emotional empathy towards the personas when interacting with our sandbox. We offered design implications for downstream applications in improving user privacy literacy.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {234},\nnumpages = {28},\nkeywords = {empathy, generated personas, privacy awareness, privacy intervention, privacy literacy, sandbox},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642713,\nauthor = {Zhang, Lotus and Stangl, Abigale and Sharma, Tanusree and Tseng, Yu-Yun and Xu, Inan and Gurari, Danna and Wang, Yang and Findlater, Leah},\ntitle = {Designing Accessible Obfuscation Support for Blind Individuals’ Visual Privacy Management},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642713},\ndoi = {10.1145/3613904.3642713},\nabstract = {Blind individuals commonly share photos in everyday life. Despite substantial interest from the blind community in being able to independently obfuscate private information in photos, existing tools are designed without their inputs. In this study, we prototyped a preliminary screen reader-accessible obfuscation interface to probe for feedback and design insights. We implemented a version of the prototype through off-the-shelf AI models (e.g., SAM, BLIP2, ChatGPT) and a Wizard-of-Oz version that provides human-authored guidance. Through a user study with 12 blind participants who obfuscated diverse private photos using the prototype, we uncovered how they understood and approached visual private content manipulation, how they reacted to frictions such as inaccuracy with existing AI models and cognitive load, and how they envisioned such tools to be better designed to support their needs (e.g., guidelines for describing visual obfuscation effects, co-creative interaction design that respects blind users’ agency).},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {235},\nnumpages = {19},\nkeywords = {accessibility, blind photography, privacy-preservation technology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642872,\nauthor = {Grover, Rohan},\ntitle = {Encoding Privacy: Sociotechnical Dynamics of Data Protection Compliance Work},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642872},\ndoi = {10.1145/3613904.3642872},\nabstract = {How do developers shape data protection regulations when they are passed from the policy arena to technical teams for compliance? This study explores data protection compliance work (DPCW) as a sociotechnical process mediated by developers’ attitudes and experiences. We draw on 14 semi-structured interviews with individuals responsible for GDPR and/or CCPA compliance to examine how developers approach DPCW and the resulting implications for user privacy. We highlight three key ways in which developers can shape compliance: by creatively interpreting ambiguous regulatory requirements; by exploiting expectations of technical expertise and low accountability; and by reducing DPCW to a one-time project. We conclude by discussing the implications for both researchers and practitioners and by recommending how to conceptualize and conduct DPCW otherwise. This article adds specificity to understanding why and how developers’ attitudes and experiences affect data protection regulations in the field.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {236},\nnumpages = {13},\nkeywords = {CCPA, GDPR, compliance, data protection, developer studies, personal data, user privacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642594,\nauthor = {Li, Tony W and Arya, Arshia and Jin, Haojian},\ntitle = {Redesigning Privacy with User Feedback: The Case of Zoom Attendee Attention Tracking},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642594},\ndoi = {10.1145/3613904.3642594},\nabstract = {Software engineers’ unawareness of user feedback in earlier stages of design contributes to privacy issues in many products. Although extensive research exists on gathering and analyzing user feedback, there is limited understanding about how developers can integrate user feedback to improve product designs to better meet users’ privacy expectations. We use Zoom’s deprecated attendee attention tracking feature to explore issues with integrating user privacy feedback into software development, presenting public online critiques about this deprecated feature to 18 software engineers in semi-structured interviews and observing how they redesign this feature. Our results suggest that while integrating user feedback for privacy is potentially beneficial, it’s also fraught with challenges of polarized design suggestions, confirmation bias, and limited scope of perceived responsibility.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {237},\nnumpages = {14},\nkeywords = {human-centered design, privacy by design, privacy engineering, software development},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641961,\nauthor = {Calambur, Veena and Jun, Dongwhan and Schiaffino, Melody K and Zhang, Zhan and Huh-Yoo, Jina},\ntitle = {A case for \"little English\" in Nurse Notes from the Telehealth Intervention Program for Seniors: Implications for Future Design and Research},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641961},\ndoi = {10.1145/3613904.3641961},\nabstract = {Community telehealth programs (CTPs) enable low-income older adults to receive telehealth services in community settings (e.g., retirement homes). The Telehealth Intervention Program for Seniors (TIPS) is a CTP that provides vital sign monitoring services managed by remote nurses. TIPS has successfully recruited and retained Limited English Proficient (LEP) participants, but lack of language services might hinder LEP participants’ equitable access to care. We conducted a two-part mixed-methods study. We first qualitatively analyzed 40 nurse notes to identify challenges nurses encounter gathering information due to language barriers and the workarounds they employed to address these. We then tested our qualitative findings on 23,975 nurse notes to quantify and compare how these challenges and workarounds scale between LEP and English-proficient TIPS participants. We present future research implications beyond low-hanging solutions, such as automated translation services, and discuss how novel technological solutions can support and ameliorate nurse workarounds and caregiver burden.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {238},\nnumpages = {16},\nkeywords = {community health, information gathering, language barrier, older adult, remote patient monitoring, telehealth},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641939,\nauthor = {Aghajari, Zhila and Baumer, Eric P. S. and Lazard, Allison and Dasgupta, Nabarun and DiFranzo, Dominic},\ntitle = {Investigating the Mechanisms by which Prevalent Online Community Behaviors Influence Responses to Misinformation: Do Perceived Norms Really Act as a Mediator?},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641939},\ndoi = {10.1145/3613904.3641939},\nabstract = {This study addresses two currently open questions about how behaviors of online community members influence others’ responses to misinformation. First, in contrast to prior work, it directly measures norm perception to address whether (1) norm perception actually acts as a mediator, (2) others’ behaviors directly influence individuals’ responses to misinformation, (3) both direct and mediated effects occur. Second, it investigates norm perceptions about a behavior that is not readily observable in online communities, but is prone to misinformation, specifically, vaccination. To do so, it experimentally manipulates the prevalence of communicating about vaccination (an unobservable behavior) within an online community. The results demonstrate no evidence of a direct effect—the causal relationship between prevalence of communicating a behavior and intentions to respond to misinformation only occurs via norm perception as a mediator. The paper highlights implications of these findings for designing community-centered interventions to influence perceived norms, thereby mitigating misinformation spread and impacts.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {239},\nnumpages = {14},\nkeywords = {Community-centered Interventions, Misinformation, Social Norms},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642161,\nauthor = {Waddell, Alex and Seguin, Joshua Paolo and Wu, Ling and Stragalinos, Peta and Wherton, Joe and Watterson, Jessica L and Prawira, Christopher Owen and Olivier, Patrick and Manning, Victoria and Lubman, Dan and Grigg, Jasmin},\ntitle = {Leveraging Implementation Science in Human-Centred Design for Digital Health},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642161},\ndoi = {10.1145/3613904.3642161},\nabstract = {There are increasing concerns that digital interventions in healthcare settings could be better designed for scalable and sustained use. Implementation science is the scientific study of how to embed evidence-based interventions in practice. Calls to integrate implementation science and Human-Centred Design methods have focused on integrating design methods within implementation science processes. By contrast, we present a novel approach to integrating implementation science within Human-Centred Design for digital health interventions. Our approach leverages the socio-technical Nonadoption, abandonment, scale-up, spread, and sustainability (NASSS) framework within the distinct phases of the Double Diamond process. To illustrate our proposal we demonstrate its application in the redesign of a brief health promotion intervention to reduce the risk of alcohol-attributable breast cancer in women attending routine mammography. We discuss reflections on the approach and implications for future research that targets implementation within design.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {240},\nnumpages = {17},\nkeywords = {Digital Health Infrastructure, Health, Implementation Science, Intervention Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642630,\nauthor = {Liaqat, Salaar and Liaqat, Daniyal and Son, Tatiana and Falk, Tiago and Wu, Robert and Gershon, Andrea S. and De Lara, Eyal and Mariakakis, Alex},\ntitle = {Promoting Engagement in Remote Patient Monitoring Using Asynchronous Messaging},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642630},\ndoi = {10.1145/3613904.3642630},\nabstract = {Remote patient monitoring is becoming increasingly instrumental to healthcare delivery but can substantially hamper the interpersonal communication that underlies standard clinical practice. In this work, we explore the benefits imparted to patients, clinicians, and researchers by an asynchronous messaging feature within a platform called COVIDFree@Home. We created COVIDFree@Home to assist the healthcare system in a large metropolitan city in North America during the COVID-19 pandemic. Clinicians used COVIDFree@Home to monitor the self-reported symptoms and vital signs of over 350 COVID-19 patients post-infection. Using thematic analysis of user-initiated messages, we found the messaging feature helped maintain protocol adherence while allowing patients to ask questions about their health and clinicians to convey empathetic care. This feedback cycle also led to higher quality data for hospitalization prediction, as the revisions significantly improved the AUROC of a machine learning model trained on demographic variables, vital signs data, and self-reported symptoms from 0.53 to 0.59.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {241},\nnumpages = {18},\nkeywords = {COVID-19, adherence, asynchronous messaging, engagement, real-world deployment, remote patient monitoring},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642912,\nauthor = {Xiong, Peixuan and Zhang, Yukai and Zhang, Nandi and Fu, Shihan and Li, Xin and Zheng, Yadan and Zhou, Jinni and Hu, Xiquan and Fan, Mingming},\ntitle = {To Reach the Unreachable: Exploring the Potential of VR Hand Redirection for Upper Limb Rehabilitation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642912},\ndoi = {10.1145/3613904.3642912},\nabstract = {Rehabilitation therapies are widely employed to assist people with motor impairments in regaining control over their affected body parts. Nevertheless, factors such as fatigue and low self-efficacy can hinder patient compliance during extensive rehabilitation processes. Utilizing hand redirection in virtual reality (VR) enables patients to accomplish seemingly more challenging tasks, thereby bolstering their motivation and confidence. While previous research has investigated user experience and hand redirection among able-bodied people, its effects on motor-impaired people remain unexplored. In this paper, we present a VR rehabilitation application that harnesses hand redirection. Through a user study and semi-structured interviews, we examine the impact of hand redirection on the rehabilitation experiences of people with motor impairments and its potential to enhance their motivation for upper limb rehabilitation. Our findings suggest that patients are not sensitive to hand movement inconsistency, and the majority express interest in incorporating hand redirection into future long-term VR rehabilitation programs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {242},\nnumpages = {11},\nkeywords = {Motor impairments, Upper limb rehabilitation, Virtual hand redirection},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642370,\nauthor = {Haliburton, Luke and Gr\\\"{u}ning, David Joachim and Riedel, Frederik and Schmidt, Albrecht and Terzimehi\\'{c}, Na\\dj{}a},\ntitle = {A Longitudinal In-the-Wild Investigation of Design Frictions to Prevent Smartphone Overuse},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642370},\ndoi = {10.1145/3613904.3642370},\nabstract = {Smartphone overuse is hyper-prevalent in society, and developing tools to prevent this overuse has become a focus of HCI. However, there is a lack of work investigating smartphone overuse interventions over the long term. We collected usage data from N = 1, 039 users of one sec over an average of 13.4 weeks and qualitative insights from 249 of the users through an online survey. We found that users overwhelmingly choose to target Social Media apps. We found that the short design frictions introduced by one sec effectively reduce how often users attempt to open target apps and lead to more intentional app-openings over time. Additionally, we found that users take periodic breaks from one sec interventions, and quickly rebound from a pattern of overuse when returning from breaks. Overall, we contribute findings from a longitudinal investigation of design frictions in the wild and identify usage patterns from real users in practice.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {243},\nnumpages = {16},\nkeywords = {behavior change, design frictions, long-term, smartphone overuse},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642505,\nauthor = {Sathya, Anup and Nakagaki, Ken},\ntitle = {Attention Receipts: Utilizing the Materiality of Receipts to Improve Screen-time Reflection on YouTube},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642505},\ndoi = {10.1145/3613904.3642505},\nabstract = {YouTube remains a site of problematic persuasive media consumption, often overriding the goals of users when on the platform. In resistance, we present Attention Receipts — artifacts that materialize the cost of being persuaded by the engagement driven design of YouTube. We design and build a browser plugin and a receipt printer that helps users critically reflect upon their time spent watching videos on YouTube. In a 3 week field-deployment with 6 participants, we evaluate how the materiality of the receipt and their agency in the reflection process affect both the quality of reflection and the time spent consuming media. We find that the materiality of the receipts positively influences time spent consuming internet media and that users were split on having agency over when and how they reflect upon their screen-time. We conclude with design recommendations for domestic artifacts that utilize materiality to reveal the effects of persuasive technology.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {244},\nnumpages = {16},\nkeywords = {agency, ambient objects, digital wellbeing, internet of things, materiality, persuasive technology, screen-time reflection, slow technology, youtube},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642317,\nauthor = {Lu, Tao and Zheng, Hongxiao and Zhang, Tianying and Xu, Xuhai “Orson” and Guo, Anhong},\ntitle = {InteractOut: Leveraging Interaction Proxies as Input Manipulation Strategies for Reducing Smartphone Overuse},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642317},\ndoi = {10.1145/3613904.3642317},\nabstract = {Smartphone overuse poses risks to people’s physical and mental health. However, current intervention techniques mainly focus on explicitly changing screen content (i.e., output) and often fail to persistently reduce smartphone overuse due to being over-restrictive or over-flexible. We present the design and implementation of InteractOut, a suite of implicit input manipulation techniques that leverage interaction proxies to weakly inhibit the natural execution of common user gestures on mobile devices. We present a design space for input manipulations and demonstrate 8 Android implementations of input interventions. We first conducted a pilot lab study (N=30) to evaluate the usability of these interventions. Based on the results, we then performed a 5-week within-subject field experiment (N=42) to evaluate InteractOut in real-world scenarios. Compared to the traditional and common timed lockout technique, InteractOut significantly reduced the usage time by an additional 15.6\\% and opening frequency by 16.5\\% on participant-selected target apps. InteractOut also achieved a 25.3\\% higher user acceptance rate, and resulted in less frustration and better user experience according to participants’ subjective feedback. InteractOut demonstrates a new direction for smartphone overuse intervention and serves as a strong complementary set of techniques with existing methods.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {245},\nnumpages = {19},\nkeywords = {Smartphone overuse, gestures, input manipulation, interaction proxy, intervention design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642583,\nauthor = {Terzimehi\\'{c}, Na\\dj{}a and Huber, Julia and Aragon-Hahner, Sarah and Mayer, Sven},\ntitle = {Real-World Winds: Micro Challenges to Promote Balance Post Smartphone Overload},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642583},\ndoi = {10.1145/3613904.3642583},\nabstract = {We present and evaluate the concept of winds – micro challenges to be done in the physical world post-smartphone overload, to encourage exiting the digital smartphone tunnel and promote refreshing breaks from the digital realm. Whereas digital detox solutions are unsustainable in everyday life, current everyday interventions such as screen time reminders or app blockers can induce negative feelings in users. We hypothesize that winds, delivered by our mobile app Real-World Wind (RWW), promote balance between the user’s physical and digital activities, as well as engagement with the intervention. RWW tracks users’ smartphone use behavior and distributes winds of five categories upon overload pattern detection. We evaluated the effectiveness of RWW in a week-long field study with 25 participants. Our findings show that winds foster a fun and engaging experience, and significantly promote balance between the digital and physical world post-smartphone overload. We discuss implications for future technology overload interventions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {246},\nnumpages = {16},\nkeywords = {balance, behavior change, digital wellbeing, intervention, micro breaks, micro challenges, mobile app, smartphone overload},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642479,\nauthor = {Li, Zhuoyang and Liang, Minhui and Lc, Ray and Luo, Yuhan},\ntitle = {StayFocused: Examining the Effects of Reflective Prompts and Chatbot Support on Compulsive Smartphone Use},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642479},\ndoi = {10.1145/3613904.3642479},\nabstract = {Amidst the increasingly prevalent smartphone addiction, we introduce StayFocused, a mobile app to help people focus on their tasks at hand by reducing compulsive smartphone use. Besides guiding people to set focus sessions for non-screen time, we incorporated reflective prompts probing individuals’ phone-checking intentions whenever they check their phones and a chatbot to deliver these prompts. To examine the effects of the reflective prompts and the chatbot support, we designed three versions of StayFocused: baseline, reflection, and reflection-chatbot, and conducted a stage-based between-subjects study with 36 college students over five weeks. We found that participants who received the reflective prompts were able to focus longer and resist distractions, and those with chatbot support seemed to better maintain their smartphone use reduction. By highlighting how participants reflected on their focus session activities and their preferences for the chatbot, we discuss the implications of designing persuasive conversational interfaces to reduce unintended behaviors.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {247},\nnumpages = {19},\nkeywords = {conversational UI, large language model, persuasive technology (PT), reflection, smartphone addiction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642790,\nauthor = {Wu, Ruolan and Yu, Chun and Pan, Xiaole and Liu, Yujia and Zhang, Ningning and Fu, Yue and Wang, Yuhan and Zheng, Zhi and Chen, Li and Jiang, Qiaolei and Xu, Xuhai and Shi, Yuanchun},\ntitle = {MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642790},\ndoi = {10.1145/3613904.3642790},\nabstract = {Problematic smartphone use negatively affects physical and mental health. Despite the wide range of prior research, existing persuasive techniques are not flexible enough to provide dynamic persuasion content based on users’ physical contexts and mental states. We first conducted a Wizard-of-Oz study (N=12) and an interview study (N=10) to summarize the mental states behind problematic smartphone use: boredom, stress, and inertia. This informs our design of four persuasion strategies: understanding, comforting, evoking, and scaffolding habits. We leveraged large language models (LLMs) to enable the automatic and dynamic generation of effective persuasion content. We developed MindShift, a novel LLM-powered problematic smartphone use intervention technique. MindShift takes users’ in-the-moment app usage behaviors, physical contexts, mental states, goals \\& habits as input, and generates personalized and dynamic persuasive content with appropriate persuasion strategies. We conducted a 5-week field experiment (N=25) to compare MindShift with its simplified version (remove mental states) and baseline techniques (fixed reminder). The results show that MindShift improves intervention acceptance rates by 4.7-22.5\\% and reduces smartphone usage duration by 7.4-9.8\\%. Moreover, users have a significant drop in smartphone addiction scale scores and a rise in self-efficacy scale scores. Our study sheds light on the potential of leveraging LLMs for context-aware persuasion in other behavior change domains.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {248},\nnumpages = {24},\nkeywords = {Problematic smartphone use, large language model, mental model, persuasion},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641979,\nauthor = {Kim, Inyeop and Lee, Uichin},\ntitle = {Navigating User-System Gaps: Understanding User-Interactions in User-Centric Context-Aware Systems for Digital Well-being Intervention},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641979},\ndoi = {10.1145/3613904.3641979},\nabstract = {In this paper, we investigate the challenges users face with a user-centric context-aware intervention system. Users often face gaps when the system’s responses do not align with their goals and intentions. We explore these gaps through a prototype system that enables users to specify context-action intervention rules as they desire. We conducted a lab study to understand how users perceive and cope with gaps while translating their intentions as rules, revealing that users experience context-mapping and context-recognition uncertainties (instant evaluation cycle). We also performed a field study to explore how users perceive gaps and make adaptations of rules when the operation of specified rules in real-world settings (delayed evaluation cycle). This research highlights the dynamic nature of user interaction with context-aware systems and suggests the potential of such systems in supporting digital well-being. It provides insights into user adaptation processes and offers guidance for designing user-centric context-aware applications.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {249},\nnumpages = {15},\nkeywords = {Context-triggered Actions, Digital Well-being Intervention, Technical Gap, User-centric Context-aware Systems},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642747,\nauthor = {Orzikulova, Adiba and Xiao, Han and Li, Zhipeng and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Ghassemi, Marzyeh and Lee, Sung-Ju and Dey, Anind K and Xu, Xuhai},\ntitle = {Time2Stop: Adaptive and Explainable Human-AI Loop for Smartphone Overuse Intervention},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642747},\ndoi = {10.1145/3613904.3642747},\nabstract = {Despite a rich history of investigating smartphone overuse intervention techniques, AI-based just-in-time adaptive intervention (JITAI) methods for overuse reduction are lacking. We develop Time2Stop, an intelligent, adaptive, and explainable JITAI system that leverages machine learning to identify optimal intervention timings, introduces interventions with transparent AI explanations, and collects user feedback to establish a human-AI loop and adapt the intervention model over time. We conducted an 8-week field experiment (N=71) to evaluate the effectiveness of both the adaptation and explanation aspects of Time2Stop. Our results indicate that our adaptive models significantly outperform the baseline methods on intervention accuracy (>32.8\\% relatively) and receptivity (>8.0\\%). In addition, incorporating explanations further enhances the effectiveness by 53.8\\% and 11.4\\% on accuracy and receptivity, respectively. Moreover, Time2Stop significantly reduces overuse, decreasing app visit frequency by 7.0 ∼ 8.9\\%. Our subjective data also echoed these quantitative measures. Participants preferred the adaptive interventions and rated the system highly on intervention time accuracy, effectiveness, and level of trust. We envision our work can inspire future research on JITAI systems with a human-AI loop to evolve with users.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {250},\nnumpages = {20},\nkeywords = {Explainable AI, Human-in-the-loop, Just-in-time adaptive intervention, Smartphone overuse},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642946,\nauthor = {Lyngs, Ulrik and Lukoff, Kai and Slovak, Petr and Inzlicht, Michael and Freed, Maureen and Andrews, Hannah and Tinsman, Claudine and Csuka, Laura and Alberts, Lize and Oldemburgo De Mello, Victoria and Makransky, Guido and Hornb\\ae{}k, Kasper and Van Kleek, Max and Shadbolt, Nigel},\ntitle = {“I finally felt I had the tools to control these urges”: Empowering Students to Achieve Their Device Use Goals With the Reduce Digital Distraction Workshop},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642946},\ndoi = {10.1145/3613904.3642946},\nabstract = {Digital self-control tools (DSCTs) help people control their time and attention on digital devices, using interventions like distraction blocking or usage tracking. Most studies of DSCTs’ effectiveness have focused on whether a single intervention reduces time spent on a single device. In reality, people may require combinations of DSCTs to achieve more subjective goals across multiple devices. We studied how DSCTs can address individual needs of university students (n = 280), using a workshop where students reflect on their goals before exploring relevant tools. At 1-3 month follow-ups, 95\\% of respondents still used at least one type of DSCT, typically applied across multiple devices, and there was substantial variation in the tool combinations chosen. We observed a large increase in self-reported digital self-control, suggesting that providing a space to articulate goals and self-select appropriate DSCTs is a powerful way to support people who struggle to self-regulate digital device use.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {251},\nnumpages = {23},\nkeywords = {attention, digital self-control, digital wellbeing, distraction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642820,\nauthor = {Hornb\\ae{}k, Kasper and Lyngs, Ulrik and Iarygina, Olga and Skov, Mikael B.},\ntitle = {“You Can Find a Part of my Life in Every Single App”: An Interview Study of What Makes Smartphone Applications Special to Their Users},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642820},\ndoi = {10.1145/3613904.3642820},\nabstract = {In the 1979 book “The Meaning of Things” Csikszentmihalyi and Rochberg-Halton studied people’s perception of the significance of things in the home. They emphasized how things influence the self, and vice versa. We propose that their method and analytical framework can help to understand the analogous question for smartphones: Why are some apps special to users? Using the framework, we conduct and analyze 60 interviews with people aged 21 to 41; with participants’ consent, we made the anonymized transcripts publicly available. The analysis of the interviews shows that participants find apps special because they are convenient, support personal goals and social communication, help them remember, and serve emotional functions. Participants report that their identity is intertwined with certain apps, even if they are annoying or cause dependency. Importantly, we also find that participants actively regulate their use of apps through their organization and particular use strategies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {252},\nnumpages = {16},\nkeywords = {Smartphones, meaning, user experience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642876,\nauthor = {Wen, Shaoyue and Ping, Songming and Wang, Jialin and Liang, Hai-Ning and Xu, Xuhai and Yan, Yukang},\ntitle = {AdaptiveVoice: Cognitively Adaptive Voice Interface for Driving Assistance},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642876},\ndoi = {10.1145/3613904.3642876},\nabstract = {Current voice assistants present messages in a predefined format without considering users’ mental states. This paper presents an optimization-based approach to alleviate this issue which adjusts the level of details and speech speed of the voice messages according to the estimated cognitive load of the user. In the first user study (N = 12), we investigated the impact of cognitive load on user performance. The findings reveal significant differences in preferred message formats across five cognitive load levels, substantiating the need for voice message adaptation. We then implemented AdaptiveVoice, an algorithm based on combinatorial optimization to generate adaptive voice messages in real time. In the second user study (N = 30) conducted in a VR-simulated driving environment, we compare AdaptiveVoice with a fixed format baseline, with and without visual guidance on the Heads-up display (HUD). Results indicate that users benefit from AdaptiveVoice with reduced response time and improved driving performance, particularly when it is augmented with HUD.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {253},\nnumpages = {18},\nkeywords = {Voice interface, adaptive user interface, driving assistance, workload},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642300,\nauthor = {Goodge, Thomas Alexander and Pollick, Frank and Brewster, Stephen Anthony},\ntitle = {Can You Hazard a Guess?: Evaluating the Effect of Augmented Reality Cues on Driver Hazard Prediction},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642300},\ndoi = {10.1145/3613904.3642300},\nabstract = {Semi-autonomous vehicles allow drivers to engage with non-driving related tasks (NDRTs). However, these tasks interfere with the driver’s situational awareness, key when they need to safely retake control of the vehicle. This paper investigates if Augmented Reality (AR) could be used to present NDRTs to reduce their impact on situational awareness. Two experiments compared driver performance on a hazard prediction task whilst interacting with an NDRT, presented either as an AR Heads-Up Display or a traditional Heads-Down Display. The results demonstrate that an AR display including a novel dynamic attentional cue improves situational awareness, depending on the workload of the NDRT and design of the cue. The results provide novel insights for designers of in-car systems about how to design NDRTs to aid driver situational awareness in future vehicles.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {254},\nnumpages = {28},\nkeywords = {Attention, Augmented Reality, Autonomous vehicles, Cueing, In-car, Situational Awareness, Takeover Request},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641997,\nauthor = {Islami, Lejla and Kitkowska, Agnieszka and Fischer-H\\\"{u}bner, Simone},\ntitle = {Inter-regional Lens on the Privacy Preferences of Drivers for ITS and Future VANETs},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641997},\ndoi = {10.1145/3613904.3641997},\nabstract = {Intelligent Transportation Systems (ITS) are on the rise, yet the knowledge about privacy preferences by different types of drivers in this context needs to be improved. This paper presents survey-based research (N = 528) focusing on preferences of drivers from South Africa and the Nordic countries for data processing and sharing by ITS, including future vehicular ad hoc networks. Our results indicate regionally framed drivers’ privacy attitudes and behaviours. South African participants have higher privacy concerns and risk perception. However, their preferences to share location data with police, family and friends, emergency services, and insurance companies are higher. Moreover, the region significantly affects preferences for transparency and control and sharing frequency, as well as willingness to pay for privacy, which are higher among the South Africans. We discuss how our results on factors, including region, impacting drivers’ privacy preferences can contribute to the design of usable privacy and identity management for ITS.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {255},\nnumpages = {20},\nkeywords = {Intelligent transportation, cross-regional comparison, privacy preferences, privacy-enhancing technologies (PETs), vehicular communication},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642341,\nauthor = {Bu, Fanjun and Li, Stacey and Goedicke, David and Colley, Mark and Sharma, Gyanendra and Ju, Wendy},\ntitle = {Portobello: Extending Driving Simulation from the Lab to the Road},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642341},\ndoi = {10.1145/3613904.3642341},\nabstract = {In automotive user interface design, testing often starts with lab-based driving simulators and migrates toward on-road studies to mitigate risks. Mixed reality (XR) helps translate virtual study designs to the real road to increase ecological validity. However, researchers rarely run the same study in both in-lab and on-road simulators due to the challenges of replicating studies in both physical and virtual worlds. To provide a common infrastructure to port in-lab study designs on-road, we built a platform-portable infrastructure, Portobello, to enable us to run twinned physical-virtual studies. As a proof-of-concept, we extended the on-road simulator XR-OOM with Portobello. We ran a within-subjects, autonomous-vehicle crosswalk cooperation study (N=32) both in-lab and on-road to investigate study design portability and platform-driven influences on study outcomes. To our knowledge, this is the first system that enables the twinning of studies originally designed for in-lab simulators to be carried out in an on-road platform.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {256},\nnumpages = {13},\nkeywords = {Driving Simulations, Human-Autonomous Vehicle Interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642941,\nauthor = {Elsharkawy, Ahmed Ibrahim Ahmed Mohamed and Ataya, Aya Abdulnasser Saed and Yeo, Dohyeon and An, Eunsol and Hwang, Seokhyun and Kim, SeungJun},\ntitle = {SYNC-VR: Synchronizing Your Senses to Conquer Motion Sickness for Enriching In-Vehicle Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642941},\ndoi = {10.1145/3613904.3642941},\nabstract = {Passengers can engage more in nondriving-related tasks owing to recent advancements in autonomous vehicles (AVs), making immersive tools such as virtual reality (VR) appealing; however, motion sickness (MS) remains a significant challenge. We present SYNC-VR, a system that aligns with visual, haptic, and auditory cues and provides proprioceptive feedback to illustrate its effect on MS and presence within the in-vehicle VR. We conducted an experiment with 24 participants using a real vehicle along a route with known MS-triggering events. Using subjective and physiological measures, we assessed participants’ presence and MS under four conditions by gradually varying the level of synchronized input sensations. Results reveal that SYNC-VR reduces MS and increases the sense of presence. Additionally, it emphasizes the impact of our interactive VR content and its role in achieving proprioceptive feedback with haptic feedback through electrical muscle stimulation, introducing an innovative approach to MS mitigation in in-vehicle VR.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {257},\nnumpages = {17},\nkeywords = {autonomous vehicles, haptic feedback, kinesthetic feedback, motion sickness, presence, sensory alignment, virtual reality, visual cues},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642219,\nauthor = {Gerber, Michael A. and Schroeter, Ronald and Johnson, Daniel and Janssen, Christian P. and Rakotonirainy, Andry and Kuo, Jonny and Lenn\\'{e}, Mike},\ntitle = {An Eye Gaze Heatmap Analysis of Uncertainty Head-Up Display Designs for Conditional Automated Driving},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642219},\ndoi = {10.1145/3613904.3642219},\nabstract = {This paper reports results from a high-fidelity driving simulator study (N=215) about a head-up display (HUD) that conveys a conditional automated vehicle’s dynamic “uncertainty” about the current situation while fallback drivers watch entertaining videos. We compared (between-group) three design interventions: display (a bar visualisation of uncertainty close to the video), interruption (interrupting the video during uncertain situations), and combination (a combination of both), against a baseline (video-only). We visualised eye-tracking data to conduct a heatmap analysis of the four groups’ gaze behaviour over time. We found interruptions initiated a phase during which participants interleaved their attention between monitoring and entertainment. This improved monitoring behaviour was more pronounced in combination compared to interruption, suggesting pre-warning interruptions have positive effects. The same addition had negative effects without interruptions (comparing baseline \\& display). Intermittent interruptions may have safety benefits over placing additional peripheral displays without compromising usability.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {258},\nnumpages = {16},\nkeywords = {Conditional Automated Driving, Driving Simulator Study, Eye-tracking, Fallback Readiness, Head Up Display, Heatmap Analysis, Non-Driving Related Activity, Robot Supervision, Task Switch},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642298,\nauthor = {P\\\"{o}hlmann, Katharina Margareta Theresa and Wilson, Graham and Li, Gang and Mcgill, Mark and Brewster, Stephen Anthony},\ntitle = {From Slow-Mo to Ludicrous Speed: Comfortably Manipulating the Perception of Linear In-Car VR Motion Through Vehicular Translational Gain and Attenuation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642298},\ndoi = {10.1145/3613904.3642298},\nabstract = {To prevent motion sickness, Virtual Reality (VR) experiences for vehicle passengers typically present “matched motion”: real vehicle movements are replicated 1:1 by movements in VR. This significantly limits virtual applications. We provide foundations for in-car VR experiences that break this constraint by manipulating the passenger’s visual perception of linear velocity through amplifying and reducing the virtual speed. In two on-the-road studies, we examined the application of Vehicular Translational Gain (1.5-9.5x) and Attenuation (0.66-0.14x) to real car speeds (~50km/h) across two VR tasks (reading and gaming), exploring journey perception, impact on motion sickness, travel experience and tasks. We found that vehicular gain/attenuation can be applied without significantly increasing motion sickness. Gain was more noticeable and affected perceived speed, distance, safety, relaxation and excitement, being well-suited to gaming, while attenuation was more suitable for productivity. Our work unlocks new ways that VR applications can enhance and alter the passenger experience through novel perceptual manipulations of vehicle velocity.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {259},\nnumpages = {20},\nkeywords = {Automated Vehicles, In-Car, Motion Sickness, Perceptual Manipulation, Translational Gain, Vehicular Attenuation, Vehicular Gain, Velocity, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642365,\nauthor = {Colley, Mark and Rajabi, Omid and Rukzio, Enrico},\ntitle = {Investigating the Effects of External Communication and Platoon Behavior on Manual Drivers at Highway Access},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642365},\ndoi = {10.1145/3613904.3642365},\nabstract = {Automated vehicles are expected to improve traffic safety and efficiency. One approach to achieve this is via platooning, that is, (automated) vehicles can drive behind each other at very close proximity to reduce air resistance. However, this behavior could lead to difficulties in mixed traffic, for example, when manual drivers try to enter a highway. Therefore, we report the results of a within-subject Virtual Reality study (N=29) evaluating different platoon behaviors (single vs. multiple, i.e., four, gaps) and communication strategies (HUD, AR, attached displays). Results show that AR communication reduced mental workload, improved perceived safety, and a single big gap led to the safest merging behavior. Our work helps to incorporate novel behavior enabled by automation into general traffic better.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {260},\nnumpages = {15},\nkeywords = {Automated vehicles, Virtual Reality., external communication, self-driving vehicles},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642053,\nauthor = {Zhang, Yutong and Awad, Edmond and Frank, Morgan R. and Liu, Peng and Du, Na},\ntitle = {Understanding Human-machine Cooperation in Game-theoretical Driving Scenarios amid Mixed Traffic},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642053},\ndoi = {10.1145/3613904.3642053},\nabstract = {Introducing automated vehicles (AVs) on roads may challenge established norms as drivers of human-driven vehicles (HVs) interact with AVs. Our study explored drivers’ decisions in game-theoretical scenarios amid mixed traffic using an online survey study. We manipulated factors including interaction types (HV-HV vs. HV-AV), scenario types (chicken game vs. public goods game), vehicle driving styles (aggressive vs. conservative), and time constraints (high vs. low). The quantitative results showed that human drivers tended to “defect” more, that is, not cooperate, against vehicles with conservative driving styles. The effect of vehicle driving styles was pronounced when interacting with AVs and in chicken game scenarios. Drivers exhibited more “defection” in public goods game scenarios and the effect of scenario types was weakened under high time constraints. Only drivers with moderate driving styles “defected” more in HV-AV interaction. Our qualitative findings provide essential insights into how drivers perceived conditions and formulated strategies for decision-making.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {261},\nnumpages = {13},\nkeywords = {automated vehicles, game theory, human-machine cooperation, mixed-traffic environment},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642896,\nauthor = {Zhang, Min and Bandara, Arosha K},\ntitle = {Understanding Pedestrians’ Perception of Safety and Safe Mobility Practices},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642896},\ndoi = {10.1145/3613904.3642896},\nabstract = {Walking is one of the greenest and most common travel modes. However, evidence shows a trend of decreased walking, and safety is a key barrier preventing many people from walking. Additionally, there is a limited understanding of pedestrians’ safe mobility practices and safety perception. Drawing on 449 survey responses from a representative sample in the United Kingdom, our work highlights how identities and walking situations intersect with individuals’ safety perceptions and diverse practices of pedestrians’ safe mobility. The role of technology used for negotiating safety and current challenges in both safe route planning and walking are also highlighted. Our work extends existing insights into pedestrians’ perception of safety and practices by adding empirical evidence and more nuanced contexts. This paper proposes two implications for design in response to design opportunities that surfaced from our mixed-method data analysis. Both the contributions and limitations of our work are also discussed.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {262},\nnumpages = {17},\nkeywords = {Pedestrian safety, perceived safety, personal safety, safe mobility, safe route navigation, safe walk, safety},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642345,\nauthor = {Dong, Kaixu and Zhang, Zhiyuan and Chang, Xiaoyu and Chirarattananon, Pakpong and LC, RAY},\ntitle = {Dances with Drones: Spatial Matching and Perceived Agency in Improvised Movements with Drone and Human Partners},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642345},\ndoi = {10.1145/3613904.3642345},\nabstract = {As drones become interwoven in human activities, increasingly taking on tasks interpreted as creative and performative, such as choreographed light shows, there is emerging interest in understanding how drones and humans can perform together. Humans have different habits when performing with partners as opposed to solo. How do people adapt their behaviors and perspectives when improvising with robotic partners? To explore these questions, we conducted a study investigating dancer-drone interactions using a system of micro aerial vehicles designed to facilitate improvised solo and partnered dances. Through solo and tandem dances with one or two robots, we analyzed the performers’ perceived workflow from semi-structured interviews and quantified their movement patterns during the improvisation. We found that the dancers perceived drone movements through spatial metaphors like the ceiling and gravity, anthropomorphizing drones as props on a stage through position and generated sound. The dancers felt a greater connection in single-drone scenarios and showed heightened avoidance behavior in two-drone situations. Our work shows how a robotic system can energize human dancers to improvise individually and in pairs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {263},\nnumpages = {16},\nkeywords = {human-drone interactions, improvisational dancing, micro aerial vehicles},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642791,\nauthor = {Wang, Ziming and Wu, Yiqian and Yang, Shiwei and Chen, Xiaowei and Rohles, Bj\\\"{o}rn and Fjeld, Morten},\ntitle = {Exploring Intended Functions of Indoor Flying Robots Interacting With Humans in Proximity},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642791},\ndoi = {10.1145/3613904.3642791},\nabstract = {What will people experience when drones become common in home environments? How will their functions and distances impact human experiences? To explore the potential usage of indoor drones, we conducted a mixed-methods study (N=60) on the reported perceptions of a small flying robot. We employed a factorial experimental design, involving four intended drone functions (camera, education, pet, unknown) at two distances (near, far). Our findings suggest that intended functions significantly influence participants’ perceptions. Among the functions examined, participants found the camera useful but annoying, and the pet useless but pleasant. The education emerged as the most favored function, while the unknown function was the least preferred one. Based on these findings, we discuss implications for designing positive interactions between humans and indoor drones, considering aspects such as context, transparency, privacy, technical factors, and personalization.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {264},\nnumpages = {16},\nkeywords = {Indoor drone, artificial intelligence (AI)., drone function, human-drone interaction (HDI), proxemics, user experience (UX)},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642061,\nauthor = {Li, Moyi and Katsiuba, Dzmitry and Dolata, Mateusz and Schwabe, Gerhard},\ntitle = {Firefighters' Perceptions on Collaboration and Interaction with Autonomous Drones: Results of a Field Trial},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642061},\ndoi = {10.1145/3613904.3642061},\nabstract = {Applications of drones in emergency response, like firefighting, have been promoted in the past decade. As the autonomy of drones continues to improve, the ways in which they are integrated into firefighting teams and their impact on crews are changing. This demands more understanding of how firefighters perceive and interact with autonomous drones. This paper presents a drone-based system for emergency operations with which firefighters can interact through sound, lights, and a graphical user interface. We use interviews with stakeholders collected in two field trials to explore their perceptions of the interaction and collaboration with drones. Our result shows that firefighters perceived visual interaction as adequate. However, for audio instructions and interfaces, information overload emerges as an essential problem. The potential impact of drones on current work configurations may involve shifting the position of humans closer to supervisory decision-makers and changing the training structure and content.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {265},\nnumpages = {19},\nkeywords = {Autonomous drone, Emergency intervention, Field trial, Firefighting},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642958,\nauthor = {Chambers, Theodore and Vierhauser, Michael and Agrawal, Ankit and Murphy, Michael and Brauer, Jason Matthew and Purandare, Salil and Cohen, Myra B and Cleland-Huang, Jane},\ntitle = {HIFuzz: Human Interaction Fuzzing for Small Unmanned Aerial Vehicles},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642958},\ndoi = {10.1145/3613904.3642958},\nabstract = {Small Unmanned Aerial Systems (sUAS) must meet rigorous safety standards when deployed in high-stress emergency response scenarios; however many reported accidents have involved humans in the loop. In this paper, we, therefore, present the HiFuzz testing framework, which uses fuzz testing to identify system vulnerabilities associated with human interactions. HiFuzz includes three distinct levels that progress from a low-cost, limited-fidelity, large-scale, no-hazard environment, using fully simulated Proxy Human Agents, via an intermediate level, where proxy humans are replaced with real humans, to a high-stakes, high-cost, real-world environment. Through applying HiFuzz to an autonomous multi-sUAS system-under-test, we show that each test level serves a unique purpose in revealing vulnerabilities and making the system more robust with respect to human mistakes. While HiFuzz is designed for testing sUAS systems, we further discuss its potential for use in other Cyber-Physical Systems.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {266},\nnumpages = {14},\nkeywords = {Cyber-Physical Systems, human-interaction, sUAS, safety},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642870,\nauthor = {Ichihashi, Sosuke and Kuroki, So and Nishimura, Mai and Kasaura, Kazumi and Hiraki, Takefumi and Tanaka, Kazutoshi and Yoshida, Shigeo},\ntitle = {Swarm Body: Embodied Swarm Robots},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642870},\ndoi = {10.1145/3613904.3642870},\nabstract = {The human brain’s plasticity allows for the integration of artificial body parts into the human body. Leveraging this, embodied systems realize intuitive interactions with the environment. We introduce a novel concept: embodied swarm robots. Swarm robots constitute a collective of robots working in harmony to achieve a common objective, in our case, serving as functional body parts. Embodied swarm robots can dynamically alter their shape, density, and the correspondences between body parts and individual robots. We contribute an investigation of the influence on embodiment of swarm robot-specific factors derived from these characteristics, focusing on a hand. Our paper is the first to examine these factors through virtual reality (VR) and real-world robot studies to provide essential design considerations and applications of embodied swarm robots. Through quantitative and qualitative analysis, we identified a system configuration to achieve the embodiment of swarm robots.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {267},\nnumpages = {19},\nkeywords = {embodiment, swarm robotics, tangible interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642607,\nauthor = {Kaspersen, Magnus H\\o{}holt and Musaeus, Line Have and Bilstrup, Karl-Emil Kj\\ae{}r and Petersen, Marianne Graves and Iversen, Ole Sejer and Dindler, Christian and Dalsgaard, Peter},\ntitle = {From Primary Education to Premium Workforce: Drawing on K-12 Approaches for Developing AI Literacy},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642607},\ndoi = {10.1145/3613904.3642607},\nabstract = {Advances in artificial intelligence present a need for fostering AI literacy in workplaces. While there is a lack of research on how this can be achieved, there are documented successful approaches in child-computer interaction (CCI), albeit aimed at K-12 education. We present an in-vivo explorative case study of how CCI approaches can be adopted for adult professionals via a full-day workshop developed in collaboration with a trade union to upskill workers. Analyzing data from pre- and post-surveys, a follow-up survey, and materials produced by participants (n=53), we demonstrate how this increased participants’ knowledge of AI while their self-efficacy and empowerment did not improve. This is similar to findings from K-12 education, pointing to self-efficacy and empowerment as major challenges for AI literacy across sectors. We discuss the role of ambassadorships and professional organizations in addressing these issues, and indicate research directions for the CHI community.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {268},\nnumpages = {16},\nkeywords = {AI Literacy, K-12, Machine Learning, education, trade unions},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642592,\nauthor = {Tan, Mei and Subramonyam, Hari},\ntitle = {More than Model Documentation: Uncovering Teachers' Bespoke Information Needs for Informed Classroom Integration of ChatGPT},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642592},\ndoi = {10.1145/3613904.3642592},\nabstract = {ChatGPT has entered classrooms, circumventing typical training and vetting procedures. Unlike other educational technologies, it placed teachers in direct contact with the versatility of generative AI. Consequently, teachers are urgently tasked to assess its capabilities to inform their use of ChatGPT. However, it is unclear what support teachers have and need and whether existing documentation, such as model cards, provides adequate direction for educators in this new paradigm. By interviewing 22 middle- and high-school ELA and Social Studies teachers, we connect the discourse on AI transparency and documentation with educational technology integration, highlighting the information needs of teachers. Our findings reveal that teachers confront significant information gaps, lacking clarity on exploring ChatGPT’s capabilities for bespoke learning tasks and ensuring its fit with the needs of diverse learners. As a solution, we propose a framework for interactive model documentation that empowers teachers to navigate the interplay between pedagogical and technical knowledge.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {269},\nnumpages = {19},\nkeywords = {AI in education, chatgpt, large language models, machine learning documentation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642198,\nauthor = {Moore, Dylan Edward and Moore, Sophia R. R. and Ireen, Bansharee and Iskandar, Winston P. and Artazyan, Grigory and Murnane, Elizabeth L.},\ntitle = {Teaching artificial intelligence in extracurricular contexts through narrative-based learnersourcing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642198},\ndoi = {10.1145/3613904.3642198},\nabstract = {Collaborative technology provides powerful opportunities to engage young people in active learning experiences that are inclusive, immersive, and personally meaningful. In particular, interactive narratives have proven to be effective scaffolds for learning, and learnersourcing has emerged as a promising student-driven approach to enable personalized education and quality control at-scale. We introduce the first synthesis of these ideas in the context of teaching artificial intelligence (AI), which is now seen as a critical component of 21st-century education. Specifically, we explore the design of a narrative-based learnersourcing platform where engagement is centered around a learner-made choose-your-own-adventure story. In grounding our approach, we draw from pedagogical literature, digital storytelling, and recent work on learnersourcing. We report on our iterative, learner-centered design process as well as our study findings that demonstrate the platform’s positive effects on knowledge gains, interest in AI concepts, and the overall user experience of narrative-based learnersourcing technology.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {270},\nnumpages = {28},\nkeywords = {AI literacy, STEM education, collaborative learning, digital narratives, learnersourcing, online learning tools, storytelling},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642785,\nauthor = {Park, Hyanghee and Ahn, Daehwan},\ntitle = {The Promise and Peril of ChatGPT in Higher Education: Opportunities, Challenges, and Design Implications},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642785},\ndoi = {10.1145/3613904.3642785},\nabstract = {A growing number of students in higher education are using ChatGPT for various educational purposes, ranging from seeking information to writing essays. Although many universities have officially banned the use of ChatGPT because of its potential harm and unintended consequences, it is still important to uncover how students leverage ChatGPT for learning, what challenges emerge, and how we can make better use of ChatGPT in higher education. Thus, we conducted focus group workshops and a series of participatory design sessions with thirty students who have actively interacted with ChatGPT for one semester in university and with other five stakeholders (e.g., professors, AI experts). Based on these, this paper identifies real opportunities and challenges of utilizing and designing ChatGPT for higher education.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {271},\nnumpages = {21},\nkeywords = {AI in Education, ChatGPT, Higher education, Large Language Models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642539,\nauthor = {Bilstrup, Karl-Emil Kj\\ae{}r and Kaspersen, Magnus H\\o{}holt and Bouvin, Niels Olof and Petersen, Marianne Graves},\ntitle = {ml-machine.org: Infrastructuring a Research Product to Disseminate AI Literacy in Education},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642539},\ndoi = {10.1145/3613904.3642539},\nabstract = {ml-machine.org is a web- and micro:bit-based educational tool for building machine learning models designed to enable more widespread teaching of AI literacy in secondary education. It has been designed as a research product in collaboration with partners from the educational sector, including the Danish Broadcasting Corporation and the Micro:bit Educational Foundation. ml-machine.org currently has more than 5000 unique users and is used in schools and teacher training. It is publicly available and promoted on the broadcasting corporation’s platforms. We describe the two-year process of developing and disseminating ml-machine.org. Based on interviews with partners and educators, we report on how ml-machine.org supports inquiry into the adoption and appropriation of such educational tools. We also provide insights on working with formal education infrastructures in order to scale and integrate a research product into teacher practices. Based on these experiences, we propose infrastructure as a novel quality of research products.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {272},\nnumpages = {16},\nkeywords = {AI Literacy, Participatory Infrastructuring, Research Product},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642692,\nauthor = {Dwivedi, Utkarsh and Elsayed-Ali, Salma and Bonsignore, Elizabeth and Kacorri, Hernisa},\ntitle = {Exploring AI Problem Formulation with Children via Teachable Machines},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642692},\ndoi = {10.1145/3613904.3642692},\nabstract = {Emphasizing problem formulation in AI literacy activities with children is vital, yet we lack empirical studies on their structure and affordances. We propose that participatory design involving teachable machines facilitates problem formulation activities. To test this, we integrated problem reduction heuristics into storyboarding and invited a university-based intergenerational design team of 10 children (ages 8-13) and 9 adults to co-design a teachable machine. We find that children draw from personal experiences when formulating AI problems; they assume voice and video capabilities, explore diverse machine learning approaches, and plan for error handling. Their ideas promote human involvement in AI, though some are drawn to more autonomous systems. Their designs prioritize values like capability, logic, helpfulness, responsibility, and obedience, and a preference for a comfortable life, family security, inner harmony, and excitement as end-states. We conclude by discussing how these results can inform the design of future participatory AI activities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {273},\nnumpages = {18},\nkeywords = {cooperative inquiry, design metaphors, machine teaching, participatory machine learning, values},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642647,\nauthor = {Zhang, Chao and Liu, Xuechen and Ziska, Katherine and Jeon, Soobin and Yu, Chi-Lin and Xu, Ying},\ntitle = {Mathemyths: Leveraging Large Language Models to Teach Mathematical Language through Child-AI Co-Creative Storytelling},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642647},\ndoi = {10.1145/3613904.3642647},\nabstract = {Mathematical language is a cornerstone of a child’s mathematical development, and children can effectively acquire this language through storytelling with a knowledgeable and engaging partner. In this study, we leverage the recent advances in large language models to conduct free-form, creative conversations with children. Consequently, we developed Mathemyths, a joint storytelling agent that takes turns co-creating stories with children while integrating mathematical terms into the evolving narrative. This paper details our development process, illustrating how prompt-engineering can optimize LLMs for educational contexts. Through a user study involving 35 children aged 4-8 years, our results suggest that when children interacted with Mathemyths, their learning of mathematical language was comparable to those who co-created stories with a human partner. However, we observed differences in how children engaged with co-creation partners of different natures. Overall, we believe that LLM applications, like Mathemyths, offer children a unique conversational experience pertaining to focused learning objectives.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {274},\nnumpages = {23},\nkeywords = {Storytelling, child–AI collaboration, children, co-creativity, conversational interfaces, large language models, mathematical language},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642041,\nauthor = {Cheng, Alan Y. and Guo, Meng and Ran, Melissa and Ranasaria, Arpit and Sharma, Arjun and Xie, Anthony and Le, Khuyen N. and Vinaithirthan, Bala and Luan, Shihe (Tracy) and Wright, David Thomas Henry and Cuadra, Andrea and Pea, Roy and Landay, James A.},\ntitle = {Scientific and Fantastical: Creating Immersive, Culturally Relevant Learning Experiences with Augmented Reality and Large Language Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642041},\ndoi = {10.1145/3613904.3642041},\nabstract = {Motivating children to learn is a major challenge in education. One way to inspire motivation to learn is through immersion. We combine the immersive potential of augmented reality (AR), narrative, and large language models (LLMs) to bridge fantasy with reality in a mobile application, Moon Story, that teaches elementary schoolers astronomy and environmental science. Our system also builds upon learning theories such as culturally-relevant pedagogy. Using our application, a child embarks on a journey inspired by Chinese mythology, engages in real-world AR activities, and converses with a fictional character powered by an LLM. We conducted a controlled experiment (N = 50) with two conditions: one using an LLM and one that was hard-coded. Both conditions resulted in learning gains, high engagement levels, and increased science learning motivation. Participants in the LLM condition also wrote more relevant answers. Finally, participants of both Chinese and non-Chinese heritage found the culturally-based narrative compelling.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {275},\nnumpages = {23},\nkeywords = {Artifact or System, Children/Parents, Education/Learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642332,\nauthor = {Belghith, Yasmine and Mahdavi Goloujeh, Atefeh and Magerko, Brian and Long, Duri and Mcklin, Tom and Roberts, Jessica},\ntitle = {Testing, Socializing, Exploring: Characterizing Middle Schoolers’ Approaches to and Conceptions of ChatGPT},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642332},\ndoi = {10.1145/3613904.3642332},\nabstract = {As generative AI rapidly enters everyday life, educational interventions for teaching about AI need to cater to how young people, in particular middle schoolers who are at a critical age for reasoning skills and identity formation, conceptualize and interact with AI. We conducted nine focus groups with 24 middle school students to elicit their interests, conceptions of, and approaches to a popular generative AI tool, ChatGPT. We highlight a) personally and culturally-relevant topics to this population, b) three distinct approaches in students’ open-ended interactions with ChatGPT: AI testing-oriented, AI socializing-oriented, and content exploring-oriented, and 3) an improved understanding of youths’ conceptions and misconceptions of generative AI. While misconceptions highlight gaps in understanding what generative AI is and how it works, most learners show interest in learning about what AI is and what it can do. We discuss the implications of these conceptions for designing AI literacy interventions in museums.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {276},\nnumpages = {17},\nkeywords = {AI literacy, ChatGPT, Child-AI Interaction, Conceptions of AI, Conversational Agents (CAs), Generative AI, Informal Learning, Large Language Models (LLMs)},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642867,\nauthor = {Choi, Seulgi and Lee, Hyewon and Lee, Yoonjoo and Kim, Juho},\ntitle = {VIVID: Human-AI Collaborative Authoring of Vicarious Dialogues from Lecture Videos},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642867},\ndoi = {10.1145/3613904.3642867},\nabstract = {The lengthy monologue-style online lectures cause learners to lose engagement easily. Designing lectures in a “vicarious dialogue” format can foster learners’ cognitive activities more than monologue-style. However, designing online lectures in a dialogue style catered to the diverse needs of learners is laborious for instructors. We conducted a design workshop with eight educational experts and seven instructors to present key guidelines and the potential use of large language models (LLM) to transform a monologue lecture script into pedagogically meaningful dialogue. Applying these design guidelines, we created VIVID which allows instructors to collaborate with LLMs to design, evaluate, and modify pedagogical dialogues. In a within-subjects study with instructors (N=12), we show that VIVID helped instructors select and revise dialogues efficiently, thereby supporting the authoring of quality dialogues. Our findings demonstrate the potential of LLMs to assist instructors with creating high-quality educational dialogues across various learning stages.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {277},\nnumpages = {26},\nkeywords = {Dialogic lecture, Instructor assist tool, LLM-based authoring tool, Vicarious learning, Video-based learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642695,\nauthor = {Lakier, Matthew and Vogel, Daniel},\ntitle = {Digital Knick-Knacks: Standalone Audiovisual Digital Possessions or Embellishments in Digital Environments},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642695},\ndoi = {10.1145/3613904.3642695},\nabstract = {Inspired by physical possessions displayed in the home, we define “digital knick-knacks” as standalone audiovisual digital possessions or embellishments contained within non-game digital environments. “Neko”, a cat that chases the cursor, is a historical example. We propose a taxonomy to define and generate digital knick-knacks based on key publications on consumer behaviour and personal possessions, augmented by results of a brainstorming session with 9 HCI researchers. Using the taxonomy, we prototype three classes of digital knick-knack exemplars: an ambient noise machine, a virtual pet, and a virtual picture frame. In a 10-day diary study, 10 participants design their own variants of the prototypes, and report on their experience using them on a personal device. Our analysis shows how digital knick-knacks can bring value to users, and we suggest implications for designing playful digital embellishments.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {278},\nnumpages = {17},\nkeywords = {digital possessions, personalization, playful user interfaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642101,\nauthor = {An, Pengcheng and Zhu, Jiawen Stefanie and Zhang, Zibo and Yin, Yifei and Ma, Qingyuan and Yan, Che and Du, Linghao and Zhao, Jian},\ntitle = {EmoWear: Exploring Emotional Teasers for Voice Message Interaction on Smartwatches},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642101},\ndoi = {10.1145/3613904.3642101},\nabstract = {Voice messages, by nature, prevent users from gauging the emotional tone without fully diving into the audio content. This hinders the shared emotional experience at the pre-retrieval stage. Research scarcely explored “Emotional Teasers”—pre-retrieval cues offering a glimpse into an awaiting message’s emotional tone without disclosing its content. We introduce EmoWear, a smartwatch voice messaging system enabling users to apply 30 animation teasers on message bubbles to reflect emotions. EmoWear eases senders’ choice by prioritizing emotions based on semantic and acoustic processing. EmoWear was evaluated in comparison with a mirroring system using color-coded message bubbles as emotional cues (N=24). Results showed EmoWear significantly enhanced emotional communication experience in both receiving and sending messages. The animated teasers were considered intuitive and valued for diverse expressions. Desirable interaction qualities and practical implications are distilled for future design. We thereby contribute both a novel system and empirical knowledge concerning emotional teasers for voice messaging.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {279},\nnumpages = {16},\nkeywords = {Animation, Emotion, Emotional Teasers, Smartwatch, Voice Message},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642445,\nauthor = {Haw, Wendy and Ren, Yuan and Ng, Kianna and Arif, Ahmed Sabbir},\ntitle = {Investigating the Effects of Self-selected Pleasant Scents on Text Composition and Transcription Performance},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642445},\ndoi = {10.1145/3613904.3642445},\nabstract = {The extensive use of computers for text entry has been linked to increased stress, depression, and sleep disturbances, adversely affecting performance. Recent trends involve using scent diffusers to counter these effects. However, the impact of scents on text entry performance is not well-studied. Our empirical study investigated the effects of self-selected pleasant scents on text composition and transcription performance. Results showed that while composing, users were slower with a scent present, potentially due to heightened focus on text quality. Scent did not alter accuracy or text length. In transcription tasks, although scent did not alter typing speed, it adversely affected accuracy, likely due to its impact on concentration levels. Despite these mixed results, users felt more effective and enjoyed the scent, indicating a preference for its continued use. This study opens avenues for further research into scents’ influence on computer-based tasks, potentially contributing to the evolving field of olfactory displays.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {280},\nnumpages = {13},\nkeywords = {Olfaction, composition, data entry, odor, odorant, olfactory interfaces, scent, smell, text entry, texting, workplace, writing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642850,\nauthor = {Zhou, Suifang and Hendra, Latisha Besariani and Zhang, Qinshi and Holopainen, Jussi and LC, RAY},\ntitle = {Eternagram: Probing Player Attitudes Towards Climate Change Using a ChatGPT-driven Text-based Adventure},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642850},\ndoi = {10.1145/3613904.3642850},\nabstract = {Conventional methods of assessing attitudes towards climate change are limited in capturing authentic opinions, primarily stemming from a lack of context-specific assessment strategies and an overreliance on simplistic surveys. Game-based Assessments (GBA) have demonstrated the ability to overcome these issues by immersing participants in engaging gameplay within carefully crafted, scenario-based environments. Concurrently, advancements in AI and Natural Language Processing (NLP) show promise in enhancing the gamified testing environment, achieving this by generating context-aware, human-like dialogues that contribute to a more natural and effective assessment. Our study introduces a new technique for probing climate change attitudes by actualizing a GPT-driven chatbot system in harmony with a game design depicting a futuristic climate scenario. The correlation analysis reveals an assimilation effect, where players’ post-game climate awareness tends to align with their in-game perceptions. Key predictors of pro-climate attitudes are identified as traits like ’Openness’ and ’Agreeableness’, and a preference for democratic values.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {281},\nnumpages = {23},\nkeywords = {Conversation Analysis, Games/Play, HCI for Development, Interaction Design, Interactive Storytelling, Quantitative Methods, Survey, Text Entry},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642957,\nauthor = {Wienrich, Carolin and Vogt, Stephanie and D\\\"{o}llinger, Nina and Obremski, David},\ntitle = {Promoting Eco-Friendly Behaviour through Virtual Reality - Implementation and Evaluation of Immersive Feedback Conditions of a Virtual CO2 Calculator},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642957},\ndoi = {10.1145/3613904.3642957},\nabstract = {Climate change is one of the most pressing global challenges in the 21st century. Urgent actions favoring the environment’s well-being are essential to mitigate its potentially irreversible consequences. However, the delayed and often distant nature of the effects of sustainable behavior makes it challenging for individuals to connect with the issue personally. Immersive media are an opportunity to introduce innovative feedback mechanisms to highlight the urgency of behavior effects. We introduce a VR carbon calculator that visualizes users’ annual carbon footprint as CO2-filled balloons over multiple periods. In a 2 \\texttimes{} 2 design, participants calculated and visualized their carbon footprint numerically or as balloons over one or three years. We found no effect of our visualization but a significant impact of the visualized period on participants’ environmental self-efficacy. These findings emphasize the importance of target-oriented design in VR behavior interventions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {282},\nnumpages = {9},\nkeywords = {Virtual reality, intention-behavior gap, pro-environmental behavior.},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642720,\nauthor = {Benabdallah, Gabrielle and Peek, Nadya},\ntitle = {Technical Mentality: Principles for HCI Research and Practice},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642720},\ndoi = {10.1145/3613904.3642720},\nabstract = {This paper presents a reflection on the role of ontological inquiry in HCI research and practice. Specifically, we introduce philosopher Gilbert Simondon’s proposal of technical mentality, an onto-epistemology based on direct knowledge of technical objects and systems. This paper makes the following contributions: an analysis of Simondon’s ontological critique and its connection to technical mentality; a reflection on the ethical and practical implications of Simondon’s proposal for systems research; an example of technical mentality in practice; and a discussion of how technical mentality might be extended into a design program for HCI through four principles: extension, integration, legibility, and expression.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {283},\nnumpages = {14},\nkeywords = {Design, Gilbert Simondon, Ontology, Philosophy, Technical Mentality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642712,\nauthor = {Brailsford, Joe and Vetere, Frank and Velloso, Eduardo},\ntitle = {Exploring the Association between Moral Foundations and Judgements of AI Behaviour},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642712},\ndoi = {10.1145/3613904.3642712},\nabstract = {How do individual differences in personal morality affect perceptions and judgments of morally contentious behaviours from AI systems? By applying Moral Foundations Theory (MFT) to the context of AI, this study sought to develop a predictive Bayesian model for assessing moral judgements based on individual differences in moral constitution. Participants (N=240) were asked to assess six different scenarios, carefully designed to elicit reflection on the behaviour of AI systems. Together, with results from the Moral Foundations Questionnaire, we performed both Bayesian modelling and reflexive thematic analysis to investigate the associations between individual differences in moral foundations and judgements of the AI systems. Results revealed a mild association between individual MFT scores and judgments of AI behaviours. Qualitative responses suggested a participant’s technical understanding of AI systems, rather than intrinsic moral values, predominantly influenced their judgments, with those who judged the behaviour as wrong tending to attribute a greater degree of agency to the AI systems.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {284},\nnumpages = {15},\nkeywords = {Moral Foundations Theory, automated decision-making, ethics principles and guidelines, responsible AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642627,\nauthor = {Yang, Mingzhe and Arai, Hiromi and Yamashita, Naomi and Baba, Yukino},\ntitle = {Fair Machine Guidance to Enhance Fair Decision Making in Biased People},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642627},\ndoi = {10.1145/3613904.3642627},\nabstract = {Teaching unbiased decision-making is crucial for addressing biased decision-making in daily life. Although both raising awareness of personal biases and providing guidance on unbiased decision-making are essential, the latter topics remains under-researched. In this study, we developed and evaluated an AI system aimed at educating individuals on making unbiased decisions using fairness-aware machine learning. In a between-subjects experimental design, 99 participants who were prone to bias performed personal assessment tasks. They were divided into two groups: a) those who received AI guidance for fair decision-making before the task and b) those who received no such guidance but were informed of their biases. The results suggest that although several participants doubted the fairness of the AI system, fair machine guidance prompted them to reassess their views regarding fairness, reflect on their biases, and modify their decision-making criteria. Our findings provide insights into the design of AI systems for guiding fair decision-making in humans.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {285},\nnumpages = {18},\nkeywords = {fairness-aware machine learning, machine guidance},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642703,\nauthor = {Agnew, William and Bergman, A. Stevie and Chien, Jennifer and D\\'{\\i}az, Mark and El-Sayed, Seliem and Pittman, Jaylen and Mohamed, Shakir and McKee, Kevin R.},\ntitle = {The Illusion of Artificial Inclusion},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642703},\ndoi = {10.1145/3613904.3642703},\nabstract = {Human participants play a central role in the development of modern artificial intelligence (AI) technology, in psychological science, and in user research. Recent advances in generative AI have attracted growing interest to the possibility of replacing human participants in these domains with AI surrogates. We survey several such “substitution proposals” to better understand the arguments for and against substituting human participants with modern generative AI. Our scoping review indicates that the recent wave of these proposals is motivated by goals such as reducing the costs of research and development work and increasing the diversity of collected data. However, these proposals ignore and ultimately conflict with foundational values of work with human participants: representation, inclusion, and understanding. This paper critically examines the principles and goals underlying human participation to help chart out paths for future work that truly centers and empowers participants.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {286},\nnumpages = {12},\nkeywords = {AI development, Human participants, generative AI, inclusion, language models, participation, representation, understanding, user research},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642403,\nauthor = {Ladak, Ali and Harris, Jamie and Anthis, Jacy Reese},\ntitle = {Which Artificial Intelligences Do People Care About Most? A Conjoint Experiment on Moral Consideration},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642403},\ndoi = {10.1145/3613904.3642403},\nabstract = {Many studies have identified particular features of artificial intelligences (AI), such as their autonomy and emotion expression, that affect the extent to which they are treated as subjects of moral consideration. However, there has not yet been a comparison of the relative importance of features as is necessary to design and understand increasingly capable, multi-faceted AI systems. We conducted an online conjoint experiment in which 1,163 participants evaluated descriptions of AIs that varied on these features. All 11 features increased how morally wrong participants considered it to harm the AIs. The largest effects were from human-like physical bodies and prosociality (i.e., emotion expression, emotion recognition, cooperation, and moral judgment). For human-computer interaction designers, the importance of prosociality suggests that, because AIs are often seen as threatening, the highest levels of moral consideration may only be granted if the AI has positive intentions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {287},\nnumpages = {11},\nkeywords = {Anthropomorphism, Conjoint experiment, Human-AI interaction, Human-computer interaction, Human-likeness, Morality, Prosociality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642166,\nauthor = {Mack, Kelly Avery and Qadri, Rida and Denton, Remi and Kane, Shaun K. and Bennett, Cynthia L.},\ntitle = {“They only care to show us the wheelchair”: disability representation in text-to-image AI models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642166},\ndoi = {10.1145/3613904.3642166},\nabstract = {This paper reports on disability representation in images output from text-to-image (T2I) generative AI systems. Through eight focus groups with 25 people with disabilities, we found that models repeatedly presented reductive archetypes for different disabilities. Often these representations reflected broader societal stereotypes and biases, which our participants were concerned to see reproduced through T2I. Our participants discussed further challenges with using these models including the current reliance on prompt engineering to reach satisfactorily diverse results. Finally, they offered suggestions for how to improve disability representation with solutions like showing multiple, heterogeneous images for a single prompt and including the prompt with images generated. Our discussion reflects on tensions and tradeoffs we found among the diverse perspectives shared to inform future research on representation-oriented generative AI system evaluation metrics and development processes.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {288},\nnumpages = {23},\nkeywords = {AI harms, algorithmic harms, disability representation, generative AI, human-centered AI, text-to-image models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642436,\nauthor = {Gray, Colin M. and Santos, Cristiana Teixeira and Bielova, Nataliia and Mildner, Thomas},\ntitle = {An Ontology of Dark Patterns Knowledge: Foundations, Definitions, and a Pathway for Shared Knowledge-Building},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642436},\ndoi = {10.1145/3613904.3642436},\nabstract = {Deceptive and coercive design practices are increasingly used by companies to extract profit, harvest data, and limit consumer choice. Dark patterns represent the most common contemporary amalgamation of these problematic practices, connecting designers, technologists, scholars, regulators, and legal professionals in transdisciplinary dialogue. However, a lack of universally accepted definitions across the academic, legislative, practitioner, and regulatory space has likely limited the impact that scholarship on dark patterns might have in supporting sanctions and evolved design practices. In this paper, we seek to support the development of a shared language of dark patterns, harmonizing ten existing regulatory and academic taxonomies of dark patterns and proposing a three-level ontology with standardized definitions for 64 synthesized dark pattern types across low-, meso-, and high-level patterns. We illustrate how this ontology can support translational research and regulatory action, including transdisciplinary pathways to extend our initial types through new empirical work across application and technology domains.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {289},\nnumpages = {22},\nkeywords = {dark patterns, deceptive design, ontology, regulation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642054,\nauthor = {Pang, Rock Yuren and Santy, Sebastin and Just, Ren\\'{e} and Reinecke, Katharina},\ntitle = {BLIP: Facilitating the Exploration of Undesirable Consequences of Digital Technologies},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642054},\ndoi = {10.1145/3613904.3642054},\nabstract = {Digital technologies have positively transformed society, but they have also led to undesirable consequences not anticipated at the time of design or development. We posit that insights into past undesirable consequences can help researchers and practitioners gain awareness and anticipate potential adverse effects. To test this assumption, we introduce Blip, a system that extracts real-world undesirable consequences of technology from online articles, summarizes and categorizes them, and presents them in an interactive, web-based interface. In two user studies with 15 researchers in various computer science disciplines, we found that Blip substantially increased the number and diversity of undesirable consequences they could list in comparison to relying on prior knowledge or searching online. Moreover, Blip helped them identify undesirable consequences relevant to their ongoing projects, made them aware of undesirable consequences they “had never considered,” and inspired them to reflect on their own experiences with technology.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {290},\nnumpages = {18},\nkeywords = {NLP, computer ethics, societal impacts, undesirable consequences},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642781,\nauthor = {Caragay, Evan and Xiong, Katherine and Zong, Jonathan and Jackson, Daniel},\ntitle = {Beyond Dark Patterns: A Concept-Based Framework for Ethical Software Design},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642781},\ndoi = {10.1145/3613904.3642781},\nabstract = {Current dark pattern research tells designers what not to do, but how do they know what to do? In contrast to prior approaches that focus on patterns to avoid and their underlying principles, we present a framework grounded in positive expected behavior against which deviations can be judged. To articulate this expected behavior, we use concepts—abstract units of functionality that compose applications. We define a design as dark when its concepts violate users’ expectations, and benefit the application provider at the user’s expense. Though user expectations can differ, users tend to develop common expectations as they encounter the same concepts across multiple applications, which we can record in a concept catalog as standard concepts. We evaluate our framework and concept catalog through three studies, illustrating their ability to describe existing dark patterns, evaluate nuanced designs, and document common application functionality.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {291},\nnumpages = {16},\nkeywords = {concepts, dark patterns, design, ethics, procedural and substantive theory, user expectations},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642678,\nauthor = {Chong, Andrew and Yoo, Ji Su and Cheshire, Coye},\ntitle = {Perceptions of Fairness in Technology-Mediated Marketplaces},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642678},\ndoi = {10.1145/3613904.3642678},\nabstract = {Consumers increasingly interact with workers through technology-mediated marketplaces (TMMs)—environments where third-party companies manage interactions, control information, and constrain behavioral choices. We argue that opacity in how TMMs operate can make it difficult for consumers to judge what is fair when interacting with other economic actors. To better understand how consumers perceive and act on fairness in TMMs, we examine the practice of tipping—a consumer behavior in the United States that is strongly associated with assessments of fairness. Through interviews with consumers, we find three distinct ways that consumers discuss fairness in tipping in third-party food delivery: fairness as supporting a living wage, fairness as reciprocity, and fairness in distribution of payments. We discuss how TMMs codify economic interactions and change consumers’ social meaning of a tip, how consumers perceive an obligation to tip drivers differently in TMMs, and how TMMs alter information consumers use to determine accountability.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {292},\nnumpages = {13},\nkeywords = {fair tipping, fairness, gig economy, technology-mediated marketplaces, technology-mediated markets, third-party food delivery platforms},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642111,\nauthor = {Kabir, Samia and Li, Lixiang and Zhang, Tianyi},\ntitle = {STILE: Exploring and Debugging Social Biases in Pre-trained Text Representations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642111},\ndoi = {10.1145/3613904.3642111},\nabstract = {The recent success of Natural Language Processing (NLP) relies heavily on pre-trained text representations such as word embeddings. However, pre-trained text representations may exhibit social biases and stereotypes, e.g., disproportionately associating gender with occupations. Though prior work presented various bias detection algorithms, they are limited to pre-defined biases and lack effective interaction support. In this work, we propose Stile, an interactive system that supports mixed-initiative bias discovery and debugging in pre-trained text representations. Stile provides users the flexibility to interactively define and customize biases to detect based on their interests. Furthermore, it provides a bird’s-eye view of detected biases in a Chord diagram and allows users to dive into the training data to investigate how a bias was developed. Our lab study and expert review confirm the usefulness and usability of Stile as an effective aid in identifying and understanding biases in pre-trained text representations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {293},\nnumpages = {20},\nkeywords = {AI Fairness, Natural Language Processing, Word Embedding},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642398,\nauthor = {Berman, Glen and Goyal, Nitesh and Madaio, Michael},\ntitle = {A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642398},\ndoi = {10.1145/3613904.3642398},\nabstract = {Responsible design of AI systems is a shared goal across HCI and AI communities. Responsible AI (RAI) tools have been developed to support practitioners to identify, assess, and mitigate ethical issues during AI development. These tools take many forms (e.g., design playbooks, software toolkits, documentation protocols). However, research suggests that use of RAI tools is shaped by organizational contexts, raising questions about how effective such tools are in practice. To better understand how RAI tools are—and might be—evaluated, we conducted a qualitative analysis of 37 publications that discuss evaluations of RAI tools. We find that most evaluations focus on usability, while questions of tools’ effectiveness in changing AI development are sidelined. While usability evaluations are an important approach to evaluate RAI tools, we draw on evaluation approaches from other fields to highlight developer- and community-level steps to support evaluations of RAI tools’ effectiveness in shaping AI development practices and outcomes.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {294},\nnumpages = {24},\nkeywords = {AI, effectiveness, ethics, evaluation, fairness, responsibility, toolkits},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642302,\nauthor = {Gray, Colin M. and Obi, Ike and Chivukula, Shruthi Sai and Li, Ziqing and Carlock, Thomas V and Will, Matthew S and Pivonka, Anne C and Johns, Janna and Rigsbee, Brookley and Menon, Ambika R and Bharadwaj, Aayushi},\ntitle = {Building an Ethics-Focused Action Plan: Roles, Process Moves, and Trajectories},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642302},\ndoi = {10.1145/3613904.3642302},\nabstract = {Design and technology practitioners are increasingly aware of the ethical impact of their work practices, desiring tools to support their ethical awareness across a range of contexts. In this paper, we report on findings from a series of six co-creation workshops with 26 technology and design practitioners that supported their creation of a bespoke ethics-focused action plan. Using a qualitative content analysis and thematic analysis approach, we identified a range of roles and process moves that practitioners and design students with professional experience employed and illustrate the interplay of these elements that impacted the creation of their action plan and revealed aspects of their ethical design complexity. We conclude with implications for supporting ethics in socio-technical practice and opportunities for the further development of methods that support ethical engagement and are resonant with the realities of practice.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {295},\nnumpages = {17},\nkeywords = {design and technology practice, design method, ethics, instrumental judgment},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642661,\nauthor = {Sch\\\"{a}fer, Ren\\'{e} and Preuschoff, Paul Miles and R\\\"{o}pke, Ren\\'{e} and Sahabi, Sarah and Borchers, Jan},\ntitle = {Fighting Malicious Designs: Towards Visual Countermeasures Against Dark Patterns},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642661},\ndoi = {10.1145/3613904.3642661},\nabstract = {Dark patterns are malicious UI design strategies that nudge users towards decisions going against their best interests. To create technical countermeasures against them, dark patterns must be automatically detectable. While researchers have devised algorithms to detect some patterns automatically, there has only been little work to use obtained results to technically counter the effects of dark patterns when users face them on their devices. To address this, we tested three visual countermeasures against 13 common dark patterns in an interactive lab study. The countermeasures we tested either (a) highlighted and explained the manipulation, (b) hid it from the user, or (c) let the user switch between the original view and the hidden version. From our data, we were able to extract multiple clusters of dark patterns where participants preferred specific countermeasures for similar reasons. To support creating effective countermeasures, we discuss our findings with a recent ontology of dark patterns.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {296},\nnumpages = {13},\nkeywords = {dark patterns, deceptive design, lab study, visual countermeasures},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641945,\nauthor = {Zhang, Haoqi},\ntitle = {Searching for the Non-Consequential: Dialectical Activities in HCI and the Limits of Computers},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641945},\ndoi = {10.1145/3613904.3641945},\nabstract = {This paper examines the pervasiveness of consequentialist thinking in human-computer interaction (HCI), and forefronts the value of non-consequential, dialectical activities in human life. Dialectical activities are human endeavors in which the value of the activity is intrinsic to itself, including being a good friend or parent, engaging in art-making or music-making, conducting research, and so on. I argue that computers—the ultimate consequentialist machinery for reliably transforming inputs into outputs—cannot be the be-all and end-all for promoting human values rooted in dialectical activities. I examine how HCI as a field of study might reconcile the consequentialist machines we have with the dialectical activities we value, and propose computational ecosystems as a vision for HCI that makes proper space for dialectical activities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {297},\nnumpages = {13},\nkeywords = {Dialectical activities, consequentialism, human values in HCI, philosophy in HCI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642881,\nauthor = {Sheil, Ashley and Acar, Gunes and Schraffenberger, Hanna and Gellert, Raphael and Malone, David},\ntitle = {Staying at the Roach Motel: Cross-Country Analysis of Manipulative Subscription and Cancellation Flows},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642881},\ndoi = {10.1145/3613904.3642881},\nabstract = {Subscribing to online services is typically a straightforward process, but cancelling them can be arduous and confusing — causing many to resign and continue paying for services they no longer use. Making the cancellation process intentionally difficult is recognized as a dark pattern called Roach Motel. This paper characterizes the subscription and cancellation flows of popular news websites from four different countries, and discusses them in the context of recent regulatory changes. We study the design features that make it difficult to cancel a subscription and find several cancellation flows that feature intentional barriers, such as forcing users to call a representative or type in a phrase. Further, we find many subscription flows that do not adequately inform users about recurring charges. Our results point to a growing need for effective regulation of designs that trick, coerce, or manipulate users into paying for subscriptions they do not want.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {298},\nnumpages = {24},\nkeywords = {Dark Patterns, Deceptive Designs, Newspapers, Roach Motel, Subscriptions},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642633,\nauthor = {Kloft, Agnes Mercedes and Welsch, Robin and Kosch, Thomas and Villa, Steeven},\ntitle = {\"AI enhances our performance, I have no doubt this one will do the same\": The Placebo effect is robust to negative descriptions of AI},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642633},\ndoi = {10.1145/3613904.3642633},\nabstract = {Heightened AI expectations facilitate performance in human-AI interactions through placebo effects. While lowering expectations to control for placebo effects is advisable, overly negative expectations could induce nocebo effects. In a letter discrimination task, we informed participants that an AI would either increase or decrease their performance by adapting the interface, when in reality, no AI was present in any condition. A Bayesian analysis showed that participants had high expectations and performed descriptively better irrespective of the AI description when a sham-AI was present. Using cognitive modeling, we could trace this advantage back to participants gathering more information. A replication study verified that negative AI descriptions do not alter expectations, suggesting that performance expectations with AI are biased and robust to negative verbal descriptions. We discuss the impact of user expectations on AI interactions and evaluation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {299},\nnumpages = {24},\nkeywords = {Artificial Intelligence, Decision-making, Performance expectation, Placebo},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642564,\nauthor = {Salikutluk, Vildan and Sch\\\"{o}pper, Janik and Herbert, Franziska and Scheuermann, Katrin and Frodl, Eric and Balfanz, Dirk and J\\\"{a}kel, Frank and Koert, Dorothea},\ntitle = {An Evaluation of Situational Autonomy for Human-AI Collaboration in a Shared Workspace Setting},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642564},\ndoi = {10.1145/3613904.3642564},\nabstract = {Designing interactions for human-AI teams (HATs) can be challenging due to an AI agent’s potential autonomy. Previous work suggests that higher autonomy does not always improve team performance, and situation-dependent autonomy adaptation might be beneficial. However, there is a lack of systematic empirical evaluations of such autonomy adaptation in human-AI interaction. Therefore, we propose a cooperative task in a simulated shared workspace to investigate effects of fixed levels of AI autonomy and situation-dependent autonomy adaptation on team performance and user satisfaction. We derive adaptation rules for AI autonomy from previous work and a pilot study. We implement these rule for our main experiment and find that team performance was best when humans collaborated with an agent adjusting its autonomy based on the situation. Additionally, users rated this agent highest in terms of perceived intelligence. From these results, we discuss the influence of varying autonomy degrees on HATs in shared workspaces.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {300},\nnumpages = {17},\nkeywords = {AI Autonomy, Human-AI Interaction, Shared Workspace},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642738,\nauthor = {Haque, MD Romael and Saxena, Devansh and Weathington, Katy and Chudzik, Joseph and Guha, Shion},\ntitle = {Are We Asking the Right Questions?: Designing for Community Stakeholders’ Interactions with AI in Policing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642738},\ndoi = {10.1145/3613904.3642738},\nabstract = {Research into recidivism risk prediction in the criminal justice system has garnered significant attention from HCI, critical algorithm studies, and the emerging field of human-AI decision-making. This study focuses on algorithmic crime mapping, a prevalent yet underexplored form of algorithmic decision support (ADS) in this context. We conducted experiments and follow-up interviews with 60 participants, including community members, technical experts, and law enforcement agents (LEAs), to explore how lived experiences, technical knowledge, and domain expertise shape interactions with the ADS, impacting human-AI decision-making. Surprisingly, we found that domain experts (LEAs) often exhibited anchoring bias, readily accepting and engaging with the first crime map presented to them. Conversely, community members and technical experts were more inclined to engage with the tool, adjust controls, and generate different maps. Our findings highlight that all three stakeholders were able to provide critical feedback regarding AI design and use - community members questioned the core motivation of the tool, technical experts drew attention to the elastic nature of data science practice, and LEAs suggested redesign pathways such that the tool could complement their domain expertise.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {301},\nnumpages = {20},\nkeywords = {algorithmic crime mapping, human-AI decision-making, problem formulation, public sector algorithms},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642446,\nauthor = {Zhang, Dongping and Chatzimparmpas, Angelos and Kamali, Negar and Hullman, Jessica},\ntitle = {Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642446},\ndoi = {10.1145/3613904.3642446},\nabstract = {As deep neural networks are more commonly deployed in high-stakes domains, their black-box nature makes uncertainty quantification challenging. We investigate the effects of presenting conformal prediction sets—a distribution-free class of methods for generating prediction sets with specified coverage—to express uncertainty in AI-advised decision-making. Through a large online experiment, we compare the utility of conformal prediction sets to displays of Top-1 and Top-k predictions for AI-advised image labeling. In a pre-registered analysis, we find that the utility of prediction sets for accuracy varies with the difficulty of the task: while they result in accuracy on par with or less than Top-1 and Top-k displays for easy images, prediction sets excel at assisting humans in labeling out-of-distribution (OOD) images, especially when the set size is small. Our results empirically pinpoint practical challenges of conformal prediction sets and provide implications on how to incorporate them for real-world decision-making.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {302},\nnumpages = {19},\nkeywords = {comparative user experiment, conformal prediction, image labeling, semi-supervised learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641960,\nauthor = {Wang, Xinru and Kim, Hannah and Rahman, Sajjadur and Mitra, Kushan and Miao, Zhengjie},\ntitle = {Human-LLM Collaborative Annotation Through Effective Verification of LLM Labels},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641960},\ndoi = {10.1145/3613904.3641960},\nabstract = {Large language models (LLMs) have shown remarkable performance across various natural language processing (NLP) tasks, indicating their significant potential as data annotators. Although LLM-generated annotations are more cost-effective and efficient to obtain, they are often erroneous for complex or domain-specific tasks and may introduce bias when compared to human annotations. Therefore, instead of completely replacing human annotators with LLMs, we need to leverage the strengths of both LLMs and humans to ensure the accuracy and reliability of annotations. This paper presents a multi-step human-LLM collaborative approach where (1) LLMs generate labels and provide explanations, (2) a verifier assesses the quality of LLM-generated labels, and (3) human annotators re-annotate a subset of labels with lower verification scores. To facilitate human-LLM collaboration, we make use of LLM’s ability to rationalize its decisions. LLM-generated explanations can provide additional information to the verifier model as well as help humans better understand LLM labels. We demonstrate that our verifier is able to identify potentially incorrect LLM labels for human re-annotation. Furthermore, we investigate the impact of presenting LLM labels and explanations on human re-annotation through crowdsourced studies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {303},\nnumpages = {21},\nkeywords = {Human-LLM collaborative annotation, LLM annotation, NLP, self-rationalization, text annotation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642016,\nauthor = {Arawjo, Ian and Swoopes, Chelse and Vaithilingam, Priyan and Wattenberg, Martin and Glassman, Elena L.},\ntitle = {ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642016},\ndoi = {10.1145/3613904.3642016},\nabstract = {Evaluating outputs of large language models (LLMs) is challenging, requiring making—and making sense of—many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {304},\nnumpages = {18},\nkeywords = {auditing, language models, prompt engineering, toolkits, visual programming environments},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642472,\nauthor = {Ha, Juhye and Jeon, Hyeon and Han, Daeun and Seo, Jinwook and Oh, Changhoon},\ntitle = {CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642472},\ndoi = {10.1145/3613904.3642472},\nabstract = {Large language models (LLMs) have facilitated significant strides in generating conversational agents, enabling seamless, contextually relevant dialogues across diverse topics. However, the existing LLM-driven conversational agents have fixed personalities and functionalities, limiting their adaptability to individual user needs. Creating personalized agent personas with distinct expertise or traits can address this issue. Nonetheless, we lack knowledge of how people customize and interact with agent personas. In this research, we investigated how users customize agent personas and their impact on interaction quality, diversity, and dynamics. To this end, we developed CloChat, an interface supporting easy and accurate customization of agent personas in LLMs. We conducted a study comparing how participants interact with CloChat and ChatGPT. The results indicate that participants formed emotional bonds with the customized agents, engaged in more dynamic dialogues, and showed interest in sustaining interactions. These findings contribute to design implications for future systems with conversational agents using LLMs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {305},\nnumpages = {24},\nkeywords = {Conversational Agents, Large Language Models, Persona, Persona Customization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642216,\nauthor = {Kim, Tae Soo and Lee, Yoonjoo and Shin, Jamin and Kim, Young-Ho and Kim, Juho},\ntitle = {EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642216},\ndoi = {10.1145/3613904.3642216},\nabstract = {By simply composing prompts, developers can prototype novel generative applications with Large Language Models (LLMs). To refine prototypes into products, however, developers must iteratively revise prompts by evaluating outputs to diagnose weaknesses. Formative interviews (N=8) revealed that developers invest significant effort in manually evaluating outputs as they assess context-specific and subjective criteria. We present EvalLM, an interactive system for iteratively refining prompts by evaluating multiple outputs on user-defined criteria. By describing criteria in natural language, users can employ the system’s LLM-based evaluator to get an overview of where prompts excel or fail, and improve these based on the evaluator’s feedback. A comparative study (N=12) showed that EvalLM, when compared to manual evaluation, helped participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory prompts with 59\\% fewer revisions. Beyond prompts, our work can be extended to augment model evaluation and alignment in specific application contexts.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {306},\nnumpages = {21},\nkeywords = {Evaluation, Human-AI Interaction, Large Language Models, Natural Language Generation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642542,\nauthor = {Mildner, Thomas and Cooney, Orla and Meck, Anna-Maria and Bartl, Marion and Savino, Gian-Luca and Doyle, Philip R and Garaialde, Diego and Clark, Leigh and Sloan, John and Wenig, Nina and Malaka, Rainer and Niess, Jasmin},\ntitle = {Listening to the Voices: Describing Ethical Caveats of Conversational User Interfaces According to Experts and Frequent Users},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642542},\ndoi = {10.1145/3613904.3642542},\nabstract = {Advances in natural language processing and understanding have led to a rapid growth in the popularity of conversational user interfaces (CUIs). While CUIs introduce novel benefits, they also yield risks that may exploit people’s trust. Although research looking at unethical design deployed through graphical user interfaces (GUIs) established a thorough understanding of so-called dark patterns, there is a need to continue this discourse within the CUI community to understand potentially problematic interactions. Addressing this gap, we interviewed 27 participants from three cohorts: researchers, practitioners, and frequent users of CUIs. Applying thematic analysis, we construct five themes reflecting each cohort’s insights about ethical design challenges and introduce the CUI Expectation Cycle, bridging system capabilities and user expectations while considering each theme’s ethical caveats. This research aims to inform future development of CUIs to consider ethical constraints while adopting a human-centred approach.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {307},\nnumpages = {18},\nkeywords = {CUI, chatbots, conversational agents, conversational user interfaces, dark patterns, deceptive design patterns, ethical design, thematic analysis, voice agents},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641946,\nauthor = {Erlei, Alexander and Sharma, Abhinav and Gadiraju, Ujwal},\ntitle = {Understanding Choice Independence and Error Types in Human-AI Collaboration},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641946},\ndoi = {10.1145/3613904.3641946},\nabstract = {The ability to make appropriate delegation decisions is an important prerequisite of effective human-AI collaboration. Recent work, however, has shown that people struggle to evaluate AI systems in the presence of forecasting errors, falling well short of relying on AI systems appropriately. We use a pre-registered crowdsourcing study (N = 611) to extend this literature by two underexplored crucial features of human AI decision-making: choice independence and error type. Subjects in our study repeatedly complete two prediction tasks and choose which predictions they want to delegate to an AI system. For one task, subjects receive a decision heuristic that allows them to make informed and relatively accurate predictions. The second task is substantially harder to solve, and subjects must come up with their own decision rule. We systematically vary the AI system’s performance such that it either provides the best possible prediction for both tasks or only for one of the two. Our results demonstrate that people systematically violate choice independence by taking the AI’s performance in an unrelated second task into account. Humans who delegate predictions to a superior AI in their own expertise domain significantly reduce appropriate reliance when the model makes systematic errors in a complementary expertise domain. In contrast, humans who delegate predictions to a superior AI in a complementary expertise domain significantly increase appropriate reliance when the model systematically errs in the human expertise domain. Furthermore, we show that humans differentiate between error types and that this effect is conditional on the considered expertise domain. This is the first empirical exploration of choice independence and error types in the context of human-AI collaboration. Our results have broad and important implications for the future design, deployment, and appropriate application of AI systems.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {308},\nnumpages = {19},\nkeywords = {Algorithm Aversion, Complementary AI Systems, Crowdsourcing Study, Decision Support System, Errors, Human-AI Collaboration, Interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642124,\nauthor = {Karaosmanoglu, Sukran and Cmentowski, Sebastian and Nacke, Lennart E. and Steinicke, Frank},\ntitle = {Born to Run, Programmed to Play: Mapping the Extended Reality Exergames Landscape},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642124},\ndoi = {10.1145/3613904.3642124},\nabstract = {Many people struggle to exercise regularly, raising the risk of serious health-related issues. Extended reality (XR) exergames address these hurdles by combining physical exercises with enjoyable, immersive gameplay. While a growing body of research explores XR exergames, no previous review has structured this rapidly expanding research landscape. We conducted a scoping review of the current state of XR exergame research to (i) provide a structured overview, (ii) highlight trends, and (iii) uncover knowledge gaps. After identifying 1318 papers in human-computer interaction and medical databases, we ultimately included 186 papers in our analysis. We provide a quantitative and qualitative summary of XR exergame research, showing current trends and potential future considerations. Finally, we provide a taxonomy of XR exergames to help future design and methodological investigation and reporting.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {309},\nnumpages = {28},\nkeywords = {active games, active video games, augmented reality, exercise, exergames, extended reality, games, mixed reality, motion games, movement games, review, sports games, taxonomy, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642936,\nauthor = {Chen, Xinyu and Li, Yuqi and Chen, Jintao and Li, Jiabao and Wang, Chong and Tang, Pinyan},\ntitle = {Enhancing Home Exercise Experiences with Video Motion-Tracking for Automatic Display Height Adjustment},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642936},\ndoi = {10.1145/3613904.3642936},\nabstract = {The increasing demand for home fitness solutions underscores the need for interactive displays that enhance user experiences. This study introduces a technology that autonomously adjusts display height using the skeletal information of demonstrators from videos, catering to home fitness needs. A user study involving thirty participants compared fixed height, manual adjustment, and automatic adjustment conditions. Head flexion angles and NASA-TLX survey responses were used for evaluation. Results showed a significant reduction in head flexion angles with automatic adjustment, promoting proper spinal alignment. NASA-TLX responses indicated lower mental, effort, and frustration ratings, along with improved performance and perceived support in the automatic adjustment condition compared to other conditions. These findings confirm that motion-based height adjustment improves posture and enhances the overall interactive experience. This research demonstrates the feasibility of integrating responsive ergonomics into interactive displays and suggests the importance of further personalization, conducting diverse user studies, and refining algorithms to fully leverage the potential of this technology.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {310},\nnumpages = {13},\nkeywords = {Automatic Height Adjustment, Head Flexion Angle, Interactive Exercise Displays, NASA-TLX survey, User Experience Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641897,\nauthor = {Adiwangsa, Michelle and Sweetser, Penny and Stevenson, Duncan and Suominen, Hanna and Xi, Mingze},\ntitle = {Exploring Opportunities for Augmenting Homes to Support Exercising},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641897},\ndoi = {10.1145/3613904.3641897},\nabstract = {Although exercising at home has benefits, it is not always engaging or motivating. Augmented Reality (AR) head-mounted displays (HMDs) offer the potential to make in-home exercising and exergaming more inclusive and immersive, but there is limited research investigating how such systems can be designed. We employed a participatory design approach involving semi-structured interviews to investigate how homes can be augmented to facilitate exercising experiences. We developed 10 recommendations for developing home-based exercising experiences using AR HMDs. Our results further contribute to the existing body of research on the use of AR for exercising, home applications, and everyday objects by presenting the first foundational study investigating the wide range of exercises that can be supported through AR HMDs in home environments and the different ways home elements may support these exercises, and laying the groundwork for future work developing home-based exergaming through AR HMDs to increase people’s physical activity levels.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {311},\nnumpages = {14},\nkeywords = {Augmented reality, Exercising, Exergaming, Home environment, Participatory Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642050,\nauthor = {Elvitigala, Don Samitha and Karahano\\u{g}lu, Arma\\u{g}an and Matviienko, Andrii and Turmo Vidal, Laia and Postma, Dees and Jones, Michael D and Montoya, Maria F. and Harrison, Daniel and Elb\\ae{}k, Lars and Daiber, Florian and Burr, Lisa Anneke and Patibanda, Rakesh and Buono, Paolo and H\\\"{a}m\\\"{a}l\\\"{a}inen, Perttu and Van Delden, Robby and Bernhaupt, Regina and Ren, Xipei and Van Rheden, Vincent and Zambetta, Fabio and Van Den Hoven, Elise and Lallemand, Carine and Reidsma, Dennis and Mueller, Florian ‘Floyd’},\ntitle = {Grand Challenges in SportsHCI},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642050},\ndoi = {10.1145/3613904.3642050},\nabstract = {The field of Sports Human-Computer Interaction (SportsHCI) investigates interaction design to support a physically active human being. Despite growing interest and dissemination of SportsHCI literature over the past years, many publications still focus on solving specific problems in a given sport. We believe in the benefit of generating fundamental knowledge for SportsHCI more broadly to advance the field as a whole. To achieve this, we aim to identify the grand challenges in SportsHCI, which can help researchers and practitioners in developing a future research agenda. Hence, this paper presents a set of grand challenges identified in a five-day workshop with 22 experts who have previously researched, designed, and deployed SportsHCI systems. Addressing these challenges will drive transformative advancements in SportsHCI, fostering better athlete performance, athlete-coach relationships, spectator engagement, but also immersive experiences for recreational sports or exercise motivation, and ultimately, improve human well-being.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {312},\nnumpages = {20},\nkeywords = {Physical Activity, Sports technology, grand challenges},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642051,\nauthor = {Karahano\\u{g}lu, Arma\\u{g}an and Coskun, Aykut and Postma, Dees and Scheltinga, Bouke Leonard and Gouveia, R\\'{u}ben and Reidsma, Dennis and Reenalda, Jasper},\ntitle = {Is it just a score? Understanding Training Load Management Practices Beyond Sports Tracking},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642051},\ndoi = {10.1145/3613904.3642051},\nabstract = {Training Load Management (TLM) is crucial for achieving optimal athletic performance and preventing chronic sports injuries. Current sports trackers provide runners with data to manage their training load. However, little is known about the extent and the way sports trackers are used for TLM. We conducted a survey (N=249) and interviews (N=24) with runners to understand sports tracker use in TLM practices. We found that runners possess some understanding of training load and generally trust their trackers to provide accurate training load-related data. Still, they hesitate to strictly follow trackers’ suggestions in managing their training load, often relying on their intuitions and body signals to determine and adapt training plans. Our findings contribute to SportsHCI research by shedding light on how sports trackers are incorporated into TLM practices and providing implications for developing trackers that better support runners in managing their training load.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {313},\nnumpages = {18},\nkeywords = {SportsHCI, Training load management, human-data interaction, personal informatics, running, sports tracking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642106,\nauthor = {Bhattacharya, Aditya and Stumpf, Simone and Gosak, Lucija and Stiglic, Gregor and Verbert, Katrien},\ntitle = {EXMOS: Explanatory Model Steering through Multifaceted Explanations and Data Configurations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642106},\ndoi = {10.1145/3613904.3642106},\nabstract = {Explanations in interactive machine-learning systems facilitate debugging and improving prediction models. However, the effectiveness of various global model-centric and data-centric explanations in aiding domain experts to detect and resolve potential data issues for model improvement remains unexplored. This research investigates the influence of data-centric and model-centric global explanations in systems that support healthcare experts in optimising models through automated and manual data configurations. We conducted quantitative (n=70) and qualitative (n=30) studies with healthcare experts to explore the impact of different explanations on trust, understandability and model improvement. Our results reveal the insufficiency of global model-centric explanations for guiding users during data configuration. Although data-centric explanations enhanced understanding of post-configuration system changes, a hybrid fusion of both explanation types demonstrated the highest effectiveness. Based on our study results, we also present design implications for effective explanation-driven interactive machine-learning systems.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {314},\nnumpages = {27},\nkeywords = {Explainable AI, Explanatory Interactive Learning, Human-centered AI, IML, Interactive Machine Learning, Interpretable AI, Model Steering, Responsible AI, XAI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642689,\nauthor = {Bo, Jessica Y and Hao, Pan and Lim, Brian Y},\ntitle = {Incremental XAI: Memorable Understanding of AI with Incremental Explanations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642689},\ndoi = {10.1145/3613904.3642689},\nabstract = {Many explainable AI (XAI) techniques strive for interpretability by providing concise salient information, such as sparse linear factors. However, users either only see inaccurate global explanations, or highly-varying local explanations. We propose to provide more detailed explanations by leveraging the human cognitive capacity to accumulate knowledge by incrementally receiving more details. Focusing on linear factor explanations (factors \\texttimes{} values = outcome), we introduce Incremental XAI to automatically partition explanations for general and atypical instances by providing Base + Incremental factors to help users read and remember more faithful explanations. Memorability is improved by reusing base factors and reducing the number of factors shown in atypical cases. In modeling, formative, and summative user studies, we evaluated the faithfulness, memorability and understandability of Incremental XAI against baseline explanation methods. This work contributes towards more usable explanation that users can better ingrain to facilitate intuitive engagement with AI.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {315},\nnumpages = {17},\nkeywords = {cognitive load, explainable AI, explanations, memory},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642474,\nauthor = {Ehsan, Upol and Passi, Samir and Liao, Q. Vera and Chan, Larry and Lee, I-Hsiang and Muller, Michael and Riedl, Mark O},\ntitle = {The Who in XAI: How AI Background Shapes Perceptions of AI Explanations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642474},\ndoi = {10.1145/3613904.3642474},\nabstract = {Explainability of AI systems is critical for users to take informed actions. Understanding who opens the black-box of AI is just as important as opening it. We conduct a mixed-methods study of how two different groups—people with and without AI background—perceive different types of AI explanations. Quantitatively, we share user perceptions along five dimensions. Qualitatively, we describe how AI background can influence interpretations, elucidating the differences through lenses of appropriation and cognitive heuristics. We find that (1) both groups showed unwarranted faith in numbers for different reasons and (2) each group found value in different explanations beyond their intended design. Carrying critical implications for the field of XAI, our findings showcase how AI generated explanations can have negative consequences despite best intentions and how that could lead to harmful manipulation of trust. We propose design interventions to mitigate them.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {316},\nnumpages = {32},\nkeywords = {Explainable AI, Human-Centered Explainable AI, User Characteristics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642352,\nauthor = {Nimmo, Robert and Constantinides, Marios and Zhou, Ke and Quercia, Daniele and Stumpf, Simone},\ntitle = {User Characteristics in Explainable AI: The Rabbit Hole of Personalization?},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642352},\ndoi = {10.1145/3613904.3642352},\nabstract = {As Artificial Intelligence (AI) becomes ubiquitous, the need for Explainable AI (XAI) has become critical for transparency and trust among users. A significant challenge in XAI is catering to diverse users, such as data scientists, domain experts, and end-users. Recent research has started to investigate how users’ characteristics impact interactions with and user experience of explanations, with a view to personalizing XAI. However, are we heading down a rabbit hole by focusing on unimportant details? Our research aimed to investigate how user characteristics are related to using, understanding, and trusting an AI system that provides explanations. Our empirical study with 149 participants who interacted with an XAI system that flagged inappropriate comments showed that very few user characteristics mattered; only age and the personality trait openness influenced actual understanding. Our work provides evidence to reorient user-focused XAI research and question the pursuit of personalized XAI based on fine-grained user characteristics. Disclaimer: This paper contains examples of language that some people may find offensive.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {317},\nnumpages = {13},\nkeywords = {explainable AI, human-centered artificial intelligence, personalization, trust, user characteristics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642535,\nauthor = {Aljuneidi, Saja and Heuten, Wilko and Abdenebaoui, Larbi and Wolters, Maria K and Boll, Susanne},\ntitle = {Why the Fine, AI? The Effect of Explanation Level on Citizens' Fairness Perception of AI-based Discretion in Public Administrations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642535},\ndoi = {10.1145/3613904.3642535},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {318},\nnumpages = {18},\nkeywords = {Administrative discretion, Algorithmic decision-making, Distributive fairness, Explainable AI, Informational fairness},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642613,\nauthor = {Li, Ke and Zhang, Ruidong and Chen, Siyuan and Chen, Boao and Sakashita, Mose and Guimbretiere, Francois and Zhang, Cheng},\ntitle = {EyeEcho: Continuous and Low-power Facial Expression Tracking on Glasses},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642613},\ndoi = {10.1145/3613904.3642613},\nabstract = {In this paper, we introduce EyeEcho, a minimally-obtrusive acoustic sensing system designed to enable glasses to continuously monitor facial expressions. It utilizes two pairs of speakers and microphones mounted on glasses, to emit encoded inaudible acoustic signals directed towards the face, capturing subtle skin deformations associated with facial expressions. The reflected signals are processed through a customized machine-learning pipeline to estimate full facial movements. EyeEcho samples at 83.3 Hz with a relatively low power consumption of 167mW. Our user study involving 12 participants demonstrates that, with just four minutes of training data, EyeEcho achieves highly accurate tracking performance across different real-world scenarios, including sitting, walking, and after remounting the devices. Additionally, a semi-in-the-wild study involving 10 participants further validates EyeEcho’s performance in naturalistic scenarios while participants engage in various daily activities. Finally, we showcase EyeEcho’s potential to be deployed on a commercial-off-the-shelf (COTS) smartphone, offering real-time facial expression tracking.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {319},\nnumpages = {24},\nkeywords = {Acoustic Sensing, Eye-mounted Wearable, Facial Expression Tracking, Low-power},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642348,\nauthor = {Pandey, Laxmi and Arif, Ahmed Sabbir},\ntitle = {MELDER: The Design and Evaluation of a Real-time Silent Speech Recognizer for Mobile Devices},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642348},\ndoi = {10.1145/3613904.3642348},\nabstract = {Silent speech is unaffected by ambient noise, increases accessibility, and enhances privacy and security. Yet current silent speech recognizers operate in a phrase-in/phrase-out manner, thus are slow, error prone, and impractical for mobile devices. We present MELDER, a Mobile Lip Reader that operates in real-time by splitting the input video into smaller temporal segments to process them individually. An experiment revealed that this substantially improves computation time, making it suitable for mobile devices. We further optimize the model for everyday use by exploiting the knowledge from a high-resource vocabulary using a transfer learning model. We then compare MELDER in both stationary and mobile settings with two state-of-the-art silent speech recognizers, where MELDER demonstrated superior overall performance. Finally, we compare two visual feedback methods of MELDER with the visual feedback method of Google Assistant. The outcomes shed light on how these proposed feedback methods influence users’ perceptions of the model’s performance.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {320},\nnumpages = {23},\nkeywords = {Silent speech, deep learning, digital lip reading, image processing, language modeling, text input, transfer learning, visual feedback},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642095,\nauthor = {Dong, Xuefu and Chen, Yifei and Nishiyama, Yuuki and Sezaki, Kaoru and Wang, Yuntao and Christofferson, Ken and Mariakakis, Alex},\ntitle = {ReHEarSSE: Recognizing Hidden-in-the-Ear Silently Spelled Expressions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642095},\ndoi = {10.1145/3613904.3642095},\nabstract = {Silent speech interaction (SSI) allows users to discreetly input text without using their hands. Existing wearable SSI systems typically require custom devices and are limited to a small lexicon, limiting their utility to a small set of command words. This work proposes ReHEarSSE, an earbud-based ultrasonic SSI system capable of generalizing to words that do not appear in its training dataset, providing support for nearly an entire dictionary’s worth of words. As a user silently spells words, ReHEarSSE uses autoregressive features to identify subtle changes in ear canal shape. ReHEarSSE infers words using a deep learning model trained to optimize connectionist temporal classification (CTC) loss with an intermediate embedding that accounts for different letters and transitions between them. We find that ReHEarSSE recognizes 100 unseen words with an accuracy of 89.3\\%.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {321},\nnumpages = {16},\nkeywords = {acoustic sensing, autoregressive model, earable computing, silent speech interface, text entry},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642086,\nauthor = {Grootjen, Jesse W. and Weing\\\"{a}rtner, Henrike and Mayer, Sven},\ntitle = {Uncovering and Addressing Blink-Related Challenges in Using Eye Tracking for Interactive Systems},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642086},\ndoi = {10.1145/3613904.3642086},\nabstract = {Currently, interactive systems use physiological sensing to enable advanced functionalities. While eye tracking is a promising means to understand the user, eye tracking data inherently suffers from missing data due to blinks, which may result in reduced system performance. We conducted a literature review to understand how researchers deal with this issue. We uncovered that researchers often implemented their use-case-specific pipeline to overcome the issue, ranging from ignoring missing data to artificial interpolation. With these first insights, we run a large-scale analysis on 11 publicly available datasets to understand the impact of the various approaches on data quality and accuracy. By this, we highlight the pitfalls in data processing and which methods work best. Based on our results, we provide guidelines for handling eye tracking data for interactive systems. Further, we propose a standard data processing pipeline that allows researchers and practitioners to pre-process and standardize their data efficiently.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {322},\nnumpages = {23},\nkeywords = {blinks, eye tracking, human computer interaction, interactive systems},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642092,\nauthor = {Wang, Xue and Su, Zixiong and Rekimoto, Jun and Zhang, Yang},\ntitle = {Watch Your Mouth: Silent Speech Recognition with Depth Sensing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642092},\ndoi = {10.1145/3613904.3642092},\nabstract = {Silent speech recognition is a promising technology that decodes human speech without requiring audio signals, enabling private human-computer interactions. In this paper, we propose Watch Your Mouth, a novel method that leverages depth sensing to enable accurate silent speech recognition. By leveraging depth information, our method provides unique resilience against environmental factors such as variations in lighting and device orientations, while further addressing privacy concerns by eliminating the need for sensitive RGB data. We started by building a deep-learning model that locates lips using depth data. We then designed a deep learning pipeline to efficiently learn from point clouds and translate lip movements into commands and sentences. We evaluated our technique and found it effective across diverse sensor locations: On-Head, On-Wrist, and In-Environment. Watch Your Mouth outperformed the state-of-the-art RGB-based method, demonstrating its potential as an accurate and reliable input technique.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {323},\nnumpages = {15},\nkeywords = {Deep Learning, Depth Sensing, Input Techniques, Lip Reading, Silent Speech Recognition, Visual Speech Recognition},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642949,\nauthor = {Lin, Yuyu and Gonzalez, Jesse T and Cui, Zhitong and Banka, Yash Rajeev and Ion, Alexandra},\ntitle = {ConeAct: A Multistable Actuator for Dynamic Materials},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642949},\ndoi = {10.1145/3613904.3642949},\nabstract = {Complex actuators in a small form factor are essential for dynamic interfaces. In this paper, we propose ConeAct, a cone-shaped actuator that can extend, contract, and bend in multiple directions to support rich expression in dynamic materials. A key benefit of our actuator is that it is self-contained and portable as the whole system. We designed our actuator’s structure to be multistable to hold its shape passively, while we control its transition between states using active materials, i.e., shape memory alloys. We present the design space by showcasing our actuator module as part of self-rolling robots, reconfigurable deployable structures, volumetric shape-changing objects and tactile displays. To assist users in designing such structures, we present an interactive editor including simulation to design such interactive capabilities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {324},\nnumpages = {16},\nkeywords = {Dynamic materials, Fabrication, Programmable Matter},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642802,\nauthor = {Deshpande, Himani and Han, Bo and Moon, Kongpyung (Justin) and Bianchi, Andrea and Zheng, Clement and Kim, Jeeeun},\ntitle = {Reconfigurable Interfaces by Shape Change and Embedded Magnets},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642802},\ndoi = {10.1145/3613904.3642802},\nabstract = {Reconfigurable physical interfaces empower users to swiftly adapt to tailored design requirements or preferences. Shape-changing interfaces enable such reconfigurability, avoiding the cost of refabrication or part replacements. Nonetheless, reconfigurable interfaces are often bulky, expensive, or inaccessible. We propose a reversible shape-changing mechanism that enables reconfigurable 3D printed structures via translations and rotations of parts. We investigate fabrication techniques that enable reconfiguration using magnets and the thermoplasticity of heated polymer. Proposed interfaces achieve tunable haptic feedback and adjustment of different user affordances by reconfiguring input motions. The design space is demonstrated through applications in rehabilitation, embodied communication, accessibility, safety, and gaming.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {325},\nnumpages = {12},\nkeywords = {Fabrication, Magnets, Shape Change},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642891,\nauthor = {Cui, Zhitong and Wang, Shuhong and Han, Violet Yinuo and Rae-Grant, Tucker and Yang, Willa Yunqi and Zhu, Alan and Hudson, Scott E and Ion, Alexandra},\ntitle = {Robotic Metamaterials: A Modular System for Hands-On Configuration of Ad-Hoc Dynamic Applications},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642891},\ndoi = {10.1145/3613904.3642891},\nabstract = {We propose augmenting initially passive structures built from simple repeated cells, with novel active units to enable dynamic, shape-changing, and robotic applications. Inspired by metamaterials that can employ mechanisms, we build a framework that allows users to configure cells of this passive structure to allow it to perform complex tasks. A key benefit is that our structures can be repeatedly (re)configured by users inserting our configuration units to turn the passive material into, e.g., locomotion robots, integrated motion platforms, or interactive interfaces, as we demonstrate in this paper. To this end, we present a mechanical system consisting of a flexible, passive, shearing lattice structure, as well as rigid and active unit cells to be inserted into the lattice for configuration. The active unit is a closed-loop pneumatically controlled shearing cell to dynamically actuate the macroscopic movement of the structure. The passive rigid cells redirect the forces to create complex motion with a reduced number of active cells. Since the placement of the rigid and active units is challenging, we offer a computational design tool. The tool optimizes the cell placement to match the macroscopic, user-defined target motions and generates the control code for the active cells.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {326},\nnumpages = {15},\nkeywords = {3D printing, HCI, elasticity, fabrication, metamaterials, programmable matter, reconfigure},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641958,\nauthor = {Lyu, Yahui and Urata, Taiga and Garzanti, Alessandro and Jiang, Ziyuan and Garcia Fernandez, Carlos and Kakehi, Yasuaki},\ntitle = {TensionFab: Fabrication of Room-scale Surface Structures From the Tension-Active Form of Planar Modules},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641958},\ndoi = {10.1145/3613904.3641958},\nabstract = {We propose TensionFab, a novel technique for creating shape-changeable room-scale structures. This method employs easily available planar materials (plywood, MDF), cuts them into multiple 2D shapes, and then connects the pieces manually to create a unified structure capable of 2D and 3D deformations. Constructed TensionFab structures are characterized by easy achievability of target surfaces, substantial structural strength, shape changeability, time and material savings, and easy storage and transportation. In this paper, we introduce the basic principles of shape-making, module combination, and structural characteristics of TensionFab. We also developed a design assistance tool to allow users to automate design based on the target shape. We evaluate the design results for shape validation and structural performance. Finally, we validate the approach with actual construction and propose a variety of application scenarios. Overall, TensionFab is an efficient strategy for spatial design and structural organization. This paper contributes to research and exploration in the HCI room-scale interaction field.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {327},\nnumpages = {16},\nkeywords = {2D cutting, Fabrication, Room-scale structure, Shape inverse design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642243,\nauthor = {Stemasov, Evgeny and Wagner, Tobias and Askari, Ali and Janek, Jessica and Rajabi, Omid and Schikorr, Anja and Frommel, Julian and Gugenheimer, Jan and Rukzio, Enrico},\ntitle = {DungeonMaker: Embedding Tangible Creation and Destruction in Hybrid Board Games through Personal Fabrication Technology},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642243},\ndoi = {10.1145/3613904.3642243},\nabstract = {Hybrid board games (HBGs) augment their analog origins digitally (e.g., through apps) and are an increasingly popular pastime activity. Continuous world and character development and customization, known to facilitate engagement in video games, remain rare in HBGs. If present, they happen digitally or imaginarily, often leaving physical aspects generic. We developed DungeonMaker, a fabrication-augmented HBG bridging physical and digital game elements: 1) the setup narrates a story and projects a digital game board onto a laser cutter; 2) DungeonMaker assesses player-crafted artifacts; 3) DungeonMaker’s modified laser head senses and moves player- and non-player figures, and 4) can physically damage figures. An evaluation (n = 4 \\texttimes{} 3) indicated that DungeonMaker provides an engaging experience, may support players’ connection to their figures, and potentially spark novices’ interest in fabrication. DungeonMaker provides a rich constellation to play HBGs by blending aspects of craft and automation to couple the physical and digital elements of an HBG tightly.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {328},\nnumpages = {20},\nkeywords = {3D-Printers, Board Games, Craft Games, Fabrication Games, Hybrid Board Games, Laser Cutters, Personal Fabrication, Playful Fabrication},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642734,\nauthor = {Campos Zamora, Daniel and Dogan, Mustafa Doga and Siu, Alexa F and Koh, Eunyee and Xiao, Chang},\ntitle = {Moir\\'{e}Widgets: High-Precision, Passive Tangible Interfaces via Moir\\'{e} Effect},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642734},\ndoi = {10.1145/3613904.3642734},\nabstract = {We introduce Moir\\'{e}Widgets, a novel approach for tangible interaction that harnesses the Moir\\'{e} effect—a prevalent optical phenomenon—to enable high-precision event detection on physical widgets. Unlike other electronics-free tangible user interfaces which require close coupling with external hardware, Moir\\'{e}Widgets can be used at greater distances while maintaining high-resolution sensing of interactions. We define a set of interaction primitives, e.g., buttons, sliders, and dials, which can be used as standalone objects or combined to build complex physical controls. These consist of 3D printed structural mechanisms with patterns printed on two layers—one on paper and the other on a plastic transparency sheet—which create a visual signal that amplifies subtle movements, enabling the detection of user inputs. Our technical evaluation shows that our method outperforms standard fiducial markers and maintains sub-millimeter accuracy at 100 cm distance and wide viewing angles. We demonstrate our approach by creating an audio console and indicate how our approach could extend to other domains.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {329},\nnumpages = {10},\nkeywords = {Fabrication, Moir\\'{e} Effect, Moir\\'{e} Pattern, Tangible Interaction, Vision-Based Sensing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642571,\nauthor = {Ye, Qian and Yong, Zhen Zhou and Han, Bo and Yen, Ching Chiuan and Zheng, Clement},\ntitle = {PaperTouch: Tangible Interfaces through Paper Craft and Touchscreen Devices},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642571},\ndoi = {10.1145/3613904.3642571},\nabstract = {Paper and touchscreen devices are two common objects found around us, and we investigated the potential of their intersection for tangible interface design. In this research, we developed PaperTouch, an approach to design paper based mechanisms that translate a variety of physical interactions to touch events on a capacitive touchscreen. These mechanisms act as switches that close during interaction, connecting the touchscreen to the device’s ground bus. To develop PaperTouch, we explored different types of paper along with the making process around them. We also built a range of applications to showcase different tangible interfaces facilitated with PaperTouch, including music instruments, educational dioramas, and playful products. By reflecting on this exploration, we uncovered the emerging design dimensions that considers the interactions, materiality, and embodiment of PaperTouch interfaces. We also surfaced the tacit know-how that we gained through our design process through annotations for others to refer to.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {330},\nnumpages = {15},\nkeywords = {Making, Paper, Tangible Interfaces, Touch Screens},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641952,\nauthor = {Steer, Cameron and Sauv\\'{e}, Kim and Jain, Anika and Lawal, Omosunmisola and Proulx, Michael J and Jicol, Crescent and Alexander, Jason},\ntitle = {Squishy, Yet Satisfying: Exploring Deformable Shapes' Cross-Modal Correspondences with Colours and Emotions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641952},\ndoi = {10.1145/3613904.3641952},\nabstract = {Surfaces with deformable and shape-changing properties seek to enhance and diversify tangible interactions with computing systems. However, we currently lack fundamental knowledge and user interface design principles that connect the inherent properties of deformable shapes with our human senses and cognitive associations. To address this knowledge gap, we systematically explored deformable shapes’ cross-modal correspondences (CC) with colours and emotions. In our CC study, 52 participants were presented with deformable shape stimuli that varied in stiffness and angularity. They were asked to associate these stimuli with colours and emotions under (i) visuo-tactile and; (ii) tactile-only conditions. For the first time, our findings reveal (1) how stiffness level primarily influences the CC associations and; (2) that stiffness and angularity play a significant role in CC associations over the visibility of the shapes. The results were distilled into design guidelines for future deformable, shape-changing interfaces that engage specific human senses and responses.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {331},\nnumpages = {20},\nkeywords = {Colour, Crossmodal Correspondences, Deformable, Emotion, Force, Shape, Tactile-only, Touch, Visuo-tactile},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642304,\nauthor = {Shi, Yonghao and Li, Chenzheng and Su, Yuning and Yang, Xing-Dong and Wu, Te-Yen},\ntitle = {WooDowel: Electrode Isolation for Electromagnetic Shielding in Triboelectric Plywood Sensors},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642304},\ndoi = {10.1145/3613904.3642304},\nabstract = {We present a new approach to address the challenges associated with maintaining the functionality of triboelectric vibration sensors in smart plywood during woodworking operations involving nails and screws. The current state-of-the-art sensor design employs non-overlapping electrodes, which unfortunately leads to significant compromises in terms of signal strength and clarity, particularly in real-world scenarios that involve electromagnetic (EM) interference. To overcome these limitations, we propose a method that enables the woodworker to manually isolate short-circuited electrodes. This method facilitates the creation of sensors using overlapping electrodes, while also incorporating EM shielding, thereby resulting in a substantial improvement in the sensor’s robustness when detecting user activities. To validate the effectiveness of our proposed approach, we conducted a series of experiments, which not only shed light on the drawbacks of non-overlapping electrode designs but also demonstrated the significant improvements achieved through our method.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {332},\nnumpages = {17},\nkeywords = {Smart Environment, TENG, computational material},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642708,\nauthor = {Hanton, Ollie and Fraser, Mike and Roudaut, Anne},\ntitle = {DisplayFab: The State of the Art and a Roadmap in the Personal Fabrication of Free-Form Displays Using Active Materials and Additive Manufacturing.},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642708},\ndoi = {10.1145/3613904.3642708},\nabstract = {Over recent years, there has been significant research within HCI towards free-form physical interactive devices. However, such devices are not straightforward to design, produce and deploy on demand. Traditional development revolves around iterative prototyping through component-based assembly, limiting device structure and implementation. Material-centric personal display fabrication (DisplayFab) opens the possibility of decentralised, configurable production by low-skill makers. Currently, DisplayFab is severely limited by its embryonic stage of development, the complexity of involved processes and materials, and the challenges around designing interactive structures. We present a development framework to provide a path for future research. DisplayFab has been developed by identifying 4 key breakpoints in the existing “Personal Fabrication” framework: Material and Deposition, Conception and Software, Feedback and Interactivity and Responsible Innovation. We use these breakpoints to form a targeted literature review of relevant work. Doing this we identify 30 challenges that act as roadmap for future research in DisplayFab.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {333},\nnumpages = {24},\nkeywords = {Active Materials, Additive Manufacturing, Display Fabrication, Interactive Devices, Personal Fabrication, Printed Electronics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642751,\nauthor = {Tran O'Leary, Jasper and Ramesh, Thrisha and Zhang, Octi and Peek, Nadya},\ntitle = {Tandem: Reproducible Digital Fabrication Workflows as Multimodal Programs},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642751},\ndoi = {10.1145/3613904.3642751},\nabstract = {Experimental digital fabrication workflows are increasingly common in human-computer interaction research, but are difficult to reproduce. We present Tandem, a software library that lets a fabricator implement an end-to-end fabrication workflow as a computational notebook program that others can run to physically reproduce the workflow. Tandem notebook programs read and write to CAD and CAM software, project augmented reality interfaces onto machines for manual interventions, and directly control fabrication machines. Fabricators can also denote potential mismatches between the physical and the digital as explicit assertions in code. Using two-sided CNC milling as an example, we demonstrate how to implement a complex workflow as a single program that can be re-run by others while supporting quality control and improving reproducibility.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {334},\nnumpages = {16},\nkeywords = {Digital fabrication, computational notebooks, open-source software, programming languages},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642401,\nauthor = {Pardomuan, Jefferson and Miyafuji, Shio and Takahashi, Nobuhiro and Koike, Hideki},\ntitle = {VabricBeads : Variable Stiffness Structured Fabric using Artificial Muscle in Woven Beads},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642401},\ndoi = {10.1145/3613904.3642401},\nabstract = {Woven beads, a structured fabric category, comprises interconnected rows of beads joined by fiber strands. While the stiffness of woven beads can be adjusted by relying on fiber tension during fabrication, the resulting shape and stiffness properties remain fixed. This study explores the potential of tunable shape and stiffness in woven beads, offering adaptability in comfort, functionality, and form factor. By leveraging Pneumatic Artificial Muscles (PAMs), we employ a state-of-the-art technique for dynamically modulating fabric stiffness through mechanical constraints in bead form. This approach enables a modular and scalable fabrication process, fostering programmability in mechanical properties. Our investigation encompasses diverse bead iterations and stitching patterns to broaden their applicability in fabric behavior including degree of freedom, stretchability, permeability, and textures. We evaluate the mechanical properties to differentiate design capabilities, and present techniques for locally adjusting stiffness. We showcase the versatility through applications, including variable stiffness wearables and shape-changing everyday objects.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {335},\nnumpages = {17},\nkeywords = {Fabrication, Shape-changing Interface, Soft Robotics, Variable Stiffness, Wearable Interface},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642232,\nauthor = {Panigrahy, Sai Nandan and Lee, Chang Hyeon and Nagoria, Vrahant and Janghorban, Mohammad and Pandey, Richa and Nittala, Aditya Shekhar},\ntitle = {ecSkin: Low-Cost Fabrication of Epidermal Electrochemical Sensors for Detecting Biomarkers in Sweat},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642232},\ndoi = {10.1145/3613904.3642232},\nabstract = {The development of low-cost and non-invasive biosensors for monitoring electrochemical biomarkers in sweat holds great promise for personalized healthcare and early disease detection. In this work, we present ecSkin, a novel fabrication approach for realizing epidermal electrochemical sensors that can detect two vital biomarkers in sweat: glucose and cortisol. We contribute the synthesis of functional reusable inks, that can be formulated using simple household materials. Electrical characterization of inks indicates that they outperform commercially available carbon inks. Cyclic voltammetry experiments show that our inks are electrochemically active and detect glucose and cortisol at activation voltages of -0.36 V and -0.22 V, respectively. Chronoamperometry experiments show that the sensors can detect the full range of glucose and cortisol levels typically found in sweat. Results from a user evaluation show that ecSkin sensors successfully function on the skin. Finally, we demonstrate three applications to illustrate how ecSkin devices can be deployed for various interactive applications.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {336},\nnumpages = {20},\nkeywords = {Electrochemical devices Sensing, Epidermal Devices, Physiological Sensing, Wearables},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642083,\nauthor = {Stemasov, Evgeny and Demharter, Simon and R\\\"{a}dler, Max and Gugenheimer, Jan and Rukzio, Enrico},\ntitle = {pARam: Leveraging Parametric Design in Extended Reality to Support the Personalization of Artifacts for Personal Fabrication},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642083},\ndoi = {10.1145/3613904.3642083},\nabstract = {Extended Reality (XR) allows in-situ previewing of designs to be manufactured through Personal Fabrication (PF). These in-situ interactions exhibit advantages for PF, like incorporating the environment into the design process. However, design-for-fabrication in XR often happens through either highly complex 3D-modeling or is reduced to rudimentary adaptations of crowd-sourced models. We present pARam, a tool combining parametric designs (PDs) and XR, enabling in-situ configuration of artifacts for PF. In contrast to modeling- or search-focused approaches, pARam supports customization through embodied and practical inputs (e.g., gestures, recommendations) and evaluation (e.g., lighting estimation) without demanding complex 3D-modeling skills. We implemented pARam for HoloLens 2 and evaluated it (n = 20), comparing XR and desktop conditions. Users succeeded in choosing context-related parameters and took their environment into account for their configuration using pARam. We reflect on the prospects and challenges of PDs in XR to streamline complex design methods for PF while retaining suitable expressivity.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {337},\nnumpages = {22},\nkeywords = {3D-modeling, Customizer Interfaces, Design Customization, In-Situ Design, In-Situ Modeling, Mixed Reality, Parametric Designs, Personal Fabrication, Remixing, pARam},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642009,\nauthor = {Lin, Richard and Ramesh, Rohit and Pandhare, Parth Nitin and Tay, Kai Jun and Dutta, Prabal and Hartmann, Bjoern and Mehta, Ankur},\ntitle = {Design Space Exploration for Board-level Circuits: Exploring Alternatives in Component-based Design},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642009},\ndoi = {10.1145/3613904.3642009},\nabstract = {While recent work explores novel tools to make electronics and device design easier and more accessible, these tend to be either highly automated (great for novices, but limiting for more advanced users) or highly manual (suitable for experts, but imposes a higher skill barrier to entry). In this work, we examine a middle ground: user-guided design space exploration to bridge an intuitive-but-ambiguous high-level representation to a fully-specified, fabrication-ready circuit. Our system helps users understand and make design choices by sweeping the design space of alternatives for electronics parts (e.g., choice of microcontroller), marking invalid options, and plotting points to visualize trade-offs (e.g., for power and size). We discuss the overall system and its structure, report on the results of a small but in-depth user study with participants from a wide range of electronics backgrounds, and draw insights on future directions for improving electronics design for everyone.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {338},\nnumpages = {14},\nkeywords = {design space exploration, electronics design, printed circuit board (PCB) design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642010,\nauthor = {Han, Bo and Liu, Xin and Yen, Ching Chiuan and Zheng, Clement},\ntitle = {E-Acrylic: Electronic-Acrylic Composites for Making Interactive Artifacts},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642010},\ndoi = {10.1145/3613904.3642010},\nabstract = {Electronic composites incorporate computing into physical materials, expanding the materiality of interactive systems for designers. In this research, we investigated acrylic as a substrate for electronics. Acrylic is valued for its visual and structural properties and is used widely in industrial design. We propose e-acrylic, an electronic composite that incorporates electronic circuits with acrylic sheets. Our approach to making this composite is centered on acrylic making practices that industrial designers are familiar with. We outline this approach systematically, including leveraging laser cutting to embed circuits into acrylic sheets, as well as different ways to shape e-acrylic into 3D objects. With this approach, we explored using e-acrylic to design interactive artifacts. We reflect on these applications to surface a design space of tangible interactive artifacts possible with this composite. We also discuss the implications of aligning electronics to an existing making practice, and working with the holistic materiality that e-acrylic embodies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {339},\nnumpages = {15},\nkeywords = {Acrylic, Digital Fabrication, Electronic Composites, Interactive Artifacts, Laser Cutting},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642888,\nauthor = {Buch, Niels Christian and Tejada, Carlos and Ashbrook, Daniel and Savage, Valkyrie},\ntitle = {LaCir: A multilayered laser-cuttable material to co-fabricate circuitry and structural components.},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642888},\ndoi = {10.1145/3613904.3642888},\nabstract = {Rapid prototyping is an important tool for designers, but many fabrication techniques are slow and create bulky components requiring multiple machines and processes to achieve desired device shape and electronic functionality. Prior work explored ways to ease fabricating shapes or designing electronics, but we focus on creating shape and electrical pathways at the same time from a single material and machine. LaCir leverages a three-layered, laser-cuttable material to incorporate circuits into the structural substrate of the design using laser cutters. Our substrate features a layer of conductive material sandwiched between thermoplastic sheets, allowing designers to cut electrical traces and assembleable, 3D object geometry in a single pass. We evaluate different composite materials, weighing their cuttability, ease of assembly, and conductivity; we also show using fully laser-cut joints as structural and electrical connections. We demonstrate LaCir’s flexibility through several example artifacts.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {340},\nnumpages = {10},\nkeywords = {Circuit Joinery, Circuitry, Digital Fabrication, Laser Cutter, Multi-material Stack, Prototyping},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642226,\nauthor = {Tokuda, Yutaka and Kobayashi, Tatsuya},\ntitle = {Painting Inferno: Novel Heat and Stiffness Control Methods with Carbon Nanomaterial Conductive Heating Paint},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642226},\ndoi = {10.1145/3613904.3642226},\nabstract = {We introduce Painting Inferno, a novel method for controlling heat and stiffness using highly electrically conductive carbon nanomaterial heating paint. Heat has found widespread applications in thermochromic displays, shape-changing interfaces, haptic devices, and materials with adjustable stiffness. Although Joule heaters based on heating circuits using electrically conductive materials have been widely used, the complex design and fabrication processes limit the freedom to create custom heaters in scale, shape, and material. As an alternative Joule heating method, we explore the potential of carbon nanomaterial heating paints, which enable the rapid generation of uniform heat at low voltages. We present simple fabrication methods for creating handmade heaters using off-the-shelf materials and cutting machines and demonstrate the feasibility of crafting heaters with complex shapes and grid-array configurations. Leveraging the heating paint’s compatibility with various materials, we showcase the versatile applications for interactive thermal displays, stiffness modulation devices, and pneumatic interfaces for stiffness-shape transformations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {341},\nnumpages = {17},\nkeywords = {heating paint, metamaterial, programmable material, shape memory, shape-changing interface, stiffness control, thermal display},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642933,\nauthor = {Yang, Yue and Ren, Lei and Chen, Chuang and Hu, Bin and Zhang, Zhuoyi and Li, Xinyan and Shen, Yanchen and Zhu, Kuangqi and Ji, Junzhe and Zhang, Yuyang and Ni, Yongbo and Wu, Jiayi and Wang, Qi and Wu, Jiang and Sun, Lingyun and Tao, Ye and Wang, Guanyun},\ntitle = {SnapInflatables: Designing Inflatables with Snap-through Instability for Responsive Interaction},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642933},\ndoi = {10.1145/3613904.3642933},\nabstract = {Snap-through instability, like the rapid closure of the Venus flytrap, is gaining attention in robotics and HCI. It offers rapid shape reconfiguration, self-sensing, actuation, and enhanced haptic feedback. However, conventional snap-through structures face limitations in fabrication efficiency, scale, and tunability. We introduce SnapInflatables, enabling safe, multi-scale interaction with adjustable sensitivity and force reactions, utilizing the snap-through instability of inflatables. We designed six types of heat-sealing structures enabling versatile snap-through passive motion of inflatables with diverse reaction and trigger directions. A block structure enables ultra-sensitive states for rapid energy release and force amplification. The motion range is facilitated by geometry parameters, while force feedback properties are tunable through internal pressure settings. Based on experiments, we developed a design tool for creating desired inflatable snap-through shapes and motions, offering previews and inflation simulations. Example applications, including a self-locking medical stretcher, interactive animals, a bounce button, and a large-scale light demonstrate enhanced passive interaction with inflatables.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {342},\nnumpages = {15},\nkeywords = {inflatable, responsive interaction, shape-changing interface, snap-through instability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642537,\nauthor = {Moon, Kongpyung (Justin) and Marciniak, Zofia and Suzuki, Ryo and Bianchi, Andrea},\ntitle = {3D Printing Locally Activated Visual-Displays Embedded in 3D Objects via Electrically Conductive and Thermochromic Materials},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642537},\ndoi = {10.1145/3613904.3642537},\nabstract = {3D printed displays promise to create unique visual interfaces for physical objects. However, current methods for creating 3D printed displays either require specialized post-fabrication processes (e.g., electroluminescence spray and silicon casting) or function as passive elements that simply react to environmental factors (e.g., body and air temperature). These passive displays offer limited control over when, where, and how the colors change. In this paper, we introduce ThermoPixels, a method for designing and 3D printing actively controlled and visually rich thermochromic displays that can be embedded in arbitrary geometries. We investigate the color-changing and thermal properties of thermochromic and conductive filaments. Based on these insights, we designed ThermoPixels and an accompanying software tool that allows embedding ThermoPixels in arbitrary 3D geometries, creating displays of various shapes and sizes (flat, curved, or matrix displays) or displays that embed textures, multiple colors, or that are flexible.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {343},\nnumpages = {15},\nkeywords = {3D Displays, Color-changing Interfaces, Local Heating, Multi-material Printing, Thermochromic Filaments},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642684,\nauthor = {Frost, Devon and Lee, Raina and Paek, Eun-Ha and Jacobs, Jennifer},\ntitle = {SketchPath: Using Digital Drawing to Integrate the Gestural Qualities of Craft in CAM-Based Clay 3D Printing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642684},\ndoi = {10.1145/3613904.3642684},\nabstract = {This paper presents the design and outcomes of SketchPath, a system that uses hand-drawn toolpaths to design for clay 3D printing. Drawing, as a direct manipulation technique, allows artists to design with the expressiveness of CAM-based tools without needing to work with a numerical system or constrained system. SketchPath works to provide artists with direct control over the outcomes of their form by not abstracting away machine operations or constraining the kinds of artifacts that can be produced. Artifacts produced with SketchPath emerge at a unique intersection of manual qualities and machine precision, creating works that blend handmade and machine aesthetics. In interactions with our system, ceramicists without a background in CAD/CAM were able to produce more complex forms with limited training, suggesting the future of CAM-based fabrication design can take on a wider range of modalities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {344},\nnumpages = {16},\nkeywords = {Artists Residency, Clay 3D Printing, Computer-Aided Machining, Digital Fabrication},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642765,\nauthor = {Yan, Zeyu and Li, Jiasheng and Zhang, Zining and Peng, Huaishu},\ntitle = {SolderlessPCB: Reusing Electronic Components in PCB Prototyping through Detachable 3D Printed Housings},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642765},\ndoi = {10.1145/3613904.3642765},\nabstract = {The iterative prototyping process for printed circuit boards (PCBs) frequently employs surface-mounted device (SMD) components, which are often discarded rather than reused due to the challenges associated with desoldering, leading to unnecessary electronic waste. This paper introduces SolderlessPCB, a collection of techniques for solder-free PCB prototyping, specifically designed to promote the recycling and reuse of electronic components. Central to this approach are custom 3D-printable housings that allow SMD components to be mounted onto PCBs without soldering. We detail the design of SolderlessPCB and the experiments conducted to evaluate its design parameters, electrical performance, and durability. To illustrate the potential for reusing SMD components with SolderlessPCB, we discuss two scenarios: the reuse of components from earlier design iterations and from obsolete prototypes. We also provide examples demonstrating that SolderlessPCB can handle high-current applications and is suitable for high-speed data transmission. The paper concludes by discussing the limitations of our approach and suggesting future directions to overcome these challenges.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {345},\nnumpages = {17},\nkeywords = {Electronic Component, PCB prototyping, Reuse, Soldering, Sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642719,\nauthor = {Puerta, Eduardo and Crnovrsanin, Tarik and South, Laura and Dunne, Cody},\ntitle = {The Effect of Orientation on the Readability and Comfort of 3D-Printed Braille},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642719},\ndoi = {10.1145/3613904.3642719},\nabstract = {Fused Deposition Modeling (FDM) is a low-cost method of 3D printing that involves stacking horizontal layers of plastic. FDM is used to produce tactile graphics and interfaces for people with visual impairments. Unfortunately, the print orientation can alter the structure and quality of braille and text. The difference between printing braille vertically and horizontally has been documented. However, we found no comprehensive study of these angles or the angles in between, nor any study providing a quantitative and qualitative user evaluation. We conducted two mixed-methods studies to evaluate the performance of braille printed at different angles. We measured reading time and subjective preference and performed a thematic analysis of participants’ responses. Our participants were faster using and preferred 75° and vertical braille over horizontal braille. These results provide makers with guidelines for creating models with readable 3D-printed braille.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {346},\nnumpages = {15},\nkeywords = {3D Printing, Braille, Visual Accessibility},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642361,\nauthor = {Moyer, Ilan E and Bourgault, Samuelle and Frost, Devon and Jacobs, Jennifer},\ntitle = {Throwing Out Conventions: Reimagining Craft-Centered CNC Tool Design through the Digital Pottery Wheel},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642361},\ndoi = {10.1145/3613904.3642361},\nabstract = {Skilled potters use manual tools with direct material engagement. In contrast, the design of clay 3D printers and workflows reinforces industrial CNC manufacturing conventions. To understand how digital fabrication can serve skilled craft practitioners, we ask: how might clay 3D printing function if it had evolved from traditional pottery tools? To examine this question, we created the Digital Pottery Wheel (DPW), a throwing wheel with 3D printing capabilities. The DPW consists of a polar mechanical architecture that looks and functions like a pottery wheel while supporting 3D printing and a real-time modular control system that blends automated and manual control. We worked with ceramicists to develop interactions that include printing onto thrown forms, throwing to manipulate printed forms, and integrating manual control, recording, and playback to re-execute manually produced forms. We demonstrate how using a physical metaphor to guide digital fabrication machine design results in new products, workflows, and perceptions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {347},\nnumpages = {22},\nkeywords = {clay 3D printing, craft, digital fabrication, hardware prototyping},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642155,\nauthor = {Buechley, Leah and Gould, Jaime and Bell, Fiona},\ntitle = {CeraMetal: A New Approach to Low-Cost Metal 3D Printing with Bronze Clay},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642155},\ndoi = {10.1145/3613904.3642155},\nabstract = {This paper introduces CeraMetal, a low-cost and robust approach to desktop metal 3D printing based on a custom \"metal clay\". We present three recipes for 3D printable bronze clay along with a workflow that includes print parameters and a sintering schedule. We introduce custom slicing software that generates continuous extrusion toolpaths for metal clay printing. We analyze the shrinkage, density, tensile strength and flexibility of prints produced with Cerametal and find the material’s performance comparable to parts produced via other bronze 3D printing methods. Finally, we provide several examples of 3D printed metal objects and a discussion of limitations and future research opportunities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {348},\nnumpages = {16},\nkeywords = {3D Printing, Digital Fabrication, Metal 3D Printing, Metal Clay, Slicing Software, Toolpaths},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642909,\nauthor = {Lipkowitz, Gabriel and Desimone, Joseph},\ntitle = {Palette-PrintAR: augmented reality design and simulation for multicolor resin 3D printing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642909},\ndoi = {10.1145/3613904.3642909},\nabstract = {While 3D printing affords designers unprecedented geometrical complexity, fewer interactive design tools for multimaterial platforms exist. Recent work in resin 3D printing specifically promises fast, multicolor printing by growing fluidic channels concurrent with the object itself, infusing different resins spatioselectively into the vat; however, no design tools have been developed enabling users to interact with such novel personal fabrication machines in situ. Here, we introduce an augmented reality-based design tool allowing users to engage with this multicolor fabrication method so as to \"paint\" growing 3D objects. We define the design process and mode of user interaction with our tool, Palette-PrintAR, which integrates situated 3D model manipulation with real-time computational fluid dynamics simulation and computer vision-based tracking and analysis. We detail our 3D printer hardware add-on implementation and AR software architecture, along with characterizing the design flexibilities and limitations of our AR-based multicolor fabrication method.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {349},\nnumpages = {12},\nkeywords = {3D printing, augmented reality, interactive fabrication},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642906,\nauthor = {Sun, Lingyun and Pan, Deying and Zhang, Yuyang and Hu, Hongyi and Ji, Junzhe and Tao, Yue and Lou, Shanghua and Lian, Boyi and Fan, Yitao and Tao, Ye and Wang, Guanyun},\ntitle = {Touch-n-Go: Designing and Fabricating Touch Fastening Structures by FDM 3D Printing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642906},\ndoi = {10.1145/3613904.3642906},\nabstract = {Touch fastening structures are widely used to quickly assemble and disassemble an object with multiple parts. However, such structures are under-explored in the context of additive manufacturing for personal fabrication. We proposed Touch-n-Go, a method for designing touch-fastening structures with customizable mechanical properties such as holding capacities or shearing strength. Additionally, the customization of fastener patterns enables both static and dynamic connections, and the dynamic connections grant the freedom of rotation and translation. To facilitate the customization process, we developed a design tool that allows the integration of fastening structures on the surface of a 3D-printed object. Furthermore, we validated the fastening properties of Touch-n-Go through a series of experiments, and the result exhibits performances that match or even surpass off-the-shelf fasteners. Finally, we demonstrated the implementation of Touch-n-Go through a collection of applications.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {350},\nnumpages = {14},\nkeywords = {3D Printing, Parametric Modeling, Personal Fabrication, Surface Adhesion},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642566,\nauthor = {Gonzalez Avila, J Felipe and Pietrzak, Thomas and Girouard, Audrey and Casiez, G\\'{e}ry},\ntitle = {Understanding the Challenges of OpenSCAD Users for 3D Printing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642566},\ndoi = {10.1145/3613904.3642566},\nabstract = {Direct manipulation has been established as the main interaction paradigm for Computer-Aided Design (CAD) for decades. It provides fast, incremental, and reversible actions that allow for an iterative process on a visual representation of the result. Despite its numerous advantages, some users prefer a programming-based approach where they describe the 3D model they design with a specific programming language, such as OpenSCAD. It allows users to create complex structured geometries and facilitates abstraction. Unfortunately, most current knowledge about CAD practices only focuses on direct manipulation programs. In this study, we interviewed 20 programming-based CAD users to understand their motivations and challenges. Our findings reveal that this programming-oriented population presents difficulties in the design process in tasks such as 3D spatial understanding, validation and code debugging, creation of organic shapes, and code-view navigation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {351},\nnumpages = {20},\nkeywords = {3D printing, Maker culture, OpenSCAD, Programming-based CAD},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642622,\nauthor = {Friedman-Gerlicz, Camila and Gelosi, Deanna and Bell, Fiona and Buechley, Leah},\ntitle = {WeaveSlicer: Expanding the Range of Printable Geometries in Clay},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642622},\ndoi = {10.1145/3613904.3642622},\nabstract = {Clay 3D printing is a relatively new technology and only a narrow range of geometries is 3D printable if one is employing commercially available slicing software. We experienced these limitations in an artist residency program where artists discovered that many desired geometries failed to print successfully. This motivated us to develop WeaveSlicer, a slicer optimized for 3D printing in clay that maintains constant wall thickness throughout the form. We achieve constant wall thickness by generating an oscillating path where the amplitude of the oscillation is determined by the form’s overhang angle. We demonstrate the effectiveness of our approach by comparing a range of successful prints, sliced by WeaveSlicer, to failed prints of the same forms sliced by Cura, a widely used slicing software. We then showcase a collection of complex artifacts designed by artists in residence that were constructed with WeaveSlicer.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {352},\nnumpages = {16},\nkeywords = {Artist Residency, Ceramics, Clay 3D Printing, Digital Fabrication, Slicing Software, Toolpath Generation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642534,\nauthor = {Yu, Yaman and Sharma, Tanusree and Das, Sauvik and Wang, Yang},\ntitle = {\"Don't put all your eggs in one basket\": How Cryptocurrency Users Choose and Secure Their Wallets},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642534},\ndoi = {10.1145/3613904.3642534},\nabstract = {Cryptocurrency wallets come in various forms, each with unique usability and security features. Through interviews with 24 users, we explore reasons for selecting wallets in different contexts. Participants opt for smart contract wallets to simplify key management, leveraging social interactions. However, they prefer personal devices over individuals as guardians to avoid social cybersecurity concerns in managing guardian relationships. When engaging in high-stakes or complex transactions, they often choose browser-based wallets, leveraging third-party security extensions. For simpler transactions, they prefer the convenience of mobile wallets. Many participants avoid hardware wallets due to usability issues and security concerns with respect to key recovery service provided by manufacturer and phishing attacks. Social networks play a dual role: participants seek security advice from friends, but also express security concerns in soliciting this help. We offer novel insights into how and why users adopt specific wallets. We also discuss design recommendations for future wallet technologies based on our findings.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {353},\nnumpages = {17},\nkeywords = {Cryptocurrency, blockchain, crypto wallets, privacy, security, user behavior},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642033,\nauthor = {Bhattacharya, Arkaprabha and Lee, Kevin and Ravi, Vineeth and Staddon, Jessica and Bellini, Rosanna},\ntitle = {Shortchanged: Uncovering and Analyzing Intimate Partner Financial Abuse in Consumer Complaints},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642033},\ndoi = {10.1145/3613904.3642033},\nabstract = {Digital financial services can introduce new digital-safety risks for users, particularly survivors of intimate partner financial abuse (IPFA). To offer improved support for such users, a comprehensive understanding of their support needs and the barriers they face to redress by financial institutions is essential. Drawing from a dataset of 2.7 million customer complaints, we implement a bespoke workflow that utilizes language-modeling techniques and expert human review to identify complaints describing IPFA. Our mixed-method analysis provides insight into the most common digital financial products involved in these attacks, and the barriers consumers report encountering when doing so. Our contributions are twofold; we offer the first human-labeled dataset for this overlooked harm and provide practical implications for technical practice, research, and design for better supporting and protecting survivors of IPFA.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {354},\nnumpages = {20},\nkeywords = {financial abuse, intimate partner violence, technology-enabled abuse},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642715,\nauthor = {Kawai, Daisuke and Soska, Kyle and Routledge, Bryan and Zetlin-Jones, Ariel and Christin, Nicolas},\ntitle = {Stranger Danger? Investor Behavior and Incentives on Cryptocurrency Copy-Trading Platforms},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642715},\ndoi = {10.1145/3613904.3642715},\nabstract = {Several large financial trading platforms have recently begun implementing “copy trading,” a process by which a leader allows copiers to automatically mirror their trades in exchange for a share of the profits realized. While it has been shown in many contexts that platform design considerably influences user choices—users tend to disproportionately trust rankings presented to them—we would expect that here, copiers exercise due diligence given the money at stake, typically USD 500–2 000 or more. We perform a quantitative analysis of two major cryptocurrency copy-trading platforms, with different default leader ranking algorithms. One of these platforms additionally changed the information displayed during our study. In all cases, we show that the platform UI significantly influences copiers’ decisions. Besides being sub-optimal, this influence is problematic as rankings are often easily gameable by unscrupulous leaders who prey on novice copiers, and they create perverse incentives for all platform users.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {355},\nnumpages = {20},\nkeywords = {Bitcoin, Copy Trading, Cryptocurrency, Derivatives, Online Markets, Social Trading, Trading},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642645,\nauthor = {Brozena, Jeff and Blair, Johnna and Richardson, Thomas and Matthews, Mark and Mukherjee, Dahlia and Saunders, Erika F. H. and Abdullah, Saeed},\ntitle = {Supportive Fintech for Individuals with Bipolar Disorder: Financial Data Sharing Preferences for Longitudinal Care Management},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642645},\ndoi = {10.1145/3613904.3642645},\nabstract = {Financial stability is a key challenge for individuals living with bipolar disorder (BD). Symptomatic periods in BD are associated with poor financial decision-making, contributing to a negative cycle of worsening symptoms and an increased risk of bankruptcy. There has been an increased focus on designing supportive financial technologies (fintech) to address varying and intermittent needs across different stages of BD. However, little is known about this population’s expectations and privacy preferences related to financial data sharing for longitudinal care management. To address this knowledge gap, we have deployed a factorial vignette survey using the Contextual Integrity framework. Our data from individuals with BD (N=480) shows that they are open to sharing financial data for long term care management. We have also identified significant differences in sharing preferences across age, gender, and diagnostic subtype. We discuss the implications of these findings in designing equitable fintech to support this marginalized community.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {356},\nnumpages = {15},\nkeywords = {Financial technologies, bipolar disorder, fintech, mental health, privacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642768,\nauthor = {Kou, Yubo and Moradzadeh, Sam and Gui, Xinning},\ntitle = {Trading as Gambling: Social Investing and Financial Risks on the r/WallStreetBets Subreddit},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642768},\ndoi = {10.1145/3613904.3642768},\nabstract = {Financial trading has become commonplace, involving the purchase and sale of securities such as stocks and bonds. While HCI research has investigated people’s financial literacy and decision-making and how to design for it, little is known as to how people form financial conversations on social media. To answer this question, we used a grounded theory approach to analyzing financial conversations in the YOLO (‘you only live once’) posts on the r/WallStreetBets subreddit (WSB), one of today’s largest financial online communities. We describe how WSB’s discursive culture portrays its gambling-like, high-risk trading by likening trading to gambling, celebrating it, and normalizing financial risk-taking. We discuss the rise of social investing, including how individual investors’ affective relationships encourage their outsized risk-taking, as well as reflect on its looming financial risks, especially to already marginalized groups. Lastly, we propose implications for design and policymaking.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {357},\nnumpages = {17},\nkeywords = {Community of Practice, Financial HCI, Investment, Trading},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642372,\nauthor = {Kato, Kunihiro and Ikematsu, Kaori and Nakamura, Hiromi and Suzaki, Hinako and Igarashi, Yuki},\ntitle = {FoodSkin: Fabricating Edible Gold Leaf Circuits on Food Surfaces},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642372},\ndoi = {10.1145/3613904.3642372},\nabstract = {We present FoodSkin, a technique for adding interactive elements to foods by implementing edible circuits on the surface of the food. The circuit is easily fabricated using commercially available materials. Existing approaches to enhance the eating experience, such as presenting an electrical taste by making food part of an electronic circuit, are challenging to apply to foods with low water content due to their low conductivity. Our technique enables the integration of dry foods into an electronic circuit and provides displaying (e.g., smell or taste) and sensing (e.g., eating activity) functionalities. We describe our fabrication technique with a library of food materials that we can utilize, evaluate the conductivity and adhesion of the gold-leaf traces, introduce demonstrative applications, and conclude with a workshop we conducted to evaluate the accessibility of our technique. FoodSkin enriches the design space for the computer-augmented eating experience by enabling the digital fabrication of electronics on versatile materials, surfaces, and shapes of foods.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {358},\nnumpages = {17},\nkeywords = {Human-Food Interaction (HFI), eating experience, fabrication technique, gold leaf},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642516,\nauthor = {Deng, Jialin and Overdevest, Nathalie and Olivier, Patrick and Mueller, Florian ‘Floyd’},\ntitle = {From Plating to Tasting: Towards Understanding the Choreography of Computational Food},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642516},\ndoi = {10.1145/3613904.3642516},\nabstract = {The emerging concept of “computational food” focusing on the material affordances when designing food interactions is gaining traction in Human-Food Interaction (HFI). However, prior HFI research has not yet substantively investigated the dynamic nature of computational food from its creation to consumption, limiting our understanding of the complex interactions among creators, computational food, and consumers. In response, our paper shifts the perspective towards the dynamics of computational food interactions through a study in cooperation with chefs and gastronomists. Utilizing “Dancing Delicacies” as a research artifact – a system that facilitates dynamic dining trajectories – we adopted the concept of “choreography” to unravel the experiential dynamics of computational food. Our study resulted in six themes concerning computational food experiences and detailed four design implications central to culinary choreography. Our work aspires to leverage the choreographic potential of computational food design, paving the way for future HFI innovations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {359},\nnumpages = {17},\nkeywords = {Choreography, Computational food, Food Design, Human-Food Interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642709,\nauthor = {Song, Katherine W and Tung, Szu Ting and Kim, Alexis and Paulos, Eric},\ntitle = {F\\\"{u}pop: \"Real Food\" Flavor Delivery via Focused Ultrasound},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642709},\ndoi = {10.1145/3613904.3642709},\nabstract = {Food and flavors are integral to our existence in the world. Nonetheless, taste remains an under-explored sense in interaction design. We present F\\\"{u}pop, a technical platform for delivering in-mouth flavors that leverages advances in electronics and molecular gastronomy. F\\\"{u}pop comprises a fully edible pouch placed inside the mouth against a cheek that programmatically releases different flavors when wirelessly triggered by a focused ultrasound transducer from outside the cheek. F\\\"{u}pop does not interfere with activities such as chewing and drinking, and its electronics may be integrated into devices already used near the cheek, such as mobile phones, audio headphones, and head-mounted displays. F\\\"{u}pop’s flavors are from “real foods,” not ones imitated with synthetic reagents, providing authentic, nutritive flavors. We envision that with F\\\"{u}pop, flavors may be synced to music, a phone call, or events in virtual reality to enhance a user’s experience of their food and the world.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {360},\nnumpages = {14},\nkeywords = {edible, gastronomy, human-food interaction, taste interactions, ultrasound},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642182,\nauthor = {Wang, Yan and Obie, Humphrey O and Li, Zhuying and Salim, Flora D. and Grundy, John and Mueller, Florian ‘Floyd’},\ntitle = {GustosonicSense: Towards understanding the design of playful gustosonic eating experiences},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642182},\ndoi = {10.1145/3613904.3642182},\nabstract = {The pleasure that often comes with eating can be further enhanced with intelligent technology, as the field of human-food interaction suggests. However, knowledge on how to design such pleasure-supporting eating systems is limited. To begin filling this knowledge gap, we designed “GustosonicSense”, a novel gustosonic eating system that utilizes wireless earbuds for sensing different eating and drinking actions with a machine learning algorithm and trigger playful sounds as a way to facilitate pleasurable eating experiences. We present the findings from our design and a study that revealed how we can support the \"stimulation\", \"hedonism\", and \"reflexivity\" for playful human-food interactions. Ultimately, with our work, we aim to support interaction designers in facilitating playful experiences with food.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {361},\nnumpages = {12},\nkeywords = {Human-food interaction, earbuds, eating, gustosonic experiences, play},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642484,\nauthor = {Maram, Sai Siddartha and Kleinman, Erica and Villareale, Jennifer and Zhu, Jichen and Seif El-Nasr, Magy},\ntitle = {\"Ah! I see\" - Facilitating Process Reflection in Gameplay through a Novel Spatio-Temporal Visualization System},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642484},\ndoi = {10.1145/3613904.3642484},\nabstract = {Educational games have emerged as potent tools for helping students understand complex concepts and are now ubiquitous in global classrooms, amassing vast data. However, there is a notable gap in research concerning the effective visualization of this data to serve two key functions: (a) guiding students in reflecting upon their game-based learning and (b) aiding them in analyzing peer strategies. In this paper, we engage educators, students, and researchers as essential stakeholders. Taking a Design-Based Research (DBR) approach, we incorporate UX design methods to develop an innovative visualization system that helps players learn through gaining insights from their own and peers’ gameplay and strategies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {362},\nnumpages = {19},\nkeywords = {Game Mechanics, Input Modalities, Parallel Programming, Serious Games},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642603,\nauthor = {Waldenmeier, Karla and Poeller, Susanne and Dechant, Martin Johannes and Baumann, Nicola and Mandryk, Regan L},\ntitle = {Cheat Codes as External Support for Players Navigating Fear of Failure and Self-Regulation Challenges In Digital Games},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642603},\ndoi = {10.1145/3613904.3642603},\nabstract = {Failure is an integral element of most games, and while some players may benefit from external support, such as cheat codes, to prompt self-soothing, most games lack supportive elements. We asked participants (N=88) to play Anno 1404 in single-player mode, and presented a money-generating cheat code in a challenging situation, also measuring the personality trait of action-state orientation, which explains differences in self-regulation ability (i.e., self-soothing) in response to threats of failure. Individuals higher in state orientation were more likely to take the offer, and used the cheat code more frequently. The cheat code also acted as an external support, as differences in experienced pressure between action- and state-oriented participants vanished when it was used. We found no negative consequences of using external support in intrinsic motivation, needs satisfaction, flow, or performance. We argue that external support mechanisms can help state-oriented players to self-regulate in gaming, when faced with failure.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {363},\nnumpages = {13},\nkeywords = {PSI theory, action-state orientation, cheat codes, cheating, digital games, fear of failure, player experience, pressure, self-regulation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642880,\nauthor = {Yang, Yeonsun and Shin, Ahyeon and Kim, Nayoung and Woo, Huidam and Chung, John Joon Young and Song, Jean Y},\ntitle = {Find the Bot!: Gamifying Facial Emotion Recognition for Both Human Training and Machine Learning Data Collection},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642880},\ndoi = {10.1145/3613904.3642880},\nabstract = {Facial emotion recognition (FER) constitutes an essential social skill for both humans and machines to interact with others. To this end, computer interfaces serve as valuable tools for training individuals to improve FER abilities, while also serving as tools for gathering labels to train FER machine learning datasets. However, existing tools have limitations on the scope and methods of training non-clinical populations and also on collecting labels for machines. In this study, we introduce Find the Bot!, an integrated game that effectively engages the general population to support not only human FER learning on spontaneous expressions but also the collection of reliable judgment-based labels. We incorporated design guidelines from gamification, education, and crowdsourcing literature to engage and motivate players. Our evaluation (N=59) shows that the game encourages players to learn emotional social norms on perceived facial expressions with a high agreement rate, facilitating effective FER learning and reliable label collection all while enjoying gameplay.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {364},\nnumpages = {20},\nkeywords = {computer interfaces for training, dataset collection, facial emotion recognition (FER), gamification},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642656,\nauthor = {Kao, Dominic and Ballou, Nick and Gerling, Kathrin and Breitsohl, Heiko and Deterding, Sebastian},\ntitle = {How does Juicy Game Feedback Motivate? Testing Curiosity, Competence, and Effectance},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642656},\ndoi = {10.1145/3613904.3642656},\nabstract = {‘Juicy’ or immediate abundant action feedback is widely held to make video games enjoyable and intrinsically motivating. Yet we do not know why it works: Which motives are mediating it? Which features afford it? In a pre-registered (n=1,699) online experiment, we tested three motives mapping prior practitioner discourse—effectance, competence, and curiosity—and connected design features. Using a dedicated action RPG and a 2x2+control design, we varied feedback amplification, success-dependence, and variability and recorded self-reported effectance, competence, curiosity, and enjoyment as well as free-choice playtime. Structural equation models show curiosity as the strongest enjoyment and only playtime predictor and support theorised competence pathways. Success dependence enhanced all motives, while amplification unexpectedly reduced them, possibly because the tested condition unintentionally impeded players’ sense of agency. Our study evidences uncertain success affording curiosity as an underappreciated moment-to-moment engagement driver, directly supports competence-related theory, and suggests that prior juicy game feel guidance ties to legible action-outcome bindings and graded success as preconditions of positive ‘low-level’ user experience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {365},\nnumpages = {16},\nkeywords = {Video games, competence, curiosity, effectance, engagement, enjoyment, juiciness, juicy feedback},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642077,\nauthor = {Berns, Sebastian and Volz, Vanessa and Tokarchuk, Laurissa and Snodgrass, Sam and Guckelsberger, Christian},\ntitle = {Not All the Same: Understanding and Informing Similarity Estimation in Tile-Based Video Games},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642077},\ndoi = {10.1145/3613904.3642077},\nabstract = {Similarity estimation is essential for many game AI applications, from the procedural generation of distinct assets to automated exploration with game-playing agents. While similarity metrics often substitute human evaluation, their alignment with our judgement is unclear. Consequently, the result of their application can fail human expectations, leading to e.g. unappreciated content or unbelievable agent behaviour. We alleviate this gap through a multi-factorial study of two tile-based games in two representations, where participants (N=456) judged the similarity of level triplets. Based on this data, we construct domain-specific perceptual spaces, encoding similarity-relevant attributes. We compare 12 metrics to these spaces and evaluate their approximation quality through several quantitative lenses. Moreover, we conduct a qualitative labelling study to identify the features underlying the human similarity judgement in this popular genre. Our findings inform the selection of existing metrics and highlight requirements for the design of new similarity metrics benefiting game development and research.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {366},\nnumpages = {23},\nkeywords = {Computer Vision, Empirical Study, Games/Play, Quantitative Methods},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642306,\nauthor = {Lei, Qinyuan and Tang, Ran and Ho, Hiu Man and Zhou, Han and Guo, Jingyi and Tang, Zilu},\ntitle = {A Game of Love for Women: Social Support in Otome Game Mr. Love: Queen’s Choice in China},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642306},\ndoi = {10.1145/3613904.3642306},\nabstract = {Otome games (also known as romantic video games, or RVGs) are story-based video games that are designed for young women, simulating the experience of a romantic relationship. Players are invited to adopt the female avatar’s perspective in the story and date one or more of the male characters. Our empirical study focuses on the different types of social support among the players of the Chinese otome game Mr. Love: Queen’s Choice. We discovered that although the game was initially designed to be a consumer product aiming to profit from a largely marginalized and stigmatized group of gamers, i.e., young female gamers, the game has created a gaming community in which the players seek and provide each other with social support. We primarily use ethnographic methods, including participant observation and in-depth interviews. Our study contributes to HCI research on mediated social support in game.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {367},\nnumpages = {15},\nkeywords = {Empirical study that tells us about people, Ethnography, Games/Play, Social Media/Online Communities},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642643,\nauthor = {Pfau, Johannes and Charan, Manik and Kleinman, Erica and Seif El-Nasr, Magy},\ntitle = {Damage Optimization in Video Games: A Player-Driven Co-Creative Approach},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642643},\ndoi = {10.1145/3613904.3642643},\nabstract = {The concept of dealing damage is established and widespread in video games. With growing complexity and countless interactions in modern games, capturing how damage unfolds becomes an intricate problem - for developers just as for players. Misunderstanding how to optimize damage potentials includes risks of game imbalances, game-breaking exploits, mismatches between player skill and challenge (harming flow), and impaired perceived competence. All of these considerably impact player experience, game reception, success, and retention, yet polishing optimal strategies remains often a player community effort. To accelerate, inform and ease this process, we implemented an interactive tool capable of simulating, visualizing, planning and comparing damage strategies in video games. Following a case study within the Guild Wars 2 community, we contribute a player-driven perspective on the problem of damage optimization, as well as an artifact that resulted in empirical improvements – advancing the fields of game analytics, game evaluation methods and self-regulated learning.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {368},\nnumpages = {16},\nkeywords = {Damage Optimization, Game Analytics, Video Game Simulation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641909,\nauthor = {Colley, Mark and Wanner, Beate and R\\\"{a}dler, Max and R\\\"{o}tzer, Marcel and Frommel, Julian and Hirzle, Teresa and Jansen, Pascal and Rukzio, Enrico},\ntitle = {Effects of a Gaze-Based 2D Platform Game on User Enjoyment, Perceived Competence, and Digital Eye Strain},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641909},\ndoi = {10.1145/3613904.3641909},\nabstract = {Gaze interaction is a promising interaction method to increase variety, challenge, and fun in games.We present “Shed Some Fear”, a 2D platform game including numerous eye-gaze-based interactions. “Shed Some Fear” includes control with eye-gaze and traditional keyboard input. The eye-gaze interactions are partially based on eye exercises reducing digital eye strain but also on employing peripheral vision. By employing eye-gaze as a necessary input mechanism, we explore the effects on and tradeoffs between user enjoyment and digital eye strain in a five-day longitudinal between-subject study (N=17) compared to interaction with a traditional mouse. We found that perceived competence was significantly higher with eye gaze interaction and significantly higher internal eye strain. With this work, we contribute to the not straightforward inclusion of eye tracking as a useful and fun input method for games.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {369},\nnumpages = {14},\nkeywords = {2D platform, digital eye strain, eye-gaze, gamification},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642270,\nauthor = {Perrig, Sebastian A. C. and Scharowski, Nicolas and Br\\\"{u}hlmann, Florian and von Felten, Nick and Opwis, Klaus and Aeschbach, Lena Fanya},\ntitle = {Independent Validation of the Player Experience Inventory: Findings from a Large Set of Video Game Players},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642270},\ndoi = {10.1145/3613904.3642270},\nabstract = {Measuring the subjective experience of digital game players is essential to player experience research. Recently, the Player Experience Inventory (PXI) was developed, which assesses both functional and psychosocial consequences of digital gameplay. We present a pre-registered independent online study with a large sample to provide additional evidence of psychometric quality for the PXI. Responses from 1518 participants were collected, rating a recent or memorable experience playing a digital game using the PXI and related measures. While our results from standard psychometric reliability and validity analyses generally favored the PXI, we also identified challenges with the immersion construct. Further, we find a ten-factor model, or alternatively, an 11-factor should enjoyment be measured, to fit our collected data best. In sum, the PXI is a valuable tool to measure a variety of constructs central to player experience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {370},\nnumpages = {13},\nkeywords = {Games user research, Measurement instrument, Player Experience Inventory, Player experience, Psychometrics, Questionnaires, Scale validation, Survey scales},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642441,\nauthor = {Gon\\c{c}alves, David and Barros, Daniel and Pais, Pedro and Guerreiro, Jo\\~{a}o and Guerreiro, Tiago and Rodrigues, Andr\\'{e}},\ntitle = {The Trick is to Stay Behind?: Defining and Exploring the Design Space of Player Balancing Mechanics},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642441},\ndoi = {10.1145/3613904.3642441},\nabstract = {In multiplayer gaming, skill disparity can lead to frustrating and excluding experiences. Balancing approaches exist to level the playing field (e.g., providing aim assistance to low-performing players), but it is unclear how different design choices affect individual player experience. We first introduce a design space for balancing mechanics encompassing six categories: Determination, Timing, Targeting, Effect, Feedback, and Information. We then present a mixed-methods study, focused on the effect of two subcategories: Targeting Direction and Effect Dependency on skill. In this study, eight pairs of participants played a game prototype and experienced seven balancing mechanics. We collected data from questionnaires and group interviews, revealing implications for future designs, including the importance of 1) merited victory that does not ignore individual achievements, 2) sense of agency when determining the balancing before and during gameplay, and 3) balancing as an intrinsic part of the game that does not disrupt the core gameplay.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {371},\nnumpages = {16},\nkeywords = {balancing, competition, design space, difficulty adjustment, fairness, gaming, multiplayer},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642589,\nauthor = {Zhang, Chenyang and Chen, Tiansu and Shaffer, Eric and Soltanaghai, Elahe},\ntitle = {FocusFlow: 3D Gaze-Depth Interaction in Virtual Reality Leveraging Active Visual Depth Manipulation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642589},\ndoi = {10.1145/3613904.3642589},\nabstract = {Gaze interaction presents a promising avenue in Virtual Reality (VR) due to its intuitive and efficient user experience. Yet, the depth control inherent in our visual system remains underutilized in current methods. In this study, we introduce FocusFlow, a hands-free interaction method that capitalizes on human visual depth perception within the 3D scenes of Virtual Reality. We first develop a binocular visual depth detection algorithm to understand eye input characteristics. We then propose a layer-based user interface and introduce the concept of “Virtual Window” that offers an intuitive and robust gaze-depth VR interaction, despite the constraints of visual depth accuracy and precision spatially at further distances. Finally, to help novice users actively manipulate their visual depth, we propose two learning strategies that use different visual cues to help users master visual depth control. Our user studies on 24 participants demonstrate the usability of our proposed virtual window concept as a gaze-depth interaction method. In addition, our findings reveal that the user experience can be enhanced through an effective learning process with adaptive visual cues, helping users to develop muscle memory for this brand-new input mechanism. We conclude the paper by discussing potential future research topics of gaze-depth interaction.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {372},\nnumpages = {18},\nkeywords = {3D User Interface, Eye Tracking, Gaze Interaction, Virtual Reality, Visual Depth},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642915,\nauthor = {Manakhov, Pavel and Sidenmark, Ludwig and Pfeuffer, Ken and Gellersen, Hans},\ntitle = {Gaze on the Go: Effect of Spatial Reference Frame on Visual Target Acquisition During Physical Locomotion in Extended Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642915},\ndoi = {10.1145/3613904.3642915},\nabstract = {Spatial interaction relies on fast and accurate visual acquisition. In this work, we analyse how visual acquisition and tracking of targets presented in a head-mounted display is affected by the user moving linearly at walking and jogging paces. We study four reference frames in which targets can be presented: Head and World where targets are affixed relative to the head and environment, respectively; HeadDelay where targets are presented in the head coordinate system but follow head movement with a delay, and novel Path where targets remain at fixed distance in front of the user, in the direction of their movement. Results of our study in virtual reality demonstrate that the more stable the target is relative to the environment, the faster and more precise it can be fixated. The results have practical significance as head-mounted displays enable interaction during mobility, and in particular when eye tracking is considered as input.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {373},\nnumpages = {16},\nkeywords = {UI placement, extended reality, eye tracking, gaze interaction, physical locomotion, reference frames, spatial UIs},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642577,\nauthor = {Kohtani, Arisa and Miyafuji, Shio and Uragaki, Keishiro and Katsuyama, Hidetaka and Koike, Hideki},\ntitle = {MOSion: Gaze Guidance with Motion-triggered Visual Cues by Mosaic Patterns},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642577},\ndoi = {10.1145/3613904.3642577},\nabstract = {We propose a gaze-guiding method called MOSion to adjust the guiding strength reacted to observers’ motion based on a high-speed projector and the afterimage effect in the human vision system. Our method decomposes the target area into mosaic patterns to embed visual cues in the perceived images. The patterns can only direct the attention of the moving observers to the target area. The stopping observer can see the original image with little distortion because of light integration in the visual perception. The precomputation of the patterns provides the adaptive guiding effect without tracking devices and computational costs depending on the movements. The evaluation and the user study show that the mosaic decomposition enhances the perceived saliency with a few visual artifacts, especially in moving conditions. Our method embedded in white lights works in various situations such as planar posters, advertisements, and curved objects.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {374},\nnumpages = {11},\nkeywords = {afterimage effect, computer vision, gaze guidance, high speed projection, saliency, visual perception},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642838,\nauthor = {Lee, Hock Siang and Weidner, Florian and Sidenmark, Ludwig and Gellersen, Hans},\ntitle = {Snap, Pursuit and Gain: Virtual Reality Viewport Control by Gaze},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642838},\ndoi = {10.1145/3613904.3642838},\nabstract = {Head-mounted displays let users explore virtual environments through a viewport that is coupled with head movement. In this work, we investigate gaze as an alternative modality for viewport control, enabling exploration of virtual worlds with less head movement. We designed three techniques that leverage gaze based on different eye movements: Dwell Snap for viewport rotation in discrete steps, Gaze Gain for amplified viewport rotation based on gaze angle, and Gaze Pursuit for central viewport alignment of gaze targets. All three techniques enable 360-degree viewport control through naturally coordinated eye and head movement. We evaluated the techniques in comparison with controller snap and head amplification baselines, for both coarse and precise viewport control, and found them to be as fast and accurate. We observed a high variance in performance which may be attributable to the different degrees to which humans tend to support gaze shifts with head movement.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {375},\nnumpages = {14},\nkeywords = {Eye tracking, eye-head coordination, gaze interaction, gaze-based interaction, user study, viewport control, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641925,\nauthor = {Reddy, G S Rajshekar and Proulx, Michael J and Hirshfield, Leanne and Ries, Anthony},\ntitle = {Towards an Eye-Brain-Computer Interface: Combining Gaze with the Stimulus-Preceding Negativity for Target Selections in XR},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641925},\ndoi = {10.1145/3613904.3641925},\nabstract = {Gaze-assisted interaction techniques enable intuitive selections without requiring manual pointing but can result in unintended selections, known as Midas touch. A confirmation trigger eliminates this issue but requires additional physical and conscious user effort. Brain-computer interfaces (BCIs), particularly passive BCIs harnessing anticipatory potentials such as the Stimulus-Preceding Negativity (SPN) - evoked when users anticipate a forthcoming stimulus - present an effortless implicit solution for selection confirmation. Within a VR context, our research uniquely demonstrates that SPN has the potential to decode intent towards the visually focused target. We reinforce the scientific understanding of its mechanism by addressing a confounding factor - we demonstrate that the SPN is driven by the user’s intent to select the target, not by the stimulus feedback itself. Furthermore, we examine the effect of familiarly placed targets, finding that SPN may be evoked quicker as users acclimatize to target locations; a key insight for everyday BCIs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {376},\nnumpages = {17},\nkeywords = {Assistive technology, Brain-Computer Interfaces, EEG, Eye-tracking, Gaze interaction, Menu selection, Midas touch, Pointing, Spatial Computing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642480,\nauthor = {Chen, Qing and Shuai, Wei and Zhang, Jiyao and Sun, Zhida and Cao, Nan},\ntitle = {Beyond Numbers: Creating Analogies to Enhance Data Comprehension and Communication with Generative AI},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642480},\ndoi = {10.1145/3613904.3642480},\nabstract = {Unfamiliar measurements usually hinder readers from grasping the scale of the numerical data, understanding the content, and feeling engaged with the context. To enhance data comprehension and communication, we leverage analogies to bridge the gap between abstract data and familiar measurements. In this work, we first conduct semi-structured interviews with design experts to identify design problems and summarize design considerations. Then, we collect an analogy dataset of 138 cases from various online sources. Based on the collected dataset, we characterize a design space for creating data analogies. Next, we build a prototype system, AnalogyMate, that automatically suggests data analogies, their corresponding design solutions, and generated visual representations powered by generative AI. The study results show the usefulness of AnalogyMate in aiding the creation process of data analogies and the effectiveness of data analogy in enhancing data comprehension and communication.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {377},\nnumpages = {14},\nkeywords = {creativity support, interview, lab study, prototyping/implementation, qualitative methods, quantitative methods},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642466,\nauthor = {Weisz, Justin D. and He, Jessica and Muller, Michael and Hoefer, Gabriela and Miles, Rachel and Geyer, Werner},\ntitle = {Design Principles for Generative AI Applications},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642466},\ndoi = {10.1145/3613904.3642466},\nabstract = {Generative AI applications present unique design challenges. As generative AI technologies are increasingly being incorporated into mainstream applications, there is an urgent need for guidance on how to design user experiences that foster effective and safe use. We present six principles for the design of generative AI applications that address unique characteristics of generative AI UX and offer new interpretations and extensions of known issues in the design of AI applications. Each principle is coupled with a set of design strategies for implementing that principle via UX capabilities or through the design process. The principles and strategies were developed through an iterative process involving literature review, feedback from design practitioners, validation against real-world generative AI applications, and incorporation into the design process of two generative AI applications. We anticipate the principles to usefully inform the design of generative AI applications by driving actionable design recommendations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {378},\nnumpages = {22},\nkeywords = {Generative AI, design principles, foundation models, human-centered AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642901,\nauthor = {Wang, Shun-Yu and Su, Wei-Chung and Chen, Serena and Tsai, Ching-Yi and Misztal, Marta and Cheng, Katherine M. and Lin, Alwena and Chen, Yu and Chen, Mike Y.},\ntitle = {RoomDreaming: Generative-AI Approach to Facilitating Iterative, Preliminary Interior Design Exploration},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642901},\ndoi = {10.1145/3613904.3642901},\nabstract = {Interior design aims to create aesthetically pleasing and functional environments within an architectural space. For a simple room, the preliminary design exploration currently takes multiple meetings and days of work for interior designers to incorporate homeowners’ personal preferences through layout, furnishings, form, colors, and materials. We present RoomDreaming, a generative AI-based approach designed to facilitate preliminary interior design exploration. It empowers owners and designers to rapidly and efficiently iterate through a broad range of AI-generated, photo-realistic design alternatives, each uniquely tailored to fit actual space layouts and individual design preferences. We conducted a series of formative and summative studies with a total of 18 homeowners and 20 interior designers to help design, improve, and evaluate RoomDreaming. Owners reported that RoomDreaming effectively increased the breadth and depth of design exploration with higher efficiency and satisfaction. Designers reported that one hour of collaborative designing with RoomDreaming yielded results comparable to several days of traditional owner-designer meetings, plus days to weeks worth of designer work to develop and refine designs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {379},\nnumpages = {20},\nkeywords = {architecture, generative-AI, human-centered AI, interior design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642919,\nauthor = {Wadinambiarachchi, Samangi and Kelly, Ryan M. and Pareek, Saumya and Zhou, Qiushi and Velloso, Eduardo},\ntitle = {The Effects of Generative AI on Design Fixation and Divergent Thinking},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642919},\ndoi = {10.1145/3613904.3642919},\nabstract = {Generative AI systems have been heralded as tools for augmenting human creativity and inspiring divergent thinking, though with little empirical evidence for these claims. This paper explores the effects of exposure to AI-generated images on measures of design fixation and divergent thinking in a visual ideation task. Through a between-participants experiment (N=60), we found that support from an AI image generator during ideation leads to higher fixation on an initial example. Participants who used AI produced fewer ideas, with less variety and lower originality compared to a baseline. Our qualitative analysis suggests that the effectiveness of co-ideation with AI rests on participants’ chosen approach to prompt creation and on the strategies used by participants to generate ideas in response to the AI’s suggestions. We discuss opportunities for designing generative AI systems for ideation support and incorporating these AI tools into ideation workflows.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {380},\nnumpages = {18},\nkeywords = {Creativity support tools, Design fixation, Generative-AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642114,\nauthor = {Li, Jie and Cao, Hancheng and Lin, Laura and Hou, Youyang and Zhu, Ruihao and El Ali, Abdallah},\ntitle = {User Experience Design Professionals’ Perceptions of Generative Artificial Intelligence},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642114},\ndoi = {10.1145/3613904.3642114},\nabstract = {Among creative professionals, Generative Artificial Intelligence (GenAI) has sparked excitement over its capabilities and fear over unanticipated consequences. How does GenAI impact User Experience Design (UXD) practice, and are fears warranted? We interviewed 20 UX Designers, with diverse experience and across companies (startups to large enterprises). We probed them to characterize their practices, and sample their attitudes, concerns, and expectations. We found that experienced designers are confident in their originality, creativity, and empathic skills, and find GenAI’s role as assistive. They emphasized the unique human factors of “enjoyment” and “agency”, where humans remain the arbiters of “AI alignment’’. However, skill degradation, job replacement, and creativity exhaustion can adversely impact junior designers. We discuss implications for human-GenAI collaboration, specifically copyright and ownership, human creativity and agency, and AI literacy and access. Through the lens of responsible and participatory AI, we contribute a deeper understanding of GenAI fears and opportunities for UXD.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {381},\nnumpages = {18},\nkeywords = {Generative AI, Human-AI Collaboration, Responsible AI, UX Designers, User Experience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642151,\nauthor = {Hernandez, Rie Helene (Lindy) and Song, Qiurong and Kou, Yubo and Gui, Xinning},\ntitle = {\"At the end of the day, I am accountable\": Gig Workers' Self-Tracking for Multi-Dimensional Accountability Management},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642151},\ndoi = {10.1145/3613904.3642151},\nabstract = {Tracking is inherent in and central to the gig economy. Platforms track gig workers’ performance through metrics such as acceptance rate and punctuality, while gig workers themselves engage in self-tracking. Although prior research has extensively examined how gig platforms track workers through metrics – with some studies briefly acknowledging the phenomenon of self-tracking among workers – there is a dearth of studies that explore how and why gig workers track themselves. To address this, we conducted 25 semi-structured interviews, revealing how gig workers self-track to manage accountabilities to themselves and external entities across three identities: the holistic self, the entrepreneurial self, and the platformized self. We connect our findings to neoliberalism, through which we contextualize gig workers’ self-accountability and the invisible labor of self-tracking. We further discuss how self-tracking mitigates information and power asymmetries in gig work and offer design implications to support gig workers’ multi-dimensional self-tracking.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {382},\nnumpages = {20},\nkeywords = {Gig workers, accountability, gig economy, personal informatics, self-tracking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642416,\nauthor = {Yadav, Deepika and Karlgren, Kasper and Shaikh, Riyaj and Helms, Karey and Mcmillan, Donald and Brown, Barry and Lampinen, Airi},\ntitle = {Bodywork at Work: Attending to Bodily Needs in Gig, Shift, and Knowledge Work},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642416},\ndoi = {10.1145/3613904.3642416},\nabstract = {The concept of ‘bodywork´ refers to the work individuals undertake on their own bodies and the bodies of others. One aspect is attending to bodily needs, which is often overlooked in the workplace and HCI/CSCW research on work practices. Yet, this labour can be a significant barrier to work, consequential to work, and prone to spill over into other aspects of life. We present three empirical cases of bodywork: gig-based food delivery, shift work in hospitals and bars, and office-based knowledge work. We describe what attending to bodily needs at work entails and illustrate tactics employed so that work can be carried on, even when the body (or technology optimising it) breaks down. Arguing that all systems are bodily systems, we conclude with a call to acknowledge the centrality of bodies in all work and the roles technologies can play in supporting or constraining bodywork differently for different workers.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {383},\nnumpages = {13},\nkeywords = {Bodywork, Health and Wellbeing, Interview Studies, Workplaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642614,\nauthor = {Do, Kimberly and De Los Santos, Maya and Muller, Michael and Savage, Saiph},\ntitle = {Designing Gig Worker Sousveillance Tools},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642614},\ndoi = {10.1145/3613904.3642614},\nabstract = {As independently-contracted employees, gig workers disproportionately suffer the consequences of workplace surveillance, which include increased pressures to work, breaches of privacy, and decreased digital autonomy. Despite the negative impacts of workplace surveillance, gig workers lack the tools, strategies, and workplace social support to protect themselves against these harms. Meanwhile, some critical theorists have proposed sousveillance as a potential means of countering such abuses of power, whereby those under surveillance monitor those in positions of authority (e.g., gig workers collect data about requesters/platforms). To understand the benefits of sousveillance systems in the gig economy, we conducted semi-structured interviews and led co-design activities with gig workers. We use “care ethics” as a guiding concept to understand our interview and co-design data, while also focusing on empathic sousveillance technology design recommendations. Through our study we identify gig workers’ attitudes towards and past experiences with sousveillance. We also uncover the type of sousveillance technologies imagined by workers, provide design recommendations, and finish by discussing how to create empowering, empathic spaces on gig platforms.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {384},\nnumpages = {19},\nkeywords = {crowd work, freelancers, gig work, job market, labor market, sousveillance *Both authors contributed equally to this research., surveillance},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641918,\nauthor = {Shaikh, Riyaj and Singh, Anubha and Brown, Barry and Lampinen, Airi},\ntitle = {Not Just A Dot on The Map: Food Delivery Workers as Infrastructure},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641918},\ndoi = {10.1145/3613904.3641918},\nabstract = {Food delivery platforms are location-based services that rely on minimal, quantifiable data points, such as GPS location, to represent and manage labor. Drawing upon an ethnographic study of food delivery work in India during the COVID-19 pandemic, we illustrate the challenges gig workers face when working with a platform that uses their (phone’s) GPS location to monitor and control their movement. Further, we describe how these, along with the platform’s opaque, location-based logics, shape the delivery workflow. We also document how the platform selectively represented workers’ bodies during the pandemic to portray them as safe and sterile, describing workers’ tactics in responding to issues arising from asymmetric platform policies. In discussion, we consider what we can learn from understanding gig workers as ‘infrastructure’, commonly overlooked but visible upon breakdown. We conclude by reflecting on how we might center gig workers’ well-being and bodily needs in design.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {385},\nnumpages = {15},\nkeywords = {COVID-19, algorithmic management, food delivery, gig work, infrastructure, location, location-based HCI, worker-centered design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642801,\nauthor = {Chen, Shi and Wang, Xiaodong and Li, Weijun and Zhang, Jingao and Qi, Yuge and Teng, Jiaqi and Zeng, Zhihan},\ntitle = {Silent Delivery: Practices and Challenges of Delivering Among Deaf or Hard of Hearing Couriers},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642801},\ndoi = {10.1145/3613904.3642801},\nabstract = {This paper explores the motivations, practices, and challenges of Deaf and Hard of Hearing (DHH) couriers in China’s food delivery industry. Interviews reveal a preference for this industry due to better pay, job satisfaction, and community belonging. DHH couriers tend to and frequently disclose their DHH disability using platform tags and text messages. They also utilize accessible communication tools provided by the delivery platforms, such as AI voice calls, voice-to-text technologies, and electronic communication cards, to facilitate communication during the delivery process. Despite these technological aids, human intervention remains crucial throughout the delivery process. Challenges encountered include safety risks when riding mopeds, the complexities of multitasking, and user mistrust in AI voice calls. Our findings offer valuable insights for designing more inclusive delivery platforms and have broader implications for creating employment opportunities for DHH, particularly in developing countries.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {386},\nnumpages = {17},\nkeywords = {Accessibility, Assistive Information and Communication Technologies, Empirical study that tells us about people, Individuals with Disabilities, Qualitative Methods},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641935,\nauthor = {Erickson, Jacob and Yan, Bei},\ntitle = {Affective Design: The Influence of Facebook Reactions on the Emotional Expression of the 114th US Congress},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641935},\ndoi = {10.1145/3613904.3641935},\nabstract = {Political communication is critical for democracy, but polarized emotions in communication may make careful deliberation difficult. Much of modern political communication occurs on social media, which may exacerbate these challenges. This study examines how the design of social media features impact political communication. We examined how the introduction of Facebook Reactions influenced the posts of the 114th US Congress on the platform. We start by analyzing the emotional content of posts, finding that politicians generally increased their usage of negative emotions in their posts after the feature’s launch. Further analysis showed that increased user engagement preceded the rise in negative emotions, suggesting that politicians were making adjustments based on user feedback. Our results show that the design features of social media can shape online political communication.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {387},\nnumpages = {9},\nkeywords = {Feature Design, Natural Field Experiment, Social Media, Text Analysis},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642000,\nauthor = {Zhang, Angie and Rana, Rocita and Boltz, Alexander and Dubal, Veena and Lee, Min Kyung},\ntitle = {Data Probes as Boundary Objects for Technology Policy Design: Demystifying Technology for Policymakers and Aligning Stakeholder Objectives in Rideshare Gig Work},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642000},\ndoi = {10.1145/3613904.3642000},\nabstract = {Despite the evidence of harm that technology can inflict, commensurate policymaking to hold tech platforms accountable still lags. This is pertinent to app-based gig workers, where unregulated algorithms continue to dictate their work, often with little human recourse. While past HCI literature has investigated workers’ experiences under algorithmic management and how to design interventions, rarely are the perspectives of stakeholders who inform or craft policy sought. To bridge this, we propose using data probes—interactive visualizations of workers’ data that show the impact of technology practices on people—exploring them in 12 semi-structured interviews with policy informers, (driver-)organizers, litigators, and a lawmaker in the rideshare space. We show how data probes act as boundary objects to assist stakeholder interactions, demystify technology for policymakers, and support worker collective action. We discuss the potential for data probes as training tools for policymakers, and considerations around data access and worker risks when using data probes.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {388},\nnumpages = {21},\nkeywords = {Algorithmic Management, Data Probes, Gig Work, Policymaking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642371,\nauthor = {Yang, Fumeng and Mortenson, Chloe Rose and Nisbet, Erik and Diakopoulos, Nicholas and Kay, Matthew},\ntitle = {In Dice We Trust: Uncertainty Displays for Maintaining Trust in Election Forecasts Over Time},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642371},\ndoi = {10.1145/3613904.3642371},\nabstract = {Trust in high-profile election forecasts influences the public’s confidence in democratic processes and electoral integrity. Yet, maintaining trust after unexpected outcomes like the 2016 U.S. presidential election is a significant challenge. Our work confronts this challenge through three experiments that gauge trust in election forecasts. We generate simulated U.S. presidential election forecasts, vary win probabilities and outcomes, and present them to participants in a professional-looking website interface. In this website interface, we explore (1) four different uncertainty displays, (2) a technique for subjective probability correction, and (3) visual calibration that depicts an outcome with its forecast distribution. Our quantitative results suggest that text summaries and quantile dotplots engender the highest trust over time, with observable partisan differences. The probability correction and calibration show small-to-null effects on average. Complemented by our qualitative results, we provide design recommendations for conveying U.S. presidential election forecasts and discuss long-term trust in uncertainty communication. We provide preregistration, code, data, model files, and videos at https://doi.org/10.17605/OSF.IO/923E7.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {389},\nnumpages = {24},\nkeywords = {election forecasts, political communication, trust, uncertainty visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642750,\nauthor = {Ge, Lily W. and Easterday, Matthew and Kay, Matthew and Dimara, Evanthia and Cheng, Peter and Franconeri, Steven L},\ntitle = {V-FRAMER: Visualization Framework for Mitigating Reasoning Errors in Public Policy},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642750},\ndoi = {10.1145/3613904.3642750},\nabstract = {Existing data visualization design guidelines focus primarily on constructing grammatically-correct visualizations that faithfully convey the values and relationships in the underlying data. However, a designer may create a grammatically-correct visualization that still leaves audiences susceptible to reasoning misleaders, e.g. by failing to normalize data or using unrepresentative samples. Reasoning misleaders are especially pernicious when presenting public policy data, where data-driven decisions can affect public health, safety, and economic development. Through textual analysis, a formative evaluation, and iterative design with 19 policy communicators, we construct an actionable visualization design framework, V-FRAMER, that effectively synthesizes ways of mitigating reasoning misleaders. We discuss important design considerations for frameworks like V-FRAMER, including using concrete examples to help designers understand reasoning misleaders, and using a hierarchical structure to support example-based accessing. We further describe V-FRAMER’s congruence with current practice and how practitioners might integrate the framework into their existing workflows. Related materials available at: https://osf.io/q3uta/.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {390},\nnumpages = {15},\nkeywords = {Framework, Public policy, Reasoning, Visualization design guidelines},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642329,\nauthor = {Cai, Mandi and Kay, Matthew},\ntitle = {Watching the Election Sausage Get Made: How Data Journalists Visualize the Vote Counting Process in U.S. Elections},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642329},\ndoi = {10.1145/3613904.3642329},\nabstract = {Election results in the United States are visualized online in real time by news outlets as vote counting persists over days or weeks. They are a massive public-facing exercise in managing audience understanding of uncertainty in partial data, breaking news web traffic records as the public seeks information about winners. We categorize designs of real-time election results from 19 U.S. news outlets and election results providers for the 2020 and 2022 general elections to create a visual vocabulary of live results. We then use this vocabulary to guide interviews with data journalists who worked on these designs to understand their design goals and challenges. Tying these conversations back to our visual vocabulary, we map out how communication goals like balancing certainty and uncertainty in the journey towards finding out winners, alongside challenges like determining thresholds at which information is shown, manifest in the designs displayed.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {391},\nnumpages = {19},\nkeywords = {United States elections, dashboard visualization, design space, progressive visualization, qualitative methods, uncertainty visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642507,\nauthor = {Wani, Asra Sakeen and Joshi, Ishika and Nahvi, Nadia Ishfaq and Singh, Pushpendra},\ntitle = {\"Unrest and trauma stays with you!\": Navigating mental health and professional service-seeking in Kashmir},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642507},\ndoi = {10.1145/3613904.3642507},\nabstract = {Mental health well-being is a global concern, with disparities in treatment services being a challenge. Though, digital mental health interventions are proposed to bridge the gaps and supplement support and assistance. Yet, many individuals still struggle with mental health issues, particularly in regions encountering socio-political unrest, and face obstacles in seeking professional assistance. Situating our work in Kashmir, India, a region with a long history of socio-political unrest, we conducted 18 semi-structured interviews with participants seeking professional support to explore how individuals navigate mental health and professional help-seeking. Our findings identify the struggles in seeking support rooted in the context through socio-political and socio-cultural influences, strategies and methods adopted to navigate these struggles, and the role of technology in seeking support. Using a social-ecological approach to mental health care, we emphasize accounting for the socio-political realities that shape support-seeking in politically disturbed contexts and offer socio-technical design recommendations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {392},\nnumpages = {17},\nkeywords = {conflict, disruption, mental health, technology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642422,\nauthor = {Awwad, Ghadeer and Toyama, Kentaro},\ntitle = {Digital Repression in Palestine},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642422},\ndoi = {10.1145/3613904.3642422},\nabstract = {While Israeli suppression of Palestinian voices is well-understood, much less is known about the Palestinian authorities’ repression of Palestinians – the very people they are supposed to represent. This paper investigates digital repression by Hamas and the Palestinian Authority through semi-structured interviews – in-person and online – with 19 Palestinian activists who post on social media. Many of our findings echo those from other repressive contexts, but the unusual Palestinian context also gives rise to several unique elements. For example, Palestinian authorities, while incorporating some high-tech methods, appear to rely primarily on a low-tech, labor-intensive apparatus to monitor, intimidate, and censor their targets, some of which involves highly personalized forms of repression. We also heard credible accusations of Palestinian authorities’ collaboration with Iranian and Israeli governments, the latter typically viewed as an adversary by Palestinians. We consider the implications of these findings and offer recommendations both for activists and social media platforms.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {393},\nnumpages = {15},\nkeywords = {Online activism, Palestinian governance, digital repression, internal repression},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641903,\nauthor = {Ayoka, Gifty and Barbareschi, Giulia and Cave, Richard and Holloway, Catherine},\ntitle = {Enhancing Communication Equity: Evaluation of an Automated Speech Recognition Application in Ghana},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641903},\ndoi = {10.1145/3613904.3641903},\nabstract = {In Ghana people who struggle to articulate speech as a result of different conditions experience barriers in interacting with others due to difficulties in being understood. Automatic speech recognition software can be used to help listeners understand people with communication difficulties. However, studies have not looked at the practical feasibility of these technologies beyond the Global North. We present a novel user study examining the introduction of one such technology, Google Project Relate, to Ghana. This freely available mobile application can create personalised speech recognition models in English for non-standard speech to support communication. Our user study spans the training of local speech and language therapists and 20 people with communication difficulties. We utilise the Technology Amplification Theory to contribute insights on the need for technological adaptations, awareness and support to reduce differential gaps of access, capacity and motivation to expand the reach of these technologies rather than exacerbating inequalities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {394},\nnumpages = {16},\nkeywords = {Accessibility, Automatic Speech Recognition, Disability, Ghana, Global South, Speech, Technology Amplification Theory},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642532,\nauthor = {Saha, Manika and Lindsay, Stephen and Watterson, Jessica L and Bartindale, Tom and Varghese, Delvin and Saha, Mallika and Oliver, Gillian C. and Ahmed, Syed Ishtiaque and Olivier, Patrick},\ntitle = {Hearing Community Voices in HCI4D: Establishing Safe Places to Co-Create Counter-Collective Narratives with Women Farmers in Bangladesh},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642532},\ndoi = {10.1145/3613904.3642532},\nabstract = {Although listening to community voice is a core value in HCI4D, we have limited methods to capture the community voice of marginalized groups within disadvantaged communities. Working with NGOs and 24 marginalized women farmers in Bangladesh, we promoted psychological safety and empowerment through our configuration of the process. Our stakeholders decided to record and produce a radio-style audio recording that presented their counter-collective narratives for development projects. We reflect on this process using the Benefits of Community Voice framework to document rich insights into community contexts, lived experiences, local knowledge, and building trust and buy-in and through interviews with three NGO workers. We discuss the fundamental need of stakeholders for a safe place to share, the value of letting stakeholders guide method selection, the significance of counter-collective narratives, the benefits of participatory audio to hear community voices for democratizing and sustaining development and design implications of our work for HCI4D.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {395},\nnumpages = {17},\nkeywords = {HCI for development, audio, qualitative methods, sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642504,\nauthor = {Tripathi, Gautami and Sabherwal, Medhavi and Singh, Pushpendra},\ntitle = {“I know I have this till my Last Breath”: Unmasking the Gaps in Chronic Obstructive Pulmonary Disease (COPD) Care in India},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642504},\ndoi = {10.1145/3613904.3642504},\nabstract = {COPD is an incurable Chronic Respiratory Disease that results in restricted airflow and respiratory issues. India faces rising pollution and health infrastructure challenges, significantly contributing to the prevalence of respiratory diseases like COPD. Studies have reported that COPD is India’s second leading cause of death and Disability Adjusted Life Years. Our study delves into the current state of COPD care and awareness in India. Our mixed-methods research, encompassing online surveys with medical personnel (n=9) and individuals (n=141) and semi-structured interviews (n=13) with various stakeholders (patients, doctors and caregivers), revealed a noteworthy lack of COPD awareness amongst the public, consequently affecting the COPD diagnosis, treatment and management strategies. We further explored how COPD affects patients’ self-perception and quality of life while identifying the barriers to COPD care. Finally, we conclude with design recommendations for technology-based interventions which can support the management of COPD patients in the Indian context.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {396},\nnumpages = {16},\nkeywords = {Chronic Obstructive Pulmonary Disease, Chronic Respiratory Diseases, India, Non Communicable Diseases},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642737,\nauthor = {Kaur, Sukhnidh and Swaminathan, Manohar and Bali, Kalika and Vashistha, Aditya},\ntitle = {Challenges to Online Disability Rights Advocacy in India},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642737},\ndoi = {10.1145/3613904.3642737},\nabstract = {People with disabilities experience high levels of social discrimination worldwide. But, these harms are more pronounced in the Global South due to the intense stigma around disability and its intersections with structural embeddings of patriarchy. The massive growth of social media in the Global South provides people with disabilities a unique opportunity to advocate for disability rights and challenge regressive ableist norms. Yet, little is known about the challenges they face in doing their advocacy work on social media. Through interviews with 20 disability advocates in India with diverse gender identities and abilities, we found that disability advocates routinely face ableist hate and harassment, patronizing and invalidating comments, and lack of visibility and support, which forces them to self-censor as a form of self-protection, leading to low advocacy outcomes. We draw on these findings to illuminate the role of social media in the invisibilization of people with disabilities in the online sphere.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {397},\nnumpages = {15},\nkeywords = {HCI, disability, online ableism, online advocacy, social media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642871,\nauthor = {Batool, Amna and Naseem, Mustafa and Toyama, Kentaro},\ntitle = {Expanding Concepts of Non-Consensual Image-Disclosure Abuse: A Study of NCIDA in Pakistan},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642871},\ndoi = {10.1145/3613904.3642871},\nabstract = {Non-Consensual Image-Disclosure Abuse (NCIDA) represents a subset of technology-facilitated sexual abuse where imagery and video with romantic or sexual connotations are used to control, extort, and otherwise harm victims. Despite considerable research on NCIDA, little is known about them in non-Western contexts. We investigate NCIDA in Pakistan, through interviews with victims, their relatives, and investigative officers; and observations of NCIDA cases being processed at a law enforcement agency. We find, first, that what constitutes NCIDA is much broader in Pakistan’s patriarchal society, and that its effects can be more severe than in Western contexts. On every dimension – types of content, perpetrators, impact on victims, and desired response by victims – our findings suggest an expansion of the concepts associated with NCIDA. We conclude by making technical and policy-level recommendations, both to address the specific context of Pakistan, and to enable a more global conception of NCIDA.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {398},\nnumpages = {17},\nkeywords = {Gender-Based Violence, Image Based Sexual Abuse, Non-Consensual Disclosures, Non-Consensual Image-Disclosure Abuse (NCIDA), Online Harassment, Social Media Platforms, South Asia, Technology-Facilitated Sexual Abuse, Women},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642733,\nauthor = {Solano-Kamaiko, Ian Ren\\'{e} and Mishra, Dibyendu and Dell, Nicola and Vashistha, Aditya},\ntitle = {Explorable Explainable AI: Improving AI Understanding for Community Health Workers in India},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642733},\ndoi = {10.1145/3613904.3642733},\nabstract = {AI technologies are increasingly deployed to support community health workers (CHWs) in high-stakes healthcare settings, from malnutrition diagnosis to diabetic retinopathy. Yet, little is known about how such technologies are understood by CHWs with low digital literacy and what can be done to make AI more understandable for them. This paper examines the potential of explorable explanations in improving AI understanding for CHWs in rural India. Explorable explanations integrate visual heuristics and written explanations to promote active learning. We conducted semi-structured interviews with CHWs who interacted with a design probe in which AI predictions of child malnutrition were accompanied by explorable explanations. Our findings show that explorable explanations shift CHWs’ AI-related folk theories, help develop a more nuanced understanding of AI, augment CHWs’ learning and occupational capabilities, and enhance their ability to contest AI decisions. We also uncover the effects of CHWs’ sociopolitical environments on AI understanding and argue for a more holistic conception of AI explainability that goes beyond cognition and literacy.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {399},\nnumpages = {21},\nkeywords = {Explainable AI, Explorable Explanations, Global South, Human-centered Explainable AI, community health workers, user study},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642952,\nauthor = {Lee, Chaeeun and Kim, Jinwook and Yi, Hyeonbeom and Lee, Woohun},\ntitle = {Viewer2Explorer: Designing a Map Interface for Spatial Navigation in Linear 360 Museum Exhibition Video},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642952},\ndoi = {10.1145/3613904.3642952},\nabstract = {The pandemic has contributed to the increased digital content development for remote experiences. Notably, museums have begun creating virtual exhibitions using 360-videos, providing a sense of presence and high level of immersion. However, 360-video content often uses a linear timeline interface that requires viewers to follow the path decided by the video creators. This format limits viewers’ ability to actively engage with and explore the virtual space independently. Therefore, we designed a map-based video interface, Viewer2Explorer, that enables the user to perceive and explore virtual spaces autonomously. We then conducted a study to compare the overall experience between the existing linear timeline and map interfaces. Viewer2Explorer enhanced users’ spatial controllability and enabled active exploration in virtual museum exhibition spaces. Additionally, based on our map interface, we discuss a new type of immersion and assisted autonomy that can be experienced through a 360-video interface and provide design insights for future content.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {400},\nnumpages = {15},\nkeywords = {360 Video, Map Interface, Spatial Exploration, Video Interface, Video Navigation, Virtual Exhibition},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642836,\nauthor = {Ashwini, B and Ghoshal, Atmadeep and Suri, Venkata Ratnadeep and Achary, Krishnaveni and Shukla, Jainendra},\ntitle = {“It looks useful, works just fine, but will it replace me ?\" Understanding Special Educators’ Perception of Social Robots for Autism Care in India},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642836},\ndoi = {10.1145/3613904.3642836},\nabstract = {Social robots, particularly in assisting children with autism, have exhibited positive impacts on mental health. While prior studies concentrated on social robots in the Global North, there’s limited exploration in the Global South. It’s essential to comprehend special educators’ perspectives for effective integration in resource-constrained settings. Our mixed-methods approach, involving interviews, workshops, and a panel discussion with 25 educators in India, uncovers challenges and opportunities in integrating social robots into autism interventions. The findings highlight the urgent need to democratise the benefits of social robotics. Special educators express concerns about their functional capacity and fear potential redundancy due to the replacement of human efforts by social robots. Despite initial scepticism, professionals suggest various ways to incorporate social robots, emphasising the importance of technological innovation in reshaping and enhancing their roles in autism therapy. We discuss the implications of these findings for developing context-aware solutions and policy-level initiatives necessary in resource-constrained settings.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {401},\nnumpages = {23},\nkeywords = {ASD, HCI4D, Human-robot interaction, social robots},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642663,\nauthor = {Kyu, Alexander and Mao, Hongyu and Zhu, Junyi and Goel, Mayank and Ahuja, Karan},\ntitle = {EITPose: Wearable and Practical Electrical Impedance Tomography for Continuous Hand Pose Estimation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642663},\ndoi = {10.1145/3613904.3642663},\nabstract = {Real-time hand pose estimation has a wide range of applications spanning gaming, robotics, and human-computer interaction. In this paper, we introduce EITPose, a wrist-worn, continuous 3D hand pose estimation approach that uses eight electrodes positioned around the forearm to model its interior impedance distribution during pose articulation. Unlike wrist-worn systems relying on cameras, EITPose has a slim profile (12 mm thick sensing strap) and is power-efficient (consuming only 0.3 W of power), making it an excellent candidate for integration into consumer electronic devices. In a user study involving 22 participants, EITPose achieves with a within-session mean per joint positional error of 11.06 mm. Its camera-free design prioritizes user privacy, yet it maintains cross-session and cross-user accuracy levels comparable to camera-based wrist-worn systems, thus making EITPose a promising technology for practical hand pose estimation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {402},\nnumpages = {10},\nkeywords = {Electrical Impedance Tomography, Extended Reality, Hand Gesture, Hand Pose, Input, Interaction Technique, Natural User Interfaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642910,\nauthor = {Lee, Chi-Jung and Zhang, Ruidong and Agarwal, Devansh and Yu, Tianhong Catherine and Gunda, Vipin and Lopez, Oliver and Kim, James and Yin, Sicheng and Dong, Boao and Li, Ke and Sakashita, Mose and Guimbretiere, Francois and Zhang, Cheng},\ntitle = {EchoWrist: Continuous Hand Pose Tracking and Hand-Object Interaction Recognition Using Low-Power Active Acoustic Sensing On a Wristband},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642910},\ndoi = {10.1145/3613904.3642910},\nabstract = {Our hands serve as a fundamental means of interaction with the world around us. Therefore, understanding hand poses and interaction contexts is critical for human-computer interaction (HCI). We present EchoWrist, a low-power wristband that continuously estimates 3D hand poses and recognizes hand-object interactions using active acoustic sensing. EchoWrist is equipped with two speakers emitting inaudible sound waves toward the hand. These sound waves interact with the hand and its surroundings through reflections and diffractions, carrying rich information about the hand’s shape and the objects it interacts with. The information captured by the two microphones goes through a deep learning inference system that recovers hand poses and identifies various everyday hand activities. Results from the two 12-participant user studies show that EchoWrist is effective and efficient at tracking 3D hand poses and recognizing hand-object interactions. Operating at 57.9 mW, EchoWrist can continuously reconstruct 20 3D hand joints with MJEDE of 4.81 mm and recognize 12 naturalistic hand-object interactions with 97.6\\% accuracy.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {403},\nnumpages = {21},\nkeywords = {Acoustic Sensing, Hand Pose, Hand-Object Interaction, Smartwatch, Wearable},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642255,\nauthor = {Luo, Yuhan and Yu, Junnan and Liang, Minhui and Wan, Yichen and Zhu, Kening and Santosa, Shannon Sie},\ntitle = {Emotion Embodied: Unveiling the Expressive Potential of Single-Hand Gestures},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642255},\ndoi = {10.1145/3613904.3642255},\nabstract = {Hand gestures are widely used in daily life for expressing emotions, yet gesture input is not part of existing emotion tracking systems. To seek a practical and effortless way of using gestures to inform emotions, we explore the relationships between gestural features and commonly experienced emotions by focusing on single-hand gestures that are easy to perform and capture. First, we collected 756 gestures (in photo and video pairs) from 63 participants who expressed different emotions in a survey, and then interviewed 11 of them to understand their gesture-forming rationales. We found that the valence and arousal level of the expressed emotions significantly correlated with participants’ finger-pointing direction and their gesture strength, and synthesized four channels through which participants externalized their expressions with gestures. Reflecting on the findings, we discuss how emotions can be characterized and contextualized with gestural cues and implications for designing multimodal emotion tracking systems and beyond.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {404},\nnumpages = {17},\nkeywords = {Emotion capture, gesture elicitation, gesture interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642602,\nauthor = {Khanna, Prerna and Ramakrishnan, IV and Jain, Shubham and Bi, Xiaojun and Balasubramanian, Aruna},\ntitle = {Hand Gesture Recognition for Blind Users by Tracking 3D Gesture Trajectory},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642602},\ndoi = {10.1145/3613904.3642602},\nabstract = {Hand gestures provide an alternate interaction modality for blind users and can be supported using commodity smartwatches without requiring specialized sensors. The enabling technology is an accurate gesture recognition algorithm, but almost all algorithms are designed for sighted users. Our study shows that blind user gestures are considerably different from sighted users, rendering current recognition algorithms unsuitable. Blind user gestures have high inter-user variance, making learning gesture patterns difficult without large-scale training data. Instead, we design a gesture recognition algorithm that works on a 3D representation of the gesture trajectory, capturing motion in free space. Our insight is to extract a micro-movement in the gesture that is user-invariant and use this micro-movement for gesture classification. To this end, we develop an ensemble classifier that combines image classification with geometric properties of the gesture. Our evaluation demonstrates a 92\\% classification accuracy, surpassing the next best state-of-the-art which has an accuracy of 82\\%.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {405},\nnumpages = {15},\nkeywords = {Accessibility, Blind users, Gesture recognition, Sensing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642554,\nauthor = {Yeh, Yen-Ting and Irudayaraj, Antony Albert Raj and Vogel, Daniel},\ntitle = {Single-handed Folding Interactions with a Modified Clamshell Flip Phone},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642554},\ndoi = {10.1145/3613904.3642554},\nabstract = {We explore and evaluate single-handed folding interactions suitable for “modified clamshell flip phones” with a full screen touch display that folds in half along the short dimension. Three categories of interactions are identified: only-fold, touch-enhanced fold, and fold-enhanced touch; in which gestures are created using fold direction, fold magnitude, and touch position. A prototype evaluation device is built to resemble clamshell flip phones, but with a modified hinge and spring system to enable folding in both directions. A study investigates performance and preference for 30 fold gestures to discover which are most promising. To demonstrate how folding interactions could be incorporated into flip phone interfaces, applications such as map browsing, text editing, and menu shortcuts are described.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {406},\nnumpages = {14},\nkeywords = {Flip phone, Foldable input, Interaction techniques},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642518,\nauthor = {Lin, Yilong and Zhang, Peng and Ofek, Eyal and Je, Seungwoo},\ntitle = {ArmDeformation: Inducing the Sensation of Arm Deformation in Virtual Reality Using Skin-Stretching},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642518},\ndoi = {10.1145/3613904.3642518},\nabstract = {With the development of virtual reality (VR) technology, research is being actively conducted on how incorporating multisensory feedback can create the illusion that virtual avatars are perceived as an extension of the body in VR. In line with this research direction, we introduce ArmDeformation, a wearable device employing skin-stretching to enhance virtual forearm ownership during arm deformation illusion. We conducted five user studies with 98 participants. Using a developed tabletop device, we confirmed the optimal number of actuators and the ideal skin-stretching design effectively increases the user’s body ownership. Additionally, we explored the maximum visual threshold for forearm bending and the minimum detectable bending direction angle when using skin-stretching in VR. Finally, our study demonstrates that using ArmDeformation in VR applications enhances user realism and enjoyment compared to relying on visual feedback alone.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {407},\nnumpages = {18},\nkeywords = {Body Illusion, Body Ownership, Skin-stretching, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642230,\nauthor = {Lee, Jaewook and Wang, Jun and Brown, Elizabeth and Chu, Liam and Rodriguez, Sebastian S. and Froehlich, Jon E.},\ntitle = {GazePointAR: A Context-Aware Multimodal Voice Assistant for Pronoun Disambiguation in Wearable Augmented Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642230},\ndoi = {10.1145/3613904.3642230},\nabstract = {Voice assistants (VAs) like Siri and Alexa are transforming human-computer interaction; however, they lack awareness of users’ spatiotemporal context, resulting in limited performance and unnatural dialogue. We introduce GazePointAR, a fully-functional context-aware VA for wearable augmented reality that leverages eye gaze, pointing gestures, and conversation history to disambiguate speech queries. With GazePointAR, users can ask “what’s over there?” or “how do I solve this math problem?” simply by looking and/or pointing. We evaluated GazePointAR in a three-part lab study (N=12): (1) comparing GazePointAR to two commercial systems, (2) examining GazePointAR’s pronoun disambiguation across three tasks; (3) and an open-ended phase where participants could suggest and try their own context-sensitive queries. Participants appreciated the naturalness and human-like nature of pronoun-driven queries, although sometimes pronoun use was counter-intuitive. We then iterated on GazePointAR and conducted a first-person diary study examining how GazePointAR performs in-the-wild. We conclude by enumerating limitations and design considerations for future context-aware VAs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {408},\nnumpages = {20},\nkeywords = {LLM, augmented reality, gaze tracking, multimodal input, pointing gesture recognition, voice assistants},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642067,\nauthor = {Kim, Taejun and Shim, Youngbo Aram and Kim, Youngin and Kim, Sunbum and Lee, Jaeyeon and Lee, Geehyuk},\ntitle = {QuadStretcher: A Forearm-Worn Skin Stretch Display for Bare-Hand Interaction in AR/VR},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642067},\ndoi = {10.1145/3613904.3642067},\nabstract = {The paradigm of bare-hand interaction has become increasingly prevalent in Augmented Reality (AR) and Virtual Reality (VR) environments, propelled by advancements in hand tracking technology. However, a significant challenge arises in delivering haptic feedback to users’ hands, due to the necessity for the hands to remain bare. In response to this challenge, recent research has proposed an indirect solution of providing haptic feedback to the forearm. In this work, we present QuadStretcher, a skin stretch display featuring four independently controlled stretching units surrounding the forearm. While achieving rich haptic expression, our device also eliminates the need for a grounding base on the forearm by using a pair of counteracting tactors, thereby reducing bulkiness. To assess the effectiveness of QuadStretcher in facilitating immersive bare-hand experiences, we conducted a comparative user evaluation (n = 20) with a baseline solution, Squeezer. The results confirmed that QuadStretcher outperformed Squeezer in terms of expressing force direction and heightening the sense of realism, particularly in 3-DoF VR interactions such as pulling a rubber band, hooking a fishing rod, and swinging a tennis racket. We further discuss the design insights gained from qualitative user interviews, presenting key takeaways for future forearm-haptic systems aimed at advancing AR/VR bare-hand experiences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {409},\nnumpages = {15},\nkeywords = {AR, Bare-Hand Interaction, Haptics, Skin Stretch, VR, Wearable},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642071,\nauthor = {Liao, Yi-Chi and Desai, Ruta and Pierce, Alec M and Taylor, Krista E. and Benko, Hrvoje and Jonker, Tanya R. and Gupta, Aakar},\ntitle = {A Meta-Bayesian Approach for Rapid Online Parametric Optimization for Wrist-based Interactions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642071},\ndoi = {10.1145/3613904.3642071},\nabstract = {Wrist-based input often requires tuning parameter settings in correspondence to between-user and between-session differences, such as variations in hand anatomy, wearing position, posture, etc. Traditionally, users either work with predefined parameter values not optimized for individuals or undergo time-consuming calibration processes. We propose an online Bayesian Optimization (BO)-based method for rapidly determining the user-specific optimal settings of wrist-based pointing. Specifically, we develop a meta-Bayesian optimization (meta-BO) method, differing from traditional human-in-the-loop BO: By incorporating meta-learning of prior optimization data from a user population with BO, meta-BO enables rapid calibration of parameters for new users with a handful of trials. We evaluate our method with two representative and distinct wrist-based interactions: absolute and relative pointing. On a weighted-sum metric that consists of completion time, aiming error, and trajectory quality, meta-BO improves absolute pointing performance by <Formula format=\"inline\"><TexMath><?TeX $22.92\\%$?></TexMath><AltText>Math 1</AltText><File name=\"chi24-182-inline1\" type=\"svg\"/></Formula> and <Formula format=\"inline\"><TexMath><?TeX $21.35\\%$?></TexMath><AltText>Math 2</AltText><File name=\"chi24-182-inline2\" type=\"svg\"/></Formula> compared to BO and manual calibration, and improves relative pointing performance by <Formula format=\"inline\"><TexMath><?TeX $25.43\\%$?></TexMath><AltText>Math 3</AltText><File name=\"chi24-182-inline3\" type=\"svg\"/></Formula> and <Formula format=\"inline\"><TexMath><?TeX $13.60\\%$?></TexMath><AltText>Math 4</AltText><File name=\"chi24-182-inline4\" type=\"svg\"/></Formula>.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {410},\nnumpages = {38},\nkeywords = {Bayesian optimization, adaptive interface., calibration, human-in-the-loop optimization, meta-Bayesian optimization, meta-learning, target selection, wrist-based interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642483,\nauthor = {Tanaka, Yudai and Serfaty, Jacob and Lopes, Pedro},\ntitle = {Haptic Source-Effector: Full-Body Haptics via Non-Invasive Brain Stimulation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642483},\ndoi = {10.1145/3613904.3642483},\nabstract = {We propose a novel concept for haptics in which one centralized on-body actuator renders haptic effects on multiple body parts by stimulating the brain, i.e., the source of the nervous system—we call this a haptic source-effector, as opposed to the traditional wearables’ approach of attaching one actuator per body part (end-effectors). We implement our concept via transcranial-magnetic-stimulation (TMS)—a non-invasive technique from neuroscience/medicine in which electromagnetic pulses safely stimulate brain areas. Our approach renders ∼15 touch/force-feedback sensations throughout the body (e.g., hands, arms, legs, feet, and jaw—which we found in our first user study), all by stimulating the user's sensorimotor cortex with a single magnetic coil moved mechanically across the scalp. In our second user study, we probed into participants’ experiences while using our haptic display in VR. Finally, as the first implementation of full-body haptics based on non-invasive brain stimulation, we discuss the roadmap to extend its interactive opportunities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {411},\nnumpages = {15},\nkeywords = {Electrical Muscle Stimulation, Haptics, Transcranial Magnetic Stimulation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642225,\nauthor = {Shen, Xiyuan and Yu, Chun and Wang, Xutong and Liang, Chen and Chen, Haozhan and Shi, Yuanchun},\ntitle = {MouseRing: Always-available Touchpad Interaction with IMU Rings},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642225},\ndoi = {10.1145/3613904.3642225},\nabstract = {Tracking fine-grained finger movements with IMUs for continuous 2D-cursor control poses significant challenges due to limited sensing capabilities. Our findings suggest that finger-motion patterns and the inherent structure of joints provide beneficial physical knowledge, which lead us to enhance motion perception accuracy by integrating physical priors into ML models. We propose MouseRing, a novel ring-shaped IMU device that enables continuous finger-sliding on unmodified physical surfaces like a touchpad. A motion dataset was created using infrared cameras, touchpads, and IMUs. We then identified several useful physical constraints, such as joint co-planarity, rigid constraints, and velocity consistency. These principles help refine the finger-tracking predictions from an RNN model. By incorporating touch state detection as a cursor movement switch, we achieved precise cursor control. In a Fitts’ Law study, MouseRing demonstrated input efficiency comparable to touchpads. In real-world applications, MouseRing ensured robust, efficient input and good usability across various surfaces and body postures.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {412},\nnumpages = {19},\nkeywords = {Finger Tracking, IMU, Input, Pointing Technique, Ring Interaction, Touch Interface, Wearable},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642292,\nauthor = {Dufresne, Florian and Nilsson, Tommy and Gorisse, Geoffrey and Guerra, Enrico and Zenner, Andr\\'{e} and Christmann, Olivier and Bensch, Leonie and Callus, Nikolai Anton and Cowley, Aidan},\ntitle = {Touching the Moon: Leveraging Passive Haptics, Embodiment and Presence for Operational Assessments in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642292},\ndoi = {10.1145/3613904.3642292},\nabstract = {Space agencies are in the process of drawing up carefully thought-out Concepts of Operations (ConOps) for future human missions on the Moon. These are typically assessed and validated through costly and logistically demanding analogue field studies. While interactive simulations in Virtual Reality (VR) offer a comparatively cost-effective alternative, they have faced criticism for lacking the fidelity of real-world deployments. This paper explores the applicability of passive haptic interfaces in bridging the gap between simulated and real-world ConOps assessments. Leveraging passive haptic props (equipment mockup and astronaut gloves), we virtually recreated the Apollo 12 mission procedure and assessed it with experienced astronauts and other space experts. Quantitative and qualitative findings indicate that haptics increased presence and embodiment, thus improving perceived simulation fidelity and validity of user reflections. We conclude by discussing the potential role of passive haptic modalities in facilitating early-stage ConOps assessments for human endeavours on the Moon and beyond.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {413},\nnumpages = {18},\nkeywords = {concepts of operations, embodiment, passive haptic feedback, presence, scenario assessment, space exploration, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642828,\nauthor = {Vega, Gabriela and Martinez-Missir, Valentin and Wittchen, Dennis and Sabnis, Nihar and Girouard, Audrey and Cochrane, Karen Anne and Strohmeier, Paul},\ntitle = {vARitouch: Back of the Finger Device for Adding Variable Compliance to Rigid Objects},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642828},\ndoi = {10.1145/3613904.3642828},\nabstract = {We present vARitouch, a back-of-the-finger wearable that can modify the perceived tactile material properties of the uninstrumented world around us: vARitouch can modulate the perceived softness of a rigid object through a vibrotactile compliance illusion. As vARitouch does not cover the fingertip, all-natural tactile properties are preserved. We provide three contributions: (1) We demonstrate the feasibility of the concept through a psychophysics study, showing that virtual compliance can be continuously modulated, and perceived softness can be increased by approximately 30 Shore A levels. (2) A qualitative study indicates the desirability of such a device, showing that a back-of-the-finger haptic device has many attractive qualities. (3) To implement vARitouch, we identify a novel way to measure pressure from the back of the finger by repurposing a pulse oximetry sensor. Based on these contributions, we present the finalized vARitouch system, accompanied by a series of application scenarios.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {414},\nnumpages = {20},\nkeywords = {compliance, finger wearable, haptic illusion, nail, softness, tactile, wearable haptics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642271,\nauthor = {Van Brummelen, Jessica and Urwin, Liv Piper and Johnston, Oliver James and Sayed, Mohamed and Brostow, Gabriel},\ntitle = {Don’t Look Now: Audio/Haptic Guidance for 3D Scanning of Landmarks},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642271},\ndoi = {10.1145/3613904.3642271},\nabstract = {People are increasingly using their smartphones to 3D scan objects and landmarks. On one hand, users have intrinsic motivations to scan well, i.e. keeping the object in-frame while walking around it to achieve coverage. On the other, users can lose interest when filming inanimate objects, and feel rushed and uncertain of their progress when watching their step in public, seeking to avoid attention. We set out to guide users while reducing their stress and increasing engagement, by moving away from the on-screen feedback ubiquitous in existing products and apps meant for 3D scanning. Specifically, our novel interface gives users audio/haptic guidance while they scan statue-type landmarks in public. The interface results from a conceptual design process and a pilot study. Ultimately, we tested 50 users in an ultra-high-traffic area of central London. Compared to regular on-screen feedback, users were more engaged, had unchanged stress levels, and produced better scans.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {415},\nnumpages = {20},\nkeywords = {3D scanning, AR, audio feedback, engagement, guidance, haptic feedback, mesh reconstruction accuracy, multimodal feedback, user safety, visual feedback},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642008,\nauthor = {Hwang, Seokhyun and Oh, Jeongseok and Kang, Seongjun and Seong, Minwoo and Elsharkawy, Ahmed Ibrahim Ahmed Mohamed and Kim, Seungjun},\ntitle = {ErgoPulse: Electrifying Your Lower Body With Biomechanical Simulation-based Electrical Muscle Stimulation Haptic System in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642008},\ndoi = {10.1145/3613904.3642008},\nabstract = {This study presents ErgoPulse, a system that integrates biomechanical simulation with electrical muscle stimulation (EMS) to provide kinesthetic force feedback to the lower-body in virtual reality (VR). ErgoPulse features two main parts: a biomechanical simulation part that calculates the lower-body joint torques to replicate forces from VR environments, and an EMS part that translates torques into muscle stimulations. In the first experiment, we assessed users’ ability to discern haptic force intensity and direction, and observed variations in perceived resolution based on force direction. The second experiment evaluated ErgoPulse’s ability to increase haptic force accuracy and user presence in both continuous and impulse force VR game environments. The experimental results showed that ErgoPulse’s biomechanical simulation increased the accuracy of force delivery compared to traditional EMS, enhancing the overall user presence. Furthermore, the interviews proposed improvements to the haptic experience by integrating additional stimuli such as temperature, skin stretch, and impact.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {416},\nnumpages = {21},\nkeywords = {Biomechanics, Electrical Muscle Stimulation, Haptic, Simulation, Virtual Reality, Wearable Device},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642156,\nauthor = {Teng, Shan-Yuan and Gupta, Aryan and Lopes, Pedro},\ntitle = {Haptic Permeability: Adding Holes to Tactile Devices Improves Dexterity},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642156},\ndoi = {10.1145/3613904.3642156},\nabstract = {Feeling haptics with our fingerpads is how we achieve manual tasks (e.g., operate a needle or press buttons). Following this, research started adding actuators atop the users’ fingerpads to render haptic feedback for interactive virtual environments. Recently, many have moved away from thick actuators (e.g., vibration motors) and turned to electrode-films with electrotactile stimulation—allowing users to still feel some sensations through the devices when touching physical objects (e.g., compliance or some macro features). However, we argue \\& demonstrate that thin devices are not enough to maximize the user's dexterity. We evaluate how adding small holes to electrotactile films can allow direct contact and thus increase haptic permeability, resulting in: (1) improved perception of tactile features; and (2) improved force control in grasping tasks. Finally, we observed participants in interactive experiences and found that holes can preserve dexterity with physical tasks while still benefiting from haptic feedback.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {417},\nnumpages = {12},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642499,\nauthor = {Ding, Yuran and Sabnis, Nihar and Strohmeier, Paul},\ntitle = {Motionless Movement: Towards Vibrotactile Kinesthetic Displays},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642499},\ndoi = {10.1145/3613904.3642499},\nabstract = {Beyond visual and auditory displays, tactile displays and grounded force feedback devices have become more common. Other sensory modalities are also catered to by a broad range of display devices, including temperature, taste, and olfaction. However, one sensory modality remains challenging to represent: kinesthesia – the sense of movement. Inspired by grain-based compliance illusions, we investigate how vibrotactile cues can evoke kinesthetic experiences, even when no movement is performed. We examine the effects of vibrotactile mappings and granularity on the magnitude of perceived motion; distance-based mappings provided the greatest sense of movement. Using an implementation that combines visual feedback and our prototype kinesthetic display, we demonstrate that action-coupled vibrotactile cues are significantly better at conveying an embodied sense of movement than the corresponding visual stimulus, and that combining vibrotactile and visual feedback is best. These results point towards a future where kinesthetic displays will be used in rehabilitation, sports, virtual-reality and beyond.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {418},\nnumpages = {16},\nkeywords = {embodiment, haptic illusion, haptic rendering, human augmentation, kinesthesia, kinesthetic display, movement display, movement illusion, tactile feedback},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641941,\nauthor = {Wang, Haokun and Singhal, Yatharth and Gil, Hyunjae and Kim, Jin Ryong},\ntitle = {Thermal Masking: When the Illusion Takes Over the Real},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641941},\ndoi = {10.1145/3613904.3641941},\nabstract = {This paper reports on a thermal illusion called thermal masking. Thermal masking is a phenomenon induced by thermal referral to completely mask the original thermal sensation, providing thermal sensation only at the tactile site. Three experiments are conducted using thermal and vibrotactile actuators to investigate the nature of thermal masking on human arms. The first experiment investigates the effects of different temperatures on masking. The results show a higher percentage of thermal masking occurs in warm than hot or cold conditions. The second experiment examines how far the thermal masking can be perceived. The results show that masking can reach up to 24 cm from the thermal site. The third experiment explores the interaction space by placing the tactile actuators on the opposite side of the thermal actuator. The results confirm that thermal masking can reach the other side of the arm, and the performance was higher in warm conditions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {419},\nnumpages = {16},\nkeywords = {Thermal perception, forearm, multisensory feedback, tactile masking, thermal illusion},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642251,\nauthor = {Park, Chaeyong and Choi, Seungmoon},\ntitle = {Augmenting Perceived Length of Handheld Controllers: Effects of Object Handle Properties},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642251},\ndoi = {10.1145/3613904.3642251},\nabstract = {In the realm of virtual reality (VR), shape-changing controllers have emerged as a means to enhance visuo-haptic congruence during user interactions. The major emphasis has been placed on manipulating the inertia tensor of a shape-changing controller to control the perceived shape. This paper delves deeper by exploring how the material properties of the controller’s handle, distinct from the inertial information, affect the perceived shape, focusing on the perceived length. We conducted three perceptual experiments to examine the effects of the handle’s softness, thermal conductivity, and texture, respectively. Results demonstrated that a softer handle increases the perceived length, whereas a handle with higher thermal conductivity reduces it. Texture, in the form of varying bumps, also alters the length perception. These results provide more comprehensive knowledge of the intricate relationship between perceived length and controller handle properties, expanding the design alternatives for shape-changing controllers for immersive VR experiences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {420},\nnumpages = {11},\nkeywords = {Dynamic Touch, Handheld Controller, Haptic Perceptual Cue, Shape Perception, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642552,\nauthor = {Stellmacher, Carolin and Pujianto, Feri Irsanto and Kojic, Tanja and Voigt-Antons, Jan-Niklas and Sch\\\"{o}ning, Johannes},\ntitle = {Experiencing Dynamic Weight Changes in Virtual Reality Through Pseudo-Haptics and Vibrotactile Feedback},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642552},\ndoi = {10.1145/3613904.3642552},\nabstract = {Virtual reality (VR) objects react dynamically to users’ touch interactions in real-time. However, experiencing changes in weight through the haptic sense remains challenging with consumer VR controllers due to their limited vibrotactile feedback. While prior works successfully applied pseudo-haptics to perceive absolute weight by manipulating the control-display (C/D) ratio, we continuously adjusted the C/D ratio to mimic weight changes. Vibrotactile feedback additionally emphasises the modulation in the virtual object’s physicality. In a study (N=18), we compared our multimodal technique with pseudo-haptics alone and a baseline condition to assess participants’ experiences of weight changes. Our findings demonstrate that participants perceived varying degrees of weight change when the C/D ratio was adjusted, validating its effectiveness for simulating dynamic weight in VR. However, the additional vibrotactile feedback did not improve weight change perception. This work extends the understanding of designing haptic experiences for lightweight VR systems by leveraging perceptual mechanisms.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {421},\nnumpages = {13},\nkeywords = {haptic illusions, multisensory integration, pseudo-haptics, vibration, virtual reality, weight change, weight perception},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642176,\nauthor = {Stellmacher, Carolin and Mathis, Florian and Weiss, Yannick and Loerakker, Meagan B. and Wagener, Nadine and Sch\\\"{o}ning, Johannes},\ntitle = {Exploring Mobile Devices as Haptic Interfaces for Mixed Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642176},\ndoi = {10.1145/3613904.3642176},\nabstract = {Dedicated handheld controllers facilitate haptic experiences of virtual objects in mixed reality (MR). However, as mobile MR becomes more prevalent, we observe the emergence of controller-free MR interactions. To retain immersive haptic experiences, we explore the use of mobile devices as a substitute for specialised MR controller. In an exploratory gesture elicitation study (n = 18), we examined users’ (1) intuitive hand gestures performed with prospective mobile devices and (2) preferences for real-time haptic feedback when exploring haptic object properties. Our results reveal three haptic exploration modes for the mobile device, as an object, hand substitute, or as an additional tool, and emphasise the benefits of incorporating the device’s unique physical features into the object interaction. This work expands the design possibilities using mobile devices for tangible object interaction, guiding the future design of mobile devices for haptic MR experiences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {422},\nnumpages = {17},\nkeywords = {gesture elicitation, haptic exploration, haptic feedback, haptic interfaces, mixed reality, mobile gestures, mobile phones},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642069,\nauthor = {Gomi, Ryota and Suzuki, Ryo and Takashima, Kazuki and Fujita, Kazuyuki and Kitamura, Yoshifumi},\ntitle = {InflatableBots: Inflatable Shape-Changing Mobile Robots for Large-Scale Encountered-Type Haptics in VR},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642069},\ndoi = {10.1145/3613904.3642069},\nabstract = {We introduce InflatableBots, shape-changing inflatable robots for large-scale encountered-type haptics in VR. Unlike traditional inflatable shape displays, which are immobile and limited in interaction areas, our approach combines mobile robots with fan-based inflatable structures. This enables safe, scalable, and deployable haptic interactions on a large scale. We developed three coordinated inflatable mobile robots, each of which consists of an omni-directional mobile base and a reel-based inflatable structure. The robot can simultaneously change its height and position rapidly (horizontal: 58.5 cm/sec, vertical: 10.4 cm/sec, from 40 cm to 200 cm), which allows for quick and dynamic haptic rendering of multiple touch points to simulate various body-scale objects and surfaces in real-time across large spaces (3.5 m x 2.5 m). We evaluated our system with a user study (N = 12), which confirms the unique advantages in safety, deployability, and large-scale interactability to significantly improve realism in VR experiences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {423},\nnumpages = {14},\nkeywords = {Encountered-Type Haptics, Haptics, Inflatables, Mobile Robots, Shape-Changing Interfaces, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642064,\nauthor = {Lu, Pin-Chun and Wang, Che-Wei and Hsu, Yu Lun and Lopez, Alvaro and Tsai, Ching-Yi and Chang, Chiao-Ju and Tan, Wei Tian Mireille and Lu, Li-Chun and Chen, Mike Y.},\ntitle = {VeeR: Exploring the Feasibility of Deliberately Designing VR Motion that Diverges from Mundane, Everyday Physical Motion to Create More Entertaining VR Experiences},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642064},\ndoi = {10.1145/3613904.3642064},\nabstract = {This paper explores the feasibility of deliberately designing VR motion that diverges from users’ physical movements to turn mundane, everyday transportation motion (e.g., metros, trains, and cars) into more entertaining VR motion experiences, in contrast to prior car-based VR approaches that synchronize VR motion to physical car movement exactly. To gain insight into users’ preferences for veering rate and veering direction for turning (left/right) and pitching (up/down) during the three phases of acceleration (accelerating, cruising, and decelerating), we conducted a formative, perceptual study (n=24) followed by a VR experience evaluation (n=18), all conducted on metro trains moving in a mundane, straight-line motion. Results showed that participants preferred relatively high veering rates, and preferred pitching upward during acceleration and downward during deceleration. Furthermore, while veering decreased comfort as expected, it significantly enhanced immersion (p<.01) and entertainment (p<.001) and the overall experience, with comfort being considered, was preferred by 89\\% of participants.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {424},\nnumpages = {13},\nkeywords = {Motion Sensation, Opportunistic Haptic, User Experience Design, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642546,\nauthor = {Jiang, Chutian and Fan, Yinan and Xie, Junan and Kuang, Emily and Zhang, Kaihao and Fan, Mingming},\ntitle = {Designing Unobtrusive Modulated Electrotactile Feedback on Fingertip Edge to Assist Blind and Low Vision (BLV) People in Comprehending Charts},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642546},\ndoi = {10.1145/3613904.3642546},\nabstract = {Charts are crucial in conveying information across various fields but are inaccessible to blind and low vision (BLV) people without assistive technology. Chart comprehension tools leveraging haptic feedback have been used widely but are often bulky, expensive, and static, rendering them inefficient for conveying chart data. To increase device portability, enable multitasking, and provide efficient assistance in chart comprehension, we introduce a novel system that delivers unobtrusive modulated electrotactile feedback directly to the fingertip edge. Our three-part study with twelve participants confirmed the effectiveness of this system, demonstrating that electrotactile feedback, when applied for 0.5 seconds with a 0.12-second interval, provides the most accurate position and direction recognition. Furthermore, our electrotactile device has proven valuable in assisting BLV participants in comprehending four commonly used charts: line charts, scatterplots, bar charts, and pie charts. We also delve into the implications of our findings on recognition enhancement, presentation modes, and function synergy.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {425},\nnumpages = {20},\nkeywords = {Accessibility, Accessible Data Visualization., Electrotactile, Haptic Data Visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642841,\nauthor = {Cheng, Chia-Yu and Chen, Yu and Handani, Sitaresmi Wahyu and Balabantaray, Avijit and Chen, Mike Y.},\ntitle = {Paired-EMS: Enhancing Electrical Muscle Stimulation (EMS)-based Force Feedback Experience by Stimulating Both Muscles in Antagonistic Pairs},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642841},\ndoi = {10.1145/3613904.3642841},\nabstract = {Electrical Muscle Stimulation (EMS) has emerged as a key wearable haptic feedback technology capable of simulating a wide range of force feedback, such as the impact force of boxing punches, the weight of virtual objects, and the reaction force from pushing on a wall. To simulate these external forces, EMS stimulates the muscles that oppose (i.e. antagonistic to) the actual muscles that users activate, causing involuntary muscle contraction and haptic sensations that differ from real-world experiences. In this work, we propose Paired-EMS which simultaneously stimulates both the muscles that users activate and that prior EMS stimulates (i.e. antagonistic muscle pairs) to enhance the external force feedback experience. We first conducted a small formative study (n=8) to help design the stimulation intensity of muscle pairs, then conducted a user experience study to evaluate Paired-EMS vs. prior EMS approaches for both isometric and isotonic user actions. Study results (n=32) showed that Paired-EMS significantly improved realism, harmony, and entertainment (p<.05) with similar comfort (p>.36), and was overall preferred by 78\\% of participants (p<.01).},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {426},\nnumpages = {7},\nkeywords = {Haptic, User Experience Design, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642873,\nauthor = {Lin, Hongnan and Liu, Xuanyou and Jiang, Shengsheng and Wang, Qi and Tao, Ye and Wang, Guanyun and Sun, Wei and Han, Teng and Tian, Feng},\ntitle = {TacTex: A Textile Interface with Seamlessly-Integrated Electrodes for High-Resolution Electrotactile Stimulation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642873},\ndoi = {10.1145/3613904.3642873},\nabstract = {This paper presents TacTex, a textile-based interface that provides high-resolution haptic feedback and touch-tracking capabilities. TacTex utilizes electrotactile stimulation, which has traditionally posed challenges due to limitations in textile electrode density and quantity. TacTex overcomes these challenges by employing a multi-layer woven structure that separates conductive weft and warp electrodes with non-conductive yarns. The driving system for TacTex includes a power supply, sensing board, and switch boards to enable spatial and temporal control of electrical stimuli on the textile, while simultaneously monitoring voltage changes. TacTex can stimulate a wide range of haptic effects, including static and dynamic patterns and different sensation qualities, with a resolution of 512 \\texttimes{} 512 and based on linear electrodes spaced as closely as 2mm. We evaluate the performance of the interface with user studies and demonstrate the potential applications of TacTex interfaces in everyday textiles for adding haptic feedback.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {427},\nnumpages = {16},\nkeywords = {e-textile, electrotactile, haptic feedback, touch sensing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642585,\nauthor = {Faltaous, Sarah and Williamson, Julie R. and Koelle, Marion and Pfeiffer, Max and Keppel, Jonas and Schneegass, Stefan},\ntitle = {Understanding User Acceptance of Electrical Muscle Stimulation in Human-Computer Interaction},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642585},\ndoi = {10.1145/3613904.3642585},\nabstract = {Electrical Muscle Stimulation (EMS) has unique capabilities that can manipulate users’ actions or perceptions, such as actuating user movement while walking, changing the perceived texture of food, and guiding movements for a user learning an instrument. These applications highlight the potential utility of EMS, but such benefits may be lost if users reject EMS. To investigate user acceptance of EMS, we conducted an online survey (N = 101). We compared eight scenarios, six from HCI research applications and two from the sports and health domain. To gain further insights, we conducted in-depth interviews with a subset of the survey respondents (N = 10). The results point to the challenges and potential of EMS regarding social and technological acceptance, showing that there is greater acceptance of applications that manipulate action than those that manipulate perception. The interviews revealed safety concerns and user expectations for the design and functionality of future EMS applications.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {428},\nnumpages = {16},\nkeywords = {Acceptability, Electrical Muscle Stimulation, Social Acceptability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642113,\nauthor = {Hamazaki, Takumi and Takami, Taiki and Ushiyama, Keigo and Mizoguchi, Izumi and Kajimoto, Hiroyuki},\ntitle = {ALCool: Utilizing Alcohol's Evaporative Cooling for Ubiquitous Cold Sensation Feedback},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642113},\ndoi = {10.1145/3613904.3642113},\nabstract = {Tactile technologies are important for novel user experiences. Among several tactile submodalities, cold sensation is essential for realistically portraying materials and environments. However, current cold presentations such as Peltier devices face challenges like low energy efficiency and the need for complicated equipment. To address these, we suggest leveraging alcohol’s endothermic property during evaporation. Our prototype, a wristwatch wearable with a fan, capitalizes on alcohol’s high volatility by absorbing ambient heat upon evaporation. The device further enhances the cooling effect by circulating air around the skin. This approach simplifies the setup required for cooling technologies and is more energy-efficient than Peltier-based systems. We also integrated perfume, which is a mixture of alcohol and scent substance, and presented a unique cooling and scent experience. The use of alcohol as a cooling method was not considered conventional, but social changes after COVID-19 made it easy to obtain a tiny amount of alcohol.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {429},\nnumpages = {14},\nkeywords = {alcohol, chemical haptics, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642536,\nauthor = {Ma, Yuxin and Xie, Tianze and Zhang, Peng and Kim, Hwan and Je, Seungwoo},\ntitle = {AirPush: A Pneumatic Wearable Haptic Device Providing Multi-Dimensional Force Feedback on a Fingertip},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642536},\ndoi = {10.1145/3613904.3642536},\nabstract = {Finger wearable haptic devices enrich virtual reality experiences by offering haptic feedback corresponding to the virtual environment. However, despite the effectiveness of current finger wearable haptic devices in delivering haptic feedback, many are often constrained in their ability to provide force feedback across a diverse range of directions or to sustain it. Therefore, we present AirPush, a finger wearable haptic device capable of generating continuously adjustable force feedback in multiple directions using compressed air. To evaluate its usability, we conducted a technical evaluation and four user studies: (1) we obtained the user’s perceptual thresholds of angles under different directions on horizontal and vertical planes, (2) in perception studies, we found that users can identify five different magnitudes of force and eight different motion when using AirPush, and (3) using it in VR applications, we confirmed that users felt more realistic and immersed when using AirPush than the HTC VIVE Controller or AirPush with a fixed nozzle.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {430},\nnumpages = {13},\nkeywords = {Compressed Air, Finger Wearable Haptic Device, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642890,\nauthor = {Tsutsui, Ayaka and Fushimi, Tatsuki and Murakami, Takahito and Kojima, Ryosei and Tanaka, Kengo and Ochiai, Yoichi},\ntitle = {HIFU Embossment of Acrylic Sheets},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642890},\ndoi = {10.1145/3613904.3642890},\nabstract = {Tactile interfaces such as embossment facilitate information transfer through touch in Human-Computer Interaction (HCI). Traditional embossing methods, while enabling the creation of intricate patterns, face limitations due to mold reliance and material thickness restrictions, hindering bespoke embossment creation. In this study, we propose High-Intensity Focused Ultrasound (HIFU) as an alternative technique to produce tailored embossed designs on acrylic without the need for traditional molds. We uncover specific HIFU parameters, such as amplitude, irradiation time, and distance that directly impact essential qualities of embossment including embossment height, transparency, and line generation. Additionally, the capability of embossing without the use of molds expands the applications for quick prototyping and customization of embossed designs within HCI. Furthermore, we introduce a user interface developed to streamline the design and application of customizable tactile graphics using HIFU, aimed at non-expert users. Preliminary user studies reveal positive feedback on the interface’s intuitiveness and the quality of the HIFU embossment. Our study indicates that HIFU embossment presents a viable approach for creating embossed features in interactive systems, with the potential to offer methods for personal customization in the design of tactile materials.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {431},\nnumpages = {23},\nkeywords = {Digital Fabrication, Fablication, HIFU, Manufacturing, Productivity, Ultrasonic, User interface, User interface design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641907,\nauthor = {Jingu, Arata and Sabnis, Nihar and Strohmeier, Paul and Steimle, J\\\"{u}rgen},\ntitle = {Shaping Compliance: Inducing Haptic Illusion of Compliance in Different Shapes with Electrotactile Grains},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641907},\ndoi = {10.1145/3613904.3641907},\nabstract = {Compliance, the degree of displacement under applied force, is pivotal in determining the material perception when touching an object. Vibrotactile actuators can be used for creating grain-based virtual compliance, but they have poor spatial resolution and a limiting rigid form factor. We propose a novel electrotactile compliance illusion that renders grains of electrical pulses on an electrode array in response to finger force changes. We demonstrate its ability to render compliance in distinct shapes through a thin, lightweight, and flexible finger-worn interface. Detailed technical parameters and the implementation of our device are provided. A controlled experiment confirms the technique can (1) create virtual compliance; (2) adjust the compliance magnitude with grain and electrode parameters; and (3) render compliance with specific shapes. In three example applications, we present how this illusion can enhance physical objects, elements in graphical user interfaces, and virtual reality experiences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {432},\nnumpages = {13},\nkeywords = {Haptics, compliance, electrotactile, haptic illusion, virtual reality.},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642299,\nauthor = {Mazursky, Alex and Serfaty, Jacob and Lopes, Pedro},\ntitle = {Stick&Slip: Altering Fingerpad Friction via Liquid Coatings},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642299},\ndoi = {10.1145/3613904.3642299},\nabstract = {We present Stick&Slip, a novel approach that alters friction between the fingerpad \\& surfaces by depositing liquid droplets that coat the fingerpad. The liquid coating modifies the finger's coefficient of friction, allowing users to feel surfaces up to ±60\\% more slippery or sticky. We selected our fluids to rapidly evaporate so that the surface returns to its original friction. Unlike traditional friction-feedback, such as electroadhesion or vibration, our approach: (1) alters friction on a wide range of surfaces and geometries, making it possible to modulate nearly any non-absorbent surface; (2) scales to many objects without requiring instrumenting the target surfaces (e.g., with conductive electrode coatings or vibromotors); and (3) both in/decreases friction via a single device. We identified nine liquids and characterized their practicality by measuring evaporation rates, etc. To illustrate the applicability of our approach, we demonstrate how it enables friction in virtual/mixed-reality or, even, while using everyday objects/tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {433},\nnumpages = {14},\nkeywords = {adhesion, friction, grasp, haptics, mixed reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641983,\nauthor = {Semertzidis, Nathan and Vranic-Peters, Michaela Jayne and Fang, Xiao Zoe and Patibanda, Rakesh and Saini, Aryan and Elvitigala, Don Samitha and Zambetta, Fabio and Mueller, Florian ‘Floyd’},\ntitle = {PsiNet: Toward Understanding the Design of Brain-to-Brain Interfaces for Augmenting Inter-Brain Synchrony},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641983},\ndoi = {10.1145/3613904.3641983},\nabstract = {Underlying humanity’s social abilities is the brain’s capacity to interpersonally synchronize. Experimental, lab-based neuropsychological studies have demonstrated that inter-brain synchrony can be technologically mediated. However, knowledge in deploying these technologies in-the-wild and studying their user experience, an area HCI excels in, is lacking. With advances in mobile brain sensing and stimulation, we identify an opportunity for HCI to investigate the in-the-wild augmentation of inter-brain synchrony. We designed “PsiNet,” the first wearable brain-to-brain system aimed at augmenting inter-brain synchrony in-the-wild. Participant interviews illustrated three themes that describe the user experience of modulated inter-brain synchrony: hyper-awareness; relational interaction; and the dissolution of self. We contribute these three themes to assist HCI theorists’ discussions of inter-brain synchrony experiences. We also present three practical design tactics for HCI practitioners designing inter-brain synchrony, and hope that our work guides a HCI future of brain-to-brain experiences which fosters human connection.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {434},\nnumpages = {18},\nkeywords = {Brain-to-Brain Interface, EEG, Inter-Brain Synchrony, Neural Synchrony, tES},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642793,\nauthor = {Hwang, Angel Hsing-Chi and Adler, Dan and Friedenberg, Meir and Yang, Qian},\ntitle = {Societal-Scale Human-AI Interaction Design? How Hospitals and Companies are Integrating Pervasive Sensing into Mental Healthcare},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642793},\ndoi = {10.1145/3613904.3642793},\nabstract = {From wearable health tracking to sensor-laden cities, AI-enhanced pervasive sensing platforms promise far-reaching benefits yet also introduce societal risks. How might designers of these platforms effectively navigate their complex ecology and sociotechnical dynamics? To explore this question, we interviewed designers building mental health technologies who undertook this challenge. They are hospital chief medical information officers and startup founders together striving to create new sensors/AI platforms and integrate them into the healthcare ecosystem. We found that, while all designers aspired to build comprehensive care platforms, their efforts focused on serving either consumers or physicians, delivering a subset of healthcare interventions, and demonstrating system effectiveness one metric at a time. Consequently, breakdowns in patient journeys are emerging; societal risks loom large. We describe how the data economy, designers’ mindsets, and evaluation challenges led to these unintended design consequences. We discuss implications for designing pervasive sensing and AI platforms for social good.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {435},\nnumpages = {16},\nkeywords = {Socio-technical systems, healthcare ecosystem., mental health},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641965,\nauthor = {Calle, Paul and Shao, Ruosi and Liu, Yunlong and H\\'{e}bert, Emily T and Kendzor, Darla and Neil, Jordan and Businelle, Michael and Pan, Chongle},\ntitle = {Towards AI-Driven Healthcare: Systematic Optimization, Linguistic Analysis, and Clinicians’ Evaluation of Large Language Models for Smoking Cessation Interventions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641965},\ndoi = {10.1145/3613904.3641965},\nabstract = {Creating intervention messages for smoking cessation is a labor-intensive process. Advances in Large Language Models (LLMs) offer a promising alternative for automated message generation. Two critical questions remain: 1) How to optimize LLMs to mimic human expert writing, and 2) Do LLM-generated messages meet clinical standards? We systematically examined the message generation and evaluation processes through three studies investigating prompt engineering (Study 1), decoding optimization (Study 2), and expert review (Study 3). We employed computational linguistic analysis in LLM assessment and established a comprehensive evaluation framework, incorporating automated metrics, linguistic attributes, and expert evaluations. Certified tobacco treatment specialists assessed the quality, accuracy, credibility, and persuasiveness of LLM-generated messages, using expert-written messages as the benchmark. Results indicate that larger LLMs, including ChatGPT, OPT-13B, and OPT-30B, can effectively emulate expert writing to generate well-written, accurate, and persuasive messages, thereby demonstrating the capability of LLMs in augmenting clinical practices of smoking cessation interventions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {436},\nnumpages = {16},\nkeywords = {Computational Linguistic Analysis, Expert Review, Large Language Model, Message Generation, Smoking Cessation Intervention},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642353,\nauthor = {Hao, Yuexing and Liu, Zeyu and Riter, Robert N. and Kalantari, Saleh},\ntitle = {Advancing Patient-Centered Shared Decision-Making with AI Systems for Older Adult Cancer Patients},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642353},\ndoi = {10.1145/3613904.3642353},\nabstract = {Shared decision making (SDM) plays a vital role in clinical practice guidelines, fostering enduring therapeutic communication and patient-clinician relationships. Previous research indicates that active patient participation in decision-making improves satisfaction and treatment outcomes. However, medical decision-making can be intricate and multifaceted. To help make SDM more accessible, we designed a patient-centered Artificial Intelligence (AI) SDM system for older adult cancer patients who lack high health literacy to become more involved in the clinical decision-making process and to improve comprehension toward treatment outcomes. We conducted a pilot feasibility study through 12 preliminary interviews followed by 25 usability testing interviews after the system development, with older adult cancer survivors and clinicians. Results indicated promise in the AI system’s ability to enhance SDM, providing personalized healthcare experiences and education for cancer patients. Clinician responses also provided useful suggestions for SDM’s new design and research opportunities in mitigating medical errors and improving clinical efficiency.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {437},\nnumpages = {20},\nkeywords = {Cancer Care, Clinical Decision Making, Older Adults, Risk Communication, Shared Decision Making},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641913,\nauthor = {Li, Brenna and Gross, Ofek and Crampton, Noah and Kapoor, Mamta and Tauseef, Saba and Jain, Mohit and Truong, Khai N. and Mariakakis, Alex},\ntitle = {Beyond the Waiting Room: Patient's Perspectives on the Conversational Nuances of Pre-Consultation Chatbots},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641913},\ndoi = {10.1145/3613904.3641913},\nabstract = {Pre-consultation serves as a critical information exchange between healthcare providers and patients, streamlining visits and supporting patient-centered care. Human-led pre-consultations offer many benefits, yet they require significant time and energy from clinical staff. In this work, we identify design goals for pre-consultation chatbots given their potential to carry out human-like conversations and autonomously adapt their line of questioning. We conducted a study with 33 walk-in clinic patients to elicit design considerations for pre-consultation chatbots. Participants were exposed to one of two study conditions: an LLM-powered AI agent and a Wizard-of-Oz agent simulated by medical professionals. Our study found that both conditions were equally well-received and demonstrated comparable conversational capabilities. However, the extent of the follow-up questions and the amount of empathy impacted the chatbot’s perceived thoroughness and sincerity. Patients also highlighted the importance of setting expectations for the chatbot before and after the pre-consultation experience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {438},\nnumpages = {24},\nkeywords = {LLMs, chatbots, information gathering, patient intake, primary care},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642883,\nauthor = {Kim, Dajung and Vegt, Niko and Visch, Valentijn and Bos-De Vos, Marina},\ntitle = {How Much Decision Power Should (A)I Have?: Investigating Patients’ Preferences Towards AI Autonomy in Healthcare Decision Making},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642883},\ndoi = {10.1145/3613904.3642883},\nabstract = {Despite the growing potential of artificial intelligence (AI) in improving clinical decision making, patients' perspectives on the use of AI for their care decision making are underexplored. In this paper, we investigate patients’ preferences towards the autonomy of AI in assisting healthcare decision making. We conducted interviews and an online survey using an interactive narrative and speculative AI prototypes to elicit participants’ preferred choices of using AI in a pregnancy care context. The analysis of the interviews and in-story responses reveals that patients’ preferences for AI autonomy vary per person and context, and may change over time. This finding suggests the need for involving patients in defining and reassessing the appropriate level of AI assistance for healthcare decision making. Departing from these varied preferences for AI autonomy, we discuss implications for incorporating patient-centeredness in designing AI-powered healthcare decision making.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {439},\nnumpages = {17},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642420,\nauthor = {Jo, Eunkyung and Jeong, Yuin and Park, Sohyun and Epstein, Daniel A. and Kim, Young-Ho},\ntitle = {Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642420},\ndoi = {10.1145/3613904.3642420},\nabstract = {Recent large language models (LLMs) offer the potential to support public health monitoring by facilitating health disclosure through open-ended conversations but rarely preserve the knowledge gained about individuals across repeated interactions. Augmenting LLMs with long-term memory (LTM) presents an opportunity to improve engagement and self-disclosure, but we lack an understanding of how LTM impacts people’s interaction with LLM-driven chatbots in public health interventions. We examine the case of CareCall—an LLM-driven voice chatbot with LTM—through the analysis of 1,252 call logs and interviews with nine users. We found that LTM enhanced health disclosure and fostered positive perceptions of the chatbot by offering familiarity. However, we also observed challenges in promoting self-disclosure through LTM, particularly around addressing chronic health conditions and privacy concerns. We discuss considerations for LTM integration in LLM-driven chatbots for public health monitoring, including carefully deciding what topics need to be remembered in light of public health goals.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {440},\nnumpages = {21},\nkeywords = {Chatbot, Check-up calls, Large language models, Long-term memory, Open-domain dialog systems, Public health, Social isolation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642551,\nauthor = {Corti, Lorenzo and Oltmans, Rembrandt and Jung, Jiwon and Balayn, Agathe and Wijsenbeek, Marlies and Yang, Jie},\ntitle = {``It Is a Moving Process\": Understanding the Evolution of Explainability Needs of Clinicians in Pulmonary Medicine},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642551},\ndoi = {10.1145/3613904.3642551},\nabstract = {Clinicians increasingly pay attention to Artificial Intelligence (AI) to improve the quality and timeliness of their services. There are converging opinions on the need for Explainable AI (XAI) in healthcare. However, prior work considers explanations as stationary entities with no account for the temporal dynamics of patient care. In this work, we involve 16 Idiopathic Pulmonary Fibrosis (IPF) clinicians from a European university medical centre and investigate their evolving uses and purposes for explainability throughout patient care. By applying a patient journey map for IPF, we elucidate clinicians’ informational needs, how human agency and patient-specific conditions can influence the interaction with XAI systems, and the content, delivery, and relevance of explanations over time. We discuss implications for integrating XAI in clinical contexts and more broadly how explainability is defined and evaluated. Furthermore, we reflect on the role of medical education in addressing epistemic challenges related to AI literacy.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {441},\nnumpages = {21},\nkeywords = {Explainable AI, Healthcare, User Needs},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642024,\nauthor = {Rajashekar, Niroop Channa and Shin, Yeo Eun and Pu, Yuan and Chung, Sunny and You, Kisung and Giuffre, Mauro and Chan, Colleen E and Saarinen, Theo and Hsiao, Allen and Sekhon, Jasjeet and Wong, Ambrose H and Evans, Leigh V and Kizilcec, Rene F. and Laine, Loren and Mccall, Terika and Shung, Dennis},\ntitle = {Human-Algorithmic Interaction Using a Large Language Model-Augmented Artificial Intelligence Clinical Decision Support System},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642024},\ndoi = {10.1145/3613904.3642024},\nabstract = {Integration of artificial intelligence (AI) into clinical decision support systems (CDSS) poses a socio-technological challenge that is impacted by usability, trust, and human-computer interaction (HCI). AI-CDSS interventions have shown limited benefit in clinical outcomes, which may be due to insufficient understanding of how health-care providers interact with AI systems. Large language models (LLMs) have the potential to enhance AI-CDSS, but haven’t been studied in either simulated or real-world clinical scenarios. We present findings from a randomized controlled trial deploying AI-CDSS for the management of upper gastrointestinal bleeding (UGIB) with and without an LLM interface within realistic clinical simulations for physician and medical student participants. We find evidence that LLM augmentation improves ease-of-use, that LLM-generated responses with citations improve trust, and HCI varies based on clinical expertise. Qualitative themes from interviews suggest the perception of LLM-augmented AI-CDSS as a team-member used to confirm initial clinical intuitions and help evaluate borderline decisions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {442},\nnumpages = {20},\nkeywords = {Artificial Intelligence, Clinical Decision Support Systems, Electronic Health Record, Health-Clinical, Machine Learning, Medical: Nursing Homes/Hospitals, Qualitative Methods, Quantitative Methods, Workflows},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642117,\nauthor = {Ding, Xiaohan and Carik, Buse and Gunturi, Uma Sushmitha and Reyna, Valerie and Rho, Eugenia Ha Rim},\ntitle = {Leveraging Prompt-Based Large Language Models: Predicting Pandemic Health Decisions and Outcomes Through Social Media Language},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642117},\ndoi = {10.1145/3613904.3642117},\nabstract = {We introduce a multi-step reasoning framework using prompt-based LLMs to examine the relationship between social media language patterns and trends in national health outcomes. Grounded in fuzzy-trace theory, which emphasizes the importance of “gists” of causal coherence in effective health communication, we introduce Role-Based Incremental Coaching (RBIC), a prompt-based LLM framework, to identify gists at-scale. Using RBIC, we systematically extract gists from subreddit discussions opposing COVID-19 health measures (Study 1). We then track how these gists evolve across key events (Study 2) and assess their influence on online engagement (Study 3). Finally, we investigate how the volume of gists is associated with national health trends like vaccine uptake and hospitalizations (Study 4). Our work is the first to empirically link social media linguistic patterns to real-world public health trends, highlighting the potential of prompt-based LLMs in identifying critical online discussion patterns that can form the basis of public health communication strategies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {443},\nnumpages = {20},\nkeywords = {covid-19, health decisions, health outcomes, large language models, online communities, pandemic, prompting, social media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642013,\nauthor = {Yildirim, Nur and Richardson, Hannah and Wetscherek, Maria Teodora and Bajwa, Junaid and Jacob, Joseph and Pinnock, Mark Ames and Harris, Stephen and Coelho De Castro, Daniel and Bannur, Shruthi and Hyland, Stephanie and Ghosh, Pratik and Ranjit, Mercy and Bouzid, Kenza and Schwaighofer, Anton and P\\'{e}rez-Garc\\'{\\i}a, Fernando and Sharma, Harshita and Oktay, Ozan and Lungren, Matthew and Alvarez-Valle, Javier and Nori, Aditya and Thieme, Anja},\ntitle = {Multimodal Healthcare AI: Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642013},\ndoi = {10.1145/3613904.3642013},\nabstract = {Recent advances in AI combine large language models (LLMs) with vision encoders that bring forward unprecedented technical capabilities to leverage for a wide range of healthcare applications. Focusing on the domain of radiology, vision-language models (VLMs) achieve good performance results for tasks such as generating radiology findings based on a patient’s medical image, or answering visual questions (e.g., “Where are the nodules in this chest X-ray?”). However, the clinical utility of potential applications of these capabilities is currently underexplored. We engaged in an iterative, multidisciplinary design process to envision clinically relevant VLM interactions, and co-designed four VLM use concepts: Draft Report Generation, Augmented Report Review, Visual Search and Querying, and Patient Imaging History Highlights. We studied these concepts with 13 radiologists and clinicians who assessed the VLM concepts as valuable, yet articulated many design considerations. Reflecting on our findings, we discuss implications for integrating VLM capabilities in radiology, and for healthcare AI more generally.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {444},\nnumpages = {22},\nkeywords = {Human-centered AI, IxD, healthcare, human-AI interaction, medical imaging, radiology, responsible AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642343,\nauthor = {Zhang, Shao and Yu, Jianing and Xu, Xuhai and Yin, Changchang and Lu, Yuxuan and Yao, Bingsheng and Tory, Melanie and Padilla, Lace M. and Caterino, Jeffrey and Zhang, Ping and Wang, Dakuo},\ntitle = {Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642343},\ndoi = {10.1145/3613904.3642343},\nabstract = {Today’s AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module in their electrical health record (EHR) system. We argue that a human-centered AI system needs to support human experts in the intermediate stages of a medical decision-making process (e.g., generating hypotheses or gathering data), instead of focusing only on the final decision. Therefore, we build SepsisLab based on a state-of-the-art AI algorithm and extend it to predict the future projection of sepsis development, visualize the prediction uncertainty, and propose actionable suggestions (i.e., which additional laboratory tests can be collected) to reduce such uncertainty. Through heuristic evaluation with six clinicians using our prototype system, we demonstrate that SepsisLab enables a promising human-AI collaboration paradigm for the future of AI-assisted sepsis diagnosis and other high-stakes medical decision making.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {445},\nnumpages = {18},\nkeywords = {Human-AI collaboration, Medical decision making, Sepsis diagnosis},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642336,\nauthor = {Cuadra, Andrea and Wang, Maria and Stein, Lynn Andrea and Jung, Malte F. and Dell, Nicola and Estrin, Deborah and Landay, James A.},\ntitle = {The Illusion of Empathy? Notes on Displays of Emotion in Human-Computer Interaction},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642336},\ndoi = {10.1145/3613904.3642336},\nabstract = {From ELIZA to Alexa, Conversational Agents (CAs) have been deliberately designed to elicit or project empathy. Although empathy can help technology better serve human needs, it can also be deceptive and potentially exploitative. In this work, we characterize empathy in interactions with CAs, highlighting the importance of distinguishing evocations of empathy between two humans from ones between a human and a CA. To this end, we systematically prompt CAs backed by large language models (LLMs) to display empathy while conversing with, or about, 65 distinct human identities, and also compare how different LLMs display or model empathy. We find that CAs make value judgments about certain identities, and can be encouraging of identities related to harmful ideologies (e.g., Nazism and xenophobia). Moreover, a computational approach to understanding empathy reveals that despite their ability to display empathy, CAs do poorly when interpreting and exploring a user’s experience, contrasting with their human counterparts.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {446},\nnumpages = {18},\nkeywords = {AI, Affective Computing, Automation, Autonomous Agents, Chatbots, Conversational Agents, Conversational User Interfaces, Disability, Emotion, Empathy, Ethics, Gender, Health, Human-AI Interaction, Human-Computer Interaction, Identity, LLMs, Marginalization, Mental Health, Natural Language Processing, Personalization, Power and Privilege, Religion, Social Robots, Technological Harm, Ubiquitous Computing, User Experience Design, Values in Design, Voice Assistants, Wellbeing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642636,\nauthor = {Yilma, Bereket A. and Kim, Chan Mi and Cupchik, Gerald C. and Leiva, Luis A.},\ntitle = {Artful Path to Healing: Using Machine Learning for Visual Art Recommendation to Prevent and Reduce Post-Intensive Care Syndrome (PICS)},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642636},\ndoi = {10.1145/3613904.3642636},\nabstract = {Staying in the intensive care unit (ICU) is often traumatic, leading to post-intensive care syndrome (PICS), which encompasses physical, psychological, and cognitive impairments. Currently, there are limited interventions available for PICS. Studies indicate that exposure to visual art may help address the psychological aspects of PICS and be more effective if it is personalized. We develop Machine Learning-based Visual Art Recommendation Systems (VA RecSys) to enable personalized therapeutic visual art experiences for post-ICU patients. We investigate four state-of-the-art VA RecSys engines, evaluating the relevance of their recommendations for therapeutic purposes compared to expert-curated recommendations. We conduct an expert pilot test and a large-scale user study (n=150) to assess the appropriateness and effectiveness of these recommendations. Our results suggest all recommendations enhance temporal affective states. Visual and multimodal VA RecSys engines compare favourably with expert-curated recommendations, indicating their potential to support the delivery of personalized art therapy for PICS prevention and treatment.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {447},\nnumpages = {19},\nkeywords = {Artwork, Health, Machine Learning, Personalization, Recommendation, User Experience, intensive care unit, rehabilitation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641998,\nauthor = {Bedmutha, Manas Satish and Tsedenbal, Anuujin and Tobar, Kelly and Borsotto, Sarah and Sladek, Kimberly R and Singh, Deepansha and Casanova-Perez, Reggie and Bascom, Emily and Wood, Brian and Sabin, Janice and Pratt, Wanda and Hartzler, Andrea and Weibel, Nadir},\ntitle = {ConverSense: An Automated Approach to Assess Patient-Provider Interactions using Social Signals},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641998},\ndoi = {10.1145/3613904.3641998},\nabstract = {Patient-provider communication influences patient health outcomes, and analyzing such communication could help providers identify opportunities for improvement, leading to better care. Interpersonal communication can be assessed through “social-signals” expressed in non-verbal, vocal behaviors like interruptions, turn-taking, and pitch. To automate this assessment, we introduce a machine-learning pipeline that ingests audio-streams of conversations and tracks the magnitude of four social-signals: dominance, interactivity, engagement, and warmth. This pipeline is embedded into ConverSense, a web-application for providers to visualize their communication patterns, both within and across visits. Our user study with 5 clinicians and 10 patient visits demonstrates ConverSense’s potential to provide feedback on communication challenges, as well as the need for this feedback to be contextualized within the specific underlying visit and patient interaction. Through this novel approach that uses data-driven self-reflection, ConverSense can help providers improve their communication with patients to deliver improved quality of care.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {448},\nnumpages = {22},\nkeywords = {healthcare, interactions, patient-provider communication, social signals},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642573,\nauthor = {Kambhamettu, Hita and Metaxa, Dana\\\"{e} and Johnson, Kevin and Head, Andrew},\ntitle = {Explainable Notes: Examining How to Unlock Meaning in Medical Notes with Interactivity and Artificial Intelligence},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642573},\ndoi = {10.1145/3613904.3642573},\nabstract = {Medical progress notes have recently become available to patients at an unprecedented scale. Progress notes offer patients insight into their care that they cannot find elsewhere. That said, reading a note requires patients to contend with the language, unspoken assumptions, and clutter common to clinical documentation. As the health system reinvents many of its interfaces to incorporate AI assistance, this paper examines what intelligent interfaces could do to help patients read their progress notes. In a qualitative study, we examine the needs of patients as they read a progress note. We then formulate a vision for the explainable note, an augmented progress note that provides support for directing attention, phrase-level understanding, and tracing lines of reasoning. This vision manifests in a set of patient-inspired opportunities for advancing intelligent interfaces for writing and reading progress notes.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {449},\nnumpages = {19},\nkeywords = {attention, augmented medical texts, intelligent reading and writing, lines of reasoning, patient-provider communication, phrase-level understanding, progress notes},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642450,\nauthor = {Zulfikar, Wazeer Deen and Chan, Samantha and Maes, Pattie},\ntitle = {Memoro: Using Large Language Models to Realize a Concise Interface for Real-Time Memory Augmentation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642450},\ndoi = {10.1145/3613904.3642450},\nabstract = {People have to remember an ever-expanding volume of information. Wearables that use information capture and retrieval for memory augmentation can help but can be disruptive and cumbersome in real-world tasks, such as in social settings. To address this, we developed Memoro, a wearable audio-based memory assistant with a concise user interface. Memoro uses a large language model (LLM) to infer the user’s memory needs in a conversational context, semantically search memories, and present minimal suggestions. The assistant has two interaction modes: Query Mode for voicing queries and Queryless Mode for on-demand predictive assistance, without explicit query. Our study of (N=20) participants engaged in a real-time conversation, demonstrated that using Memoro reduced device interaction time and increased recall confidence while preserving conversational quality. We report quantitative results and discuss the preferences and experiences of users. This work contributes towards utilizing LLMs to design wearable memory augmentation systems that are minimally disruptive.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {450},\nnumpages = {18},\nkeywords = {context-aware agent, large language models, memory assistant, minimal interfaces, voice interfaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641896,\nauthor = {Yildirim, Nur and Zlotnikov, Susanna and Sayar, Deniz and Kahn, Jeremy M. and Bukowski, Leigh A and Amin, Sher Shah and Riman, Kathryn A. and Davis, Billie S. and Minturn, John S. and King, Andrew J. and Ricketts, Dan and Tang, Lu and Sivaraman, Venkatesh and Perer, Adam and Preum, Sarah M. and McCann, James and Zimmerman, John},\ntitle = {Sketching AI Concepts with Capabilities and Examples: AI Innovation in the Intensive Care Unit},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641896},\ndoi = {10.1145/3613904.3641896},\nabstract = {Advances in artificial intelligence (AI) have enabled unprecedented capabilities, yet innovation teams struggle when envisioning AI concepts. Data science teams think of innovations users do not want, while domain experts think of innovations that cannot be built. A lack of effective ideation seems to be a breakdown point. How might multidisciplinary teams identify buildable and desirable use cases? This paper presents a first hand account of ideating AI concepts to improve critical care medicine. As a team of data scientists, clinicians, and HCI researchers, we conducted a series of design workshops to explore more effective approaches to AI concept ideation and problem formulation. We detail our process, the challenges we encountered, and practices and artifacts that proved effective. We discuss the research implications for improved collaboration and stakeholder engagement, and discuss the role HCI might play in reducing the high failure rate experienced in AI innovation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {451},\nnumpages = {18},\nkeywords = {Brainstorming, healthcare, human-centered AI, ideation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642756,\nauthor = {Bascom, Emily and Casanova-Perez, Reggie and Tobar, Kelly and Bedmutha, Manas Satish and Ramaswamy, Harshini and Pratt, Wanda and Sabin, Janice and Wood, Brian and Weibel, Nadir and Hartzler, Andrea},\ntitle = {Designing Communication Feedback Systems To Reduce Healthcare Providers’ Implicit Biases In Patient Encounters},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642756},\ndoi = {10.1145/3613904.3642756},\nabstract = {Healthcare providers’ implicit bias, based on patients’ physical characteristics and perceived identities, negatively impacts healthcare access, care quality, and outcomes. Feedback tools are needed to help providers identify and learn from their biases. To incorporate providers’ perspectives on the most effective ways to present such feedback, we conducted semi-structured design critique sessions with 24 primary care providers. We found that providers seek feedback designed with transparent metrics indicating the quality of their communication with a patient and trends in communication patterns across visits. Based on these metrics and trends, providers want this feedback presented in a dashboard paired with actionable, personalized tips about how to improve their communication behaviors. Our study provides new insights for interactive systems to help mitigate the impact of implicit biases in patient-provider communication. New systems that build upon these insights could support providers in making healthcare more equitable, particularly for patients from marginalized communities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {452},\nnumpages = {12},\nkeywords = {Communication Feedback, Healthcare, Healthcare Providers, Imlicit Bias},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642214,\nauthor = {Foong, Pin Sym and Ureyang, Natasha and Foo, Charisse and Antonyrex, Sajeban and Koh, Gerald CH},\ntitle = {Designing for Caregiver-facing Values Elicitation Tools},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642214},\ndoi = {10.1145/3613904.3642214},\nabstract = {In serious illness contexts, caregivers tasked with making decisions for incapacitated patients face unique challenges. Current tools for values elicitation, primarily patient-oriented, may not fully meet caregiver needs. This study, involving 12 caregivers, adopted a research-through-design approach to evaluate caregiver-facing values elicitation tools. The findings reveal caregivers’ need for more support in understanding their own values, discerning patient values, and reconciling these perspectives. While caregivers seek tools for consensus building within the family, they prefer using them individually rather than in a shared manner. The study also highlights a general unawareness of the importance of values in decision-making. This lack of understanding points to the need for tools that educate as well as aid in decision-making. Finally, tool designers must balance top-down directives with bottom-up values in their designs. This research informs the development of more effective caregiver-facing values elicitation tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {453},\nnumpages = {20},\nkeywords = {advance care planning, caregivers, end-of-life, reflection, values, values elicitation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642201,\nauthor = {Dawson, Joshua and Fisher, Eden and Wiese, Jason},\ntitle = {Hospital Employee Experiences Caring for Patients in Smart Patient Rooms},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642201},\ndoi = {10.1145/3613904.3642201},\nabstract = {Smart hospital patient rooms integrate smart devices for digital control of both entertainment (e.g., television and sound system) and the environment (e.g., lights, blinds, and temperature). While primarily designed to enhance the patient experience, this technology also impacts the hospital employees who work in these patient rooms. This study explores hospital employee experiences with smart patient rooms. We conducted 23 interviews with rehabilitation healthcare professionals, including nurses, doctors, psychologists, and occupational, physical, and speech therapists, to understand their perspectives on working in smart patient rooms. Drawn from thematic analysis of the interviews, our findings offer insights into employees’ current use of the technology, the benefits and drawbacks they encounter, and their suggestions for improving the technology. These findings shed light on the complex problem of building smart patient rooms that simultaneously support the needs of multiple stakeholders, including patients and employees; they also point to important considerations for future designs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {454},\nnumpages = {16},\nkeywords = {accessibility, rehabilitation hospital, smart healthcare, smart home technology, smart hospital, smart patient room},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641982,\nauthor = {Yildirim, Nur and Zlotnikov, Susanna and Venkat, Aradhana and Chawla, Gursimran and Kim, Jennifer and Bukowski, Leigh A. and Kahn, Jeremy M. and Mccann, James and Zimmerman, John},\ntitle = {Investigating Why Clinicians Deviate from Standards of Care: Liberating Patients from Mechanical Ventilation in the ICU},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641982},\ndoi = {10.1145/3613904.3641982},\nabstract = {Clinical practice guidelines, care pathways, and protocols are designed to support evidence-based practices for clinicians; however, their adoption remains a challenge. We set out to investigate why clinicians deviate from the “Wake Up and Breathe” protocol, an evidence-based guideline for liberating patients from mechanical ventilation in the intensive care unit (ICU). We conducted over 40 hours of direct observations of live clinical workflows, 17 interviews with frontline care providers, and 4 co-design workshops at three different medical intensive care units. Our findings indicate that unlike prior literature suggests, disagreement with the protocol is not a substantial barrier to adoption. Instead, the uncertainty surrounding the application of the protocol for individual patients leads clinicians to deprioritize adoption in favor of tasks where they have high certainty. Reflecting on these insights, we identify opportunities for technical systems to help clinicians in effectively executing the protocol and discuss future directions for HCI research to support the integration of protocols into clinical practice in complex, team-based healthcare settings.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {455},\nnumpages = {15},\nkeywords = {adherence, field study, healthcare, standards of care, ventilation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642035,\nauthor = {Schmidmaier, Matthias and Rupp, Jonathan and Cvetanova, Darina and Mayer, Sven},\ntitle = {Perceived Empathy of Technology Scale (PETS): Measuring Empathy of Systems Toward the User},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642035},\ndoi = {10.1145/3613904.3642035},\nabstract = {Affective computing improves rapidly, allowing systems to process human emotions. This enables systems such as conversational agents or social robots to show empathy toward users. While there are various established methods to measure the empathy of humans, there is no reliable and validated instrument to quantify the perceived empathy of interactive systems. Thus, we developed the Perceived Empathy of Technology Scale (PETS) to assess and compare how empathic users perceive technology. We followed a standardized multi-phase process of developing and validating scales. In total, we invited 30 experts for item generation, 324 participants for item selection, and 396 additional participants for scale validation. We developed our scale using 22 scenarios with opposing empathy levels, ensuring the scale is universally applicable. This resulted in the PETS, a 10-item, 2-factor scale. The PETS allows designers and researchers to evaluate and compare the perceived empathy of interactive systems rapidly.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {456},\nnumpages = {18},\nkeywords = {empathy, human-computer interaction, scale, technology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642453,\nauthor = {Yarmand, Matin and Chen, Chen and Cheng, Kexin and Murphy, James and Weibel, Nadir},\ntitle = {\"I'd be watching him contour till 10 o'clock at night\": Understanding Tensions between Teaching Methods and Learning Needs in Healthcare Apprenticeship},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642453},\ndoi = {10.1145/3613904.3642453},\nabstract = {Apprenticeship is the predominant method for transferring specialized medical skills, yet the inter-dynamics between faculty and residents, including methods of feedback exchange are under-explored. We specifically investigate contouring: outlining tumors in preparation for radiotherapy, a critical skill that when performed subpar, severely degrades patient survival. Interviews and design-thinking workshops (N = four faculty; six residents) revealed misalignment between teaching methods and residents who desired timely, relevant, and diverse feedback. We further discuss reasons: overlapping learning content and strategies to ease tensions between clinical and teaching duties, and lack of support for exchange of cognitive processes. The follow-up survey study (N = 67 practitioners from 31 countries), which contained annotation and sketching tasks, provided diverse perspective over effective feedback elements. We lastly present sociotechnical implications in supporting faculty’s teaching duties and learners’ cognitive models, such as systematically leveraging senior learners in providing case-based guidance and supporting double-sided flow of cognitive information via in-situ video snippets.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {457},\nnumpages = {19},\nkeywords = {Cognitive Apprenticeship, Contouring, Healthcare Training},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642100,\nauthor = {Liu, Chang and Tan, Felicia Fang-Yi and Zhao, Shengdong and Kanneganti, Abhiram and Tushar, Gosavi Arundhati and Khoo, Eng Tat},\ntitle = {Facilitating Virtual Reality Integration in Medical Education: A Case Study of Acceptability and Learning Impact in Childbirth Delivery Training},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642100},\ndoi = {10.1145/3613904.3642100},\nabstract = {Advancements in Virtual Reality (VR) technology have opened new frontiers in medical education, igniting interest among medical educators to incorporate it into mainstream curriculum, complementing traditional training modalities such as manikin training. Despite numerous VR simulators on the market, their uptake in medical education remains limited. This paper explores the acceptability and educational effectiveness of VR in the context of vaginal childbirth delivery training, with the simulator providing a walkthrough for the second and third stages of labour, contrasting it with established manikin-based methods. We conducted a large-scale empirical study with 117 medical students, revealing a significant 24.9\\% improvement in knowledge scores when using VR as compared to manikin. However, VR received significantly lower self-reported feasibility scores in Confidence, Usability, Enjoyment, Feedback and Presence, indicating low acceptance. The study provides critical insights into the relationship between technological innovation and educational impact, guiding future integration of VR into medical training curricula.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {458},\nnumpages = {14},\nkeywords = {Medical Education, Obstetrics \\& Gynaecology, User Experience Design, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641929,\nauthor = {Popov, Vitaliy and Chen, Xinyue and Wang, Jingying and Kemp, Michael and Sandhu, Gurjit and Kantor, Taylor and Mateju, Natalie and Wang, Xu},\ntitle = {Looking Together ≠ Seeing the Same Thing: Understanding Surgeons' Visual Needs During Intra-operative Coordination and Instruction},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641929},\ndoi = {10.1145/3613904.3641929},\nabstract = {Shared gaze visualizations have been found to enhance collaboration and communication outcomes in diverse HCI scenarios including computer supported collaborative work and learning contexts. Given the importance of gaze in surgery operations, especially when a surgeon trainer and trainee need to coordinate their actions, research on the use of gaze to facilitate intra-operative coordination and instruction has been limited and shows mixed implications. We performed a field observation of 8 surgeries and an interview study with 14 surgeons to understand their visual needs during operations, informing ways to leverage and augment gaze to enhance intra-operative coordination and instruction. We found that trainees have varying needs in receiving visual guidance which are often unfulfilled by the trainers’ instructions. It is critical for surgeons to control the timing of the gaze-based visualizations and effectively interpret gaze data. We suggest overlay technologies, e.g., gaze-based summaries and depth sensing, to augment raw gaze in support of surgical coordination and instruction.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {459},\nnumpages = {12},\nkeywords = {Intraoperative Coordination, Joint Visual Attention, Shared Gaze Visualizations, Surgical Education},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642324,\nauthor = {Tashiro, Yuka and Miyafuji, Shio and Kojima, Yusuke and Kiyofuji, Satoshi and Kin, Taichi and Igarashi, Takeo and Koike, Hideki},\ntitle = {MR Microsurgical Suture Training System with Level-Appropriate Support},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642324},\ndoi = {10.1145/3613904.3642324},\nabstract = {The integration of advanced technologies in healthcare necessitates the development of systems accommodating the daily routines in medical practices. Neurosurgeons, in particular, require extensive practice in microsurgical suturing in the long term, even in the busy routine of a medical practice. This study collaboratively developed a Mixed Reality system with neurosurgeons to support self-training in microscopic suturing. Based on the neurosurgeons’ opinions, we implemented a level-appropriate microsurgical suture training system. For novices, the system offers shadow-matching training to support the practice of precise movements under the high-sensitivity environment of the microscope. For intermediates, it provides a real-time feedback system, which allows users to practice attention to details. Evaluation involved testing the novice system on students with no medical background and the intermediate system on neurosurgery residents. The effectiveness of the system was demonstrated through the experimental results and subsequent discussion.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {460},\nnumpages = {19},\nkeywords = {CV-based tracking, MR, Neurosurgical training, Real-time feedback, Visual feedback},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642587,\nauthor = {Wang, Jingying and Tang, Haoran and Kantor, Taylor and Soltani, Tandis and Popov, Vitaliy and Wang, Xu},\ntitle = {Surgment: Segmentation-enabled Semantic Search and Creation of Visual Question and Feedback to Support Video-Based Surgery Learning},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642587},\ndoi = {10.1145/3613904.3642587},\nabstract = {Videos are prominent learning materials to prepare surgical trainees before they enter the operating room (OR). In this work, we explore techniques to enrich the video-based surgery learning experience. We propose Surgment, a system that helps expert surgeons create exercises with feedback based on surgery recordings. Surgment is powered by a few-shot-learning-based pipeline (SegGPT+SAM) to segment surgery scenes, achieving an accuracy of 92\\%. The segmentation pipeline enables functionalities to create visual questions and feedback desired by surgeons from a formative study. Surgment enables surgeons to 1) retrieve frames of interest through sketches, and 2) design exercises that target specific anatomical components and offer visual feedback. In an evaluation study with 11 surgeons, participants applauded the search-by-sketch approach for identifying frames of interest and found the resulting image-based questions and feedback to be of high educational value.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {461},\nnumpages = {18},\nkeywords = {question creation, scene segmentation, surgical learning, video navigation, video-based learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642187,\nauthor = {Du, Qiuxin and Song, Zhen and Jiang, Haiyan and Wei, Xiaoying and Weng, Dongdong and Fan, Mingming},\ntitle = {LightSword: A Customized Virtual Reality Exergame for Long-Term Cognitive Inhibition Training in Older Adults},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642187},\ndoi = {10.1145/3613904.3642187},\nabstract = {The decline of cognitive inhibition significantly impacts older adults’ quality of life and well-being, making it a vital public health problem in today’s aging society. Previous research has demonstrated that Virtual reality (VR) exergames have great potential to enhance cognitive inhibition among older adults. However, existing commercial VR exergames were unsuitable for older adults’ long-term cognitive training due to the inappropriate cognitive activation paradigm, unnecessary complexity, and unbefitting difficulty levels. To bridge these gaps, we developed a customized VR cognitive training exergame (LightSword) based on Dual-task and Stroop paradigms for long-term cognitive inhibition training among healthy older adults. Subsequently, we conducted an eight-month longitudinal user study with 12 older adults aged 60 years and above to demonstrate the effectiveness of LightSword in improving cognitive inhibition. After the training, the cognitive inhibition abilities of older adults were significantly enhanced, with benefits persisting for 6 months. This result indicated that LightSword has both short-term and long-term effects in enhancing cognitive inhibition. Furthermore, qualitative feedback revealed that older adults exhibited a positive attitude toward long-term training with LightSword, which enhanced their motivation and compliance.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {462},\nnumpages = {17},\nkeywords = {Health, Older Adults, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642390,\nauthor = {Batbold, Togtokhtur and Soro, Alessandro and Schroeter, Ronald},\ntitle = {Mentorable Interfaces for Automated Vehicles: A New Paradigm for Designing Learnable Technology for Older Adults},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642390},\ndoi = {10.1145/3613904.3642390},\nabstract = {We introduce a conceptual framework exploring the learning methods for older adults in navigating automated vehicle interfaces. Through semi-structured interviews, we observed distinct approaches to learning, and based on these, offer a novel conceptualization of a ‘mentorable’ interface to enhance technology education. The introduction of automated vehicles (AVs) to transportation has required novel lenses to technology adoption. Although AVs require less demand in cognitive, motor, and sensory acuity, there is an increasing dependence on digital literacy. While technology education has been broadly explored through the lens of learnability, this paradigm does not work well for older adults due to its inherent trial-and-error approach to independent learning. Because older adults rely heavily on additional external support in learning technologies, we present a conceptual framework for ‘mentorability’, where a network of support is emphasized, and mentorship is integrated into the design process for in-vehicle interfaces.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {463},\nnumpages = {15},\nkeywords = {automated vehicles, digital literacy, learning methods, older adults, technology education},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642842,\nauthor = {Yang, Muhe and Moffatt, Karyn},\ntitle = {Navigating the Maze of Routine Disruption: Exploring How Older Adults Living Alone Navigate Barriers to Establishing and Maintaining Physical Activity Habits},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642842},\ndoi = {10.1145/3613904.3642842},\nabstract = {Older adults, especially those living alone, are less likely to meet recommended physical activity levels than other age groups. However, current physical activity promoting technologies have seen low uptake among older adults, likely due to poor attention to their unique needs. To understand the perspectives of older adults living alone towards physical activity, including their motivations for and the challenges encountered in maintaining routines, we conducted a qualitative study with 17 participants. Through thematic analysis of semi-structured interviews and diaries, we reveal their diverse motivations for engaging in physical activity while also detailing how their intentions and routines are habitually disrupted by multidimensional and interrelated barriers, including changing personal and environmental circumstances, lack of stimulus to maintaining motivation, and limited access to resources. We suggest future PA promoting technologies to leverage social interaction to develop commitments and employ a holistic design approach to addressing the interplay between the barriers.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {464},\nnumpages = {15},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642170,\nauthor = {Wang, Yiwen and Li, Mengying and Kim, Young-Ho and Lee, Bongshin and Danilovich, Margaret and Lazar, Amanda and Conroy, David E and Kacorri, Hernisa and Choe, Eun Kyoung},\ntitle = {Redefining Activity Tracking Through Older Adults' Reflections on Meaningful Activities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642170},\ndoi = {10.1145/3613904.3642170},\nabstract = {Activity tracking has the potential to promote active lifestyles among older adults. However, current activity tracking technologies may inadvertently perpetuate ageism by focusing on age-related health risks. Advocating for a personalized approach in activity tracking technology, we sought to understand what activities older adults find meaningful to track and the underlying values of those activities. We conducted a reflective interview study following a 7-day activity journaling with 13 participants. We identified various underlying values motivating participants to track activities they deemed meaningful. These values, whether competing or aligned, shape the desirability of activities. Older adults appreciate low-exertion activities, but they are difficult to track. We discuss how these activities can become central in designing activity tracking systems. Our research offers insights for creating value-driven, personalized activity trackers that resonate more fully with the meaningful activities of older adults.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {465},\nnumpages = {15},\nkeywords = {activity tracking, interview study, meaningful activity, older adults},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642644,\nauthor = {Bennett, Christopher R. and Fink, Paul D. S. and Giudice, Nicholas A.},\ntitle = {“X-Ray Vision” as a Compensatory Augmentation for Slowing Cognitive Map Decay in Older Adults},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642644},\ndoi = {10.1145/3613904.3642644},\nabstract = {Safe and efficient navigation often relies on the development and retention of accurate cognitive maps that include inter-landmark relations. For many older adults, cognitive maps are difficult to form and remember over time, which introduces serious challenges for independence and mobility. To address this problem, we explore an innovative compensatory augmentation solution enabling enhanced inter-landmark learning via an “X-Ray Vision” simulation. Results with (n=45) user study participants suggest superior older adult cognitive map retention over time from a single learning session with the augmentation versus a control condition without the augmentation. Furthermore, results characterize differences in decay of cognitive maps between older adults and a control of younger adults. These findings suggest important implications for future augmented reality devices and the ways in which they can be used to promote memory and independence among older adults.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {466},\nnumpages = {13},\nkeywords = {Augmented Reality, Cognitive Maps, Older Adults, X-Ray Vision},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641902,\nauthor = {Jin, Angela and Salehi, Niloufar},\ntitle = {(Beyond) Reasonable Doubt: Challenges that Public Defenders Face in Scrutinizing AI in Court},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641902},\ndoi = {10.1145/3613904.3641902},\nabstract = {Accountable use of AI systems in high-stakes settings relies on making systems contestable. In this paper we study efforts to contest AI systems in practice by studying how public defenders scrutinize AI in court. We present findings from interviews with 17 people in the U.S. public defense community to understand their perceptions of and experiences scrutinizing computational forensic software (CFS) — automated decision systems that the government uses to convict and incarcerate, such as facial recognition, gunshot detection, and probabilistic genotyping tools. We find that our participants faced challenges assessing and contesting CFS reliability due to difficulties (a) navigating how CFS is developed and used, (b) overcoming judges and jurors’ non-critical perceptions of CFS, and (c) gathering CFS expertise. To conclude, we provide recommendations that center the technical, social, and institutional context to better position interventions such as performance evaluations to support contestability in practice.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {467},\nnumpages = {19},\nkeywords = {algorithmic decision systems, artificial intelligence, contestability, criminal justice, performance evaluations},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642362,\nauthor = {Sa\\ss{}mannshausen, Sheree May and Ontika, Nazmun Nisat and Pinatti De Carvalho, Aparecido Fabiano and Rouncefield, Mark and Pipek, Volkmar},\ntitle = {Amplifying Human Capabilities in Prostate Cancer Diagnosis: An Empirical Study of Current Practices and AI Potentials in Radiology},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642362},\ndoi = {10.1145/3613904.3642362},\nabstract = {This paper examines the potential of Human-Centered AI (HCAI) solutions to support radiologists in diagnosing prostate cancer. Prostate cancer is one of the most prevalent and increasing cancers among men. The scarcity of radiologists raises concerns about their ability to address the growing demand for prostate cancer diagnosis, leading to a significant surge in the workload of radiologists. Drawing on an HCAI approach, we sought to understand the current practices concerning radiologists’ work on detecting and diagnosing prostate cancer, as well as the challenges they face. The findings from our empirical studies point toward the potential that AI has to expedite informed decision-making and enhance accuracy, efficiency, and consistency. This is particularly beneficial for collaborative prostate cancer diagnosis processes. We discuss these results and introduce design recommendations and HCAI concepts for the domain of prostate cancer diagnosis, with the aim of amplifying the professional capabilities of radiologists.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {468},\nnumpages = {20},\nkeywords = {contextual inquiry, human-centered AI, prostate cancer diagnosis, radiologists},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642296,\nauthor = {Berney, Manon and Ouaazki, Abdessalam and Macko, Vladimir and Kocher, Bruno and Holzer, Adrian},\ntitle = {Care-Based Eco-Feedback Augmented with Generative AI: Fostering Pro-Environmental Behavior through Emotional Attachment},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642296},\ndoi = {10.1145/3613904.3642296},\nabstract = {Lights out! With the escalating climate crisis, eco-feedback has gained prominence over the last decade. However, traditional approaches could be underperforming as they often use data-driven strategies and assume that people only need additional information about their consumption to change behavior. A proposed path to overcome this issue is to design eco-feedback to foster emotional connections with users. However, not much is known about the effectiveness of such designs. In this paper, we propose a novel care-based eco-feedback system. Central to the system is a Tamagotchi-inspired digital character named Infi who gets its life force from the user’s energy savings. Additionally, we harness the latest advancements in generative artificial intelligence to enhance emotional attachment through conversational interactions that users can have with Infi. The results of a randomized controlled experiment (N=420) convey the fact that this design increases emotional attachment, which in turn increases energy-saving behavior.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {469},\nnumpages = {15},\nkeywords = {care-based intervention, conversational interaction, eco-feedback, emotional attachment, gamification, generative AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642402,\nauthor = {Hanschke, Vanessa Aisyahsari and Rees, Dylan and Alanyali, Merve and Hopkinson, David and Marshall, Paul},\ntitle = {Data Ethics Emergency Drill: A Toolbox for Discussing Responsible AI for Industry Teams},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642402},\ndoi = {10.1145/3613904.3642402},\nabstract = {Researchers urge technology practitioners such as data scientists to consider the impacts and ethical implications of algorithmic decisions. However, unlike programming, statistics, and data management, discussion of ethical implications is rarely included in standard data science training. To begin to address this gap, we designed and tested a toolbox called the data ethics emergency drill (DEED) to help data science teams discuss and reflect on the ethical implications of their work. The DEED is a roleplay of a fictional ethical emergency scenario that is contextually situated in the team’s specific workplace and applications. This paper outlines the DEED toolbox and describes three studies carried out with two different data science teams that iteratively shaped its design. Our findings show that practitioners can apply lessons learnt from the roleplay to real-life situations, and how the DEED opened up conversations around ethics and values.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {470},\nnumpages = {17},\nkeywords = {data science, responsible innovation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642125,\nauthor = {Liu, Zhihao and Li, Yu and Tu, Fangyuan and Zhang, Ruiyuan and Cheng, Zhanglin and Yokoya, Naoto},\ntitle = {DeepTreeSketch: Neural Graph Prediction for Faithful 3D Tree Modeling from Sketches},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642125},\ndoi = {10.1145/3613904.3642125},\nabstract = {We present DeepTreeSketch, a novel AI-assisted sketching system that enables users to create realistic 3D tree models from 2D freehand sketches. Our system leverages a tree graph prediction network, TGP-Net, to learn the underlying structural patterns of trees from a large collection of 3D tree models. The TGP-Net simulates the iterative growth of botanical trees and progressively constructs the 3D tree structures in a bottom-up manner. Furthermore, our system supports a flexible sketching mode for both precise and coarse control of the tree shapes by drawing branch strokes and foliage strokes, respectively. Combined with a procedural generation strategy, users can freely control the foliage propagation with diverse and fine details. We demonstrate the expressiveness, efficiency, and usability of our system through various experiments and user studies. Our system offers a practical tool for 3D tree creation, especially for natural scenes in games, movies, and landscape applications.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {471},\nnumpages = {19},\nkeywords = {3D modeling interface, ideation, neural networks, sketching system},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642810,\nauthor = {Sadek, Malak and Constantinides, Marios and Quercia, Daniele and Mougenot, Celine},\ntitle = {Guidelines for Integrating Value Sensitive Design in Responsible AI Toolkits},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642810},\ndoi = {10.1145/3613904.3642810},\nabstract = {Value Sensitive Design (VSD) is a framework for integrating human values throughout the technology design process. In parallel, Responsible AI (RAI) advocates for the development of systems aligning with ethical values, such as fairness and transparency. In this study, we posit that a VSD approach is not only compatible, but also advantageous to the development of RAI toolkits. To empirically assess this hypothesis, we conducted four workshops involving 17 early-career AI researchers. Our aim was to establish links between VSD and RAI values while examining how existing toolkits incorporate VSD principles in their design. Our findings show that collaborative and educational design features within these toolkits, including illustrative examples and open-ended cues, facilitate an understanding of human and ethical values, and empower researchers to incorporate values into AI systems. Drawing on these insights, we formulated six design guidelines for integrating VSD values into the development of RAI toolkits.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {472},\nnumpages = {20},\nkeywords = {ethical AI, responsible AI, toolkits, value sensitive design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642280,\nauthor = {Zhang, Xiaoping and Cheng, Xusen},\ntitle = {I lose vs. I earn: Consumer perceived price fairness toward algorithmic (vs. human) price discrimination},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642280},\ndoi = {10.1145/3613904.3642280},\nabstract = {Many companies are turning to algorithms to determine prices. However, little research has been done to investigate consumers’ perceived price fairness when price discrimination is implemented by either a human or an algorithm. The results of two experiments with 2 (price-setting agent: algorithm vs. human) \\texttimes{} 2 (price discrimination: advantaged vs. disadvantaged) between-subjects design reveal that consumers perceive disadvantaged price discrimination as being more unfair when it is implemented by a human (vs. algorithm). Conversely, they perceive advantaged price discrimination as being more unfair when it is implemented by an algorithm (vs. human). This difference is caused by distinct attribution processes. Consumers are more likely to externalize disadvantaged price discrimination implemented by a human than an algorithm (i.e., attributing it to the unintentionality of price-setting agents), while they are more likely to internalize advantaged price discrimination implemented by a human than an algorithm (i.e., attributing it to perceived personal luck). Based on these findings, we discuss how designers and managers can design and utilize algorithms to implement price discrimination that reduces consumer perception of price unfairness. We believe that reasonable disclosure of algorithmic clues to consumers can maximize the benefits of price discrimination strategies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {473},\nnumpages = {17},\nkeywords = {Algorithmic pricing, Attribution, Price discrimination, Price fairness},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641951,\nauthor = {Mim, Nusrat Jahan and Nandi, Dipannita and Khan, Sadaf Sumyia and Dey, Arundhuti and Ahmed, Syed Ishtiaque},\ntitle = {In-Between Visuals and Visible: The Impacts of Text-to-Image Generative AI Tools on Digital Image-making Practices in the Global South},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641951},\ndoi = {10.1145/3613904.3641951},\nabstract = {This paper joins the growing body of HCI work on critical AI studies and focuses on the impact of Generative Artificial Intelligence (GAI) tools in Bangladesh. While the West has started to examine the limitations and risks associated with these tools, their impacts on the Global South have remained understudied. Based on our interviews, focus group discussions (FGD), and social media-based qualitative study, this paper reports how popular text-to-image GAI tools (ex., DALL-E, Midjourney, Stable Diffusion, Firefly) are affecting various image-related local creative fields. We report how these tools limit the creative explorations of marginal artists, struggle to understand linguistic nuances, fail to generate local forms of art and architecture, and misrepresent the diversity among citizens in the image production process. Drawing from a rich body of work on critical image theory, postcolonial computing, and design politics, we explain how our findings are pertinent to HCI’s broader interest in social justice, decolonization, and global development.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {474},\nnumpages = {18},\nkeywords = {Architecture, Art, Artificial Intelligence, Generative AI, Image, Urban Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642755,\nauthor = {Harrison, Galen and Bryson, Kevin and Bamba, Ahmad Emmanuel Balla and Dovichi, Luca and Binion, Aleksander Herrmann and Borem, Arthur and Ur, Blase},\ntitle = {JupyterLab in Retrograde: Contextual Notifications That Highlight Fairness and Bias Issues for Data Scientists},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642755},\ndoi = {10.1145/3613904.3642755},\nabstract = {Current algorithmic fairness tools focus on auditing completed models, neglecting the potential downstream impacts of iterative decisions about cleaning data and training machine learning models. In response, we developed Retrograde, a JupyterLab environment extension for Python that generates real-time, contextual notifications for data scientists about decisions they are making regarding protected classes, proxy variables, missing data, and demographic differences in model performance. Our novel framework uses automated code analysis to trace data provenance in JupyterLab, enabling these notifications. In a between-subjects online experiment, 51 data scientists constructed loan-decision models with Retrograde providing notifications continuously throughout the process, only at the end, or never. Retrograde’s notifications successfully nudged participants to account for missing data, avoid using protected classes as predictors, minimize demographic differences in model performance, and exhibit healthy skepticism about their models.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {475},\nnumpages = {19},\nkeywords = {Jupyter Notebook, computational notebooks, data science, fairness},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642531,\nauthor = {Schor, Bianca G. S. and Norval, Chris and Charlesworth, Ellen and Singh, Jatinder},\ntitle = {Mind The Gap: Designers and Standards on Algorithmic System Transparency for Users},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642531},\ndoi = {10.1145/3613904.3642531},\nabstract = {Many call for algorithmic systems to be more transparent, yet it is often unclear for designers how to do so in practice. Standards are emerging that aim to support designers in building transparent systems, e.g by setting testable transparency levels, but their efficacy in this regard is not yet understood. In this paper, we use the ‘Standard for Transparency of Autonomous Systems’ (IEEE 7001) to explore designers’ understanding of algorithmic system transparency, and the degree to which their perspectives align with the standard’s recommendations. Our mixed-method study reveals participants consider transparency important, difficult to implement, and welcome support. However, despite IEEE 7001’s potential, many did not find its recommendations particularly appropriate. Given the importance and increased attention on transparency, and because standards like this purport to guide system design, our findings reveal the need for ‘bridging the gap’, through (i) raising designers’ awareness about the importance of algorithmic system transparency, alongside (ii) better engagement between stakeholders (i.e. standards bodies, designers, users). We further identify opportunities towards developing transparency best practices, as means to help drive more responsible systems going forward.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {476},\nnumpages = {16},\nkeywords = {algorithmic systems, artificial intelligence, design guidelines, design practice, human-AI interaction, standards, transparency},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642501,\nauthor = {Kommiya Mothilal, Ramaravind and Guha, Shion and Ahmed, Syed Ishtiaque},\ntitle = {Towards a Non-Ideal Methodological Framework for Responsible ML},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642501},\ndoi = {10.1145/3613904.3642501},\nabstract = {Though ML practitioners increasingly employ various Responsible ML (RML) strategies, their methodological approach in practice is still unclear. In particular, the constraints, assumptions, and choices of practitioners with technical duties–such as developers, engineers, and data scientists—are often implicit, subtle, and under-scrutinized in HCI and related fields. We interviewed 22 technically oriented ML practitioners across seven domains to understand the characteristics of their methodological approaches to RML through the lens of ideal and non-ideal theorizing of fairness. We find that practitioners’ methodological approaches fall along a spectrum of idealization. While they structured their approaches through ideal theorizing, such as by abstracting RML workflow from the inquiry of applicability of ML, they did not systematically document nor pay deliberate attention to their non-ideal approaches, such as diagnosing imperfect conditions. We end our paper with a discussion of a new methodological approach, inspired by elements of non-ideal theory, to structure technical practitioners’ RML process and facilitate collaboration with other stakeholders.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {477},\nnumpages = {17},\nkeywords = {Fairness, Ideal Theory, Justice, ML Practitioners, Machine Learning, Non-Ideal Theory, Responsible ML},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641898,\nauthor = {Karusala, Naveena and Upadhyay, Sohini and Veeraraghavan, Rajesh and Gajos, Krzysztof Z.},\ntitle = {Understanding Contestability on the Margins: Implications for the Design of Algorithmic Decision-making in Public Services},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641898},\ndoi = {10.1145/3613904.3641898},\nabstract = {Policymakers have established that the ability to contest decisions made by or with algorithms is core to responsible artificial intelligence (AI). However, there has been a disconnect between research on contestability of algorithms, and what the situated practice of contestation looks like in contexts across the world, especially amongst communities on the margins. We address this gap through a qualitative study of follow-up and contestation in accessing public services for land ownership in rural India and affordable housing in the urban United States. We find there are significant barriers to exercising rights and contesting decisions, which intermediaries like NGO workers or lawyers work with communities to address. We draw on the notion of accompaniment in global health to highlight the open-ended work required to support people in navigating violent social systems. We discuss the implications of our findings for key aspects of contestability, including building capacity for contestation, human review, and the role of explanations. We also discuss how sociotechnical systems of algorithmic decision-making can embody accompaniment by taking on a higher burden of preventing denials and enabling contestation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {478},\nnumpages = {16},\nkeywords = {India, United States, algorithmic decision-making, contestability, public services},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642714,\nauthor = {Akridge, Hunter and Fan, Bonnie and Tang, Alice Xiaodi and Mehta, Chinar and Martelaro, Nikolas and Fox, Sarah E},\ntitle = {“The bus is nothing without us”: Making Visible the Labor of Bus Operators amid the Ongoing Push Towards Transit Automation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642714},\ndoi = {10.1145/3613904.3642714},\nabstract = {This paper describes how the circumstances bus operators manage presents unique challenges to the feasibility of high-level automation in public transit. Avoiding an overly rationalized view of bus operators’ labor is critical to ensure the introduction of automation technologies does not compromise public wellbeing, the dignity of transit workers, or the integrity of critical public infrastructure. Our findings from a group interview study show that bus operators take on work — undervalued by those advancing automation technologies — to ensure the well-being of passengers and communities. Notably, bus operators are positioned to function as shock absorbers during social crises in their communities and in moments of technological breakdown as new systems come on board. These roles present a critical argument against the rapid push toward driverless automation in public transit. We conclude by identifying opportunities for participatory design and collaborative human-machine teaming for a more just future of transit.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {479},\nnumpages = {16},\nkeywords = {autonomous vehicle (AV) technology, bus operators, critical HCI, human-machine teaming, invisible work, political economy, transit automation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642326,\nauthor = {Bertrand, Astrid and Eagan, James R. and Maxwell, Winston and Brand, Joshua},\ntitle = {AI is Entering Regulated Territory: Understanding the Supervisors' Perspective for Model Justifiability in Financial Crime Detection},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642326},\ndoi = {10.1145/3613904.3642326},\nabstract = {Artificial intelligence (AI) has the potential to bring significant benefits to highly regulated industries such as healthcare or banking. Adoption, however, remains low. AI’s entry into complex socio-techno-legal systems raises issues of transparency, specifically for regulators. However, the perspective of supervisors, regulators who monitor compliance with applicable financial regulations, has rarely been studied. This paper focuses on understanding the needs of supervisors in anti-money laundering (AML) to better inform the design of AI justifications and explanations in highly regulated fields. Through scenario-based workshops with 13 supervisors and 6 banking professionals, we outline the auditing practices and socio-technical context of the supervisor. By combining the workshops’ insights with an analysis of compliance requirements, we identify the AML obligations that conflict with AI opacity. We then formulate seven needs that supervisors have for model justifiability. We discuss the role of explanations as reliable evidence on which to base justifications.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {480},\nnumpages = {21},\nkeywords = {AI regulation, anti-money laundering, explainability, highly-regulated environment, justifiability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642122,\nauthor = {Metzger, Luise and Miller, Linda and Baumann, Martin and Kraus, Johannes},\ntitle = {Empowering Calibrated (Dis-)Trust in Conversational Agents: A User Study on the Persuasive Power of Limitation Disclaimers vs. Authoritative Style},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642122},\ndoi = {10.1145/3613904.3642122},\nabstract = {While conversational agents based on Large Language Models (LLMs) can drive progress in many domains, they are prone to generating faulty information. To ensure an efficient, safe, and satisfactory user experience maximizing benefits of these systems, users must be empowered to judge the reliability of system outputs. In this, both disclaimers and agents’ communicative style are pivotal design instances. In an online study with 594 participants, we investigated how these affect users’ trust and a mock-up agent’s persuasiveness, based on an established framework from social psychology. While prior information on potential inaccuracies or faulty information did not affect trust, an authoritative communicative style elicited more trust. Also, a trusted agent was more persuasive resulting in more positive attitudes regarding the subject of the conversation. Results imply that disclaimers on agents’ limitations fail to effectively alter users’ trust but can be supported by appropriate communicative style during interaction.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {481},\nnumpages = {19},\nkeywords = {ChatGPT, chatbots, communicative style, conversational agents, elaboration likelihood model, large language models, trust in automation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642428,\nauthor = {Leiser, Florian and Eckhardt, Sven and Leuthe, Valentin and Knaeble, Merlin and M\\\"{a}dche, Alexander and Schwabe, Gerhard and Sunyaev, Ali},\ntitle = {HILL: A Hallucination Identifier for Large Language Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642428},\ndoi = {10.1145/3613904.3642428},\nabstract = {Large language models (LLMs) are prone to hallucinations, i.e., nonsensical, unfaithful, and undesirable text. Users tend to overrely on LLMs and corresponding hallucinations which can lead to misinterpretations and errors. To tackle the problem of overreliance, we propose HILL, the \"Hallucination Identifier for Large Language Models\". First, we identified design features for HILL with a Wizard of Oz approach with nine participants. Subsequently, we implemented HILL based on the identified design features and evaluated HILL’s interface design by surveying 17 participants. Further, we investigated HILL’s functionality to identify hallucinations based on an existing question-answering dataset and five user interviews. We find that HILL can correctly identify and highlight hallucinations in LLM responses which enables users to handle LLM responses with more caution. With that, we propose an easy-to-implement adaptation to existing LLMs and demonstrate the relevance of user-centered designs of AI artifacts.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {482},\nnumpages = {13},\nkeywords = {Artifact Development, Artificial Hallucinations, ChatGPT, Large Language Models, Wizard of Oz},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642517,\nauthor = {Yang, Jackie (Junrui) and Shi, Yingtian and Zhang, Yuhan and Li, Karina and Rosli, Daniel Wan and Jain, Anisha and Zhang, Shuning and Li, Tianshi and Landay, James A. and Lam, Monica S.},\ntitle = {ReactGenie: A Development Framework for Complex Multimodal Interactions Using Large Language Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642517},\ndoi = {10.1145/3613904.3642517},\nabstract = {By combining voice and touch interactions, multimodal interfaces can surpass the efficiency of either modality alone. Traditional multimodal frameworks require laborious developer work to support rich multimodal commands where the user’s multimodal command involves possibly exponential combinations of actions/function invocations. This paper presents ReactGenie, a programming framework that better separates multimodal input from the computational model to enable developers to create efficient and capable multimodal interfaces with ease. ReactGenie translates multimodal user commands into NLPL (Natural Language Programming Language), a programming language we created, using a neural semantic parser based on large-language models. The ReactGenie runtime interprets the parsed NLPL and composes primitives in the computational model to implement complex user commands. As a result, ReactGenie allows easy implementation and unprecedented richness in commands for end-users of multimodal apps. Our evaluation showed that 12 developers can learn and build a non-trivial ReactGenie application in under 2.5 hours on average. In addition, compared with a traditional GUI, end-users can complete tasks faster and with less task load using ReactGenie apps.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {483},\nnumpages = {23},\nkeywords = {development frameworks, large-language model, multimodal interactions, natural language processing, programming framework},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642545,\nauthor = {Zhang, Yu and Sun, Jingwei and Feng, Li and Yao, Cen and Fan, Mingming and Zhang, Liuxin and Wang, Qianying and Geng, Xin and Rui, Yong},\ntitle = {See Widely, Think Wisely: Toward Designing a Generative Multi-agent System to Burst Filter Bubbles},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642545},\ndoi = {10.1145/3613904.3642545},\nabstract = {The proliferation of AI-powered search and recommendation systems has accelerated the formation of “filter bubbles” that reinforce people’s biases and narrow their perspectives. Previous research has attempted to address this issue by increasing the diversity of information exposure, which is often hindered by a lack of user motivation to engage with. In this study, we took a human-centered approach to explore how Large Language Models (LLMs) could assist users in embracing more diverse perspectives. We developed a prototype featuring LLM-powered multi-agent characters that users could interact with while reading social media content. We conducted a participatory design study with 18 participants and found that multi-agent dialogues with gamification incentives could motivate users to engage with opposing viewpoints. Additionally, progressive interactions with assessment tasks could promote thoughtful consideration. Based on these findings, we provided design implications with future work outlooks for leveraging LLMs to help users burst their filter bubbles.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {484},\nnumpages = {24},\nkeywords = {diverse information, filter bubble, interaction design, large language model, multi-agent system},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642046,\nauthor = {Yoshimura, Kazumi and Chen, Dominique and Witkowski, Olaf},\ntitle = {Synlogue with Aizuchi-bot: Investigating the Co-Adaptive and Open-Ended Interaction Paradigm},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642046},\ndoi = {10.1145/3613904.3642046},\nabstract = {In contrast to dialogue, wherein the exchange of completed messages occurs through turn-taking, synlogue is a mode of conversation characterized by co-creative processes, such as mutually complementing incomplete utterances and cooperative overlaps of backchannelings. Such co-creative conversations have the potential to alleviate social divisions in contemporary information environments. This study proposed the design concept of a synlogue based on literature in linguistics and anthropology and explored features that facilitate synlogic interactions in computer-mediated interfaces. Through an experiment, we focused on aizuchi, an important backchanneling element that drives synlogic conversation, and compared the speech and perceptual changes of participants when a bot dynamically uttered aizuchi or otherwise silent in a situation simulating an online video call. Consequently, we discussed the implications for interaction design based on our qualitative and quantitative analysis of the experiment. The synlogic perspective presented in this study is expected to facilitate HCI researchers to achieve more convivial forms of communication.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {485},\nnumpages = {21},\nkeywords = {AI-human interaction, Computer-Mediated-Communication, aizuchi, co-adaptation, overlap, synlogue},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642333,\nauthor = {Schaffner, Brennan and Bhagoji, Arjun Nitin and Cheng, Siyuan and Mei, Jacqueline and Shen, Jay L and Wang, Grace and Chetty, Marshini and Feamster, Nick and Lakier, Genevieve and Tan, Chenhao},\ntitle = {\"Community Guidelines Make this the Best Party on the Internet\": An In-Depth Study of Online Platforms' Content Moderation Policies},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642333},\ndoi = {10.1145/3613904.3642333},\nabstract = {Moderating user-generated content on online platforms is crucial for balancing user safety and freedom of speech. Particularly in the United States, platforms are not subject to legal constraints prescribing permissible content. Each platform has thus developed bespoke content moderation policies, but there is little work towards a comparative understanding of these policies across platforms and topics. This paper presents the first systematic study of these policies from the 43 largest online platforms hosting user-generated content, focusing on policies around copyright infringement, harmful speech, and misleading content. We build a custom web-scraper to obtain policy text and develop a unified annotation scheme to analyze the text for the presence of critical components. We find significant structural and compositional variation in policies across topics and platforms, with some variation attributable to disparate legal groundings. We lay the groundwork for future studies of ever-evolving content moderation policies and their impact on users.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {486},\nnumpages = {16},\nkeywords = {content moderation, dataset, qualitative analysis, quantitative analysis},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641963,\nauthor = {Zhang, Bowen and Anderson, Jennings and Sarkar, Dipto and Soden, Robert},\ntitle = {A Quantitative Approach to Identifying Emergent Editor Roles in Open Street Map},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641963},\ndoi = {10.1145/3613904.3641963},\nabstract = {The objective of this study was to investigate and classify the roles, or distinct contribution styles, adopted by participants within the OpenStreetMap (OSM) community. Using a quantitative analysis of mapping behaviors, we devised a methodology to identify distinct features associated with specific roles. We used an unsupervised clustering approach and unveiled eight discernible roles, or types of mapper in OSM. Each role displays specific patterns of mapping behaviors related to their habits and preferences for adding or editing map objects over time. We validated our roles, in part, using known affiliations with humanitarian and corporate organizations. Using these roles, we examine community composition and contributor retention over time. Our contributions include applying existing methods on the analysis of contributor behavior in online platforms to OSM, the identification of eight roles that can guide future research and design within OSM, and further understanding into the overall trajectory of the world’s largest geospatial peer production community.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {487},\nnumpages = {14},\nkeywords = {Contributor Roles, OpenStreetMap (OSM), Peer Production, Quantitative Methods},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641906,\nauthor = {Huang, Xiaoshan and Wu, Haolun and Liu, Xue and Lajoie, Susanne},\ntitle = {Examining the Role of Peer Acknowledgements on Social Annotations: Unraveling the Psychological Underpinnings},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641906},\ndoi = {10.1145/3613904.3641906},\nabstract = {This study explores the impact of peer acknowledgement on learner engagement and implicit psychological attributes in written annotations on an online social reading platform. Participants included 91 undergraduates from a large North American University. Using log file data, we analyzed the relationship between learners’ received peer acknowledgement and their subsequent annotation behaviours using cross-lag regression. Higher peer acknowledgements correlate with increased initiation of annotations and responses to peer annotations. By applying text mining techniques and calculating Shapley values to analyze 1,969 social annotation entries, we identified prominent psychological themes within three dimensions (i.e., affect, cognition, and motivation) that foster peer acknowledgment in digital social annotation. These themes include positive affect, openness to learning and discussion, and expression of motivation. The findings assist educators in improving online learning communities and provide guidance to technology developers in designing effective prompts, drawing from both implicit psychological cues and explicit learning behaviours.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {488},\nnumpages = {9},\nkeywords = {Digital social annotation, Shapley value, learner behavior, peer acknowledgment, psychological themes, text mining},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642620,\nauthor = {Thorn, Emily and Spence, Jocelyn and Koleva, Boriana and Benford, Steven David},\ntitle = {Insights Into Legacy: Issues of Handover from a Partner-Initiated Project},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642620},\ndoi = {10.1145/3613904.3642620},\nabstract = {We report on a six-year collaboration with a small community organisation to develop and deploy a permanent physical / digital locative media experience as part on an ongoing community regeneration project. We describe how this unfolded over four phases: approach and pilot; public deployment; supporting subsequent community-led spin-off experiences; and planning legacy and technology handovers. The project was distinctive for being a Knowledge Exchange project in which we were approached and formally contracted by the community to deliver the digital technology, rather than instigating and leading a research project. We identify seven considerations for handing over technologies that combine both digital and physical elements to communities of stakeholders that encompass businesses, councils, and volunteers, and how this illuminates the unique strengths and weaknesses of Knowledge Exchange projects within the wider design research landscape.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {489},\nnumpages = {15},\nkeywords = {Community, Digital, Handover, In-The-Wild, Installation, Knowledge Exchange, Legacy, Locative Media, Physical, Stakeholders},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642449,\nauthor = {Panagiotidou, Georgia and Costanza, Enrico and Potapov, Kyrill and Nkatha, Sonia and Fell, Michael J. and Samanani, Farhan and Knox, Hannah},\ntitle = {SolarClub: Supporting Renewable Energy Communities through an Interactive Coordination System},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642449},\ndoi = {10.1145/3613904.3642449},\nabstract = {Energy communities are a key focus for governments around the world in support of more sustainable energy practices. However, interactive systems for supporting energy communities to coordinate around renewable energy resources are still lacking. We present SolarClub, a demand-shifting visualization system that supported households in coordinating their energy usage by booking energy-hungry activities when solar energy was available. We deployed SolarClub with four groups of neighbors (N=15) for a month. SolarClub successfully enabled neighbors to coordinate, even when some of those participating households were less flexible. While participants reported that SolarClub did not foster a feeling of community, it helped them empathize with their neighbors. Our findings demonstrate the potential of sensor- and visualization-based technology to help understand the relation between everyday practices and resources consumption, beyond individual eco-feedback. This work thus contributes to the development of a next generation of practices and technologies that support collective action for environmental sustainability.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {490},\nnumpages = {19},\nkeywords = {coordination, data visualization, demand-shifting, energy communities, sensors},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642805,\nauthor = {Zavolokina, Liudmila and Sprenkamp, Kilian and Katashinskaya, Zoya and Jones, Daniel Gordon and Schwabe, Gerhard},\ntitle = {Think Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642805},\ndoi = {10.1145/3613904.3642805},\nabstract = {In today's digital age, characterized by rapid news consumption and increasing vulnerability to propaganda, fostering citizens' critical thinking is crucial for stable democracies. This paper introduces the design of ClarifAI, a novel automated propaganda detection tool designed to nudge readers towards more critical news consumption by activating the analytical mode of thinking, following Kahneman's dual-system theory of cognition. Using Large Language Models, ClarifAI detects propaganda in news articles and provides context-rich explanations, enhancing users' understanding and critical thinking. Our contribution is threefold: first, we propose the design of ClarifAI; second, in an online experiment, we demonstrate that this design effectively encourages news readers to engage in more critical reading; and third, we emphasize the value of explanations for fostering critical thinking. The study thus offers both a practical tool and useful design knowledge for mitigating propaganda in digital news.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {491},\nnumpages = {24},\nkeywords = {digital nudging, dual-system thinking, propaganda detection},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642177,\nauthor = {McDonnell, Emma J and Eagle, Tessa and Sinlapanuntakul, Pitch and Moon, Soo Hyun and Ringland, Kathryn E. and Froehlich, Jon E. and Findlater, Leah},\ntitle = {“Caption It in an Accessible Way That Is Also Enjoyable”: Characterizing User-Driven Captioning Practices on TikTok},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642177},\ndoi = {10.1145/3613904.3642177},\nabstract = {As user-generated video dominates media landscapes, it poses an accessibility challenge. While disability advocacy groups globally have secured hard-won accessibility regulations for broadcast media, no such regulation of user-generated content exists. Yet, one major player in this shift, TikTok, has a culture of user-generated, creative captioning. We sought to understand how TikTok videos are captioned and the impact current practices have on those who need captions to access audio content. Therefore, we conducted a content analysis of 300 open-captioned TikToks and contextualized these findings by interviewing nine caption users. We found that the current state of TikTok captioning does facilitate access to the platform but that a user-generated, social video-specific standard for captioning could improve caption quality and expand access. We contribute an empirical account of the state of TikTok captioning and outline steps toward a standard for user-generated captioning.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {492},\nnumpages = {16},\nkeywords = {TikTok, accessibility, captioning, user-generated video},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642171,\nauthor = {Morrison, Landon and McPherson, Andrew},\ntitle = {Entangling Entanglement: A Diffractive Dialogue on HCI and Musical Interactions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642171},\ndoi = {10.1145/3613904.3642171},\nabstract = {If, as several recent papers claim, we have entered a new wave of “Entanglement HCI,” then we are still at a liminal stage prior to consensus around which sources underpin this paradigm shift or how they might inform actionable approaches to design practice. Now is the time to interpret technosocial mediation from a range of disciplinary perspectives, rather than settling on a narrow canon of literature. To this end, our paper enacts a diffractive dialogue between researchers from different disciplines, focusing on digital musical instruments to examine how technical knowledge from design and engineering can be read against the grain of critical theories from music, media, and cultural studies. Drawing on two object lessons—keyboards and step sequencers, plus their remediations in recent musical interaction research—we highlight interdependencies of theory, design, and practice, and we show how the idea of entanglement is itself entangled in a cross-disciplinary web.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {493},\nnumpages = {17},\nkeywords = {Diffraction, Engineering, Entanglement HCI, Music and Media Studies, Musical Interaction, New Materialism, Social Studies of Science},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642309,\nauthor = {Sparrow, Lucy A. and Galwey, Caiti and Loveridge, Ben and Glasser, Solange and Osborne, Margaret S. and Kelly, Ryan M.},\ntitle = {Heart and Soul: The Ethics of Biometric Capture in Immersive Artistic Performance},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642309},\ndoi = {10.1145/3613904.3642309},\nabstract = {Biometric data plays a multifaceted role in innovative artistic endeavours. As artists continue to break new ground by integrating performers’ biometric data into live performances, others collect biometric data from audiences to measure engagement. Given the sensitive and personal nature of biometric data, particularly in relation to immersive technology, it is imperative to ethically consider how this data should be handled in performative contexts. To clarify these ethical considerations, we conducted a scoping review of sources related to immersive biometric performance in HCI, Performing Arts, and Social Sciences published over the past 30+ years. We detail how and why biometric data is being used in immersive artistic performance, identify associated ethical questions and concerns, and develop a framework of ethical considerations for artists and researchers in this space. In doing so, we emphasise an ‘ethics by design’ approach that considers values such as privacy and autonomy alongside artistic merit.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {494},\nnumpages = {23},\nkeywords = {Artistic Performance, Biometrics, Ethics, Immersive Technology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642655,\nauthor = {Moon, Alice and Bos, Maarten W.},\ntitle = {The Illusion of Increased Customization: Framing Choices as a Creative Process Increases Perceived Customization},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642655},\ndoi = {10.1145/3613904.3642655},\nabstract = {People are increasingly able to receive customized options. Despite this abundance of options, people may not view products as customized to their wants and needs. Across five experiments, we provide evidence for a possible solution. We find evidence for the Illusion of increased customization: Framing choices as a creative process increases a chosen option’s perceived customization. Even with a constant choice set, choosing by (creative) attributes rather than choosing from all available options produces the Illusion of increased customization. The Illusion of increased customization arises in part because people feel a greater sense of co-creation when choosing via a seemingly creative process. Consequently, the Illusion of increased customization extends to choices that express preferences (e.g., liking blue over red) but is significantly diminished with choices that describe objective needs (e.g., needing a small versus large T-shirt). Additionally, heightened perceived customization from the Illusion of increased customization results in greater willingness-to-pay for the chosen option.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {495},\nnumpages = {12},\nkeywords = {Empirical study, customization, psychology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642616,\nauthor = {Robson, Nicole and McPherson, Andrew and Bryan-Kinns, Nick},\ntitle = {Thinking with Sound: Exploring the Experience of Listening to an Ultrasonic Art Installation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642616},\ndoi = {10.1145/3613904.3642616},\nabstract = {Entanglement theories are well established in HCI discourse. These involve a commitment to view human experience in encounters with technology as relational and contingent, and research apparatuses as co-producers rather than passive observers of phenomena. In this paper, we argue that sound is the sensory modality best suited to the investigation of entanglements. Materialist theories of sound and listening guide both the design of a novel interactive sound installation and the methodological approach of a participant study exploring the experience of listening. We present a diffractive analysis whereby micro-phenomenological interview data is read with sonic theories, generating accounts that might otherwise remain mute: the temporal fluctuation and physical feeling of proximity in listener entanglements with sound, somatic intention setting, and plural interpretations of interactivity. Finally, we offer a series of provocations for HCI to embrace qualities of the sonic and consider epistemological positions grounded in other sense modalities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {496},\nnumpages = {14},\nkeywords = {Diffraction, Entanglement, Epistemology, Experience-Centred Design, Listening, Micro-Phenomenology, Sonic Interaction, Sound Installation, Sound Studies, Ultrasound},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642854,\nauthor = {Hsueh, Stacy and Ciolfi Felice, Marianela and Alaoui, Sarah Fdili and Mackay, Wendy E.},\ntitle = {What Counts as ‘Creative’ Work? Articulating Four Epistemic Positions in Creativity-Oriented HCI Research},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642854},\ndoi = {10.1145/3613904.3642854},\nabstract = {This paper examines prevailing understandings of creativity in creative computing research through the lens of feminist epistemology. We analyze “creativity support” as a construct that encodes different definitions of creative work. Drawing on existing literature and practices, the paper surfaces four views about creative work that underpin current creative technologies and HCI research: problem-solving, cognitive emergence, embodied action, and tool-mediated expert activity. Each view makes different claims about the role of computing in creative work and the creative subject assumed. We articulate the attendant politics of each view and illustrate how critical feminist epistemology can serve as an analytical tool to reason about the trade-offs of various creativity definitions. The paper concludes with recommendations for integrating feminist values into creativity-oriented HCI research.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {497},\nnumpages = {15},\nkeywords = {creative work, creativity definitions, epistemic positions},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642572,\nauthor = {Velykoivanenko, Lev and Salehzadeh Niksirat, Kavous and Teofanovic, Stefan and Chapuis, Bertil and Mazurek, Michelle L. and Huguenin, K\\'{e}vin},\ntitle = {Designing a Data-Driven Survey System: Leveraging Participants' Online Data to Personalize Surveys},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642572},\ndoi = {10.1145/3613904.3642572},\nabstract = {User surveys are essential to user-centered research in many fields, including human-computer interaction (HCI). Survey personalization—specifically, adapting questionnaires to the respondents’ profiles and experiences—can improve reliability and quality of responses. However, popular survey platforms lack usable mechanisms for seamlessly importing participants’ data from other systems. This paper explores the design of a data-driven survey system to fill this gap. First, we conducted formative research, including a literature review and a survey of researchers (N = 52), to understand researchers’ practices, experiences, needs, and interests in a data-driven survey system. Then, we designed and implemented a minimum viable product called Data-Driven Surveys (DDS), which enables including respondents’ data from online service accounts (Fitbit, Instagram, and GitHub) in survey questions, answers, and flow/logic on existing survey platforms (Qualtrics and SurveyMonkey). Our system is open source and can be extended to work with more online service accounts and survey platforms. It can enhance the survey research experience for both researchers and respondents. A demonstration video is available here: https://doi.org/10.17605/osf.io/vedbj},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {498},\nnumpages = {22},\nkeywords = {artefact, online accounts, surveys, user interfaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642808,\nauthor = {Bressa, Nathalie and Louis, Jordan and Willett, Wesley and Huron, Samuel},\ntitle = {Input Visualization: Collecting and Modifying Data with Visual Representations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642808},\ndoi = {10.1145/3613904.3642808},\nabstract = {We examine input visualizations, visual representations that are designed to collect (and represent) new data rather than encode preexisting datasets. Information visualization is commonly used to reveal insights and stories within existing data. As a result, most contemporary visualization approaches assume existing datasets as the starting point for design, through which that data is mapped to visual encodings. Meanwhile, the implications of visualizations as inputs and as data sources have received little attention—despite the existence of visual and physical examples stretching back centuries. In this paper, we present a design space of 50 input visualizations analyzing their visual representation, data, artifact, context, and input. Based on this, we identify input modalities, purposes of input visualizations, and a set of design considerations. Finally, we discuss the relationship between input visualization and traditional visualization design and suggest opportunities for future research to better understand these visual representations and their potential.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {499},\nnumpages = {18},\nkeywords = {data collection, data discussion, input visualization, interaction, participation, physicalization, public engagement, survey, visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3643055,\nauthor = {Dash, Punyashlok and Axtell, Benett and Geiskkovitch, Denise Y. and Neustaedter, Carman and Stuerzlinger, Wolfgang},\ntitle = {Multimedia-Enabled 911: Exploring 911 Callers’ Experience of Call Taker Controlled Video Calling in Simulated Emergencies},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3643055},\ndoi = {10.1145/3613904.3643055},\nabstract = {Emergency response to large-scale disasters is often supported with multimedia from social media. However, while these features are common in everyday video calls, the complex needs of 911 and other systems make it difficult to directly incorporate these features. We assess an ME911 (Multimedia-Enabled 911) app to understand how the design will need to deviate from common norms and how callers will respond to those non-standard choices. We expand the role of 911 call taker control over emergency situations to the calling interface while incorporating key features like map-based location finding. Participants’ experiences in mock emergencies show the non-standard design helps callers in the unfamiliar setting of emergency calling yet it also causes confusion and delays. We find the need for emergency-specific deviations from design norms is supported by participant feedback. We discuss how broader system changes will support callers to use these non-standard designs during emergencies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {500},\nnumpages = {15},\nkeywords = {911 calling, Emergency response, Non-standard design, User assessment, Video call},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642470,\nauthor = {Agrawaal, Taneea S and Chauhan, Aarjav and Nobre, Carolina and Soden, Robert},\ntitle = {What's the Rush?: Alternative Values in Navigation Technologies for Urban Placemaking},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642470},\ndoi = {10.1145/3613904.3642470},\nabstract = {In the design of contemporary mapping technologies, effective navigation has become synonymous with the quickest route, limiting the extent to which people engage with the places they move in and around. This paper unsettles the prevalent focus on efficiency and explores opportunities to support placemaking during everyday practices of navigation. Drawing on 16 interviews and using the Value Sensitive Design framework, we identify seven alternative values, beyond efficiency, that hold significance for people navigating the city. Through a series of two design workshops, we further examine how and when these values come to matter during navigation. Our findings suggest four ways in which the prevalent design standards of navigational apps work against these values, and highlight their potential contribution to placemaking during technology mediated navigation. In doing so, this paper contributes to placemaking research and ongoing questions of efficiency and optimization within HCI.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {501},\nnumpages = {17},\nkeywords = {cities, design, efficiency, mapping, maps, mobile technologies, navigation, placemaking, urban design, values, wayfinding},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641926,\nauthor = {Jit, Sophia S and Spinney, Jennifer and Chandra, Priyank and Chilton, Lydia B and Soden, Robert},\ntitle = {Writing out the Storm: Designing and Evaluating Tools for Weather Risk Messaging},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641926},\ndoi = {10.1145/3613904.3641926},\nabstract = {Communicating risk to the public in the lead-up to and during severe weather events has the potential to reduce the impacts of these events on lives and property. Globally, these events are anticipated to increase due to climate change, rendering effective risk communication an integral component of climate adaptation policies. Research in risk communications literature has developed substantial knowledge and best practices for the design of risk messaging. This study considers the potential for quantifying the compliance of severe weather risk messages with these best practices, individually and at scale, and developing tools to improve risk communication messaging. The current work makes two contributions. First, we develop a string-matching approach to evaluate whether messaging complies with best practices and suggest areas for improvement. Second, we conduct an interview study with risk communication professionals to inform the design space of authoring tools and other technologies to support severe weather risk communicators.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {502},\nnumpages = {16},\nkeywords = {Creativity Support, Crisis/Disaster, Empirical study that tells us about how people use a system},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642099,\nauthor = {Raghunath, Ananditha and Metzger, Alexander Le and Easton, Hans and Liu, Xunmei and Wang, Fanchong and Wang, Yunqi and Zhao, Yunwei and Mpogole, Hosea and Anderson, Richard},\ntitle = {eKichabi v2: Designing and Scaling a Dual-Platform Agricultural Technology in Rural Tanzania},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642099},\ndoi = {10.1145/3613904.3642099},\nabstract = {Although farmers in Sub-Saharan Africa are accessing feature phones and smartphones at historically high rates, they face challenges finding a robust network of agricultural contacts. With collaborators, we conduct a quantitative survey of 1014 agricultural households in Kagera, Tanzania to characterize technology access, use, and comfort levels in the region. Recognizing the paucity of research on dual-platform technologies that cater to both feature phone and smartphone users, we develop and deploy eKichabi v2, a searchable directory of 9833 agriculture-related enterprises accessible via a USSD application and an Android application. To bridge the gap in affordances between the two applications, we conduct a mixed methods pilot leveraging mobile money agents as intermediators for our USSD application’s users. Through our investigations, we identify the advantages, obstacles, and critical considerations in the design, implementation, and scalability of agricultural information systems tailored to both feature phone and smartphone users in Sub-Saharan Africa.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {503},\nnumpages = {16},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642126,\nauthor = {Park, Jeongeun and Shin, Hyorim and Oh, Changhoon and Kim, Ha Young},\ntitle = {“Is Text-Based Music Search Enough to Satisfy Your Needs?” A New Way to Discover Music with Images},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642126},\ndoi = {10.1145/3613904.3642126},\nabstract = {Music is intrinsically connected to human experience, yet the plethora of choices often renders the search for the ideal piece perplexing, especially when the search terms are ambiguous. This study questions the viability of employing visual data, specifically images, in innovative queries for music search, and it aims to better align search results with users’ moods and situational context. We designed and evaluated three prototype systems for music search—TTTune (text-based), VisTune (image-based), and VTTune (hybrid)—to comparatively assess user experience and system usability. In a comprehensive user study involving 236 participants, each participant interacted with one of the systems and subsequently completed post-experimental surveys. A subset of participants also participated in in-depth interviews to further elucidate the potential and the advantages of image-based music retrieval (IMR) systems. Our findings reveal a marked preference for the user experience and usability offered by the IMR approach, as compared with the traditional text-based method. This underscores the potential of the image in an effective search query. Based on these findings, we discuss interface design guidelines tailored for IMR systems and factors affecting system performance, contributing to the evolving landscape of music search methods.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {504},\nnumpages = {21},\nkeywords = {Image-to-music retrieval, multimodal, music search, system usability, user experience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642784,\nauthor = {Fadrigon, Beatrice and Gordon, Princess and Lupica, Jane and Ames, Morgan G.},\ntitle = {A Playbook to be Proud of: Making the Case for LGBTQ+ Inclusive User Account Design},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642784},\ndoi = {10.1145/3613904.3642784},\nabstract = {Digital platforms often require users to select from a limited set of options that may force them to misrepresent their gender identities and sexual orientation, which disproportionately affects the LGBTQ+ population. To provide digital product teams with a feasible industry-focused tool to ensure the inclusion of this population, we surveyed 151 participants, including 81 who identify as LGBTQ+; conducted five interviews with LGBTQ+ participants and 11 interviews with product managers in the technology industry; and analyzed the user account creation processes of 45 digital platforms commonly used or mentioned in survey responses to understand LGBTQ+ users’ wants, needs, and pain points in navigating user account sign-up. Participants recounted instances of microaggressions or micro-affirmations, and often had strong feelings about companies based on their account creation experience. Based on these results, we present a ‘Playbook’ of design recommendations, which is online at bit.ly/LGBTInclusive_UAGuide.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {505},\nnumpages = {22},\nkeywords = {LGBTQ+ user experience, account creation, digital platforms, inclusive design, micro-affirmations, microaggressions},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642199,\nauthor = {Sobey, Aisha},\ntitle = {Conceptualising Fatness within HCI: A Call for Fat Liberation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642199},\ndoi = {10.1145/3613904.3642199},\nabstract = {Fatness sits at the intersection of many systems of oppression, such as race, gender, class, and (dis)ability. Anti-fat bias happens out in the open and is prevalent in Western society, yet there has been little to no consideration for the wider impact of digital systems in exacerbating, recreating, and repurposing anti-fat bias, or any engagement with designing for fat justice. Therefore, this paper argues that there needs to be a consideration for fat dignity in the design of digital systems, and an investigation of the (un)intended consequences of the datafication of fat lives. This paper offers a scoping literature review of HCI and Fat Studies to identify research gaps and argues that both disciplines would benefit from collaboration. Specifically, the standard of design justice would be increased through radical acceptance, and new questions could be asked to critique how technologies have been leveraged to exercise control over our bodies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {506},\nnumpages = {14},\nkeywords = {Anti-Fat Bias, Design Justice, Fat Liberation, Harms, Scoping Review},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642494,\nauthor = {Taylor, Jordan and Simpson, Ellen and Tran, Anh-Ton and Brubaker, Jed R. and Fox, Sarah E and Zhu, Haiyi},\ntitle = {Cruising Queer HCI on the DL: A Literature Review of LGBTQ+ People in HCI},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642494},\ndoi = {10.1145/3613904.3642494},\nabstract = {LGBTQ+ people have received increased attention in HCI research, paralleling a greater emphasis on social justice in recent years. However, there has not been a systematic review of how LGBTQ+ people are researched or discussed in HCI. In this work, we review all research mentioning LGBTQ+ people across the HCI venues of CHI, CSCW, DIS, and TOCHI. Since 2014, we find a linear growth in the number of papers substantially about LGBTQ+ people and an exponential increase in the number of mentions. Research about LGBTQ+ people tends to center experiences of being politicized, outside the norm, stigmatized, or highly vulnerable. LGBTQ+ people are typically mentioned as a marginalized group or an area of future research. We identify gaps and opportunities for (1) research about and (2) the discussion of LGBTQ+ in HCI and provide a dataset to facilitate future Queer HCI research.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {507},\nnumpages = {21},\nkeywords = {LGBTQ+ people, Literature Review, Marginalized Communities, Queer HCI, Queer People},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642240,\nauthor = {Falk, Jeanette and Blumenkranz, Anna and Kubesch, Moritz and Vetter, Ralf and Hofer, Lisa and Frauenberger, Christopher},\ntitle = {Designing Diverse Pathways for Participation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642240},\ndoi = {10.1145/3613904.3642240},\nabstract = {In HCI there have been calls for diversity-driven research and insights into how this may be carried out in practice. One way of conducting diversity-driven HCI research is by doing participatory design. In this paper we contribute with lessons identified from organizing a PD workshop that enabled diverse ways of participating for our participants. The workshop design is based on insights from two years of doing diversity-driven PD with two middle school classes, which are particularly interesting settings to explore as diverse children spend substantial time together in a period of their development that is formative for their socialisation. We describe the workshop itself before reflecting on its structure and facilitation as well as the role of the physical space and the choice of design materials with the aim to distil insights and recommendations about what researchers can do to enable diverse pathways of participation in design processes.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {508},\nnumpages = {16},\nkeywords = {diversity, participation, participatory design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642312,\nauthor = {Riggs, Alexandra Teixeira and Janicki, Sylvia and Howell, Noura and Sullivan, Anne},\ntitle = {Designing an Archive of Feelings: Queering Tangible Interaction with Button Portraits},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642312},\ndoi = {10.1145/3613904.3642312},\nabstract = {How can tangible, wearable design encourage affective, embodied reflections on queer history? We expand Queer HCI scholarship, using queer theory to inform the design of wearable experiences that explore archives of gender and sexuality. Our project, “Button Portraits,” invites individuals to listen to oral histories from prominent queer activists by pinning archival buttons to a wearable audio player, eliciting moving personal impressions. We observed 17 participants’ experiences with “Button Portraits,” and with semi-structured interviews, surfaced reflections on how our design evoked personal connections to history, queer self-identification, and relatability to archival materials. We offer the following design directions: (1) designing tangible archives of feeling; (2) queering tangible, wearable interactions in design; (3) designing for personal, archival experiences; and (4) designing within difference. Through this work, we foreground queer stories to affect emotional reflections on marginalized histories, entangling the complex connections between bodies, feelings, histories, and shared queer experiences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {509},\nnumpages = {17},\nkeywords = {Cultural Heritage Design, Queer Archives, Queer HCI, Sexual and Gender Minorities, Tangible Embodied Interaction Design, Wearable Computing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642036,\nauthor = {Salminen, Joni and Liu, Chang and Pian, Wenjing and Chi, Jianxing and H\\\"{a}yh\\\"{a}nen, Essi and Jansen, Bernard J},\ntitle = {Deus Ex Machina and Personas from Large Language Models: Investigating the Composition of AI-Generated Persona Descriptions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642036},\ndoi = {10.1145/3613904.3642036},\nabstract = {Large language models (LLMs) can generate personas based on prompts that describe the target user group. To understand what kind of personas LLMs generate, we investigate the diversity and bias in 450 LLM-generated personas with the help of internal evaluators (n=4) and subject-matter experts (SMEs) (n=5). The research findings reveal biases in LLM-generated personas, particularly in age, occupation, and pain points, as well as a strong bias towards personas from the United States. Human evaluations demonstrate that LLM persona descriptions were informative, believable, positive, relatable, and not stereotyped. The SMEs rated the personas slightly more stereotypical, less positive, and less relatable than the internal evaluators. The findings suggest that LLMs can generate consistent personas perceived as believable, relatable, and informative while containing relatively low amounts of stereotyping.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {510},\nnumpages = {20},\nkeywords = {AI, HCI, LLMs, evaluation, user personas},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642303,\nauthor = {Fujii, Takao and Seaborn, Katie and Steeds, Madeleine},\ntitle = {Silver-Tongued and Sundry: Exploring Intersectional Pronouns with ChatGPT},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642303},\ndoi = {10.1145/3613904.3642303},\nabstract = {ChatGPT is a conversational agent built on a large language model. Trained on a significant portion of human output, ChatGPT can mimic people to a degree. As such, we need to consider what social identities ChatGPT simulates (or can be designed to simulate). In this study, we explored the case of identity simulation through Japanese first-person pronouns, which are tightly connected to social identities in intersectional ways, i.e., intersectional pronouns. We conducted a controlled online experiment where people from two regions in Japan (Kanto and Kinki) witnessed interactions with ChatGPT using ten sets of first-person pronouns. We discovered that pronouns alone can evoke perceptions of social identities in ChatGPT at the intersections of gender, age, region, and formality, with caveats. This work highlights the importance of pronoun use for social identity simulation, provides a language-based methodology for culturally-sensitive persona development, and advances the potential of intersectional identities in intelligent agents.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {511},\nnumpages = {14},\nkeywords = {ChatGPT, Japan, conversational user interface, first-person pronouns, gender, human-machine dialogue, identity perception, intersectionality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642704,\nauthor = {Chordia, Ishita and Baltaxe-Admony, Leya Breanna and Boone, Ashley and Sheehan, Alyssa and Dombrowski, Lynn and Le Dantec, Christopher A and Ringland, Kathryn E. and Smith, Angela D. R.},\ntitle = {Social Justice in HCI: A Systematic Literature Review},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642704},\ndoi = {10.1145/3613904.3642704},\nabstract = {Given the renewed attention on politics, values, and ethics within our field and the wider cultural milieu, now is the time to take stock of social justice research in HCI. We surveyed 124 papers explicitly pursuing social justice between 2009 and 2022 to better reflect on the current state of justice-oriented work within our discipline. We identified (1) how researchers understood the social justice-relevant harms and benefits, (2) the approaches researchers used to address harm, and (3) the tools that researchers leveraged to pursue justice. Our analysis highlights gaps in social justice work, such as the need for our community to conceptualize benefits, and identifies concrete steps the HCI community can take to pursue just futures. By providing a comprehensive overview of and reflection on HCI’s current social justice landscape, we seek to help our research community strategize, collaborate, and collectively act toward justice.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {512},\nnumpages = {33},\nkeywords = {activism, benefits, harms, justice, marginalization, oppression, positionality, reflexivity, resistance, social justice, structural, systemic},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642404,\nauthor = {Zabel, Sarah and Otto, Siegmar},\ntitle = {SustAInable: How Values in the Form of Individual Motivation Shape Algorithms’ Outcomes. An Example Promoting Ecological and Social Sustainability},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642404},\ndoi = {10.1145/3613904.3642404},\nabstract = {When thinking about algorithms, cold lines of code and purely rational decisions may come to mind. However, this picture is incomplete. Numerous examples illustrate how human aspects shape algorithmic output (e.g., via biased training data). This study delves into how developers’ and users’ individual differences can influence algorithmic output, focusing on environmental and altruistic motivation. In an online survey, (N = 766) participants rated different emails on their likelihood of being spam as input for a hypothetical spam-filter algorithm. Participants’ environmental motivation was negatively correlated with classifying emails from environmental and humanitarian organizations as spam. Thus, individuals with a stronger environmental motivation rated the emails in such a way that the spam filter was biased toward the common good. However, altruistic motivation had no impact on the ratings. These findings suggest that environmental motivation extends beyond pro-environmental behaviors by also influencing prosocial behaviors, thus offering insights for developing sustainable algorithms.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {513},\nnumpages = {11},\nkeywords = {Empirical study that tells us about people, Humanities, Quantitative Methods, Sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642600,\nauthor = {O'Neill, Daisy and Birk, Max V. and Mandryk, Regan L},\ntitle = {Unpacking Norms, Narratives, and Nourishment: A Feminist HCI Critique on Food Tracking Technologies},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642600},\ndoi = {10.1145/3613904.3642600},\nabstract = {Food tracking applications (apps) can provide benefits (e.g., helping diagnose food intolerances) but can also create harm (e.g., facilitating disordered eating). However, food tracking apps—viewed as a women’s health issue, and critically examined through the lens of feminist HCI—are absent from the discourse of sociocultural, ethical, and political implications of apps designed to track bodily data. We use a walkthrough method to critically analyze three commercial food tracking apps with differing marketing narratives and designs, applying a reflexive feminist lens grounded in a perspective of fat liberation. We articulate how these apps reproduce normativities of food and nutrition, health, and bodies, and how they perpetuate narratives of embodiment, simplification and quantification of health, and neoliberalism and the individualization of health. Our work exposes the normativities of bodies being propagated by food tracking apps, spotlighting how designs and interaction features are situated within prevalent anti-fat narratives.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {514},\nnumpages = {20},\nkeywords = {feminist HCI, food, food tracking apps, nutrition, self tracking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642396,\nauthor = {Margariti, Eleni and Vlachokyriakos, Vasilis and Durrant, Abigail C and Kirk, David},\ntitle = {Evaluating ActuAir: Building Occupants' Experiences of a Shape-Changing Air Quality Display},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642396},\ndoi = {10.1145/3613904.3642396},\nabstract = {With workplace buildings becoming increasingly sensor-rich environments and amidst climate change and global pandemic pressures, there is novel opportunity for utilizing climatic data within buildings for awareness and wellbeing purposes. Interaction design research, including on large, shared-displays, rarely addresses building occupants’ experiences of air quality (AQ); and to-date there are no studies evaluating such interventions in the context of communicating climatic data in the workplace. Responding to these research gaps, three prototype-led studies were conducted with 21 occupants of a smart office building over June-August 2022, evaluating occupants’ experiences of a large shape- and color-changing display responding to AQ data. A thematic analysis resulted in design implications for improving shape- and color-changing displays for communicating AQ data; linking biomimicry to data interpretation. Contributing to Human-Building Interaction (HBI) research in the Human-Computer Interaction (HCI) field, we provide design directions for future shape-changing and responsive architectures for climate awareness in smart buildings.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {515},\nnumpages = {21},\nkeywords = {Empirical Study that tells us how people use a system, Prototyping/Implementation, Public Displays, Workplaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642565,\nauthor = {Rae-Grant, Tucker and Wang, Shuhong and Yao, Lining},\ntitle = {ExCell: High Expansion Ratio Moisture-Responsive Wooden Actuators for DIY Shape-Changing and Deployable Structures},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642565},\ndoi = {10.1145/3613904.3642565},\nabstract = {While there has been sustained interest in shape-changing materials and deployable structures, many existing systems require engineering materials, precision fabrication, and computationally modeled kinematics in order to work. Additionally, many rely on external power sources in order to deploy. In light of these factors, we perceive a need for deployable materials that are easy to design, prototype, and deploy, and that can transform themselves in response to environmental stimuli, making them appropriate for ecological applications. In this paper, we present ExCell, a DIY-able system of water-responsive wooden linear actuators for self-actuating deployable structures. We show that ExCell can be used to develop a wide range of geometries, we present a prototyping method that can create accurate models of ExCell structures, and we suggest four possible applications for this system.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {516},\nnumpages = {14},\nkeywords = {active materials, deployable structures, human-nature interaction, morphing materials, shape-changing interface, sustainable HCI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642848,\nauthor = {Haynes, Alice C and Steimle, J\\\"{u}rgen},\ntitle = {Flextiles: Designing Customisable Shape-Change in Textiles with SMA-Actuated Smocking Patterns},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642848},\ndoi = {10.1145/3613904.3642848},\nabstract = {Shape Memory Alloys (SMAs) afford the seamless integration of shape-changing behaviour into textiles, enabling designers to augment apparel with dynamic shaping and styling. However, existing works fall short of providing versatile methods adaptable to varying scales, materials, and applications, curtailing designers’ capacity to prototype customised solutions. To address this, we introduce Flextiles, parameterised SMA design schema that leverage the traditional craft of smocking to integrate planar shape-change seamlessly into diverse textile projects. The conception of Flextiles stems from material experimentation and consultative dialogues with designers, whose insights inspired strategies for customising scale, elasticity, geometry, and actuation of Flextiles. To support the practical implementation of Flextiles, we provide a design tool and experimentally characterise their material properties. Lastly, through a workshop with practitioners, we explore the multifaceted applications and perspectives surrounding Flextiles, and subsequently realise four scenarios that illustrate the creative potential of these modular, customisable patterns.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {517},\nnumpages = {17},\nkeywords = {Actuated Textiles, Fabrication, Shape Memory Alloys},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642581,\nauthor = {Guridi, Sof\\'{\\i}a and Iannacchero, Matteo and Pouta, Emmi},\ntitle = {Towards More Sustainable Interactive Textiles: A Literature Review on The Use of Biomaterials for eTextiles.},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642581},\ndoi = {10.1145/3613904.3642581},\nabstract = {The development of functional fibres, active materials, and flexible electrical components has introduced new ways of embedding interactive capabilities within textiles. However, this seamless integration poses challenges in terms of materials, disassembly, and disposal, revealing an urgent need to address the issue of sustainability when creating new electronic textiles. Authors have proposed eco-design guidelines that emphasise the use of renewable and biodegradable materials. Despite these recommendations, the potential of biomaterials in eTextiles remains largely unexplored. This integrative literature review showcases how biomaterials emerged as catalysts for expanding possibilities within eTextiles and HCI, not only through their environmental sustainability but also through their dynamic and transformative nature, fostering a realm of novel interactive experiences. We suggest the potential of developing fully bio-based eTextile systems, the need for broader sustainability and aesthetic studies, the relevance of DIY methods, and the urgency of textile knowledge integration.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {518},\nnumpages = {19},\nkeywords = {biomaterials, eTextiles, smart textiles, sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642455,\nauthor = {Miller, Josh Aaron and Gandhi, Kutub and Whitby, Matthew Alexander and Kosa, Mehmet and Cooper, Seth and Mekler, Elisa D. and Iacovides, Ioanna},\ntitle = {A Design Framework for Reflective Play},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642455},\ndoi = {10.1145/3613904.3642455},\nabstract = {Recent research has begun exploring games as a medium for reflection due to their affordances as interactive systems of challenge. However, little effort has been put into (1) synthesizing insights across studies and disciplines and (2) translating the academic work on reflective play into practical takeaways for game developers. This article takes the first steps toward summarizing existing work on reflective play and translating insights for practical implementation by identifying key game elements present in games that evoke reflection. We divide these elements into five approaches: Disruptions, Slowdowns, Questioning, Revisiting, and Enhancers. Finally, we provide an actionable supplement for practicing game developers to apply these concepts to their games.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {519},\nnumpages = {21},\nkeywords = {games, meaningful games, reflection, reflective game design, reflective informatics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642275,\nauthor = {Park, Solip and H\\\"{a}m\\\"{a}l\\\"{a}inen, Perttu and Kultima, Annakaisa},\ntitle = {Comic-making to Study Game-making: Using Comics in Qualitative Longitudinal Research on Game Development},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642275},\ndoi = {10.1145/3613904.3642275},\nabstract = {This paper reports the research method of the “Game Expats Story (GES)” project that used qualitative longitudinal research (“QLR”) incorporated with art-based research (“ABR”) in the context of game research. To facilitate greater participant engagement and a higher retention rate of longitudinal participants, we created comic artworks simultaneously while researching the case of migrant/expatriate game developers (“game expats”) in Finland 2020-2023 in two phases: (i) art creation as part of the qualitative data analysis to supplement the researcher’s inductive abstraction of the patterns, and (ii) artwork as a communication and recall tool when periodically engaging with the informants over the multi-year project span. Our findings suggest that the method of QLR-ABR helps game research as it positively influences the researcher’s abstractions of longitudinal data and participants’ continuous engagement with a high retention rate of 89\\%. We conclude that incorporating artistic methods provides new opportunities for ethnographic research on game development.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {520},\nnumpages = {11},\nkeywords = {Comic, Ethnography, Game Development, Game expats, Longitudinal research, Qualitative research, art-based research},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642679,\nauthor = {Larsen, Bjarke Alexander and Carstensdottir, Elin},\ntitle = {Community, Storytelling, and Play: Making and Breaking Rituals in Destiny 2},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642679},\ndoi = {10.1145/3613904.3642679},\nabstract = {Modern video game development relies increasingly on live service models and storytelling, putting strain on developer-player interactions and community management, the success of which is key to the success of such games. In this paper we report on a 2.5-year ethnography study on the Destiny player community, specifically on how players and developers interact and communicate about the game, and how is this interaction is affected by and affects ongoing rituals and storytelling in that game. Our findings indicate that rituals of play are fundamental. They reinforce the players’ collective experiences, created by the ongoing relationship between the players and developers. Players test and break boundaries of rituals, and developers continually adjust and experiment with those boundaries in turn. Our findings show that developers create positive feedback loops from the community when they lean into creativity efforts and boundary-breaking from players, and use storytelling directly as a community management tool.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {521},\nnumpages = {18},\nkeywords = {Destiny, community management, developer interaction, ethnography, narrative, online communities, perennial games, ritual},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642659,\nauthor = {Spors, Velvet and Buruk, O\\u{g}uz 'Oz' and Hamari, Juho},\ntitle = {Ecological In/Congruence: Becoming Sensitised to Nature in Video Games through Humanistic First-Person Research},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642659},\ndoi = {10.1145/3613904.3642659},\nabstract = {The ongoing ecological crisis is the current biggest threat for our species. As we attempt to address the situation through policy, interventions, and education, we urgently need to understand how people encounter and relate to nature: As it is, in the world, and portrayed through different media. As an exemplary medium facilitating digital nature, this paper focuses on video games. Using first-person research methods, we report on the first author sensitising themselves to nature as a ubiquitous feature, theme, and actor in video games. They played eight nature-focused games for three months. Through auto-ethnography, close reading and “noticing” (after Tsing), we make sense of their experiences using the humanistic concept of ecological (in)congruence: We draw out the relational gap and potential meanings between real nature and its virtual equivalent. Based on these insights, we outline two design impulses for how the HCI community might approach nature—within games and beyond.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {522},\nnumpages = {16},\nkeywords = {auto ethnography, ecology, first-person-research, human-nature interaction, humanistic psychology, sustainability, video games},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642048,\nauthor = {Kirchner-Krath, Jeanine and Schmidt-Kraepelin, Manuel and Sch\\\"{o}bel, Sofia and Ullrich, Mathias and Sunyaev, Ali and Von Korflesch, Harald F. O.},\ntitle = {Outplay Your Weaker Self: A Mixed-Methods Study on Gamification to Overcome Procrastination in Academia},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642048},\ndoi = {10.1145/3613904.3642048},\nabstract = {Procrastination is the deliberate postponing of tasks knowing that it will have negative consequences in the future. Despite the potentially serious impact on mental and physical health, research has just started to explore the potential of information systems to help students combat procrastination. Specifically, while existing learning systems increasingly employ elements of game design to transform learning into an enjoyable and purposeful adventure, little is known about the effects of gameful approaches to overcome procrastination in academic settings. This study advances knowledge on gamification to counter procrastination by conducting a mixed-methods study among higher education students. Our results shed light on usage patterns and outcomes of gamification on self-efficacy, self-control, and procrastination behaviors. The findings contribute to theory by providing a better understanding of the potential of gamification to tackle procrastination. Practitioners are supported by implications on how to design gamified learning systems to support learners in self-organized work.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {523},\nnumpages = {19},\nkeywords = {Gameful design, Gamification, Higher education, Learning, Procrastination, Task management},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642804,\nauthor = {Martinez, Jesse J and Froehlich, Jon E. and Fogarty, James},\ntitle = {Playing on Hard Mode: Accessibility, Difficulty and Joy in Video Game Adoption for Gamers with Disabilities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642804},\ndoi = {10.1145/3613904.3642804},\nabstract = {Video games often pose accessibility barriers to gamers with disabilities, yet there is no standard method for identifying which games have barriers, what those barriers are, and whether and how they can be overcome. We propose and explore three phases of the “game adoption process”: Discovery, Evaluation, and Adaptation. To advance understanding of how gamers with disabilities experience this process, the resources and strategies they use, and the challenges experienced, we conducted an interview study with thirteen gamers with disabilities with differing backgrounds. We then engage with existing theories of consequence-based accessibility, of difficulty, and of identity-based gaming to better understand how these processes manifest “access difficulty” and to characterize the experience of “disabled gaming.” Finally, we present design recommendations for game developers and distributors to better support gamers with disabilities in the game adoption process by engaging with community-made resources, supporting socially-created access, and creating customizable experiences with opportunities for unconventional play.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {524},\nnumpages = {17},\nkeywords = {accessibility, adaptation, difficulty, video games},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642809,\nauthor = {Morris, Brooke and Havlucu, Hayati and Oldfield, Alison and Metatla, Oussama},\ntitle = {Understanding Neurodiverse Social Play Between Autistic and Non-Autistic Children},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642809},\ndoi = {10.1145/3613904.3642809},\nabstract = {Social play supports children to develop essential life skills and foster friendships. However, autistic and non-autistic children often do not have equal opportunities to engage in social play. Previous research to improve these opportunities tends to invoke social skill interventions solely for autistic children or is focused on designing for only one group, rather than considering the interactions or needs of all children in neurodiverse groups1. In order to understand the different experiences of children during social play, we conducted interviews with 6 professionals who support neurodiverse social play and undertook observation sessions of 36 autistic and non-autistic children during unstructured social play. Our findings move beyond the existing characterizations of autistic social play and build upon the double empathy problem to capture and consider the needs of all children in neurodiverse playgroups. We argue these findings could be used to inform future neurodiverse social play technology design in HCI.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {525},\nnumpages = {16},\nkeywords = {Autism, Double Empathy Problem, Neurodiverse Play, Social Play},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642163,\nauthor = {Rudnik, John and Raghuraj, Sharadhi and Li, Mingyi and Brewer, Robin N.},\ntitle = {CareJournal: A Voice-Based Conversational Agent for Supporting Care Communications},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642163},\ndoi = {10.1145/3613904.3642163},\nabstract = {Effective communication between older adult care recipients and unpaid caregivers is essential to both care partners’ well-being. To understand communication in care relationships, we conducted a two-part study with older adult care recipients and caregivers. First, we conducted a two-week diary study to gain insight into care-related communication challenges. While caregivers discussed the benefits of emotional attachment, care recipients expressed concerns about emotional fluctuation and losing autonomy. These findings, along with literature on self-disclosure and conversational scaffolding informed our design of CareJournal—a voice-based conversational agent that supports care-related disclosure between care partners. We evaluated CareJournal with 40 care partners to inform future design considerations and learn more about their communication practices. Our findings highlight the impact of distance and tensions between care and independence, providing insight into how care partners imagine computer-mediated care communication impacting their relationships.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {526},\nnumpages = {22},\nkeywords = {care receiving, caregiving, conversational scaffolding, evaluative disclosure, older adults, voice assistants},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642570,\nauthor = {S\\\"{a}uberli, Andreas and Holzknecht, Franz and Haller, Patrick and Deilen, Silvana and Schiffl, Laura and Hansen-Schirra, Silvia and Ebling, Sarah},\ntitle = {Digital Comprehensibility Assessment of Simplified Texts among Persons with Intellectual Disabilities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642570},\ndoi = {10.1145/3613904.3642570},\nabstract = {Text simplification refers to the process of increasing the comprehensibility of texts. Automatic text simplification models are most commonly evaluated by experts or crowdworkers instead of the primary target groups of simplified texts, such as persons with intellectual disabilities. We conducted an evaluation study of text comprehensibility including participants with and without intellectual disabilities reading unsimplified, automatically and manually simplified German texts on a tablet computer. We explored four different approaches to measuring comprehensibility: multiple-choice comprehension questions, perceived difficulty ratings, response time, and reading speed. The results revealed significant variations in these measurements, depending on the reader group and whether the text had undergone automatic or manual simplification. For the target group of persons with intellectual disabilities, comprehension questions emerged as the most reliable measure, while analyzing reading speed provided valuable insights into participants’ reading behavior.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {527},\nnumpages = {11},\nkeywords = {Automatic text simplification, digital testing, evaluation, intellectual disabilities, readability, reading comprehension},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642562,\nauthor = {Watson, Colin and Crivellaro, Clara and Parnaby, Adam W and Kharrufa, Ahmed},\ntitle = {Hostile Systems: A Taxonomy of Harms Articulated by Citizens Living with Socio-Economic Deprivation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642562},\ndoi = {10.1145/3613904.3642562},\nabstract = {There is increasing interest in how digitalisation variously impacts different socio-economic demographics’ ability to access, and realise benefits from, public services. Centring citizens’ lived experience in the identification of harms and benefits is critical for the evaluation of digital services, and more broadly for responsible innovation. Yet this poses significant challenges, particularly when engaging those living in precarious conditions. This paper reports on a study that engaged citizens living with poverty (n=76) to articulate harms arising from digitalisation in the context of an e-government social protection service. Interviews and surveys supported by speculative scenarios of ongoing changes helped surface and express citizen-centric harm characteristics within wider ecosystems before, during and after access, beyond a narrower service-lifecycle viewpoint. Drawing on the findings, we develop a taxonomy of harms and discuss how this can be utilised by HCI practitioners concerned with responsible innovation in digital welfare contexts.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {528},\nnumpages = {17},\nkeywords = {Responsible innovation, access, e-government, ethics, harms, poverty, service design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642421,\nauthor = {Albers, Ruben and Hassenzahl, Marc},\ntitle = {Let’s Talk About Death: Existential Conversations with Chatbots},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642421},\ndoi = {10.1145/3613904.3642421},\nabstract = {Many people prefer not to think about their own death, let alone talk about it. This contributes to fear of death and reduces the acceptance of its inevitability. We hypothesized that talking about one’s own death with a specially designed chatbot reduces fear of death and strengthens the confidence to discuss the topic further with loved ones. Participants (N=100) talked with the chatbot for an average of 25 minutes. It offered conversations about planning for one’s own death, end-of-life preferences, and hopes for the afterlife. We measured participants’ fear and acceptance of death (DAP-R questionnaire) and readiness for end-of-life conversation (REOLC questionnaire) before and after the chat. Overall, attitudes toward death improved and fear decreased, while readiness for end-of-life conversations increased. Bigger changes in attitude corresponded with longer, more reflective responses in the conversations, commitment to plans, finding meaning in death, and some notion of legacy or afterlife.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {529},\nnumpages = {14},\nkeywords = {conversational agent, death, dying, end of life, thanato-technology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642200,\nauthor = {Petterson, Adrian and Jaimes Rodriguez, Isabella and Doggett, Olivia and Chandra, Priyank},\ntitle = {Networks of care in digital domestic labour economies},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642200},\ndoi = {10.1145/3613904.3642200},\nabstract = {Care work has long been relegated to private households and small communities, however, with the entry of digital marketplaces, it is becoming part of public economic spheres. While care work has been generally devalued and understudied, it is a complex practice embedded in a network of economic transactions, social relations, material conditions, and socio-cultural norms. This paper explores the care giving networks among migrant house-cleaners guided by Tronto’s ‘care ethics’ and Puig de la Bellacasa’s ‘matters of care’. We interviewed 19 Latino house-cleaners in Toronto to understand their care practices and networks. Our analysis identifies gaps in our participants’ care networks. We create a new term, lateral care, to explicate the digital communities of care practice our participants formed. We conclude with implications for the future design of technologies for labor economies that attend to concerns of care.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {530},\nnumpages = {16},\nkeywords = {Feminist HCI, care, digital communities, migrant labor},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642584,\nauthor = {Claisse, Caroline and Durrant, Abigail C and Lie, Mabel},\ntitle = {Understanding Antenatal Care Needs through Co-Creation with Roma Women to Inform the Design of mHealth Technologies},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642584},\ndoi = {10.1145/3613904.3642584},\nabstract = {Women from the Roma community experience significant health disparities during their pregnancy. Whilst Mobile Health (mHealth) technologies have the potential to improve antenatal care experiences and health outcomes, research on women from ethnically marginalised backgrounds in developed countries remains limited. We report on a series of Co-creation Workshops with 11 Roma women who have settled in the North of England. In this paper, we present thematic insights about their experiences and needs during pregnancy, and their perceptions and attitudes towards digital technologies to inform the design of culturally sensitive mHealth. We contribute to Human Computer Interaction (HCI) with new empirical research to discourses on Critical Digital Health, Intersectional HCI and Women-centred Design, highlight implications for design and encourage a more critical and intersectional design approach to accommodate better the experiences of ethnically marginalised groups whose needs arguably tend to be overlooked and stereotyped.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {531},\nnumpages = {16},\nkeywords = {Antenatal Care, Co-Creation, Design Implications, Intersectionality, Marginalised Groups, Pregnancy Journey, mHealth},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642898,\nauthor = {Lei, Ying and Ma, Shuai and Sun, Yuling},\ntitle = {Unpacking ICT-supported Social Connections and Support of Late-life Migration: From the Lens of Social Convoys},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642898},\ndoi = {10.1145/3613904.3642898},\nabstract = {Migration and aging-related dilemmas have limited the opportunities for late-life migrants to rebuild social connections and access support. While research on migrants has drawn increasing attention in HCI, limited attention has been paid to the increasing number of late-life migrants. This paper reports a qualitative study examining the social connections and support of late-life migrants. In particular, drawing on the social convoy model, we pay specific attention to the dynamic changes of late-life migrants’ social convoy, the supporting roles each convoy plays, the functions ICT plays in the process, as well as the encountered challenges and expectations of late-life migrants regarding ICT-supported social convoys. Based on these findings, we deeply discuss the role of the social convoy in supporting more targeted social support for late-life migrants, as well as broader migrant communities. Finally, we offer late-life migrant-oriented design considerations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {532},\nnumpages = {15},\nkeywords = {China, ICT, Late-life Migrant, Older Drifter, Qualitative Research, Social Connection, Social Convoy Model, Social Support},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641922,\nauthor = {Stapleton, Logan and Liu, Sunniva and Liu, Cindy and Hong, Irene and Chancellor, Stevie and Kraut, Robert E and Zhu, Haiyi},\ntitle = {\"If This Person is Suicidal, What Do I Do?\": Designing Computational Approaches to Help Online Volunteers Respond to Suicidality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641922},\ndoi = {10.1145/3613904.3641922},\nabstract = {Online platforms provide support for many kinds of distress, including suicidal thoughts and behaviors. However, because many platforms restrict suicidal talk, volunteers on these platforms struggle with how to help suicidal people who come for support. We interviewed 11 volunteer counselors in a large online support platform, including after they role-played conversations with varying severities of suicidality, to explore practices and challenges when identifying and responding to suicidality. We then presented Speed Dating design concepts around emotional preparation and support, real-time guidance, training, and suicide detection. Participants wanted more support and preparation for conversations with suicidal people, but were conflicted about AI-based technologies, including trade-offs between potential benefits of conversational agents for training and limitations of prediction or real-time response suggestions, due to the sensitive, context-dependent decisions that volunteers must make. Our work has important implications for nuanced considerations and design choices around developing digital mental health technologies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {533},\nnumpages = {21},\nkeywords = {artificial intelligence, mental health, moderation, online community, social support, suicidality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642916,\nauthor = {Zghab, Souleima and Pag\\'{e}, Gabrielle and Lussier, M\\'{e}lanie and B\\'{e}dard, Sylvain and Cheng, Jinghui},\ntitle = {\"It's Sink or Swim\": Exploring Patients' Challenges and Tool Needs for Self-Management of Postoperative Acute Pain},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642916},\ndoi = {10.1145/3613904.3642916},\nabstract = {Poorly managed postoperative acute pain can have long-lasting negative impacts and pose a major healthcare issue. There is limited investigation to understand and address the unique needs of patients experiencing acute pain. In this paper, we tackle this gap through an interview study with 14 patients who recently underwent postoperative acute pain to understand their challenges in pain self-management and their need for supportive tools. Our analysis identified various factors associated with the major aspects of acute pain self-management. Together, our findings indicated that tools for supporting these patients need to carefully consider information and support delivery to adapt to rapid changes in pain experiences, offer personalized and dynamic assistance that adapts to individual situations in context, and monitor emotion when promoting motivation. Overall, our work provided valuable knowledge to address the less-investigated but highly-needed problem of designing technology for the self-management of acute pain and similar health conditions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {534},\nnumpages = {11},\nkeywords = {Acute Pain, Consumer Health, Pain Self-Management},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642388,\nauthor = {Barbareschi, Giulia and Kumar, Tarika and Kim, Christopher Changmok and Chernyshov, George and Kunze, Kai},\ntitle = {\"Speech is Silver, Silence is Golden \" Analyzing Micro-communication Strategies between Visually Impaired Runners and their Guides},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642388},\ndoi = {10.1145/3613904.3642388},\nabstract = {Running and jogging are popular activities for many visually impaired individuals thanks to the relatively low entry barriers. Research in HCI and beyond has focused primarily on leveraging technology to enable visually impaired people to run independently. However, depending on their residual vision and personal preferences, many chose to run with a sighted guide. This study presents a comprehensive analysis of the partnership between visually impaired runners and sighted guides. Using a combination of interaction and thematic analysis on video and interview data from 6 pairs of runners and guides, we unpack the complexity and directionality of three layers of vocal communication (directive, contextual, and recreational) and distinguish between intentional and unintentional corporeal communication. Building on the understanding of the importance of synchrony we also present some exploratory data looking at physiological synchrony between 2 pairs of runners with different level of experience and articulate recommendations for the HCI community.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {535},\nnumpages = {16},\nkeywords = {Visually impaired, blind, guide, interdependence, running, sport, synchrony},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641950,\nauthor = {Sidnam-Mauch, Emily and Monge, Peter},\ntitle = {Individual Differences and Technology Affordances Combine to Predict Mobile Social Media Distraction Behaviors and Consequences},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641950},\ndoi = {10.1145/3613904.3641950},\nabstract = {Drawing upon theories from communication studies and cognitive psychology, this research develops a multitheoretical model that identifies human and technological factors that predict social media distraction engagement and explains how social media distractions can lead to negative consequences across various tasks. This model is empirically tested using data from a survey of U.S. mobile phone users (N = 1,026). The results from a structural equation modeling analysis support the model’s predictions that a person’s age, fear of missing out, smartphone checking habit strength, and the number of social media applications with notifications enabled all impact a variety of distraction behaviors and consequences. The findings show that communication technology distraction behavior is influenced by a complex intertwining of goal-driven communication and information-seeking behaviors, automatic processes in the brain, and technology affordances.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {536},\nnumpages = {18},\nkeywords = {Computer Mediated Communication, Health - Wellbeing, Social Media/Online Communities, Tasks/Interruptions/Notification},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642444,\nauthor = {M\\\"{a}der, Aurel Ruben and Meegahapola, Lakmal and Gatica-Perez, Daniel},\ntitle = {Learning About Social Context From Smartphone Data: Generalization Across Countries and Daily Life Moments},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642444},\ndoi = {10.1145/3613904.3642444},\nabstract = {Understanding how social situations unfold in people’s daily lives is relevant to designing mobile systems that can support users in their personal goals, well-being, and activities. As an alternative to questionnaires, some studies have used passively collected smartphone sensor data to infer social context (i.e., being alone or not) with machine learning models. However, the few existing studies have focused on specific daily life occasions and limited geographic cohorts in one or two countries. This limits the understanding of how inference models work in terms of generalization to everyday life occasions and multiple countries. In this paper, we used a novel, large-scale, and multimodal smartphone sensing dataset with over 216K self-reports collected from 581 young adults in five countries (Mongolia, Italy, Denmark, UK, Paraguay), first to understand whether social context inference is feasible with sensor data, and then, to know how behavioral and country-level diversity affects inferences. We found that several sensors are informative of social context, that partially personalized multi-country models (trained and tested with data from all countries) and country-specific models (trained and tested within countries) can achieve similar performance above 90\\% AUC, and that models do not generalize well to unseen countries regardless of geographic proximity. These findings confirm the importance of the diversity of mobile data, to better understand social context inference models in different countries.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {537},\nnumpages = {18},\nkeywords = {context-awareness, digital health, generalization, machine learning, mobile sensing, multimodal, personalization, smartphone sensing, social context},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642463,\nauthor = {Madapura Nagaraj, Preetham and Mo, Wen and Holloway, Catherine},\ntitle = {Mindfulness-based Embodied Tangible Interactions for Stroke Rehabilitation at Home},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642463},\ndoi = {10.1145/3613904.3642463},\nabstract = {Current approaches to designing technologies for stroke rehabilitation at home show great promise using either mindfulness-based interventions or embodied tangible interactions. However, there is an untapped potential in integrating these approaches and a lack of understanding of how to embody aspects of mindfulness in tangible interactions for stroke rehabilitation. We report the first explicit effort to explore this dimension by conducting semi-structured interviews and co-design sessions involving four physiotherapists and four mindfulness experts. The major themes ‘Awareness – The essence of mindfulness’ and ‘Tactile sensations – A pathway to mindfulness’ point us towards new ways to embody mindfulness in tangible interactions to address stroke rehabilitation challenges. This work introduces a novel approach to designing technology called ‘Mindfulness-based Embodied Tangible Interactions’ (MBETI). We present five key design principles such as 'Design to support mindful awareness’ and ‘Design for Comfort’ while discussing the future research opportunities of assistive technologies for stroke rehabilitation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {538},\nnumpages = {16},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642477,\nauthor = {Pendse, Sachin R and Kumar, Neha and De Choudhury, Munmun},\ntitle = {Quantifying the Pollan Effect: Investigating the Impact of Emerging Psychiatric Interventions on Online Mental Health Discourse},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642477},\ndoi = {10.1145/3613904.3642477},\nabstract = {Psychedelic-assisted therapy has shown significant promise in alleviating treatment-resistant mental illness, prompting excitement among people with lived experience of mental illness. The emerging collective perception of psychedelics as tools for mental health has been dubbed the Pollan Effect. We investigate whether the Pollan Effect carries to online community discussions concerning psilocybin-containing mushrooms (PCMs). Through a matched computational analysis of 676,875 longform Reddit posts describing PCM use spanning a decade, we provide evidence of the Pollan Effect in terms of increased health discourse around PCMs following two inception points—release of a book and subsequent documentary on PCMs. We then introduce the notion of a Pollan shift, which we witness through increased collective sharing of emotional and social experiences following the two inception points. Our findings offer insights into how online discourse could be representative of emerging social movements around new psychiatric treatments, and the role of platforms in sensemaking and research.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {539},\nnumpages = {22},\nkeywords = {collective sensemaking, mental health, psychedelic science, social movements},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641894,\nauthor = {Milton, Ashlee and Maestre, Juan F. and Roy, Abhishek and Umbach, Rebecca and Chancellor, Stevie},\ntitle = {Seeking in Cycles: How Users Leverage Personal Information Ecosystems to Find Mental Health Information},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641894},\ndoi = {10.1145/3613904.3641894},\nabstract = {Information is crucial to how people understand their mental health and well-being, and many turn to online sources found through search engines and social media. We present an interview study (n = 17) of participants who use online platforms to seek information about their mental illnesses. Participants use their personal information ecosystems in a cyclical process to find information. This cycle is driven by the adoption of new information and questioning the credibility of information. Privacy concerns fueled by perceptions of stigma and platform design also influence their information-seeking decisions. Our work proposes theoretical implications for social computing and information retrieval on information seeking in users’ personal information ecosystems. We offer design implications to support users in navigating personal information ecosystems to find mental health information.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {540},\nnumpages = {16},\nkeywords = {Information Seeking, Mental Health, Social Media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642557,\nauthor = {Ezer, Talia Sofia and Giron, Jonathan and Erel, Hadas and Zuckerman, Oren},\ntitle = {Somaesthetic Meditation Wearable: Exploring the Effect of Targeted Warmth Technology on Meditators' Experiences},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642557},\ndoi = {10.1145/3613904.3642557},\nabstract = {Mindfulness meditation has vast benefits, yet is challenging for many. We designed a novel targeted warmth somaesthetic wearable and evaluated how the thermal sensation is perceived during meditation. In a qualitative study, twenty participants explored the wearable during meditation. Findings reveal participants’ rich experiences, sensations, and feelings. They perceived the technology as an appropriate tool for self-exploration. Even when participants initially felt the wearable was distracting their meditation process, they easily learned how to leverage it in their introspection process. We report on four potential roles for warmth technology: functional (pulling focal of attention), behavioral (motivating to \"get back to the practice\"), emotional (comforting during the lonely process), and therapeutic feelings. We conclude with design guidelines, highlighting that warmth is a promising technology for meditation if designed to encourage self-exploration of body sensations and emotions while not compromising the natural meditation practice.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {541},\nnumpages = {14},\nkeywords = {Introspection, Meditation, Somaesthetic, Targeted Warmth, Wearable},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642694,\nauthor = {Lopez Burst, Emily and Ciolfi Felice, Marianela and O'Kane, Aisling Ann},\ntitle = {Using and Appropriating Technology to Support The Menopause Journey in the UK},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642694},\ndoi = {10.1145/3613904.3642694},\nabstract = {The menopause transition has a direct impact on half of the global population, yet it has continued to be a stigmatised topic with limited focus on supporting it with technology. Whilst attention being given to menopause in HCI may be new, people experiencing it is not and people have adopted, adapted and appropriated technologies to support their menopause journey. In this questionnaire and interview study, we examine how people in the UK are using (and not using) existing general and menopause-specific technology to support themselves through the transition. Despite limited menopause-specific technologies available, participants have found novel uses of technologies such as social media and smartwatches for 1) connecting and sharing, 2) information seeking, 3) tracking and reflecting, and 4) self-care. This work contributes design considerations for menopause specific technologies, and design opportunities and challenges for technologies that can be appropriated to support menopause.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {542},\nnumpages = {14},\nkeywords = {Menopause, aging, appropriation, care, health, qualitative, real world use, women},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642383,\nauthor = {Tran, Tanh Quang and Langlotz, Tobias and Regenbrecht, Holger},\ntitle = {A Survey On Measuring Presence in Mixed Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642383},\ndoi = {10.1145/3613904.3642383},\nabstract = {Presence is a defining element of virtual reality (VR), but it is also increasingly used when assessing mixed reality (MR) experiences. The increased interest in measuring presence in MR and recent works underpinning the specific nature of presence in MR raise the question of the current state and practice of assessing presence in MR. To address this question, we present an analysis of more than 320 studies that report on presence measurements in MR. Our analysis showed that questionnaires are the dominant measurement but also identify problematic trends that stem from the lack of a generally agreed-upon concept or measurement for presence in MR. More specifically, we show that using measurements that are not validated in MR or custom questionnaires limiting the comparability of results is commonplace and could contribute to a looming replication crisis in an increasingly relevant field.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {543},\nnumpages = {38},\nkeywords = {Augmented Reality, Extended Reality, Mixed Reality, Sense of Presence, Spatial Presence, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642123,\nauthor = {Soler-Dominguez, Jose Luis and Navas-Medrano, Samuel and Pons, Patricia},\ntitle = {ARCADIA: A Gamified Mixed Reality System for Emotional Regulation and Self-Compassion},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642123},\ndoi = {10.1145/3613904.3642123},\nabstract = {Mental health and wellbeing have become one of the significant challenges in global society, for which emotional regulation strategies hold the potential to offer a transversal approach to addressing them. However, the persistently declining adherence of patients to therapeutic interventions, coupled with the limited applicability of current technological interventions across diverse individuals and diagnoses, underscores the need for innovative solutions. We present ARCADIA, a Mixed-Reality platform strategically co-designed with therapists to enhance emotional regulation and self-compassion. ARCADIA comprises several gamified therapeutic activities, with a strong emphasis on fostering patient motivation. Through a dual study involving therapists and mental health patients, we validate the fully functional prototype of ARCADIA. Encouraging results are observed in terms of system usability, user engagement, and therapeutic potential. These findings lead us to believe that the combination of Mixed Reality and gamified therapeutic activities could be a significant tool in the future of mental health.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {544},\nnumpages = {17},\nkeywords = {emotional regulation, gamification, mental health, mixed reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642412,\nauthor = {Medlar, Alan and Lehtikari, Mari Tatsuko and Glowacka, Dorota},\ntitle = {Behind the Scenes: Adapting Cinematography and Editing Concepts to Navigation in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642412},\ndoi = {10.1145/3613904.3642412},\nabstract = {Teleportation is a popular method of navigation in virtual reality (VR) because it does not induce symptoms of VR sickness, such as nausea and disorientation. However, teleportation may reduce spatial awareness, causing users to miss important aspects of their surroundings. We present ACTIVE, a novel approach to teleportation that uses techniques from cinematography to enhance the user experience of navigation in VR. ACTIVE adapts heuristics from continuity editing to dynamically reposition and reorient the camera after teleportation. This approach aims to improve the aesthetic quality of entities and environmental features while respecting users’ intended trajectory through the virtual environment. In a user study, we found that even though ACTIVE did not improve users’ recall of which entities were present in the environment, it increased engagement by significantly improving aesthetic appeal. Lastly, despite removing some agency from users, ACTIVE had no impact on presence or VR sickness compared to teleportation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {545},\nnumpages = {12},\nkeywords = {cinematography, multi-objective optimization, navigation, user engagement, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642281,\nauthor = {Bradwell, Hannah Louise and Cooper, Leonie and Baxter, Rory and Tomaz, Simone and Edwards, Katie Jane and Whittaker, Anna C and Jones, Ray B.},\ntitle = {Implementation of Virtual Reality Motivated Physical Activity via Omnidirectional Treadmill in a Supported Living Facility for Older Adults: A Mixed-Methods Evaluation.: Virtual reality to motivate physical activity for older adults},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642281},\ndoi = {10.1145/3613904.3642281},\nabstract = {Virtual reality (VR) can support healthy ageing, but few devices have been trialed with frail older adults to increase physical activity. We conducted a preliminary mixed-methods implementation evaluation of an omnidirectional VR treadmill and a static VR experience with seven older adults over a six-week period in a supported living facility. Frequency of use and pre-post physical functioning measures were collected, mainly to establish technology suitability based on person characteristics. Diary entries following technology use, resident focus group and staff interview revealed technology acceptance and perceived potential for increasing physical activity, health and wellbeing through accessing virtual environments, which motivated continued activity. Results demonstrated technology suitability for a range of older adults with various mobility and physical impairments. However, residents noted interest in a seated treadmill for physical activity without perceived risks of falls with standing treadmills. Staff raised considerations around care home implementations including usability, cost and space.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {546},\nnumpages = {13},\nkeywords = {Social care, innovation, physical activity, technology, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642658,\nauthor = {Kalus, Alexander and Klein, Johannes and Ho, Tien-Julian and Seegets, Lee-Ann and Henze, Niels},\ntitle = {MobileGravity: Mobile Simulation of a High Range of Weight in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642658},\ndoi = {10.1145/3613904.3642658},\nabstract = {Simulating accurate weight forces in Virtual Reality (VR) is an unsolved challenge. Therefore, providing real weight sensations by transferring liquid mass has emerged as a promising approach. However, key objectives conceptually interfere with each other. In particular, previous designs that support a high range of weight or high flow rate lack mobility. In this work, we present MobileGravity, a system, that decouples the weight-changing object from the liquid supply and the pump. It enables weight changes of up to 1 kg at a rate of 235 g/s and allows the user to walk around freely. Through a study with 30 participants, we show that the system enables users to perceive the weight of different virtual objects and enhances realism, as well as enjoyment.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {547},\nnumpages = {13},\nkeywords = {haptics, virtual reality, weight perception, weight simulation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641933,\nauthor = {Feick, Martin and Zenner, Andr\\'{e} and Seibert, Simon and Tang, Anthony and Kr\\\"{u}ger, Antonio},\ntitle = {The Impact of Avatar Completeness on Embodiment and the Detectability of Hand Redirection in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641933},\ndoi = {10.1145/3613904.3641933},\nabstract = {To enhance interactions in VR, many techniques introduce offsets between the virtual and real-world position of users’ hands. Nevertheless, such hand redirection (HR) techniques are only effective as long as they go unnoticed by users—not disrupting the VR experience. While several studies consider how much unnoticeable redirection can be applied, these focus on mid-air floating hands that are disconnected from users’ bodies. Increasingly, VR avatars are embodied as being directly connected with the user’s body, which provide more visual cue anchoring, and may therefore reduce the unnoticeable redirection threshold. In this work, we studied more complete avatars and their effect on the sense of embodiment and the detectability of HR. We found that higher avatar completeness increases embodiment, and we provide evidence for the absence of practically relevant effects on the detectability of HR.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {548},\nnumpages = {9},\nkeywords = {Virtual reality, avatar embodiment, detection thresholds, hand redirection, illusions},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642272,\nauthor = {Yamanaka, Shota and Usuba, Hiroki and Sato, Junichi},\ntitle = {Behavioral Differences between Tap and Swipe: Observations on Time, Error, Touch-point Distribution, and Trajectory for Tap-and-swipe Enabled Targets},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642272},\ndoi = {10.1145/3613904.3642272},\nabstract = {Existing guidelines for designing targets on smartphones often focus on single-tap operations for accurate selection. However, smartphone interfaces can support both tap and swipe actions. We explored user-performance differences between tap and swipe in two crowdsourced experiments using bar and square targets. Results indicated longer operation times, higher error rates, and significantly shifted touch points for swipe compared to tap. Our findings imply that current target-size guidelines may not apply to swipe-operated targets, and they reveal new research opportunities for swipeable-target designs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {549},\nnumpages = {12},\nkeywords = {Human motor performance, endpoint distribution, error rate prediction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642687,\nauthor = {Hosseini, Masoumehsadat and Mueller, Heiko and Boll, Susanne},\ntitle = {Controlling the Rooms: How People Prefer Using Gestures to Control Their Smart Homes},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642687},\ndoi = {10.1145/3613904.3642687},\nabstract = {Gesture interactions have become ubiquitous, and with increasingly reliable sensing technology we can anticipate their use in everyday environments such as smart homes. Gestures must meet users’ needs and constraints in diverse scenarios to gain widespread acceptance. Although mid-air gestures have been proposed in various user contexts, it is still unclear to what extent users want to integrate them into different scenarios in their smart homes, along with the motivations driving this desire. Furthermore, it is uncertain whether users will remain consistent in their suggestions when transitioning to alternative scenarios within a smart home. This study contributes methodologically by adapting a bottom-up frame-based design process. We offer insights into preferred devices and commands in different smart home scenarios. Using our results, we can assist in designing gestures in the smart home that are consistent with individual needs across devices and scenarios, while maximizing the reuse and transferability of gestural knowledge.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {550},\nnumpages = {18},\nkeywords = {Gesture elicitation, agreement rate, scenario-based interaction, smart home, study design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642183,\nauthor = {Jaber, Razan and Zhong, Sabrina and Kuoppam\\\"{a}ki, Sanna and Hosseini, Aida and Gessinger, Iona and Brumby, Duncan P and Cowan, Benjamin R. and Mcmillan, Donald},\ntitle = {Cooking With Agents: Designing Context-aware Voice Interaction},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642183},\ndoi = {10.1145/3613904.3642183},\nabstract = {Voice Agents (VAs) are touted as being able to help users in complex tasks such as cooking and interacting as a conversational partner to provide information and advice while the task is ongoing. Through conversation analysis of 7 cooking sessions with a commercial VA, we identify challenges caused by a lack of contextual awareness leading to irrelevant responses, misinterpretation of requests, and information overload. Informed by this, we evaluated 16 cooking sessions with a wizard-led context-aware VA. We observed more fluent interaction between humans and agents, including more complex requests, explicit grounding within utterances, and complex social responses. We discuss reasons for this, the potential for personalisation, and the division of labour in VA communication and proactivity. Then, we discuss the recent advances in generative models and the VAs interaction challenges. We propose limited context awareness in VAs as a step toward explainable, explorable conversational interfaces.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {551},\nnumpages = {13},\nkeywords = {conversation analysis, conversational user interfaces, cooking, voice interfaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642022,\nauthor = {Zhao, Kaixing and Wu, Chaoyi and Xu, Tao and He, Liang and Serrano, Marcos and Roudaut, Anne},\ntitle = {Grip-Reach-Touch-Repeat: A Refined Model of Grasp to Encompass One-Handed Interaction with Arbitrary Form Factor Devices},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642022},\ndoi = {10.1145/3613904.3642022},\nabstract = {We extend grasp models to encompass one-handed interaction with arbitrary shaped touchscreen devices. Current models focus on how objects are stably held by external forces. However, with touchscreen devices, we postulate that users do a trade-off between holding securely and exploring interactively. To verify this, we first conducted a qualitative study which asked participants to grasp 3D printed objects while considering its different interactivity. Results of the study confirm our hypothesis and reveal obvious change in postures. To further verify this trade-off and design interactions, we developed a simulation software capable of computing the stability of a grasp and its reachability. We conducted the second study based on the observed predominant grasps to validate our software with a glove. Results also confirm a consistent trade-off between stability and reachability. We conclude by discussing how this research can help designing computational tools focusing on hand-held interactions with arbitrary shaped touchscreen devices.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {552},\nnumpages = {17},\nkeywords = {Freeform devices, grasp, handheld interaction, reachability, simulation.},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642037,\nauthor = {Coutrix, C\\'{e}line and Prost, Cam\\'{e}lia},\ntitle = {Impact of Fingernails Length on Mobile Tactile Interaction},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642037},\ndoi = {10.1145/3613904.3642037},\nabstract = {Mobile users have fingernails of different lengths. This paper measures the impact of fingernail length on the use of tactile mobile phones. We first conducted interviews with participants wearing long fingernails. They reported difficulties and non-satisfactory coping strategies to hold their phone securely and acquire targets accurately. We then conducted three experiments comparing different lengths of fingernails (0 mm, 5 mm, and 10 mm). Our results quantify the drop in comfort and efficiency. We measured the range of incidental pitch angle on the surface, the comfortable and useful area of the thumb, and the target acquisition efficiency. 10 mm fingernails consistently decrease by 57\\% the range of the finger pitch angle, by 36\\% the comfortable area of the thumb, and by 24\\% the throughput when acquiring targets. This paper contributes guidelines for future inclusive devices and techniques to also support users with long fingernails.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {553},\nnumpages = {21},\nkeywords = {Comfortable area of the thumb., Fingernails, Fitts pointing study, Interviews, Length, Mobile, Tactile interaction, Touchscreen},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642637,\nauthor = {Gori, Julien and Fruchard, Bruno and Bailly, Gilles},\ntitle = {Model-based Evaluation of Recall-based Interaction Techniques},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642637},\ndoi = {10.1145/3613904.3642637},\nabstract = {This article tackles two challenges of the empirical evaluation of interaction techniques that rely on user memory, such as hotkeys, here coined Recall-based interaction techniques (RBITs): (1) the lack of guidance to design the associated study protocols, and (2) the difficulty of comparing evaluations performed with different protocols. To address these challenges, we propose a model-based evaluation of RBITs. This approach relies on a computational model of human memory to (1) predict the informativeness of a particular protocol through the variance of the estimated parameters (Fisher Information) (2) compare RBITs recall performance based on the inferred parameters rather than behavioral statistics, which has the advantage of being independent of the study protocol. We also release a Python library implementing our approach to aid researchers in producing more robust and meaningful comparisons of RBITs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {554},\nnumpages = {16},\nkeywords = {experimental design, fisher information, interaction technique, maximum likelihood estimation, memory, model-based evaluation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642028,\nauthor = {Andrei, Alexandru-Tudor and Bilius, Laura-Bianca and Vatavu, Radu-Daniel},\ntitle = {Take a Seat, Make a Gesture: Charting User Preferences for On-Chair and From-Chair Gesture Input},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642028},\ndoi = {10.1145/3613904.3642028},\nabstract = {We explore the chair as a referential frame for facilitating hand gesture input to control interactive systems. First, we conduct a Systematic Literature Review on the topic of interactions supported by chairs, and uncover little research on harnessing everyday chairs for input, limited to chair rotation and tilting movements. Subsequently, to understand end users’ preferences for gestures performed on the chair’s surface (i.e., on-chair gestures) and in the space around the chair (i.e., from-chair gestures), we conduct an elicitation study involving 54 participants, 3 widespread chair variations—armchair, office-chair, and stool,—and 15 referents encompassing common actions, digital content types, and navigation commands for interactive systems. Our findings reveal a preference for unimanual gestures implemented with strokes, hand poses, and touch input, with specific nuances and kinematic profiles according to the chair type. Based on our findings, we propose a range of implications for interactive systems leveraging on-chair and from-chair gestures.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {555},\nnumpages = {17},\nkeywords = {Gesture interaction, chairs, gesture elicitation study, mid-air gestures, surface gestures, touch},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642590,\nauthor = {Mwesigwa, Daniel and Cs\\'{\\i}kszentmih\\'{a}lyi, Christopher},\ntitle = {Air/time Travel: Rethinking Appropriation in Global HCI and Futures of Electronic Exchange},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642590},\ndoi = {10.1145/3613904.3642590},\nabstract = {This paper reexamines appropriation in human-computer interaction (HCI), which refers to the unexpected alterations made to artifacts by users. We analyze when earlier informal practices of exchanging airtime for cash became enclosed into proprietary mobile money platforms, and show that this enclosure has a longer history in global telecommunications. Building on interviews with 19 experts in computing, policy, and media, we challenge teleological narratives of the inevitability of mobile money often overlooked in computing and global development. We develop an ‘appropriation matrix’ introducing a dialectic of re- and reverse- appropriation animated by three elements—users, artifacts, and imaginaries—that unexpectedly switch between production and consumption, complicating invention and innovation in formal and informal economies. This matrix may help HCI and development better understand how different values, visions, and practices might have led (or could still lead) to different designs of products like mobile money.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {556},\nnumpages = {21},\nkeywords = {airtime, appropriation, design, history, mobile money, speculation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641957,\nauthor = {Dhawka, Priya and Perera, Lauren and Willett, Wesley},\ntitle = {Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641957},\ndoi = {10.1145/3613904.3641957},\nabstract = {We explore the potential of generative AI text-to-image models to help designers efficiently craft unique, representative, and demographically diverse anthropographics that visualize data about people. Currently, creating data-driven iconic images to represent individuals in a dataset often requires considerable design effort. Generative text-to-image models can streamline the process of creating these images, but risk perpetuating designer biases in addition to stereotypes latent in the models. In response, we outline a conceptual workflow for crafting anthropographic assets for visualizations, highlighting possible sources of risk and bias as well as opportunities for reflection and refinement by a human designer. Using an implementation of this workflow with Stable Diffusion and Google Colab, we illustrate a variety of new anthropographic designs that showcase the visual expressiveness and scalability of these generative approaches. Based on our experiments, we also identify challenges and research opportunities for new AI-enabled anthropographic visualization tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {557},\nnumpages = {14},\nkeywords = {demographic data, diverse anthropographics, generative models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642547,\nauthor = {Chauhan, Aarjav and Soden, Robert},\ntitle = {Commoning as a Strategy for HCI Research and Design in South Asia},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642547},\ndoi = {10.1145/3613904.3642547},\nabstract = {Commons emerge and are reclaimed through collective, shared, and self-organized practices known as commoning. Despite their historical embeddedness in South Asian communities, commoning practices have succumbed to enclosure and destruction due to region-wide privatization and development schemes implemented over the past century. Certain HCI and ICTD research has critiqued such schemes for undermining community autonomy and well-being. To advance the development of alternative models, we conducted a literature review of HCI research involving the commons, considering both natural and digital resources, in South Asia. Additionally, we examine the social practices, rules, and institutional arrangements described in the corpus through the lens of Elinor Ostrom’s design principles for commons governance. Based on our findings, we formulate a commoning framework by synthesizing three areas of HCI research — infrastructuring, participatory design, and assets-based design — proposing it as an alternative to neoliberal development paradigms for future HCI research.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {558},\nnumpages = {18},\nkeywords = {HCI for development, Meta-Analysis/Literature Survey, commoning, commons},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641990,\nauthor = {Zhao, Yuchao},\ntitle = {Design With Rural-To-Urban Migrant Women: Opportunities and Challenges in Designing within a Precarious Marriage Context in South China},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641990},\ndoi = {10.1145/3613904.3641990},\nabstract = {This study elucidates the challenges faced by married migrant women (MMW) in South China in relation to design. Through 2-year ethnographic fieldwork and semi-structured interviews with 15 participants, I aim to understand the context and intricacies of their systemic marital problems that form the backdrop against which mobile technology design could occur. I discuss how they tactically appropriate cellphones to negotiate love, sex, and marriage while remaining stuck in gendered patterns of technological use. The marital issues raised by the participants concerning the place of technology could provide HCI researchers with valuable guidance. “Design with” implies avoiding an elite perspective, eschewing a top-down approach, and steering clear of a condescending attitude in technical design. Designers should act as collaborators, assisting MMW in uncovering and nurturing their values, collective traits, and life experiences, which could ultimately nourish the future development of MMW and the migrant community. Potential considerations include a better understanding of national history and local ecosystems, recognizing MMW's agency and initiative in technology design and decision-making, valuing and learning from their “alternative” experiences of using technologies, and design beyond individual users for a more equal and safer environment.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {559},\nnumpages = {14},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3643044,\nauthor = {Longdon, Joycelyn and Westerlaken, Michelle and Blackwell, Alan F and Gabrys, Jennifer and Ossom, Benjamin and Ashton-Butt, Adham and Acheampong, Emmanuel},\ntitle = {Justice-oriented Design Listening: Participatory Ecoacoustics with a Ghanaian Forest Community},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3643044},\ndoi = {10.1145/3613904.3643044},\nabstract = {Despite a long tradition of ‘non-expert’ participation in ecoacoustics research, asymmetrical distribution of resources and engagement between the Global North and Global South continue, extending to ecoacoustic sensing and design. Whilst there exists a growing body of work in Participatory Design (PD) addressing the technical and social challenges of ecoacoustic research, we find that popular PD methods inadequately address design justice and decolonising agendas. Through participatory ecoacoustic sensing and design engagements with a forest community in Ghana, we highlight the tensions that emerge when employing visual and written modes of PD in a context where an oral approach to creativity and communication is more appropriate. We present Justice-oriented Design Listening, an acoustically-mediated approach to PD, described through three modes: polyphony, pace and transformation. This work contributes to calls for design justice by developing a methodological approach that facilitates pluralistic participation in design when developing conservation technologies in non-Western contexts.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {560},\nnumpages = {12},\nkeywords = {Conservation technologies, decolonial design methods, design justice, ecoacoustics, participatory design, participatory sensing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642039,\nauthor = {Zhou, Jiwei and Doubrovski, Zjenja and Giaccardi, Elisa and Karana, Elvin},\ntitle = {Living with Cyanobacteria: Exploring Materiality in Caring for Microbes in Everyday Life},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642039},\ndoi = {10.1145/3613904.3642039},\nabstract = {Materiality of artefacts holds the potential to intricately and dynamically shape our daily practices. We posit this capacity can be harnessed in fostering creative unfolding of everyday care practices towards living artefacts. To explore this premise, we designed a cyanobacterial living artefact with air purifying capacity, and invited eight participants to live with and care for it for two weeks. The artefact can be situated in diverse locations within domestic spaces, wherever the participant would consider air purification necessary and certain lighting conditions beneficial for the artefact’s vitality. This versatility is supported by the artefact’s colour-changing, pliable, adhesive, and suspendable nature. We analysed visual documentation and semi-structured interviews of participants’ experiences of the artefact. Our findings suggest distinct roles of materiality for care regarding labour, knowledge, and exploration. We further highlight the intricate design space encompassing openness, temporalities and semantic fitness towards nurturing mutualistic care in human-microbe interactions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {561},\nnumpages = {20},\nkeywords = {Biological-HCI, Care, Cyanobacteria, Human-Microbe Interactions, Living Artefacts, Materiality, More-than-Human},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642605,\nauthor = {Nigatu, Hellina Hailu and Canny, John and Chasins, Sarah E.},\ntitle = {Low-Resourced Languages and Online Knowledge Repositories: A Need-Finding Study.},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642605},\ndoi = {10.1145/3613904.3642605},\nabstract = {Online Knowledge Repositories (OKRs) like Wikipedia offer communities a way to share and preserve information about themselves and their ways of living. However, for communities with low-resourced languages—including most African communities—the quality and volume of content available are often inadequate. One reason for this lack of adequate content could be that many OKRs embody Western ways of knowledge preservation and sharing, requiring many low-resourced language communities to adapt to new interactions. To understand the challenges faced by low-resourced language contributors on the popular OKR Wikipedia, we conducted (1)  a thematic analysis of Wikipedia forum discussions and (2)  a contextual inquiry study with 14 novice contributors. We focused on three Ethiopian languages: Afan Oromo, Amharic, and Tigrinya. Our analysis revealed several recurring themes; for example, contributors struggle to find resources to corroborate their articles in low-resourced languages, and language technology support, like translation systems and spellcheck, result in several errors that waste contributors’ time. We hope our study will support designers in making online knowledge repositories accessible to low-resourced language speakers.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {562},\nnumpages = {21},\nkeywords = {HCI4D, Indigenous Knowledge, Knowledge Repositories, Low-resourced languages, Need-finding studies},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642263,\nauthor = {Doggett, Olivia and Ratto, Matt and Chandra, Priyank},\ntitle = {Migrant Farmworkers' Experiences of Agricultural Technologies: Implications for Worker Sociality and Desired Change},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642263},\ndoi = {10.1145/3613904.3642263},\nabstract = {This mixed method study situated in Ontario, Canada, investigates how migrant farmworkers’ experiences with agricultural technologies (agtech) affect their attitudes, conditions, and expectations of work, and how workers envision technologies serving as supportive interventions. Through a survey and interviews, we identify that surveillance and tracking agtech (chequeadoras) affect workers, imparting negative health and safety consequences. Workers’ interactions with chequeadoras reveal three major impacts: performance expectations engender stress, surveillance causes fears of disciplinary action, and performance tracking heightens competition. These impacts demonstrate how chequeadoras erode workers’ capacity to build sociality and solidarity. In response to these impacts and to support workers’ desired workplace changes, which aim for safer environments with technical skill development opportunities, we examine tactics from HCI, critical design, and migrant justice movements. Our findings lead us to contemplate what qualifies as agtech and how we may support marginalised workers with divergent opinions regarding workplace technologies, and desired collective change.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {563},\nnumpages = {23},\nkeywords = {agriculture, agtech, digital agriculture, farming, greenhouses, labor organization, managed workforce, migrant labor, mixed methods, social justice, sociality, surveillance, worker advocacy, workplace technology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642273,\nauthor = {Seaborn, Katie and Iseya, Satoru and Hidaka, Shun and Kobuki, Sota and Chandra, Shruti},\ntitle = {Play Across Boundaries: Exploring Cross-Cultural Maldaimonic Game Experiences},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642273},\ndoi = {10.1145/3613904.3642273},\nabstract = {Maldaimonic game experiences occur when people engage in personally fulfilling play through egocentric, destructive, and/or exploitative acts. Initial qualitative work verified this orientation and experiential construct for English-speaking Westerners. In this comparative mixed methods study, we explored whether and how maldaimonic game experiences and orientations play out in Japan, an Eastern gaming capital that may have cultural values incongruous with the Western philosophical basis underlying maldaimonia. We present findings anchored to the initial frameworks on maldaimonia in game experiences that show little divergence between the Japanese and US cohorts. We also extend the qualitative findings with quantitative measures on affect, player experience, and the related constructs of hedonia and eudaimonia. We confirm this novel construct for Japan and set the stage for scale development.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {564},\nnumpages = {15},\nkeywords = {Japan, Maldaimonia, cross-cultural studies, eudaimonia, games, hedonia, maldaimonic game UX, player experience, user experience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642496,\nauthor = {Chowdhury, Nabila and Shokri, Natasha and Valera, Cibeles Herrera and Sp, Azhagu Meena and Marquez, Carolina Reyes and Rifat, Mohammad Rashidujjaman and Wong-Villacres, Marisol and Munteanu, Cosmin and Dahya, Negin and Ahmed, Syed Ishtiaque},\ntitle = {Politics of the Past: Understanding the Role of Memory, Postmemory, and Remembrance in Navigating the History of Migrant Families},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642496},\ndoi = {10.1145/3613904.3642496},\nabstract = {The importance of history as an HCI method has been gaining increasing attention in HCI literature. However, the mainstream historical sources (books, documentaries, etc.) and methods often risk (re)producing western colonial biases potentially providing a narrow one-sided perspective on history and detaching “sanitized facts” from people’s emotional accounts. While oral history and similar alternative methods are often used as a countermeasure, their applicability has remained underexplored in HCI, especially in a sensitive context, such as migration. We build on the rich body of social science work on collective memory to introduce a complementary way of navigating the past of the migrant families, and also reveal the corresponding challenges to advance this literature. Our interview study with 17 migrant families highlights how the politics of remembrance, family dynamics, and postmemory shape the past stories of migrant families. We discuss how these findings inform the HCI literature on migration, design, and postcolonial computing.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {565},\nnumpages = {17},\nkeywords = {Memory and History, Migrant Communities},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642559,\nauthor = {Chang, Michael Alan and Wong, Richmond Y. and Breideband, Thomas and Philip, Thomas M. and McKoy, Ashieda and Cortez, Arturo and D'Mello, Sidney K.},\ntitle = {Co-design Partners as Transformative Learners: Imagining Ideal Technology for Schools by Centering Speculative Relationships},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642559},\ndoi = {10.1145/3613904.3642559},\nabstract = {Emergent technologies like artificial intelligence have been proposed to address issues of inequity in schools, yet tend to ossify the status quo because they address needs within an already inequitable system. In this paper, we draw from speculative participatory approaches across HCI and the learning sciences, and present a novel approach to co-design that forefronts supporting historically minoritized youth in developing transformative agency to change their schools based on their valued hopes, practices, and concerns. We argue that when co-design spaces forefront relational development, expansive technological objects emerge as a byproduct. We present a case study of expansive dreaming with U.S. historically minoritized students about the use of artificial intelligence to support classroom collaboration. Methodologically, we demonstrate how physically visiting spaces of collective agency serves as a powerful perceptual bridge to imagining joyful, equitable possibilities for schooling. Our approach yields new visions for schooling and new metaphors for artificial intelligence.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {566},\nnumpages = {15},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641921,\nauthor = {Nicholson, Rebecca and Strachan, Rebecca and Dele-Ajayi, Opeyemi and Fasae, Kemi},\ntitle = {Emergency Remote Education in Nigeria: Challenges and Design Opportunities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641921},\ndoi = {10.1145/3613904.3641921},\nabstract = {There are currently approximately 20.2 million children in Nigeria out of school, exacerbated by ongoing conflicts demonstrating an ongoing need for Emergency Remote Education (ERE). Despite this, Nigeria remains an under-explored context and the specific challenges of providing ERE there are not fully understood. This paper reports on a mixed methods study of teachers experiences of enacting ERE in Nigeria in April 2020 with a questionnaire (n=374), diary study and follow up interviews (n=20) carried out. The contributions of the paper are two-fold; firstly, an in-depth study of ERE in Nigeria, demonstrating that teachers used WhatsApp as a tool of practical necessity, configured it to create a continued sense of place, and continued to enact largely traditional pedagogies. Secondly, through reflection on these findings, we offer initial design considerations for technology use in ERE in low resource settings before outlining continuing design challenges for HCI researchers in this context.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {567},\nnumpages = {14},\nkeywords = {Emergency Remote Education, Low Resource Settings, Mobile Learning, Nigeria},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642236,\nauthor = {Adjagbodjou, Adinawa and Kaufman, Geoff},\ntitle = {Envisioning Support-Centered Technologies for Language Practice and Use: Needs and Design Opportunities for Immigrant English Language Learners (ELLs)},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642236},\ndoi = {10.1145/3613904.3642236},\nabstract = {Immigrant English Language Learners (ELLs) who are learning the majority language in a new country are required to participate in the informal language space on a daily basis to gain access to essential economic and social resources. In contrast to formal language spaces, which extensive literature has researched, exploration of informal language spaces, which present a number of linguistic and psychological challenges without scaffolded support, remains limited. In this work, we conduct a qualitative interview study to explore the use of support tools to facilitate participation in daily life for ELLs, investigating the efficacy of these tools, obstacles encountered, and perceptions of what defines positive and negative experiences. We aim to contribute a deeper, more nuanced understanding of the experience of language use in practical scenarios for ELLs and present a set of actionable considerations for designers working with ELLs that prioritize their linguistic, affective, and social needs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {568},\nnumpages = {15},\nkeywords = {Design, Immigrants, Informal Learning, Language Learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642087,\nauthor = {Yamani, Asma Z and Al-Shammare, Haifa Abdullah and Baslyman, Malak},\ntitle = {Establishing Heuristics for Improving the Usability of GUI Machine Learning Tools for Novice Users},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642087},\ndoi = {10.1145/3613904.3642087},\nabstract = {Machine learning (ML) tools with graphical user interfaces (GUI) are facing demand from novice users who do not have the background of their underlying concepts. These tools are frequently complex and pose unique challenges in terms of interaction and comprehension by novice users. There is yet to be an established set of usability heuristics to guide and assess GUI ML tool design. To address this gap, in this paper, we extend Nielsen’s heuristics for evaluating GUI ML Tools through a set of empirical evaluations. To validate the proposed heuristics, user testing was conducted by novice users on a prototype that reflects those heuristics. Based on the results of the evaluations, our new heuristics set improves upon existing heuristics in the context of ML tools. It can serve as a resource for practitioners designing and evaluating these tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {569},\nnumpages = {19},\nkeywords = {Cognitive walkthrough (CW), GUI machine learning tools, Heuristic evaluation (HE), Knime, ML tools, Novice users, SUS, Usable, User testing, Weka},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642778,\nauthor = {Kim, Minyeong and Lee, Jiwook and Koh, Youngji and Lee, Chanhee and Lee, Uichin and Kim, Auk},\ntitle = {Interrupting for Microlearning: Understanding Perceptions and Interruptibility of Proactive Conversational Microlearning Services},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642778},\ndoi = {10.1145/3613904.3642778},\nabstract = {Significant investment of time and effort for language learning has prompted a growing interest in microlearning. While microlearning requires frequent participation in 3-to-10-minute learning sessions, the recent widespread of smart speakers in homes presents an opportunity to expand learning opportunities by proactively providing microlearning in daily life. However, such proactive provision can distract users. Despite the extensive research on proactive smart speakers and their opportune moments for proactive interactions, our understanding of opportune moments for more-than-one-minute interactions remains limited. This study aims to understand user perceptions and opportune moments for more-than-one-minute microlearning using proactive smart speakers at home. We first developed a proactive microlearning service through six pilot studies (n=29), and then conducted a three-week field study (n=28). We identified the key contextual factors relevant to opportune moments for microlearning of various durations, and discussed the design implications for proactive conversational microlearning services at home.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {570},\nnumpages = {21},\nkeywords = {Conversational Interaction, Interruptibility, Microlearning, Opportune Moment, Smart Speakers},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641985,\nauthor = {Mavali, Sahar and Yoon, Dongwook and Sinnamon, Luanne and Fels, Sidney S},\ntitle = {Time-Turner: A Bichronous Learning Environment to Support Positive In-class Multitasking of Online Learners},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641985},\ndoi = {10.1145/3613904.3641985},\nabstract = {University students engage in a substantial amount of multitasking in online classes despite being aware of its negative impacts on their learning. Depending on the learner’s goals, in-class multitasking can be a positive strategic behavior to increase productivity. In a formative pilot study (N=10), we established the structure and scope for our design by exploring students’ motivations, perceptions, and challenges in in-class multitasking and identified several promising design elements. Our design facilitates multitasking in online synchronous classes by providing a novel bichronous (blending of synchronous and asynchronous) learning environment manifested in Time-Turner that enables asynchronous guided accelerated viewing of past content during synchronous classes. A summative evaluation of our prototype showed significant improvement in learning outcomes when multitasking (N=20). Furthermore, 95\\% of users found Time-Turner helpful and expressed interest in having it in their online classes. Our findings show the great potential of supporting positive multitasking in synchronous online classes.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {571},\nnumpages = {15},\nkeywords = {bichronous learning, in-class multitasking, online learning interfaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641978,\nauthor = {Lambert, Sol\\`{e}ne and Voros, Sandrine and Canlorbe, Geoffroy and Troccaz, Jocelyne and Avellino, Ignacio},\ntitle = {Understanding Takeovers and Telestration in Laparoscopic Surgery to Inform Telementoring System Design},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641978},\ndoi = {10.1145/3613904.3641978},\nabstract = {Surgery is primarily taught through mentoring, where an expert mentor supervises a mentee performing surgery, taking over when necessary. Telementoring systems aim to provide mentees with access to remote mentors, but the physical distance between mentors and mentees poses unique challenges to surgical training. We investigate the underlying needs leading to takeovers in onsite mentoring and assess mentors’ ability to fulfill address these needs remotely using existing telestration tools, namely pointers and drawings on shared views. Through interviews and workshops with expert surgeons, we find that (1) mentors take over to convey gestures related to instrument placement, tissue displacement, force, and movement, (2) mentors gather information about location of tissue, equipment, and instruments, as well as gesture constraints, and (3) surgeons judge telestration insufficient for these needs. Based on this gap between onsite mentoring practices and telementoring tools, we discuss novel tools to address these needs and their evaluation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {572},\nnumpages = {17},\nkeywords = {gestures, remote instruction, surgical telementoring, takeovers},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642457,\nauthor = {Wang, Chenyang and Tozadore, Daniel C. and Bruno, Barbara and Dillenbourg, Pierre},\ntitle = {WriteUpRight: Regulating Children’s Handwriting Body Posture by Unobstrusively Error Amplification via Slow Visual Stimuli on Tablets},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642457},\ndoi = {10.1145/3613904.3642457},\nabstract = {Maintaining a proper body posture during interactions with educational tablet applications is crucial for children’s physical well-being and task performance, especially considering digital tablet’s increasingly pervasive use in classrooms. In this work we propose WriteUpRight, an interaction system for children’s self-regulation of posture while writing on a tablet. The system relies on slowly deforming visual stimuli appearing on the tablet screen and compares two posture correction strategies: the Error Amplification method (see Figure 1) seeks to induce self-correction by amplifying the postural error, while the Error Correction method seeks to unobtrusively nudge the child towards the correct posture. Through a formative design and a user study with 42 children, we demonstrate the effectiveness of our solution and the advantages of the Error Amplification method with respect to the Error Correction method. The system shows potential for helping children maintain a proper head-screen distance and head roll angle during reading and writing tasks on tablets.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {573},\nnumpages = {13},\nkeywords = {Children, Handwriting Body Posture Regulation, Method of Amplification of Error, Tablet},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642826,\nauthor = {Schm\\\"{u}ser, Juliane and Sri Ramulu, Harshini and W\\\"{o}hler, Noah and Stransky, Christian and Bensmann, Felix and Dimitrov, Dimitar and Schellhammer, Sebastian and Wermke, Dominik and Dietze, Stefan and Acar, Yasemin and Fahl, Sascha},\ntitle = {Analyzing Security and Privacy Advice During the 2022 Russian Invasion of Ukraine on Twitter},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642826},\ndoi = {10.1145/3613904.3642826},\nabstract = {The Russian Invasion of Ukraine in 2022 resulted in a rapidly changing cyber threat environment globally and incentivized the sharing of security and privacy advice on social media. Previous research found a strong impact of online security advice on end-user behavior. Twitter is an important platform for sharing information in crises. We examined 306 tweets with security and privacy advice related to the Ukrainian war, and created a taxonomy of 224 unique pieces of advice in seven categories, targeted at individuals or organizations in Ukraine and elsewhere. While our findings include untargeted and generic advice known from previous research, we identify novel advice specific to the invasion, offers for individual consultation, and misinformation on security and privacy advice as a new threat. Our findings highlight the strengths and shortcomings of the security and privacy advice given online during the invasion and establish areas for improvements and future research.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {574},\nnumpages = {16},\nkeywords = {security and privacy advice, twitter, ukraine},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642664,\nauthor = {Wang, Kelly and Bially Levy, Dan and Nguyen, Kien T and Lerner, Ada and Marsh, Abigail},\ntitle = {Counting Carrds: Investigating Personal Disclosure and Boundary Management in Transformative Fandom},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642664},\ndoi = {10.1145/3613904.3642664},\nabstract = {The privacy practices of transformative fandom are of interest to HCI researchers both for the community’s high proportion of queer members and for the community’s sophisticated privacy norms and behaviors. We investigated fans’ use of single-serving websites on Carrd.co (“Carrds”) as personal profiles linked from Twitter accounts. We scraped Twitter to gather 5252 Carrds from fans in a variety of fandoms, which we analyzed using a combination of keyword searches and hand-coding. Fans’ Carrds frequently disclose queer identity, and articulate a complex system of community values and boundary management. Inspired by how these findings aren’t well-explained by individual theories of privacy, we articulate first steps towards a theory of collective privacy based in a communal process of values construction, trust building, and personal disclosure that we believe helps us to understand the sophisticated nature of fans’ observed behaviors.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {575},\nnumpages = {13},\nkeywords = {Carrd.co, Twitter, fandom, norms, privacy, quantitative methods, theory},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642521,\nauthor = {Malki, Lisa Mekioussa and Kaleva, Ina and Patel, Dilisha and Warner, Mark and Abu-Salma, Ruba},\ntitle = {Exploring Privacy Practices of Female mHealth Apps in a Post-Roe World},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642521},\ndoi = {10.1145/3613904.3642521},\nabstract = {Mobile apps which support women’s health have developed rapidly alongside the increasing de-stigmatisation of female reproductive wellbeing. However, the ubiquity of these apps has advanced the practice of intimate surveillance and the commodification of sensitive user data. While the overturning of Roe v. Wade has prompted reflection on the privacy and safety implications of female mobile health (mHealth) apps, the privacy practices of these apps have yet to be thoroughly examined in a post-Roe world. We investigated the privacy practices of 20 popular female mHealth apps, combining a thematic analysis of Data safety sections and privacy policies with a privacy-focused usability inspection. Our findings revealed problematic practices, including inconsistencies across privacy policy content and privacy-related app features, flawed consent and data deletion mechanisms, and covert gathering of sensitive data. We present recommendations for improving privacy practices, and call for a dedicated focus not only on user privacy, but also safety.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {576},\nnumpages = {24},\nkeywords = {Digital health, FemTech, mobile health, privacy, safety, women’s health},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642242,\nauthor = {Bhardwaj, Divyanshu and Ponticello, Alexander and Tomar, Shreya and Dabrowski, Adrian and Krombholz, Katharina},\ntitle = {In Focus, Out of Privacy: The Wearer's Perspective on the Privacy Dilemma of Camera Glasses},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642242},\ndoi = {10.1145/3613904.3642242},\nabstract = {The rising popularity of camera glasses challenges societal norms of recording bystanders and thus requires efforts to mediate privacy preferences. We present the first study on the wearers’ perspectives and explore privacy challenges associated with wearing camera glasses when bystanders are present. We conducted a micro-longitudinal diary study (N = 15) followed by exit interviews with existing users and people without prior experience. Our results show that wearers consider the currently available privacy indicators ineffective. They believe the looks and interaction design of the glasses conceal the technology from unaware people. Due to the lack of effective privacy-mediating measures, wearers feel emotionally burdened with preserving bystanders’ privacy. We furthermore elicit how this sentiment impacts their usage of camera glasses and highlight the need for technical and non-technical solutions. Finally, we compare the wearers’ and bystanders’ perspectives and discuss the design space of a future privacy-preserving ecosystem for wearable cameras.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {577},\nnumpages = {18},\nkeywords = {camera glasses, privacy, smart glasses, wearables, wearer’s perspective},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642206,\nauthor = {Gould, Sandy J. J.},\ntitle = {Stochastic Machine Witnesses at Work: Today's Critiques of Taylorism are Inadequate for Workplace Surveillance Epistemologies of the Future},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642206},\ndoi = {10.1145/3613904.3642206},\nabstract = {I argue that epistemologies of workplace surveillance are shifting in fundamental ways, and so critiques must shift accordingly. I begin the paper by relating Scientific Management to Human-Centred Computing’s ways of knowing through a study of ‘metaverse’ virtual reality workplaces. From this, I develop two observations. The first is that today’s workplace measurement science does not resemble the science that Taylor developed for Scientific Management. Contemporary workplace science is more passive, more intermediated and less controlled. The second observation is that new forms of workplace measurement challenge the norms of empirical science. Instead of having credentialed human witnesses observe phenomena and agree facts about them, we instead make outsourced, uncredentialed stochastic machine witnesses responsible for producing facts about work. With these observations in mind, I assert that critiques of workplace surveillance still framed by Taylorism will not be fit for interrogating workplace surveillance practices of the future.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {578},\nnumpages = {12},\nkeywords = {Metaverse, Neo-Taylorism, Scientific Management, Taylorism, Ubiquitous Computing, Work Measurement, Workplace Surveillance},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642524,\nauthor = {Al Qahtani, Elham and Story, Peter and Shehab, Mohamed},\ntitle = {The Impact of Risk Appeal Approaches on Users’ Sharing Confidential Information},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642524},\ndoi = {10.1145/3613904.3642524},\nabstract = {End-to-end encrypted email can help users prevent unauthorized access of their sensitive information. However, many users struggle to utilize encryption tools due to usability issues and low understanding. Thus, we designed video messaging interventions to persuade users to use email encryption software (Virtru). Our first intervention combined Protection Motivation Theory with Anticipated Regret (PMT+AR), and was designed to help participants understand the benefits of using encrypted email. Our second intervention also included Action Planning (PMT+AR+AP), and was designed to help participants recognize opportunities to use encrypted email. We conducted online interviews with 121 participants and used a follow-up survey to evaluate our interventions. Pre-intervention, participants believed that Gmail encrypted standard email content by default. Post-intervention, both messages made participants more likely to utilize encrypted email in a simulated information sharing scenario compared to Control. Our results suggest that our interventions can help people adopt protective technologies and address their misconceptions about them.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {579},\nnumpages = {21},\nkeywords = {action planning, anticipated regret, encrypted email, nudges, protection motivation theory, technology adoption},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642368,\nauthor = {Kaufhold, Marc-Andr\\'{e} and Riebe, Thea and Bayer, Markus and Reuter, Christian},\ntitle = {‘We Do Not Have the Capacity to Monitor All Media’: A Design Case Study on Cyber Situational Awareness in Computer Emergency Response Teams},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642368},\ndoi = {10.1145/3613904.3642368},\nabstract = {Computer Emergency Response Teams (CERTs) provide advisory, preventive and reactive cybersecurity services for authorities, citizens, and businesses. However, their responsibility of monitoring, analyzing, and communicating cyber threats have become challenging due to the growing volume and varying quality of information disseminated through public channels. Based on a design case study conducted from 2021 to 2023, this paper combines three iterations of expert interviews, design workshops and cognitive walkthroughs to design an automated, cross-platform and real-time cybersecurity dashboard. By adopting the notion of cyber situational awareness, the study extracts user requirements and design heuristics for enhanced threat awareness and mission awareness in CERTs, discussing the aspects of source integration, data management, customizable visualization, relationship awareness, information assessment, software integration, (inter-)organizational collaboration, and communication of stakeholder warnings.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {580},\nnumpages = {16},\nkeywords = {Computer Emergency Response Teams, Cyber Situational Awareness, Design Case Studies, Security and Privacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642464,\nauthor = {Sharma, Tanusree and Nair, Vivek C and Wang, Henry and Wang, Yang and Song, Dawn},\ntitle = {“I Can’t Believe It’s Not Custodial!”: Usable Trustless Decentralized Key Management},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642464},\ndoi = {10.1145/3613904.3642464},\nabstract = {Key management has long remained a difficult unsolved problem in the field of usable security. While password-based key derivation functions (PBKDFs) are widely used to solve this problem in centralized applications, their low entropy and lack of a recovery mechanism make them unsuitable for use in decentralized contexts. The multi-factor key derivation function (MFKDF) is a recently proposed cryptographic primitive that aims to address these deficiencies by incorporating commonly used authentication factors into the key derivation process. In this paper, we implement an MFKDF-based Ethereum wallet and perform a user study with 27 participants to directly compare its usability against traditional cryptocurrency wallet architectures. Our results show that MFKDF-based applications outperform conventional key management approaches on both subjective and objective metrics, with a 37\\% higher average SUS score (p < 0.0001) and 71\\% faster task completion times (p < 0.0001) for the MFKDF-based wallet.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {581},\nnumpages = {16},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642290,\nauthor = {Luo, Yiming and Liu, Shihao and Wu, Di and Wang, Hao and Pan, Yushan},\ntitle = {\"Please Be Nice\": Robot Responses to User Bullying - Measuring Performance Across Aggression Levels},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642290},\ndoi = {10.1145/3613904.3642290},\nabstract = {As robots become integral to public services, addressing harmful user behaviors like bullying is crucial. Existing research often overlooks the gradual nature of human bullying. This study fills this gap by exploring how robots can counter bullying through optimized responses. Using a simulated human-robot interaction study, we manipulated robot response behaviors and styles across escalating bullying severity. Results show that empathetic verbal responses promptly reduce users’ bullying tendencies by eliciting remorse and redirecting attention to social awareness. However, users’ underlying dispositions may override these reflexive reactions, emphasizing the need for a holistic understanding. In conclusion, a comprehensive approach is essential, involving immediate reaction optimization, emotional state assessment, and ongoing behavioral adjustment through empathetic dialogue. By implementing such strategies, we can transform human-robot relationships from potential bullying situations to harmonious interactions. This study provides an empirical foundation for response protocols that discourage bullying and enhance mutual understanding.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {582},\nnumpages = {15},\nkeywords = {Bullying, Human-Computer Interaction, Human-Robot Interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642082,\nauthor = {Babel, Franziska and Welsch, Robin and Miller, Linda and Hock, Philipp and Thellman, Sam and Ziemke, Tom},\ntitle = {A Robot Jumping the Queue: Expectations About Politeness and Power During Conflicts in Everyday Human-Robot Encounters},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642082},\ndoi = {10.1145/3613904.3642082},\nabstract = {Increasing encounters between people and autonomous service robots may lead to conflicts due to mismatches between human expectations and robot behaviour. This interactive online study (N = 335) investigated human-robot interactions at an elevator, focusing on the effect of communication and behavioural expectations on participants’ acceptance and compliance. Participants evaluated a humanoid delivery robot primed as either submissive or assertive. The robot either matched or violated these expectations by using a command or appeal to ask for priority and then entering either first or waiting for the next ride. The results highlight that robots are less accepted if they violate expectations by entering first or using a command. Interactions were more effective if participants expected an assertive robot which then asked politely for priority and entered first. The findings emphasize the importance of power expectations in human-robot conflicts for the robot’s evaluation and effectiveness in everyday situations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {583},\nnumpages = {13},\nkeywords = {expectations, human-robot cooperation, persuasive technologies, power, social roles},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642038,\nauthor = {van Rijn, Pol and Mertes, Silvan and Janowski, Kathrin and Weitz, Katharina and Jacoby, Nori and Andr\\'{e}, Elisabeth},\ntitle = {Giving Robots a Voice: Human-in-the-Loop Voice Creation and open-ended Labeling},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642038},\ndoi = {10.1145/3613904.3642038},\nabstract = {Speech is a natural interface for humans to interact with robots. Yet, aligning a robot’s voice to its appearance is challenging due to the rich vocabulary of both modalities. Previous research has explored a few labels to describe robots and tested them on a limited number of robots and existing voices. Here, we develop a robot-voice creation tool followed by large-scale behavioral human experiments (N=2,505). First, participants collectively tune robotic voices to match 175 robot images using an adaptive human-in-the-loop pipeline. Then, participants describe their impression of the robot or their matched voice using another human-in-the-loop paradigm for open-ended labeling. The elicited taxonomy is then used to rate robot attributes and to predict the best voice for an unseen robot. We offer a web interface to aid engineers in customizing robot voices, demonstrating the synergy between cognitive science and machine learning for engineering tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {584},\nnumpages = {34},\nkeywords = {Crowdsourcing, Personalization, Robot, Text/Speech/Language},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642305,\nauthor = {Seo, Jiyeon and Lim, Hajin and Suh, Bongwon and Lee, Joonhwan},\ntitle = {I feel being there, they feel being together: Exploring How Telepresence Robots Facilitate Long-Distance Family Communication},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642305},\ndoi = {10.1145/3613904.3642305},\nabstract = {Many families often live geographically apart from each other due to work, education, or marriage. Therefore, long-distance families frequently use computer-mediated communication (CMC) tools to stay connected. While CMC tools have significantly improved family communication, they cannot fully mediate social presence. To examine the potential of telepresence robots for improving long-distance family communication, we conducted a two-week qualitative in situ study involving eight families. We analyzed recorded videos of their family interactions and conducted pre- and post-deployment interviews. Our findings highlight telepresence robots’ potential as family communication tools, enabling immersive, natural, and dynamic interactions through physical embodiment and autonomy. Particularly, we identified five categories of family interaction mediated by telepresence robots: engaging in multi-party family communication, exploring home, restoring family routines, providing support, and having joint physical activities. Based on our findings, we present design guidelines for leveraging telepresence robots as effective family communication tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {585},\nnumpages = {18},\nkeywords = {family communication, proxemics, relationship maintenance, robot-mediated communication, social presence, telepresence robots},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642231,\nauthor = {Wang, Chongyang and Zheng, Siqi and Zhong, Lingxiao and Yu, Chun and Liang, Chen and Wang, Yuntao and Gao, Yuan and Lam, Tin Lun and Shi, Yuanchun},\ntitle = {PepperPose: Full-Body Pose Estimation with a Companion Robot},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642231},\ndoi = {10.1145/3613904.3642231},\nabstract = {Accurate full-body pose estimation across diverse actions in a user-friendly and location-agnostic manner paves the way for interactive applications in realms like sports, fitness, and healthcare. This task becomes challenging in real-world scenarios due to factors like the user’s dynamic positioning, the diversity of actions, and the varying acceptability of the pose-capturing system. In this context, we present PepperPose, a novel companion robot system tailored for optimized pose estimation. Unlike traditional methods, PepperPose actively tracks the user and refines its viewpoint, facilitating enhanced pose accuracy across different locations and actions. This allows users to enjoy a seamless action-sensing experience. Our evaluation, involving 30 participants undertaking daily functioning and exercise actions in a home-like space, underscores the robot’s promising capabilities. Moreover, we demonstrate the opportunities that PepperPose presents for human-robot interaction, its current limitations, and future developments.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {586},\nnumpages = {16},\nkeywords = {human-robot interaction, pose estimation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642389,\nauthor = {Yu, Xinyan and Hoggenm\\\"{u}ller, Marius and Tomitsch, Martin},\ntitle = {From Agent Autonomy to Casual Collaboration: A Design Investigation on Help-Seeking Urban Robots},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642389},\ndoi = {10.1145/3613904.3642389},\nabstract = {As intelligent agents transition from controlled to uncontrolled environments, they face challenges that sometimes exceed their operational capabilities. In many scenarios, they rely on assistance from bystanders to overcome those challenges. Using robots that get stuck in urban settings as an example, we investigate how agents can prompt bystanders into providing assistance. We conducted four focus group sessions with 17 participants that involved bodystorming, where participants assumed the role of robots and bystander pedestrians in role-playing activities. Generating insights from both assumed robot and bystander perspectives, we were able to identify potential non-verbal help-seeking strategies (i.e., addressing bystanders, cueing intentions, and displaying emotions) and factors shaping the assistive behaviours of bystanders. Drawing on these findings, we offer design considerations for help-seeking urban robots and other agents operating in uncontrolled environments to foster casual collaboration, encompass expressiveness, align with agent social categories, and curate appropriate incentives.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {587},\nnumpages = {14},\nkeywords = {Human-agent collaboration, autonomous agent, casual bystanders, embodied design methods, urban robots},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642561,\nauthor = {Ogawa, Nami and Baba, Jun and Nakanishi, Junya},\ntitle = {Investigating Effect of Altered Auditory Feedback on Self-Representation, Subjective Operator Experience, and Task Performance in Teleoperation of a Social Robot},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642561},\ndoi = {10.1145/3613904.3642561},\nabstract = {Teleoperating social robots requires operators to “speak as the robot,” as local users would favor robots whose appearance and voice match. This study focuses on real-time altered auditory feedback (AAF), a method to transform the acoustic traits of one’s speech and provide feedback to the speaker, to transform the operator’s self-representation toward “becoming the robot.” To explore whether AAF with voice transformation (VT) matched to the robot’s appearance can influence the operator’s self-representation and ease the task, we experimented with three conditions: no VT (No-VT), only VT (VT-only), and VT with AAF (VT-AAF), where participants teleoperated a robot to verbally serve real passersby at a bakery. The questionnaire results demonstrate that VT-AAF changed the participants’ self-representation to match the robot’s character and improved participants’ subjective teleoperating experience, while task performance and implicit measures of self-representation were not significantly affected. Notably, 87\\% of the participants preferred VT-AAF the most.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {588},\nnumpages = {18},\nkeywords = {altered auditory feedback, avatar, self-representation, social robots, teleoperation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642911,\nauthor = {Sch\\\"{o}mbs, Sarah and Pareek, Saumya and Goncalves, Jorge and Johal, Wafa},\ntitle = {Robot-Assisted Decision-Making: Unveiling the Role of Uncertainty Visualisation and Embodiment},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642911},\ndoi = {10.1145/3613904.3642911},\nabstract = {Robots are embodied agents that act under several sources of uncertainty. When assisting humans in a collaborative task, robots need to communicate their uncertainty to help inform decisions. In this study, we examine the use of visualising a robot’s uncertainty in a high-stakes assisted decision-making task. In particular, we explore how different modalities of uncertainty visualisations (graphical display vs. the robot’s embodied behaviour) and confidence levels (low, high, 100\\%) conveyed by a robot affect the human decision-making and perception during a collaborative task. Our results show that these visualisations significantly impact how participants arrive to their decision as well as how they perceive the robot’s transparency across the different confidence levels. We highlight potential trade-offs and offer implications for robot-assisted decision-making. Our work contributes empirical insights on how humans make use of uncertainty visualisations conveyed by a robot in a critical robot-assisted decision-making scenario.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {589},\nnumpages = {16},\nkeywords = {agency, assisted decision-making, embodiment, risk communication, uncertainty visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642465,\nauthor = {Nguyen, Binh Vinh Duc and Vande Moere, Andrew},\ntitle = {The Adaptive Architectural Layout: How the Control of a Semi-Autonomous Mobile Robotic Partition was Shared to Mediate the Environmental Demands and Resources of an Open-Plan Office},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642465},\ndoi = {10.1145/3613904.3642465},\nabstract = {A typical open-plan office layout is unable to optimally host multiple collocated work activities, personal needs, and situational events, as its space exerts a range of environmental demands on workers in terms of maintaining their acoustic, visual or privacy comfort. As we hypothesise that these demands could be coped by optimising the environmental resources of the architectural layout, we deployed a mobile robotic partition that autonomously manoeuvres between predetermined locations. During a five-weeks in-the-wild study within a real-world open-plan office, we studied how 13 workers adopted four distinct adaptation strategies when sharing the spatiotemporal control of the robotic partition. Based on their logged and self-reported reasoning, we present six initiation regulating factors that determine the appropriateness of each adaptation strategy. This study thus contributes to how future human-building interaction could autonomously improve the experience, comfort, performance, and even the health and wellbeing of multiple workers that share the same workplace.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {590},\nnumpages = {20},\nkeywords = {adaptive architecture, human-building interaction, human-robot interaction, indoor autonomous driving, interactive architecture, kinetic architecture, responsive architecture, robotic architecture, robotic furniture, robotic partition, smart building, smart office, smart space, spatial layout},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642610,\nauthor = {Brown, Barry and Bu, Fanjun and Mandel, Ilan and Ju, Wendy},\ntitle = {Trash in Motion: Emergent Interactions with a Robotic Trashcan},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642610},\ndoi = {10.1145/3613904.3642610},\nabstract = {The introduction of robots in public spaces raises many questions concerning emergent interactions with robots. In this paper, we use video analysis to study two robotic trashcans deployed in a busy city square. We focus on the movement-based practices that emerged between the robot, the robot operators, and the inhabitants of the square. These practices spanned ways of attracting the robot and disposing of trash, the robot ’asking’ for trash, ’demonstrations’ by those in the square, as well as passersby in the square navigating around and in coordination with the robots. In discussion, we document these ’spontaneous simple sequential systematics’ - interactions that were systematic (they had an order), sequential (they had parts that happened one at a time), simple (in that they could be understood and copied by an observer) and spontaneous (they could be produced with no prompting or training). Building on this we discuss how we might think of robotic motion as a design space, along with HCI contributions to urban robotics.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {591},\nnumpages = {17},\nkeywords = {Public interaction, ethnomethodology, human-robot interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642875,\nauthor = {Han, Howard Ziyu and Li, Franklin Mingzhe and Baca Vazquez, Alesandra and Byrne, Daragh and Martelaro, Nikolas and Fox, Sarah E},\ntitle = {Co-design Accessible Public Robots: Insights from People with Mobility Disability, Robotic Practitioners and Their Collaborations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642875},\ndoi = {10.1145/3613904.3642875},\nabstract = {Sidewalk robots are increasingly common across the globe. Yet, their operation on public paths poses challenges for people with mobility disabilities (PwMD) who face barriers to accessibility, such as insufficient curb cuts. We interviewed 15 PwMD to understand how they perceive sidewalk robots. Findings indicated that PwMD feel they have to compete for space on the sidewalk when robots are introduced. We next interviewed eight robotics practitioners to learn about their attitudes towards accessibility. Practitioners described how issues often stem from robotic companies addressing accessibility only after problems arise. Both interview groups underscored the importance of integrating accessibility from the outset. Building on this finding, we held four co-design workshops with PwMD and practitioners in pairs. These convenings brought to bear accessibility needs around robots operating in public spaces and in the public interest. Our study aims to set the stage for a more inclusive future around public service robots.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {592},\nnumpages = {20},\nkeywords = {Accessibility, Delivery robots, Human-robot interaction, Public space, Sidewalk robots},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642115,\nauthor = {Schneiders, Eike and Benford, Steve and Chamberlain, Alan and Mancini, Clara and Castle-Green, Simon and Ngo, Victor and Row Farr, Ju and Adams, Matt and Tandavanitj, Nick and Fischer, Joel},\ntitle = {Designing Multispecies Worlds for Robots, Cats, and Humans},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642115},\ndoi = {10.1145/3613904.3642115},\nabstract = {We reflect on the design of a multispecies world centred around a bespoke enclosure in which three cats and a robot arm coexist for six hours a day during a twelve-day installation as part of an artist-led project. In this paper, we present the project’s design process, encompassing various interconnected components, including the cats, the robot and its autonomous systems, the custom end-effectors and robot attachments, the diverse roles of the humans-in-the-loop, and the custom-designed enclosure. Subsequently, we provide a detailed account of key moments during the deployment and discuss the design implications for future multispecies systems. Specifically, we argue that designing the technology and its interactions is not sufficient, but that it is equally important to consider the design of the ‘world’ in which the technology operates. Finally, we highlight the necessity of human involvement in areas such as breakdown recovery, animal welfare, and their role as audience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {593},\nnumpages = {16},\nkeywords = {animal-computer interaction, artist-led research, performance-led research},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642795,\nauthor = {Luo, Jiadi and Domova, Veronika and Kim, Lawrence H},\ntitle = {Impact of Multi-Robot Presence and Anthropomorphism on Human Cognition and Emotion},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642795},\ndoi = {10.1145/3613904.3642795},\nabstract = {Exploring how robots impact human cognition and emotions has become increasingly important as robots gradually become ubiquitous in our lives. In this study, we investigate the impact of robotic presence on human cognition and emotion by examining various robot parameters such as anthropomorphism, number of robots, and multi-robot motion patterns. 16 participants completed two cognitive tasks in the presence of anthropomorphic and non-anthropomorphic robots, alone, and with a human nearby. The non-anthropomorphic robot conditions were further varied in the number of robots and their motion patterns. We find that increasing the number of non-anthropomorphic robots generally leads to slower performance, but coordinated patterned motions can lower the completion time compared to random movements. An anthropomorphic robot induces an increased level of feelings of being judged compared to a non-anthropomorphic robot. These findings provide preliminary insights into how designers or users can purposefully integrate robots into our environment by understanding the effects of anthropomorphism, number of robots, and multi-robot motion patterns on human cognition and emotion.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {594},\nnumpages = {15},\nkeywords = {Anthropomorphism, Multi-Robot Systems, Robot Companions, Social Facilitation, Social Robotics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642905,\nauthor = {Zojaji, Sahba and Matviienko, Andrii and Leite, Iolanda and Peters, Christopher},\ntitle = {Join Me Here if You Will: Investigating Embodiment and Politeness Behaviors When Joining Small Groups of Humans, Robots, and Virtual Characters},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642905},\ndoi = {10.1145/3613904.3642905},\nabstract = {Politeness and embodiment are pivotal elements in human-agent interactions. While many previous works advocate the positive role of embodiment in enhancing these interactions, it remains unclear how embodiment and politeness affect individuals joining groups. In this paper, we explore how politeness behaviors (verbal and nonverbal) exhibited by three distinct embodiments (humans, robots, and virtual characters) influence individuals’ decisions to join a group of two agents in a controlled experiment (N=54). We assessed agent effectiveness regarding persuasiveness, perceived politeness, and participants’ trajectories when joining the group. We found that embodiment does not significantly impact agent persuasiveness and perceived politeness, but politeness does. Direct and explicit politeness strategies have a higher success rate in persuading participants to join the group at the furthest side. Lastly, participants adhered to social norms when joining at the furthest side, maintained a greater physical distance from humans, chose longer paths, and walked faster when interacting with humans.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {595},\nnumpages = {16},\nkeywords = {Free-standing conversational groups, Group dynamics, Humans, Politeness, Robots, Trajectory, Virtual characters, social norms},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642181,\nauthor = {Hwang, Hochul and Jung, Hee-Tae and Giudice, Nicholas A and Biswas, Joydeep and Lee, Sunghoon Ivan and Kim, Donghyun},\ntitle = {Towards Robotic Companions: Understanding Handler-Guide Dog Interactions for Informed Guide Dog Robot Design},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642181},\ndoi = {10.1145/3613904.3642181},\nabstract = {Dog guides are favored by blind and low-vision (BLV) individuals for their ability to enhance independence and confidence by reducing safety concerns and increasing navigation efficiency compared to traditional mobility aids. However, only a relatively small proportion of BLV individuals work with dog guides due to their limited availability and associated maintenance responsibilities. There is considerable recent interest in addressing this challenge by developing legged guide dog robots. This study was designed to determine critical aspects of the handler-guide dog interaction and better understand handler needs to inform guide dog robot development. We conducted semi-structured interviews and observation sessions with 23 dog guide handlers and 5 trainers. Thematic analysis revealed critical limitations in guide dog work, desired personalization in handler-guide dog interaction, and important perspectives on future guide dog robots. Grounded on these findings, we discuss pivotal design insights for guide dog robots aimed for adoption within the BLV community.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {596},\nnumpages = {20},\nkeywords = {Accessibility, Individuals with Disabilities \\& Assistive Technologies, Interview, Robot},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642821,\nauthor = {Rifat, Mohammad Rashidujjaman and Ayad, Reem and Asha, Ashratuz Zavin and Huang, Bingjian and Okman, Selin and Sabie, Dina and Ferdous, Hasan Shahid and Soden, Robert and Ahmed, Syed Ishtiaque},\ntitle = {Cohabitant: The Design, Implementation, and Evaluation of a Virtual Reality Application for Interfaith Learning and Empathy Building},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642821},\ndoi = {10.1145/3613904.3642821},\nabstract = {Lack of interfaith communication often gives rise to prejudice and group-based conflict in multi-faith societies. Nurturing this communication via interfaith learning may reduce this conflict by fostering interfaith empathy. HCI has a dearth of knowledge on interfaith coexistence and empathy building. To address this gap, we present the design, implementation, and usability of Cohabitant: a virtual reality (VR) application that promotes interfaith learning and empathy. Cohabitant’s design is theoretically underpinned by Allport’s intergroup contact theory and informed by insights from a participatory workshop we ran with members of three religious groups: Christians, Hindus, and Muslims. Our evaluation study, combining quantitative and qualitative data from 30 participants, suggests that Cohabitant may enhance general interpersonal empathy, but falls short for ethnocultural empathy. We discuss the possible design and policy implications of using this kind of VR technology for interfaith learning and empathy building.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {597},\nnumpages = {19},\nkeywords = {Empathy, Interfaith, Learning, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642359,\nauthor = {Mello, Beatriz and Welsch, Robin and Verbokkem, Marissa Christien and Knierim, Pascal and Dechant, Martin Johannes},\ntitle = {Navigating the Virtual Gaze: Social Anxiety's Role in VR Proxemics},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642359},\ndoi = {10.1145/3613904.3642359},\nabstract = {For individuals with Social Anxiety (SA), interacting with others can be a challenging experience, a concern that extends into the virtual world. While technology has made significant strides in creating more realistic virtual human agents (VHA), the interplay of gaze and interpersonal distance when interacting with VHAs is often neglected. This paper investigates the effect of dynamic and static Gaze animations in VHAs on interpersonal distance and their relation to SA. A Bayesian analysis shows that static centered and dynamic centering gaze led participants to stand closer to VHAs than static averted and dynamic averting gaze, respectively. In the static gaze conditions, this pattern was found to be reversed in SA: participants with higher SA kept larger distances for static-centered gaze than for averted gaze VHAs. These findings update theory, elucidate how nuanced interactions with VHAs must be designed, and offer renewed guidelines for pleasant VHA interaction design.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {598},\nnumpages = {15},\nkeywords = {Proxemics, Virtual Human Agents, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642725,\nauthor = {Cao, Yining and Kazi, Rubaiat Habib and Wei, Li-Yi and Aneja, Deepali and Xia, Haijun},\ntitle = {Elastica: Adaptive Live Augmented Presentations with Elastic Mappings Across Modalities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642725},\ndoi = {10.1145/3613904.3642725},\nabstract = {Augmented presentations offer compelling storytelling by combining speech content, gestural performance, and animated graphics in a congruent manner. The expressiveness of these presentations stems from the harmonious coordination of spoken words and graphic elements, complemented by smooth animations aligned with the presenter’s gestures. However, achieving such desired congruence in a live presentation poses significant challenges due to the unpredictability and imprecision inherent in presenters’ real-time actions. Existing methods either leveraged rigid mapping without predefined states or required the presenters to conform to predefined animations. We introduce adaptive presentations that dynamically adjust predefined graphic animations to real-time speech and gestures. Our approach leverages script following and motion warping to establish elastic mappings that generate runtime graphic parameters coordinating speech, gesture, and predefined animation state. Our evaluation demonstrated that the proposed adaptive presentation can effectively mitigate undesired visual artifacts caused by performance deviations and enhance the expressiveness of resulting presentations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {599},\nnumpages = {19},\nkeywords = {animation, augmented presentation, gestural interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642579,\nauthor = {De La Torre, Fernanda and Fang, Cathy Mengying and Huang, Han and Banburski-Fahey, Andrzej and Amores Fernandez, Judith and Lanier, Jaron},\ntitle = {LLMR: Real-time Prompting of Interactive Worlds using Large Language Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642579},\ndoi = {10.1145/3613904.3642579},\nabstract = {We present Large Language Model for Mixed Reality (LLMR), a framework for the real-time creation and modification of interactive Mixed Reality experiences using LLMs. LLMR leverages novel strategies to tackle difficult cases where ideal training data is scarce, or where the design goal requires the synthesis of internal dynamics, intuitive analysis, or advanced interactivity. Our framework relies on text interaction and the Unity game engine. By incorporating techniques for scene understanding, task planning, self-debugging, and memory management, LLMR outperforms the standard GPT-4 by 4x in average error rate. We demonstrate LLMR’s cross-platform interoperability with several example worlds, and evaluate it on a variety of creation and modification tasks to show that it can produce and edit diverse objects, tools, and scenes. Finally, we conducted a usability study (N=11) with a diverse set that revealed participants had positive experiences with the system and would use it again.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {600},\nnumpages = {22},\nkeywords = {artificial intelligence, large language model, mixed reality, spatial reasoning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642519,\nauthor = {Han, Jihae and Vande Moere, Andrew and Simeone, Adalberto L.},\ntitle = {Meaning Follows Purpose: Unravelling the Architectural Design Conventions in the Contemporary Metaverse},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642519},\ndoi = {10.1145/3613904.3642519},\nabstract = {Thousands of people regularly meet, work and play in the architectural spaces that the metaverse offers today. Yet despite the creative potential to disrupt how the built environment is represented, there exists a prevalent belief that the architectural design of the metaverse is rather conventional and reliant on simulating physical reality. We investigated this claim by conducting a design critique study of the most apparent architectural design conventions within the current most popular metaverse platforms, as determined by a scoping review and Google Trends analysis. Based on the opinions of 21 architectural experts on the design of interiors, buildings, and plazas within these platforms, we elicited three overarching design conventions that capture the representation, engagement, and purpose of metaverse architecture. By discussing the impact of these conventions on architectural quality, we inform the future design of metaverse spaces to more purposefully, and perhaps less frequently, use realism to convey meaning.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {601},\nnumpages = {22},\nkeywords = {Architectural Design, Architectural Drawing, Cyberspace, Design Science, Design Studies, Metaverse, Scoping Review, User Study, Virtual Reality, Virtual Worlds},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642491,\nauthor = {Ghamandi, Ryan Khushan and Kattoju, Ravi Kiran and Hmaiti, Yahya and Maslych, Mykola and Taranta, Eugene Matthew and McMahan, Ryan P. and LaViola, Joseph},\ntitle = {Unlocking Understanding: An Investigation of Multimodal Communication in Virtual Reality Collaboration},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642491},\ndoi = {10.1145/3613904.3642491},\nabstract = {Communication in collaboration, especially synchronous, remote communication, is crucial to the success of task-specific goals. Insufficient or excessive forms of communication may lead to detrimental effects on task performance while increasing mental fatigue. However, identifying which combinations of communication modalities provide the most efficient transfer of information in collaborative settings will greatly improve collaboration. To investigate this, we developed a remote, synchronous, asymmetric VR collaborative assembly task application, where users play the role of either mentor or mentee, and were exposed to different combinations of three communication modalities: voice, gestures, and gaze. Through task-based experiments with 25 pairs of participants (50 individuals), we evaluated quantitative and qualitative data and found that gaze did not differ significantly from multiple combinations of communication modalities. Our qualitative results indicate that mentees experienced more difficulty and frustration in completing tasks than mentors, with both types of users preferring all three modalities to be present.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {602},\nnumpages = {16},\nkeywords = {collaboration, communication, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642351,\nauthor = {Bhatia, Arpit and Pohl, Henning and Hirzle, Teresa and Seifi, Hasti and Hornb\\ae{}k, Kasper},\ntitle = {Using the Visual Language of Comics to Alter Sensations in Augmented Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642351},\ndoi = {10.1145/3613904.3642351},\nabstract = {Augmented Reality (AR) excels at altering what we see but non-visual sensations are difficult to augment. To augment non-visual sensations in AR, we draw on the visual language of comic books. Synthesizing comic studies, we create a design space describing how to use comic elements (e.g., onomatopoeia) to depict non-visual sensations (e.g., hearing). To demonstrate this design space, we built eight demos, such as speed lines to make a user think they are faster and smell lines to make a scent seem stronger. We evaluate these elements in a qualitative user study (N=20) where participants performed everyday tasks with comic elements added as augmentations. All participants stated feeling a change in perception for at least one sensation, with perceived changes detected by between four participants (touch) and 15 participants (hearing). The elements also had positive effects on emotion and user experience, even when participants did not feel changes in perception.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {603},\nnumpages = {17},\nkeywords = {augmented reality, comics, sensory augmentation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642631,\nauthor = {Katins, Christopher and Wo\\'{z}niak, Pawe\\l{} W. and Chen, Aodi and Tumay, Ihsan and Le, Luu Viet Trinh and Uschold, John and Kosch, Thomas},\ntitle = {Assessing User Apprehensions About Mixed Reality Artifacts and Applications: The Mixed Reality Concerns (MRC) Questionnaire},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642631},\ndoi = {10.1145/3613904.3642631},\nabstract = {Current research in Mixed Reality (MR) presents a wide range of novel use cases for blending virtual elements with the real world. This yet-to-be-ubiquitous technology challenges how users currently work and interact with digital content. While offering many potential advantages, MR technologies introduce new security, safety, and privacy challenges. Thus, it is relevant to understand users’ apprehensions towards MR technologies, ranging from security concerns to social acceptance. To address this challenge, we present the Mixed Reality Concerns (MRC) Questionnaire, designed to assess users’ concerns towards MR artifacts and applications systematically. The development followed a structured process considering previous work, expert interviews, iterative refinements, and confirmatory tests to analytically validate the questionnaire. The MRC Questionnaire offers a new method of assessing users’ critical opinions to compare and assess novel MR artifacts and applications regarding security, privacy, social implications, and trust.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {604},\nnumpages = {13},\nkeywords = {Concerns, Mixed Reality, Privacy, Safety, Security, Social Acceptance, Trust, User Apprehensions},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642408,\nauthor = {Ring, Patrizia and Tietenberg, Julius and Emmerich, Katharina and Masuch, Maic},\ntitle = {Development and Validation of the Collision Anxiety Questionnaire for VR Applications},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642408},\ndoi = {10.1145/3613904.3642408},\nabstract = {The high degree of sensory immersion is a distinctive feature of head-mounted virtual reality (VR) systems. While the visual detachment from the real world enables unique immersive experiences, users risk collisions due to their inability to perceive physical obstacles in their environment. Even the mere anticipation of a collision can adversely affect the overall experience and erode user confidence in the VR system. However, there are currently no valid tools for assessing collision anxiety. We present the iterative development and validation of the Collision Anxiety Questionnaire (CAQ), involving an exploratory and a confirmatory factor analysis with a total of 159 participants. The results provide evidence for both discriminant and convergent validity and a good model fit for the final CAQ with three subscales: general collision anxiety, orientation, and interpersonal collision anxiety. By utilizing the CAQ, researchers can examine potential confounding effects of collision anxiety and evaluate methods for its mitigation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {605},\nnumpages = {13},\nkeywords = {assessment, collision anxiety, discomfort, fear, user experience, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642932,\nauthor = {In, Sungwon and Krokos, Eric and Whitley, Kirsten and North, Chris and Yang, Yalong},\ntitle = {Evaluating Navigation and Comparison Performance of Computational Notebooks on Desktop and in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642932},\ndoi = {10.1145/3613904.3642932},\nabstract = {The computational notebook serves as a versatile tool for data analysis. However, its conventional user interface falls short of keeping pace with the ever-growing data-related tasks, signaling the need for novel approaches. With the rapid development of interaction techniques and computing environments, there is a growing interest in integrating emerging technologies in data-driven workflows. Virtual reality, in particular, has demonstrated its potential in interactive data visualizations. In this work, we aimed to experiment with adapting computational notebooks into VR and verify the potential benefits VR can bring. We focus on the navigation and comparison aspects as they are primitive components in analysts’ workflow. To further improve comparison, we have designed and implemented a Branching&Merging functionality. We tested computational notebooks on the desktop and in VR, both with and without the added Branching&Merging capability. We found VR significantly facilitated navigation compared to desktop, and the ability to create branches enhanced comparison.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {606},\nnumpages = {15},\nkeywords = {3D UI \\& interaction, computational notebook system, data science, immersive analytics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642375,\nauthor = {Sarma, Abhraneel and Hwang, Kyle and Hullman, Jessica and Kay, Matthew},\ntitle = {Milliways: Taming Multiverses through Principled Evaluation of Data Analysis Paths},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642375},\ndoi = {10.1145/3613904.3642375},\nabstract = {Multiverse analyses involve conducting all combinations of reasonable choices in a data analysis process. A reader of a study containing a multiverse analysis might question—are all the choices included in the multiverse reasonable and equally justifiable? How much do results vary if we make different choices in the analysis process? In this work, we identify principles for validating the composition of, and interpreting the uncertainty in, the results of a multiverse analysis. We present Milliways, a novel interactive visualisation system to support principled evaluation of multiverse analyses. Milliways provides interlinked panels presenting result distributions, individual analysis composition, multiverse code specification, and data summaries. Milliways supports interactions to sort, filter and aggregate results based on the analysis specification to identify decisions in the analysis process to which the results are sensitive. To represent the two qualitatively different types of uncertainty that arise in multiverse analyses—probabilistic uncertainty from estimating unknown quantities of interest such as regression coefficients, and possibilistic uncertainty from choices in the data analysis—Milliways uses consonance curves and probability boxes. Through an evaluative study with five users familiar with multiverse analysis, we demonstrate how Milliways can support multiverse analysis tasks, including a principled assessment of the results of a multiverse analysis.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {607},\nnumpages = {15},\nkeywords = {Multiverse analysis, Principled evaluation, Statistical analysis},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642364,\nauthor = {Neuhaus, Robin and Ringfort-Felner, Ronda and Courtney, Daniel and Kneile, Madlen and Hassenzahl, Marc},\ntitle = {Virtual Unreality: Augmentation-Oriented Ideation Through Design Cards},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642364},\ndoi = {10.1145/3613904.3642364},\nabstract = {While realism is a common design goal for virtual reality (VR), VR also offers opportunities that are impossible in the real world (e.g., telekinesis). So far, there is no design support to exploit the potential of such “impossible” augmentations, especially for serious applications. We developed a card set and a workshop format, which features 15 opportunities to facilitate the ideation of augmentation-oriented VR. We piloted the method in five workshops with people in the early stages of developing a VR application (N=35). Participants found the cards easy to use and to inspire viable new concepts that differed from earlier ideas. Analysis of the concepts with interaction criticism identified two strategies: (1) augmentations that are only loosely related to the purpose of the application, simply to increase “fun”, and (2) augmentations that are closely related to the core purpose and thereby subtly facilitate its fulfillment. The latter has the greater potential.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {608},\nnumpages = {23},\nkeywords = {augmented human capabilities, design tools, superpowers, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642394,\nauthor = {Cho, Hyunsung and Yan, Yukang and Todi, Kashyap and Parent, Mark and Smith, Missie and Jonker, Tanya R. and Benko, Hrvoje and Lindlbauer, David},\ntitle = {MineXR: Mining Personalized Extended Reality Interfaces},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642394},\ndoi = {10.1145/3613904.3642394},\nabstract = {Extended Reality (XR) interfaces offer engaging user experiences, but their effective design requires a nuanced understanding of user behavior and preferences. This knowledge is challenging to obtain without the widespread adoption of XR devices. We introduce  MineXR, a design mining workflow and data analysis platform for collecting and analyzing personalized XR user interaction and experience data.  MineXR enables elicitation of personalized interfaces from participants of a data collection: for any particular context, participants create interface elements using application screenshots from their own smartphone, place them in the environment, and simultaneously preview the resulting XR layout on a headset. Using  MineXR, we contribute a dataset of personalized XR interfaces collected from 31 participants, consisting of 695 XR widgets created from 178 unique applications. We provide insights for XR widget functionalities, categories, clusters, UI element types, and placement. Our open-source tools and data support researchers and designers in developing future XR interfaces.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {609},\nnumpages = {17},\nkeywords = {Datasets, Extended Reality, Personalized UI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642527,\nauthor = {Ye, Hui and Leng, Jiaye and Xu, Pengfei and Singh, Karan and Fu, Hongbo},\ntitle = {ProInterAR: A Visual Programming Platform for Creating Immersive AR Interactions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642527},\ndoi = {10.1145/3613904.3642527},\nabstract = {AR applications commonly contain diverse interactions among different AR contents. Creating such applications requires creators to have advanced programming skills for scripting interactive behaviors of AR contents, repeated transferring and adjustment of virtual contents from virtual to physical scenes, testing by traversing between desktop interfaces and target AR scenes, and digitalizing AR contents. Existing immersive tools for prototyping/authoring such interactions are tailored for domain-specific applications. To support programming general interactive behaviors of real object(s)/environment(s) and virtual object(s)/environment(s) for novice AR creators, we propose ProInterAR, an integrated visual programming platform to create immersive AR applications with a tablet and an AR-HMD. Users can construct interaction scenes by creating virtual contents and augmenting real contents from the view of an AR-HMD, script interactive behaviors by stacking blocks from a tablet UI, and then execute and control the interactions in the AR scene. We showcase a wide range of AR application scenarios enabled by ProInterAR, including AR game, AR teaching, sequential animation, AR information visualization, etc. Two usability studies validate that novice AR creators can easily program various desired AR applications using ProInterAR.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {610},\nnumpages = {15},\nkeywords = {AR contents, AR interactions, Visual programming},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642220,\nauthor = {Pei, Siyou and Kim, David and Olwal, Alex and Zhang, Yang and Du, Ruofei},\ntitle = {UI Mobility Control in XR: Switching UI Positionings between Static, Dynamic, and Self Entities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642220},\ndoi = {10.1145/3613904.3642220},\nabstract = {Extended reality (XR) has the potential for seamless user interface (UI) transitions across people, objects, and environments. However, the design space, applications, and common practices of 3D UI transitions remain underexplored. To address this gap, we conducted a need-finding study with 11 participants, identifying and distilling a taxonomy based on three types of UI placements — affixed to static, dynamic, or self entities. We further surveyed 113 commercial applications to understand the common practices of 3D UI mobility control, where only 6.2\\% of these applications allowed users to transition UI between entities. In response, we built interaction prototypes to facilitate UI transitions between entities. We report on results from a qualitative user study (N=14) on 3D UI mobility control using our FingerSwitches technique, which suggests that perceived usefulness is affected by types of entities and environments. We aspire to tackle a vital need in UI mobility within XR.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {611},\nnumpages = {12},\nkeywords = {Embodied Interactions, Extended Reality, Hand Gestures, Mode Switching, UI Mobility, User Interface Behaviors, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642235,\nauthor = {Wang, Zhan and Yuan, Lin-Ping and Wang, Liangwei and Jiang, Bingchuan and Zeng, Wei},\ntitle = {VirtuWander: Enhancing Multi-modal Interaction for Virtual Tour Guidance through Large Language Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642235},\ndoi = {10.1145/3613904.3642235},\nabstract = {Tour guidance in virtual museums encourages multi-modal interactions to boost user experiences, concerning engagement, immersion, and spatial awareness. Nevertheless, achieving the goal is challenging due to the complexity of comprehending diverse user needs and accommodating personalized user preferences. Informed by a formative study that characterizes guidance-seeking contexts, we establish a multi-modal interaction design framework for virtual tour guidance. We then design VirtuWander, a two-stage innovative system using domain-oriented large language models to transform user inquiries into diverse guidance-seeking contexts and facilitate multi-modal interactions. The feasibility and versatility of VirtuWander are demonstrated with virtual guiding examples that encompass various touring scenarios and cater to personalized preferences. We further evaluate VirtuWander through a user study within an immersive simulated museum. The results suggest that our system enhances engaging virtual tour experiences through personalized communication and knowledgeable assistance, indicating its potential for expanding into real-world scenarios.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {612},\nnumpages = {20},\nkeywords = {large language models, multi-modal feedback, virtual museum},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642138,\nauthor = {Wang, Huanchen and Zhao, Minzhu and Hu, Wanyang and Ma, Yuxin and Lu, Zhicong},\ntitle = {Critical Heritage Studies as a Lens to Understand Short Video Sharing of Intangible Cultural Heritage on Douyin},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642138},\ndoi = {10.1145/3613904.3642138},\nabstract = {Intangible Cultural Heritage (ICH) faces numerous threats that can lead to its destruction. While the emergence of short video platforms provides opportunities for fostering innovation and communication among ICH practitioners and viewers, it is still understudied how different stakeholders present, explain, and manage ICH via short videos. To address this, we conduct a mixed-method study of ICH-related videos on Douyin, a popular short video platform in China with an extensive user base and wealth of ICH content. By adopting the Critical Heritage Studies (CHS) framework, we propose a taxonomy of frames that construct the landscape of ICH short videos and then investigate the interactions among different groups regarding power, identity, and knowledge. Additionally, we analyze viewer responses to different frames and groups based on audience metrics (e.g., # of likes and comments) and comments. Our research reveals that government-affiliated and indigenous groups dominate the promotion and presentation of ICH on Douyin. Contrary to previous literature, viewer responses show a preference for videos from external ICH groups and ordinary individuals, suggesting a tendency to counter authority and exclusivity associated with ICH. Moreover, it highlights a lack of sustainable debates and negotiations among different groups involved in ICH discourse. Situated within CHS, we provide design implications for ICH safeguarding and sustainability through short videos and online media.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {613},\nnumpages = {21},\nkeywords = {Intangible cultural heritage, critical theory, online video platforms},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642026,\nauthor = {Reitmaier, Thomas and Raju, Dani Kalarikalayil and Klejch, Ondrej and Wallington, Electra and Markl, Nina and Pearson, Jennifer and Jones, Matt and Bell, Peter and Robinson, Simon},\ntitle = {Cultivating Spoken Language Technologies for Unwritten Languages},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642026},\ndoi = {10.1145/3613904.3642026},\nabstract = {We report on community-centered, collaborative research that weaves together HCI, natural language processing, linguistic, and design insights to develop spoken language technologies for unwritten languages. Across three visits to a Banjara farming community in India, we use participatory, technical, and creative methods to engage community members, collect spoken language photo annotations, and develop an information retrieval (IR) system. Drawing on orality theory, we interrogate assumptions and biases of current speech interfaces and create a simple application that leverages our IR system to match fluidly spoken queries with recorded annotations and surface corresponding photos. In-situ evaluations show how our novel approach returns reliable results and inspired the co-creation of media retrieval use-cases that are more appropriate in oral contexts. The very low (< 4h) spoken data requirements makes our approach adaptable to other contexts where languages are unwritten or have no digital language resources available.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {614},\nnumpages = {17},\nkeywords = {Speech/language, co-creation, field study, zero-resource information retrieval},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642682,\nauthor = {Kotut, Lindah and Bhatti, Neelma and Hassan, Taha and Haqq, Derek and Saaty, Morva},\ntitle = {Griot-Style Methodology: Longitudinal Study of Navigating Design With Unwritten Stories},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642682},\ndoi = {10.1145/3613904.3642682},\nabstract = {We describe a seven-year longitudinal study conducted in collaboration with an indigenous community in Kenya. We detail the process of conducting research with an oral community: the deliberate practice of understanding and collecting stories; working with inter-generational community to envision and design technologies that support their ways of storytelling and story preservation; and to influence the design of other technologies. We chronicle how we contended with translating oral stories with rich metaphors to new mediums, and the dimensions of trust we have established and continue to reinforce. We offer our griot-style methodology, informed by working with the community and retrofitting existing HCI approaches: as an example model of what has worked, and the dimensions of challenges at each stage of the research work. The griot-style methodology has prompted a reflection on how we approach research, and present opportunities for other HCI research and practice of handling community stories.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {615},\nnumpages = {14},\nkeywords = {Griot Methodology, Griots, HCI4D, ICH, Indigenous Knowledge, Longitudinal Study, Oral Communities, Orality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642931,\nauthor = {Noe, Kari and Kirshenbaum, Nurit},\ntitle = {Where Generalized Equitable Design Practice Meet Specific Indigenous Communities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642931},\ndoi = {10.1145/3613904.3642931},\nabstract = {There are many approaches to design frameworks that guide designers through co-designing with Indigenous communities. Designers that want to be respectful to the Indigenous communities look towards these equitable design approaches to ensure they are not perpetuating histories of harm. However, some of these approaches are prescriptive to a generalized “Indigenous community.\" Through these guidelines designers often develop a practice through their own interpretations of what is equitable for this learned idea of a generalized Indigenous community. This can be limiting, as what are respectful practices to an Indigenous community can be vastly different. This paper engages with these generalized guidelines of co-design to present and discuss a method of developing customized practice to collaborate with specific Indigenous communities. We showcase the framework with our experience of developing a design practice for the Office of Indigenous Knowledge and Innovation’s work with the Kanaka Maoli (Native Hawaiian) community.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {616},\nnumpages = {8},\nkeywords = {Collaboration, Cultural Heritage/History, Design Methods, Method},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642598,\nauthor = {Guerrero Millan, Carlos and Nissen, Bettina and Pschetz, Larissa},\ntitle = {Cosmovision Of Data: An Indigenous Approach to Technologies for Self-Determination},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642598},\ndoi = {10.1145/3613904.3642598},\nabstract = {This paper analyses practices of data perception and usage, as well as ongoing and envisioned community technology projects carried out by a Masewal Indigenous group in Mexico through their union of cooperatives, Tosepan. Through fieldwork interviews, Masewal participants expressed how they have been appropriating existing technologies for their people’s self-determination. During a workshop, they imagined how diverse knowledges and lived experiences of their worldview, passed down through generations, could be represented and translated into digital practices more broadly. We draw considerations for the HCI community to embrace novel approaches to data and information systems from the community’s concept of Cosmovision, and develop Micro-, Meso- and Macro- lenses within it. Through these lenses, we discuss how technologies could be designed for specific individual practices and connections with nature (Micro-cosmos) while supporting communal actions for autonomy and self-determination (Meso-cosmos), and considering broader worldmaking processes and implications for identity, prosperity, ecology and plural representations (Macro-cosmos).},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {617},\nnumpages = {13},\nkeywords = {Community Technologies, Cooperative data systems, Cosmovision, Decoloniality, Design research, Indigenous technologies, Latin America, Pluriversality, Self-determination},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642748,\nauthor = {Ferreira, Pedro},\ntitle = {Examining the \"Local\" in ICT4D: A Postcolonial Perspective on Participation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642748},\ndoi = {10.1145/3613904.3642748},\nabstract = {ICT4D has increasingly adopted participation and community involvement to address power imbalances, namely through the figure of the \"local\". However, this reliance makes assumptions about the nature of the \"local\" while limiting scrutiny of research approaches. Through a Postcolonial Critical Discourse Analysis, this paper argues that 1) communities are often essentialized in agency-depriving ways, 2) researchers claim substantial discretionary power in representing communities, and 3) participatory approaches are framed as inherently beneficial, obscuring compromises. The analysis suggests participation serves to maintain the status quo. Going forward, ICT4D research should ground claims in evidence, demonstrate community benefits, acknowledge complexities transparently, and question premises that empirical gaps alone justify research. Rather than participation as a panacea, a reflexive ICT4D should scrutinize representational practices and notions of empowerment that may perpetuate inequities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {618},\nnumpages = {13},\nkeywords = {discourse analysis, hci4d, ict4d, participation, postcolonial theory},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642142,\nauthor = {Elkhuizen, Willemijn S. and Love, Jeff and Parisi, Stefano and Karana, Elvin},\ntitle = {On the Role of Materials Experience for Novel Interactions with Digital Representations of Historical Pop-up and Movable Books},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642142},\ndoi = {10.1145/3613904.3642142},\nabstract = {Direct interaction with cultural heritage (CH) artefacts is frequently unavailable to visitors, offering an opportunity for HCI designers to explore integrating material aspects into digitally-mediated encounters with CH artefacts. We argue that a thorough understanding of the material experiences of CH artefacts can open a novel design space, enabling engaging and meaningful interactions with digital representations. Capitalising on this potential, we present a user study where we systematically explore the material experiences of historic pop-up and movable books. Our analysis identifies five key material qualities to inspire augmentation: fold-ability, slide-ability, tear-ability, age-ability, and print-ability. Highlighting how these material qualities can inspire novel interactions with their digital representations, we present two extended-reality (XR) prototypes of a CH book. With our work, we present HCI designers with a novel approach on designing CH experiences, firmly rooted in materiality, challenging the prevalent paradigms of ‘technology-driven’ or ‘as-realistic-as-possible’ sensory experiences often found in CH-HCI.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {619},\nnumpages = {18},\nkeywords = {Books, Cultural Heritage, Materiality, Materials Experience, Mixed Reality, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642877,\nauthor = {Zhang, Lili and Liao, Xi and Yang, Zaijia and Gao, Baihang and Wang, Chunjie and Yang, Qiuling and Li, Deshun},\ntitle = {Partiality and Misconception: Investigating Cultural Representativeness in Text-to-Image Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642877},\ndoi = {10.1145/3613904.3642877},\nabstract = {Text-to-image (T2I) models enable users worldwide to create high-definition and realistic images through text prompts, where the underrepresentation and potential misinformation of images have raised growing concerns. However, few existing works examine cultural representativeness, especially involving whether the generated content can fairly and accurately reflect global cultures. Combining automated and human methods, we investigate this issue in multiple dimensions quantificationally and conduct a set of evaluations on three prevailing T2I models (DALL-E v2, Stable Diffusion v1.5 and v2.1). Introducing attributes of cultural cluster and subject, we provide a fresh interdisciplinary perspective to bias analysis. The benchmark dataset UCOGC is presented, which encompasses authentic images of unique cultural objects from global clusters. Our results reveal that the culture of a disadvantaged country is prone to be neglected, some specified subjects often present a stereotype or a simple patchwork of elements, and over half of cultural objects are mispresented.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {620},\nnumpages = {25},\nkeywords = {bias, cultural cluster, cultural representativeness, stereotype, text-to-image generation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642141,\nauthor = {Zhang, Guanhua and Hu, Zhiming and B\\^{a}ce, Mihai and Bulling, Andreas},\ntitle = {Mouse2Vec: Learning Reusable Semantic Representations of Mouse Behaviour},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642141},\ndoi = {10.1145/3613904.3642141},\nabstract = {The mouse is a pervasive input device used for a wide range of interactive applications. However, computational modelling of mouse behaviour typically requires time-consuming design and extraction of handcrafted features, or approaches that are application-specific. We instead propose Mouse2Vec  – a novel self-supervised method designed to learn semantic representations of mouse behaviour that are reusable across users and applications. Mouse2Vec uses a Transformer-based encoder-decoder architecture, which is specifically geared for mouse data: During pretraining, the encoder learns an embedding of input mouse trajectories while the decoder reconstructs the input and simultaneously detects mouse click events. We show that the representations learned by our method can identify interpretable mouse behaviour clusters and retrieve similar mouse trajectories. We also demonstrate on three sample downstream tasks that the representations can be practically used to augment mouse data for training supervised methods and serve as an effective feature extractor.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {621},\nnumpages = {17},\nkeywords = {Behaviour retrieval, Data augmentation, Mouse input, Representation learning, Self-supervised learning, Transformer},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642316,\nauthor = {Yamanaka, Shota and Stuerzlinger, Wolfgang},\ntitle = {The Effect of Latency on Movement Time in Path-steering},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642316},\ndoi = {10.1145/3613904.3642316},\nabstract = {In current graphical user interfaces, there exists a (typically unavoidable) end-to-end latency from each pointing-device movement to its corresponding cursor response on the screen, which is known to affect user performance in target selection, e.g., in terms of movement time (MT). Previous work also reported that a long latency increases MTs in path-steering tasks, but the quantitative relationship between latency and MT had not been previously investigated for path-steering. In this work, we derive models to predict MTs for path-steering and evaluate them with five tasks: goal crossing as a preliminary task for model derivation, linear-path steering, circular-path steering, narrowing-path steering, and steering with target pointing. The results show that the proposed models yielded an adjusted R2 > 0.94, with lower AICs and smaller cross-validation RMSEs than the baseline models, enabling more accurate prediction of MTs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {622},\nnumpages = {19},\nkeywords = {Human motor performance, graphical user interface, operational time prediction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642904,\nauthor = {Lee, Dawon and Kim, Sunjun and Noh, Junyong and Lee, Byungjoo},\ntitle = {User Performance in Consecutive Temporal Pointing: An Exploratory Study},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642904},\ndoi = {10.1145/3613904.3642904},\nabstract = {A significant amount of research has recently been conducted on user performance in so-called temporal pointing tasks, in which a user is required to perform a button input at the timing required by the system. Consecutive temporal pointing (CTP), in which two consecutive button inputs must be performed while satisfying temporal constraints, is common in modern interactions, yet little is understood about user performance on the task. Through a user study involving 100 participants, we broadly explore user performance in a variety of CTP scenarios. The key finding is that CTP is a unique task that cannot be considered as two ordinary temporal pointing processes. Significant effects of button input method, motor limitations, and different hand use were also observed.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {623},\nnumpages = {15},\nkeywords = {Button Inputs, Fitts’ Law, Temporal Pointing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641942,\nauthor = {Komatsu, Takanori and Xie, Chenxi and Yamada, Seiji},\ntitle = {Waiting Time Perceptions for Faster Count-downs/ups Are More Sensitive Than Slower Ones: Experimental Investigation and Its Application},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641942},\ndoi = {10.1145/3613904.3641942},\nabstract = {Countdowns and count-ups are very useful displays that explicitly show how long users should wait and also show the current processing states of a given task. Most countdowns or count-ups decrease or increase their digit every one second exactly, and most users have an implicit assumption that the digit changes every one second exactly. However, there are no studies that investigate how users perceive wait times with these countdowns and count-ups and that consider changing users’ perception of time passing as shorter than the actual passage of time by means of countdowns and count-ups while taking into account such user assumptions. To clarify these issues, we first investigated how users perceive countdowns “from 3/5/10 to 0” and count-ups “from 0 to 3/5/10” that have different lengths of intervals from 800 to 1200 msec (Experiment 1). Next, on the basis of the results of Experiment 1, we explored a novel method for presenting countdowns to make users perceive the wait time as being shorter than the actual wait time (Experiment 2) and investigated whether such countdowns can be used in realistic applications or not (Experiment 3). As a result, we found that countdowns and count-ups that were “from 250 msec shorter to 10\\% longer” than 3, 5, or 10 sec were perceived as 3, 5, or 10 sec, respectively, and those “from 5 to 0” (their lengths were 5 sec) that first displayed extremely shorter intervals were perceived as being shorter than their actual length (5 sec). Finally, we confirmed the applicability and effectiveness of such displays in a realistic application. Thus, we strongly argue that these findings could become indispensable knowledge for researchers in this research field to reduce users’ cognitive load during wait times.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {624},\nnumpages = {13},\nkeywords = {Count-up, Countdown, Time Perception, Wait Time},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641938,\nauthor = {Hirskyj-Douglas, Ilyena and Cunha, Jennifer and Kleinberger, Rebecca},\ntitle = {Call of the Wild Web: Comparing Parrot Engagement in Live vs. Pre-Recorded Video Calls},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641938},\ndoi = {10.1145/3613904.3641938},\nabstract = {The concept of the animal Internet has flourished, with many conceptualisations proceeding from the premise that connecting animals online may enrich their social life. Yet we remain unaware of how – or even whether – online interactions (either live or with pre-recorded material) might affect how animals engage with other animals. We implemented a system for parrots to trigger live video calls with other birds or playback from a pre-recorded video call. The goal was to identify differences in engagement and behaviours. Over a six-month study, parrots triggered significantly more live calls and engaged longer in that setting relative to the playback condition, while the animals’ caregivers found greater value in the latter but preferred the live alternative for the birds under their care. The results begin to question what animals make of online remote connections, putting forward considerations as to how the internet can affect animals’ experiences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {625},\nnumpages = {14},\nkeywords = {Animal Internet, Animal–Computer Interaction, Parrot},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641994,\nauthor = {Benford, Steven David and Mancini, Clara and Chamberlain, Alan and Schneiders, Eike and Castle-Green, Simon D and Fischer, Joel E and Kucukyilmaz, Ayse and Salimbeni, Guido and Ngo, Victor Zhi Heung and Barnard, Pepita and Adams, Matt and Tandavanitj, Nick and Row Farr, Ju},\ntitle = {Charting Ethical Tensions in Multispecies Technology Research through Beneficiary-Epistemology Space},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641994},\ndoi = {10.1145/3613904.3641994},\nabstract = {While ethical challenges are widely discussed in HCI, far less is reported about the ethical processes that researchers routinely navigate. We reflect on a multispecies project that negotiated an especially complex ethical approval process. Cat Royale was an artist-led exploration of creating an artwork to engage audiences in exploring trust in autonomous systems. The artwork took the form of a robot that played with three cats. Gaining ethical approval required an extensive dialogue with three Institutional Review Boards (IRBs) covering computer science, veterinary science and animal welfare, raising tensions around the welfare of the cats, perceived benefits and appropriate methods, and reputational risk to the University. To reveal these tensions we introduce beneficiary-epistemology space, that makes explicit who benefits from research (humans or animals) and underlying epistemologies. Positioning projects and IRBs in this space can help clarify tensions and highlight opportunities to recruit additional expertise.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {626},\nnumpages = {15},\nkeywords = {Animal Ethics, Animal-Computer Interaction, Art, Artist-led research, Epistemology, Ethical Review, IRB, Medical Science, Research Ethics, Veterinary-Science},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3643654,\nauthor = {Cunha, Jennifer and Renguette, Corinne C and Singh, Nikhil and Stella, Lily and Mcmahon, Megan and Jin, Hao and Kleinberger, Rebecca},\ntitle = {Ellie Talks About the Weather: Toward Evaluating the Expressive and Enrichment Potential of a Tablet-Based Speech Board in a Single Goffin’s Cockatoo},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3643654},\ndoi = {10.1145/3613904.3643654},\nabstract = {Augmentative and alternative communication devices (AACs) are designed to assist humans with complex communication needs. Recently, AAC use has been reported in non-human animals. Such tools may potentially provide enrichment and increase interspecies connection. However, there is no evaluation framework and little data available to assess AAC potential. Here, we examine seven months of a single parrot’s sustained use of a tablet-based AAC totaling 129 sessions within 190 days. After devising a coding schema, we propose a framework to explore the expressive potential and enrichment value for the parrot. Our results suggest that the choice of destination words cannot be simply explained based on random selection or icon location alone, and 92\\% of corroborable selections are validated by behaviors. The parrot interactions also appear significantly skewed toward social and cognitive enrichment. This work is a first step toward assessment of AAC use for parrot enrichment.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {627},\nnumpages = {16},\nkeywords = {Animal Enrichment, Animal-Computer Interactions, Touchscreen Interactions},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641888,\nauthor = {Kankaanp\\\"{a}\\\"{a}, Vilma and Clark, Fay E and Hirskyj-Douglas, Ilyena},\ntitle = {LemurLounge: Lemurs' Individual-Level, Group, and Cross-Species Use of an Interactive Audio Device in Zoos},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641888},\ndoi = {10.1145/3613904.3641888},\nabstract = {Computer technology for animals is typically oriented toward isolated individuals, seldom attending to such group-living factors as accommodating differences between individuals. To address this shortcoming of research and practice, the authors designed and developed an audio-based system that lets lemurs in group accommodation voluntarily trigger audio via a novel device dubbed LemurLounge and listen to it on their own. This interactive system was deployed for 14 lemurs, of three species (black-and-white, brown, and ring-tailed), in their normal habitat. The device’s presence clearly influenced lemurs’ visits to the relevant portion of the enclosure. Alongside a general preference for audio over silence, assessment of individual- and species-level differences revealed significant differences at both levels, though no particular sound type (rainfall, traffic, either upbeat or relaxing music, or white noise) was favoured. The findings and design work highlight the need for customisable and adaptive computer technology for animals living in group settings, with important implications for lemurs and other primates, humans included.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {628},\nnumpages = {14},\nkeywords = {animal enrichment, animal–computer interaction, audio, lemur, proximity},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642119,\nauthor = {Kleinberger, Rebecca and Cunha, Jennifer and McMahon, Megan and Hirskyj-Douglas, Ilyena},\ntitle = {No More Angry Birds: Investigating Touchscreen Ergonomics to Improve Tablet-Based Enrichment for Parrots},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642119},\ndoi = {10.1145/3613904.3642119},\nabstract = {Touchscreen devices, ubiquitous in humans’ day-to-day life, offer a promising avenue for animal enrichment. With advanced cognitive abilities, keen visual perception, and adeptness to engage with capacitive screens using dexterous tongues, parrots are uniquely positioned to benefit from this technology. Additionally, pet parrots often lack appropriate stimuli, supporting the need for inexpensive solutions using off-the-shelf devices. However, the current human-centric interaction design standards of tablet applications do not optimally cater to the tactile affordances and ergonomic needs of parrots. To address this, we conducted a study with 20 pet parrots, examining their tactile interactions with touchscreens and evaluating the applicability of existing HCI interaction models. Our research highlights key ergonomic characteristics unique to parrots, which include pronounced multi-tap behavior, a critical size threshold for touch targets, and greater effectiveness of larger targets over closer proximity. Based on these insights, we propose guidelines for tablet-based enrichment systems for companion parrots.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {629},\nnumpages = {16},\nkeywords = {Animal Enrichment, Animal Usability, Animal-Computer Interactions, Fitts’ Law, Interspecies Interactions, Parrot, Touchscreen Interactions},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642486,\nauthor = {Kargut, Kim and Gutwin, Carl and Cockburn, Andy},\ntitle = {Effects of Device Environment and Information Layout on Spatial Memory and Performance in VR Selection Tasks},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642486},\ndoi = {10.1145/3613904.3642486},\nabstract = {Virtual Reality systems are increasingly proposed as a platform for everyday interactive software. Many applications are dependent on actions such as navigation and selection, but it is not clear how well immersive environments support these basic activities. Previous studies have suggested advantages for spatial learning in VR, so we carried out a study that investigated two aspects of immersion on spatial memory and selection: the degree to which the user is immersed in the data, and whether the system uses immersive input and output. The study showed that more-immersive conditions had substantially worse selection performance, and did not improve spatial learning. However, most participants believed that the immersive conditions were better for learning object locations, and most people preferred the immersive layout and the HMD. Our study suggests that designers should be cautious about assuming that everyday software applications will benefit from being deployed in an immersive VR environment.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {630},\nnumpages = {17},\nkeywords = {immersiveness, selection performance, spatial memory, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642192,\nauthor = {Hedlund, Martin and Bogdan, Cristian and Meixner, Gerrit and Matviienko, Andrii},\ntitle = {Rowing Beyond: Investigating Steering Methods for Rowing-based Locomotion in Virtual Environments},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642192},\ndoi = {10.1145/3613904.3642192},\nabstract = {Rowing has great potential in Virtual Reality (VR) exergames as it requires physical effort and uses physical motion to map the locomotion in a virtual space. However, rowing in VR is currently restricted to locomotion along one axis, leaving 2D and 3D locomotion out of the scope. To facilitate rowing-based locomotion, we implemented three steering techniques based on head, hands, and feet movements for 2D and 3D VR environments. To investigate these methods, we conducted a controlled experiment (N = 24) to assess the user performance, experience and VR sickness. We found that head steering leads to fast and precise steering in 2D and 3D, and hand steering is the most realistic. Feet steering had the largest performance difference between 2D and 3D but comparable precision to hands in 2D. Lastly, head steering is the least mentally demanding, and all methods had comparable VR sickness.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {631},\nnumpages = {17},\nkeywords = {exergame, locomotion, rowing, steering, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641974,\nauthor = {van Gemert, Thomas and Nilsson, Niels Christian and Hirzle, Teresa and Bergstr\\\"{o}m, Joanna},\ntitle = {Sicknificant Steps: A Systematic Review and Meta-analysis of VR Sickness in Walking-based Locomotion for Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641974},\ndoi = {10.1145/3613904.3641974},\nabstract = {Walking-based locomotion techniques in virtual reality (VR) can use redirection to enable walking in a virtual environment larger than the physical one. This results in a mismatch between the perceived virtual and physical movement, which is known to cause VR sickness. However, it is unclear if different types of walking techniques (e.g., resetting, reorientation, or self-overlapping spaces) affect VR sickness differently. To address this, we conducted a systematic review and meta-analysis of 96 papers published in 2016–2022 that measure VR sickness in walking-based locomotion. We find different VR sickness effects between types of redirection and between normal walking and redirection. However, we also identified several problems with the use and reporting of VR sickness measures. We discuss the challenges in understanding VR sickness differences between walking techniques and present guidelines for measuring VR sickness in locomotion studies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {632},\nnumpages = {36},\nkeywords = {locomotion, reality, sickness, ssq, virtual, vr, vrise, walking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642811,\nauthor = {Lystb\\ae{}k, Mathias N. and Pfeuffer, Ken and Langlotz, Tobias and Gr\\o{}nb\\ae{}k, Jens Emil Sloth and Gellersen, Hans},\ntitle = {Spatial Gaze Markers: Supporting Effective Task Switching in Augmented Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642811},\ndoi = {10.1145/3613904.3642811},\nabstract = {Task switching can occur frequently in daily routines with physical activity. In this paper, we introduce Spatial Gaze Markers, an augmented reality tool to support users in immediately returning to the last point of interest after an attention shift. The tool is task-agnostic, using only eye-tracking information to infer distinct points of visual attention and to mark the corresponding area in the physical environment. We present a user study that evaluates the effectiveness of Spatial Gaze Markers in simulated physical repair and inspection tasks against a no-marker baseline. The results give insights into how Spatial Gaze Markers affect user performance, task load, and experience of users with varying levels of task type and distractions. Our work is relevant to assist physical workers with simple AR techniques and render task switching faster with less effort.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {633},\nnumpages = {11},\nkeywords = {attention switching, augmented reality, eye-tracking, gaze interaction, task switching},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642147,\nauthor = {Kang, Sei and Jeong, Jaejoon and Lee, Gun A. and Kim, Soo-Hyung and Yang, Hyung-Jeong and Kim, Seungwon},\ntitle = {The RayHand Navigation: A Virtual Navigation Method with Relative Position between Hand and Gaze-Ray},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642147},\ndoi = {10.1145/3613904.3642147},\nabstract = {In this paper, we introduce a novel Virtual Reality (VR) navigation method using gaze ray and hand, named RayHand navigation. It supports controlling navigation speed and direction by quickly indicating the initial direction using gaze and then using dexterous hand movement for controlling the speed and direction based on the relative position between the gaze ray and user's hand. We conducted a user study comparing our approach to the head-hand and torso-leaning-based navigation methods, and also evaluated their learning effect. The results showed that the RayHand and head-hand navigations were less physically demanding than the torso-leaning navigation, and the RayHand supported rich navigation experience with high hedonic quality and solved the issue of the user unintentionally stepping out from the designated interaction area. In addition, our approach showed a significant improvement over time with a learning effect.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {634},\nnumpages = {15},\nkeywords = {gaze-ray, navigation, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642154,\nauthor = {G\\\"{u}nther, Sebastian and Skogseide, Alexandra and Buhlmann, Robin and M\\\"{u}hlh\\\"{a}user, Max},\ntitle = {Assessing the Influence of Visual Cues in Virtual Reality on the Spatial Perception of Physical Thermal Stimuli},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642154},\ndoi = {10.1145/3613904.3642154},\nabstract = {Advancements in haptics for Virtual Reality (VR) increased the quality of immersive content. Particularly, recent efforts to provide realistic temperature sensations have gained traction, but most often require very specialized or large complex devices to create precise thermal actuations. However, being largely detached from the real world, such a precise correspondence between the physical location of thermal stimuli and the shown visuals in VR might not be necessary for an authentic experience. In this work, we contribute the findings of a controlled experiment with 20 participants, investigating the spatial localization accuracy of thermal stimuli while having matching and non-matching visual cues of a virtual heat source in VR. Although participants were highly confident in their localization decisions, their ability to accurately pinpoint thermal stimuli was notably deficient.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {635},\nnumpages = {12},\nkeywords = {haptic feedback, temperature, thermal stimuli, user study, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642935,\nauthor = {Wang, Xizi and Lafreniere, Ben and Zhao, Jian},\ntitle = {Exploring Visualizations for Precisely Guiding Bare Hand Gestures in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642935},\ndoi = {10.1145/3613904.3642935},\nabstract = {Bare hand interaction in augmented or virtual reality (AR/VR) systems, while intuitive, often results in errors and frustration. However, existing methods, such as a static icon or a dynamic tutorial, can only inform simple and coarse hand gestures and lack corrective feedback. This paper explores various visualizations for enhancing precise hand interaction in VR. Through a comprehensive two-part formative study with 11 participants, we identified four types of essential information for visual guidance and designed different visualizations that manifest these information types. We further distilled four visual designs and conducted a controlled lab study with 15 participants to assess their effectiveness for various single- and double-handed gestures. Our results demonstrate that visual guidance significantly improved users’ gesture performance, reducing time and workload while increasing confidence. Moreover, we found that the visualization did not disrupt most users’ immersive VR experience or their perceptions of hand tracking and gesture recognition reliability.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {636},\nnumpages = {19},\nkeywords = {Virtual reality, error visualization, hand gesture recognition., visual guidance},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642091,\nauthor = {Sehrt, Jessica and Ferreira, Leonardo Leite and Weyers, Karsten and Mahmood, Amir and Kosch, Thomas and Schwind, Valentin},\ntitle = {Improving Electromyographic Muscle Response Times through Visual and Tactile Prior Stimulation in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642091},\ndoi = {10.1145/3613904.3642091},\nabstract = {Electromyography (EMG) enables hands-free interactions by detecting muscle activity at different human body locations. Previous studies have demonstrated that input performance based on isometric contractions is muscle-dependent and can benefit from synchronous biofeedback. However, it remains unknown whether stimulation before interaction can help to localize and tense a muscle faster. In a response-based VR experiment (N=21), we investigated whether prior stimulation using visual or tactile cues at four different target muscles (biceps, triceps, upper leg, calf) can help reduce the time to perform isometric muscle contractions. The results show that prior stimulation decreases EMG reaction times with visual, vibrotactile, and electrotactile cues. Our experiment also revealed important findings regarding learning and fatigue at the different body locations. We provide qualitative insights into the participants’ perceptions and discuss potential reasons for the improved interaction. We contribute with implications and use cases for prior stimulated muscle activation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {637},\nnumpages = {17},\nkeywords = {Assistive Systems, Electrical Muscle Stimulation, Electromyography, Physiological Sensing, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642437,\nauthor = {Yang, Yongjie and Chen, Tao and Huang, Yujing and Guo, Xiuzhen and Shangguan, Longfei},\ntitle = {MAF: Exploring Mobile Acoustic Field for Hand-to-Face Gesture Interactions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642437},\ndoi = {10.1145/3613904.3642437},\nabstract = {We present MAF, a novel acoustic sensing approach that leverages the commodity hardware in bone conduction earphones for hand-to-face gesture interactions. Briefly, by shining audio signals with bone conduction earphones, we observe that these signals not only propagate along the surface of the human face but also dissipate into the air, creating an acoustic field that envelops the individual’s head. We conduct benchmark studies to understand how various hand-to-face gestures and human factors influence this acoustic field. Building on the insights gained from these initial studies, we then propose a deep neural network combined with signal preprocessing techniques. This combination empowers MAF to effectively detect, segment, and subsequently recognize a variety of hand-to-face gestures, whether in close contact with the face or above it. Our comprehensive evaluation based on 22 participants demonstrates that MAF achieves an average gesture recognition accuracy of 92\\% across ten different gestures tailored to users’ preferences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {638},\nnumpages = {20},\nkeywords = {Acoustic Sensing, Gesture Detection, Wearable Computing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642582,\nauthor = {Zhu, Fengyuan and Sousa, Mauricio and Sidenmark, Ludwig and Grossman, Tovi},\ntitle = {PhoneInVR: An Evaluation of Spatial Anchoring and Interaction Techniques for Smartphone Usage in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642582},\ndoi = {10.1145/3613904.3642582},\nabstract = {When users wear a virtual reality (VR) headset, they lose access to their smartphone and accompanying apps. Past work has proposed smartphones as enhanced VR controllers, but little work has explored using existing smartphone apps and performing traditional smartphone interactions while in VR. In this paper, we consider three potential spatial anchorings for rendering smartphones in VR: On top of a tracked physical smartphone which the user holds (Phone-locked), on top of the user’s empty hand, as if holding a virtual smartphone (Hand-locked), or in a static position in front of the user (World-locked). We conducted a comparative study of target acquisition, swiping, and scrolling tasks across these anchorings using direct Touch or above-the-surface Pinch. Our findings indicate that physically holding a smartphone with Touch improves accuracy and speed for all tasks, and Pinch performed better with virtual smartphones. These findings provide a valuable foundation to enable smartphones in VR.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {639},\nnumpages = {16},\nkeywords = {Smartphones, Touch Input, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642429,\nauthor = {de Groot, Esra Cemre Su and Gadiraju, Ujwal},\ntitle = {\"Are we all in the same boat?\" Customizable and Evolving Avatars to Improve Worker Engagement and Foster a Sense of Community in Online Crowd Work},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642429},\ndoi = {10.1145/3613904.3642429},\nabstract = {Human intelligence continues to be essential in building ground-truth data, training sets, and for evaluating a plethora of systems. The democratized and distributed nature of online crowd work — an attractive and accessible feature that has led to the proliferation of the paradigm — has also meant that crowd workers may not always feel connected to their remote peers. Despite the prevalence of collaborative crowdsourcing practices, workers on many microtask crowdsourcing platforms work on tasks individually and are seldom directly exposed to other crowd workers. In this context, improving worker engagement on microtask crowdsourcing platforms is an unsolved challenge. At the same time, fostering a sense of community among workers can improve the sustainability and working conditions in crowd work. This work aims to increase worker engagement in conversational microtask crowdsourcing by leveraging evolving avatars that workers can customize as they progress through monotonous task batches. We also aim to improve group identification in individual tasks by creating a community space where workers can share their avatars and feelings on task completion. To this end, we carried out a preregistered between-subjects controlled study (N = 680) spanning five experimental conditions and two task types. We found that evolving and customizable worker avatars can increase worker retention. The prospect of sharing worker avatars and task-related feelings in a community space did not consistently affect group identification. Our exploratory analysis indicated that workers who identify themselves as crowd workers experienced greater intrinsic motivation, subjective engagement, and perceived workload. Furthermore, we discuss how task differences shape the relative effectiveness of our interventions. Our findings have important theoretical and practical implications for designing conversational crowdsourcing tasks and in shaping new directions for research to improve crowd worker experiences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {640},\nnumpages = {26},\nkeywords = {Community, Conversational Crowdsourcing, Engagement, Group Identification, Worker Avatars},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642700,\nauthor = {Woodruff, Allison and Shelby, Renee and Kelley, Patrick Gage and Rousso-Schindler, Steven and Smith-Loud, Jamila and Wilcox, Lauren},\ntitle = {How Knowledge Workers Think Generative AI Will (Not) Transform Their Industries},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642700},\ndoi = {10.1145/3613904.3642700},\nabstract = {Generative AI is expected to have transformative effects in multiple knowledge industries. To better understand how knowledge workers expect generative AI may affect their industries in the future, we conducted participatory research workshops for seven different industries, with a total of 54 participants across three US cities. We describe participants’ expectations of generative AI’s impact, including a dominant narrative that cut across the groups’ discourse: participants largely envision generative AI as a tool to perform menial work, under human review. Participants do not generally anticipate the disruptive changes to knowledge industries currently projected in common media and academic narratives. Participants do however envision generative AI may amplify four social forces currently shaping their industries: deskilling, dehumanization, disconnection, and disinformation. We describe these forces, and then we provide additional detail regarding attitudes in specific knowledge industries. We conclude with a discussion of implications and research challenges for the HCI community.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {641},\nnumpages = {26},\nkeywords = {generative AI, industries, knowledge work},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642601,\nauthor = {Jiang, Ling and Wagner, Christian},\ntitle = {How Low is Low? Crowdworker Perceptions of Microtask Payments in Work versus Leisure Situations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642601},\ndoi = {10.1145/3613904.3642601},\nabstract = {Getting paid for completing microtasks online via crowdsourcing (i.e., microworking) has become a widely accepted way to earn money. Despite disputes over low pay rates, however, little is known about the extent of “lowness” and about the perceptions of microworkers concerning the value of micro-paid online activity. In an online survey on a microtask crowdsourcing platform, respondents demonstrated the dual attitudes of work and leisure toward microworking. Although actual wage rates were lower than microworkers expected, the perceived value of the money earned from microworking was paramount. The monetary equivalent, a newly developed metric calibrating microworkers’ subjective evaluations of monetary and nonmonetary dimensions, of microworking outstripped that of alternative activities, the majority of which were leisure activities. Instead of struggling with below-expectation pay rates, microworkers tend to appreciate the value of small gains, especially in contrast to potential losses incurred by alternatives activities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {642},\nnumpages = {11},\nkeywords = {Alternative activities, Microtask crowdsourcing, Pay rates, Perceived values},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642089,\nauthor = {Li, Chu and Zhang, Zhihan and Saugstad, Michael and Safranchik, Esteban and Kulkarni, Chaitanyashareef and Huang, Xiaoyu and Patel, Shwetak and Iyer, Vikram and Althoff, Tim and Froehlich, Jon E.},\ntitle = {LabelAId: Just-in-time AI Interventions for Improving Human Labeling Quality and Domain Knowledge in Crowdsourcing Systems},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642089},\ndoi = {10.1145/3613904.3642089},\nabstract = {Crowdsourcing platforms have transformed distributed problem-solving, yet quality control remains a persistent challenge. Traditional quality control measures, such as prescreening workers and refining instructions, often focus solely on optimizing economic output. This paper explores just-in-time AI interventions to enhance both labeling quality and domain-specific knowledge among crowdworkers. We introduce LabelAId, an advanced inference model combining Programmatic Weak Supervision (PWS) with FT-Transformers to infer label correctness based on user behavior and domain knowledge. Our technical evaluation shows that our LabelAId pipeline consistently outperforms state-of-the-art ML baselines, improving mistake inference accuracy by 36.7\\% with 50 downstream samples. We then implemented LabelAId into Project Sidewalk, an open-source crowdsourcing platform for urban accessibility. A between-subjects study with 34 participants demonstrates that LabelAId significantly enhances label precision without compromising efficiency while also increasing labeler confidence. We discuss LabelAId’s success factors, limitations, and its generalizability to other crowdsourced science domains.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {643},\nnumpages = {21},\nkeywords = {community science, crowdsourcing, human-ai collaboration, machine learning, programmatic weak supervision (pws), quality control, urban accessibility},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642400,\nauthor = {Suh, Sangho and Chen, Meng and Min, Bryan and Li, Toby Jia-Jun and Xia, Haijun},\ntitle = {Luminate: Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642400},\ndoi = {10.1145/3613904.3642400},\nabstract = {Thanks to their generative capabilities, large language models (LLMs) have become an invaluable tool for creative processes. These models have the capacity to produce hundreds and thousands of visual and textual outputs, offering abundant inspiration for creative endeavors. But are we harnessing their full potential? We argue that current interaction paradigms fall short, guiding users towards rapid convergence on a limited set of ideas, rather than empowering them to explore the vast latent design space in generative models. To address this limitation, we propose a framework that facilitates the structured generation of design space in which users can seamlessly explore, evaluate, and synthesize a multitude of responses. We demonstrate the feasibility and usefulness of this framework through the design and development of an interactive system, Luminate, and a user study with 14 professional writers. Our work advances how we interact with LLMs for creative tasks, introducing a way to harness the creative potential of LLMs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {644},\nnumpages = {26},\nkeywords = {Large language models, creativity support, design space, dimensional exploration, human-AI co-creation, human-AI interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642109,\nauthor = {Hohman, Fred and Kery, Mary Beth and Ren, Donghao and Moritz, Dominik},\ntitle = {Model Compression in Practice: Lessons Learned from Practitioners Creating On-device Machine Learning Experiences},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642109},\ndoi = {10.1145/3613904.3642109},\nabstract = {On-device machine learning (ML) promises to improve the privacy, responsiveness, and proliferation of new, intelligent user experiences by moving ML computation onto everyday personal devices. However, today’s large ML models must be drastically compressed to run efficiently on-device, a hurtle that requires deep, yet currently niche expertise. To engage the broader human-centered ML community in on-device ML experiences, we present the results from an interview study with 30 experts at Apple that specialize in producing efficient models. We compile tacit knowledge that experts have developed through practical experience with model compression across different hardware platforms. Our findings offer pragmatic considerations missing from prior work, covering the design process, trade-offs, and technical strategies that go into creating efficient models. Finally, we distill design recommendations for tooling to help ease the difficulty of this work and bring on-device ML into to more widespread practice.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {645},\nnumpages = {18},\nkeywords = {Efficient machine learning, design directions, interactive systems, interview study, model compression, on-device machine learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642032,\nauthor = {Str\\\"{o}mel, Konstantin R. and Henry, Stanislas and Johansson, Tim and Niess, Jasmin and Wo\\'{z}niak, Pawe\\l{} W.},\ntitle = {Narrating Fitness: Leveraging Large Language Models for Reflective Fitness Tracker Data Interpretation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642032},\ndoi = {10.1145/3613904.3642032},\nabstract = {While fitness trackers generate and present quantitative data, past research suggests that users often conceptualise their wellbeing in qualitative terms. This discrepancy between numeric data and personal wellbeing perception may limit the effectiveness of personal informatics tools in encouraging meaningful engagement with one’s wellbeing. In this work, we aim to bridge the gap between raw numeric metrics and users’ qualitative perceptions of wellbeing. In an online survey with n = 273 participants, we used step data from fitness trackers and compared three presentation formats: standard charts, qualitative descriptions generated by an LLM (Large Language Model), and a combination of both. Our findings reveal that users experienced more reflection, focused attention and reward when presented with the generated qualitative data compared to the standard charts alone. Our work demonstrates how automatically generated data descriptions can effectively complement numeric fitness data, fostering a richer, more reflective engagement with personal wellbeing information.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {646},\nnumpages = {16},\nkeywords = {fitness trackers, generative AI, personal informatics, reflection},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641904,\nauthor = {Cheng, Furui and Zouhar, Vil\\'{e}m and Arora, Simran and Sachan, Mrinmaya and Strobelt, Hendrik and El-Assady, Mennatallah},\ntitle = {RELIC: Investigating Large Language Model Responses using Self-Consistency},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641904},\ndoi = {10.1145/3613904.3641904},\nabstract = {Large Language Models (LLMs) are notorious for blending fact with fiction and generating non-factual content, known as hallucinations. To address this challenge, we propose an interactive system that helps users gain insight into the reliability of the generated text. Our approach is based on the idea that the self-consistency of multiple samples generated by the same LLM relates to its confidence in individual claims in the generated texts. Using this idea, we design RELIC, an interactive system that enables users to investigate and verify semantic-level variations in multiple long-form responses. This allows users to recognize potentially inaccurate information in the generated text and make necessary corrections. From a user study with ten participants, we demonstrate that our approach helps users better verify the reliability of the generated text. We further summarize the design implications and lessons learned from this research for future studies of reliable human-LLM interactions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {647},\nnumpages = {18},\nkeywords = {hallucination detection, human-AI interaction, natural language processing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642628,\nauthor = {Hohman, Fred and Wang, Chaoqun and Lee, Jinmook and G\\\"{o}rtler, Jochen and Moritz, Dominik and Bigham, Jeffrey P and Ren, Zhile and Foret, Cecile and Shan, Qi and Zhang, Xiaoyi},\ntitle = {Talaria: Interactively Optimizing Machine Learning Models for Efficient Inference},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642628},\ndoi = {10.1145/3613904.3642628},\nabstract = {On-device machine learning (ML) moves computation from the cloud to personal devices, protecting user privacy and enabling intelligent user experiences. However, fitting models on devices with limited resources presents a major technical challenge: practitioners need to optimize models and balance hardware metrics such as model size, latency, and power. To help practitioners create efficient ML models, we designed and developed Talaria : a model visualization and optimization system. Talaria enables practitioners to compile models to hardware, interactively visualize model statistics, and simulate optimizations to test the impact on inference metrics. Since its internal deployment two years ago, we have evaluated Talaria using three methodologies: (1) a log analysis highlighting its growth of 800+ practitioners submitting 3,600+ models; (2) a usability survey with 26 users assessing the utility of 20 Talaria features; and (3) a qualitative interview with the 7 most active users about their experience using Talaria.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {648},\nnumpages = {19},\nkeywords = {Efficient machine learning, interactive systems, model compression, on-device machine learning, visual analytics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642229,\nauthor = {Chen, Liuqing and Xiao, Shuhong and Chen, Yunnong and Song, Yaxuan and Wu, Ruoyu and Sun, Lingyun},\ntitle = {ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642229},\ndoi = {10.1145/3613904.3642229},\nabstract = {As Computational Thinking (CT) continues to permeate younger age groups in K-12 education, established CT platforms such as Scratch face challenges in catering to these younger learners, particularly those in the elementary school (ages 6-12). Through formative investigation with Scratch experts, we uncover three key obstacles to children’s autonomous Scratch learning: artist’s block in project planning, bounded creativity in asset creation, and inadequate coding guidance during implementation. To address these barriers, we introduce ChatScratch, an AI-augmented system to facilitate autonomous programming learning for young children. ChatScratch employs structured interactive storyboards and visual cues to overcome artist’s block, integrates digital drawing and advanced image generation technologies to elevate creativity, and leverages Scratch-specialized Large Language Models (LLMs) for professional coding guidance. Our study shows that, compared to Scratch, ChatScratch efficiently fosters autonomous programming learning, and contributes to the creation of high-quality, personally meaningful Scratch projects for children.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {649},\nnumpages = {19},\nkeywords = {Children Aged 6-12, Computational Thinking, Large Language Model, Scratch},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642773,\nauthor = {Kazemitabaar, Majeed and Ye, Runlong and Wang, Xiaoning and Henley, Austin Zachary and Denny, Paul and Craig, Michelle and Grossman, Tovi},\ntitle = {CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642773},\ndoi = {10.1145/3613904.3642773},\nabstract = {Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates student’s incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AI’s unique benefits; D2) simplifying query formulation while promoting cognitive engagement; D3) avoiding direct responses while encouraging motivated learning; and D4) maintaining transparency and control for students to asses and steer AI responses.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {650},\nnumpages = {20},\nkeywords = {AI assistants, AI tutoring, class deployment, design guidelines, educational technology, generative AI, intelligent tutoring systems, large language models, programming education},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642706,\nauthor = {Nguyen, Sydney and Babe, Hannah McLean and Zi, Yangtian and Guha, Arjun and Anderson, Carolyn Jane and Feldman, Molly Q},\ntitle = {How Beginning Programmers and Code LLMs (Mis)read Each Other},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642706},\ndoi = {10.1145/3613904.3642706},\nabstract = {Generative AI models, specifically large language models (LLMs), have made strides towards the long-standing goal of text-to-code generation. This progress has invited numerous studies of user interaction. However, less is known about the struggles and strategies of non-experts, for whom each step of the text-to-code problem presents challenges: describing their intent in natural language, evaluating the correctness of generated code, and editing prompts when the generated code is incorrect. This paper presents a large-scale controlled study of how 120 beginning coders across three academic institutions approach writing and editing prompts. A novel experimental design allows us to target specific steps in the text-to-code process and reveals that beginners struggle with writing and editing prompts, even for problems at their skill level and when correctness is automatically determined. Our mixed-methods evaluation provides insight into student processes and perceptions with key implications for non-expert Code LLM use within and outside of education.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {651},\nnumpages = {26},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642349,\nauthor = {Jin, Hyoungwook and Lee, Seonghee and Shin, Hyungyu and Kim, Juho},\ntitle = {Teach AI How to Code: Using Large Language Models as Teachable Agents for Programming Education},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642349},\ndoi = {10.1145/3613904.3642349},\nabstract = {This work investigates large language models (LLMs) as teachable agents for learning by teaching (LBT). LBT with teachable agents helps learners identify knowledge gaps and discover new knowledge. However, teachable agents require expensive programming of subject-specific knowledge. While LLMs as teachable agents can reduce the cost, LLMs’ expansive knowledge as tutees discourages learners from teaching. We propose a prompting pipeline that restrains LLMs’ knowledge and makes them initiate “why” and “how” questions for effective knowledge-building. We combined these techniques into TeachYou, an LBT environment for algorithm learning, and AlgoBo, an LLM-based tutee chatbot that can simulate misconceptions and unawareness prescribed in its knowledge state. Our technical evaluation confirmed that our prompting pipeline can effectively configure AlgoBo’s problem-solving performance. Through a between-subject study with 40 algorithm novices, we also observed that AlgoBo’s questions led to knowledge-dense conversations (effect size=0.71). Lastly, we discuss design implications, cost-efficiency, and personalization of LLM-based teachable agents.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {652},\nnumpages = {28},\nkeywords = {AI and Education, Generative AI, Human-AI interaction, LLM agents},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642070,\nauthor = {Figueiredo, Vanessa and Cameron, Catherine Ann},\ntitle = {EXPLORA: A teacher-apprentice methodology for eliciting natural child-computer interactions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642070},\ndoi = {10.1145/3613904.3642070},\nabstract = {Investigating child-computer interactions within their contexts is vital for designing technology that caters to children’s needs. However, determining what aspects of context are relevant for designing child-centric technology remains a challenge. We introduce EXPLORA, a multimodal, multistage online methodology comprising three pivotal stages: ( 1)  building a teacher-apprentice relationship, ( 2)  learning from child-teachers, and ( 3)  assessing and reinforcing researcher-apprentice learning. Central to EXPLORA is the collection of attitudinal data through pre-observation interviews, offering researchers a deeper understanding of children’s characteristics and contexts. This informs subsequent online observations, allowing researchers to focus on frequent interactions. Furthermore, researchers can validate preliminary assumptions with children. A means-ends analysis framework aids in the systematic analysis of data, shedding light on context, agency and homework-information searching processes children employ in their activities. To illustrate EXPLORA’s capabilities, we present nine single case studies investigating Brazilian child-caregiver dyads ( children ages 9-11)  use of technology in homework information-searching.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {653},\nnumpages = {11},\nkeywords = {Methods, child-computer interaction, means-ends analysis.},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642717,\nauthor = {Nisser, Martin and Gaetz, Marisa and Fishberg, Andrew and Soicher, Raechel N. and Faruqi, Faraz and Long, Joshua},\ntitle = {From Prisons to Programming: Fostering Self-Efficacy via Virtual Web Design Curricula in Prisons and Jails},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642717},\ndoi = {10.1145/3613904.3642717},\nabstract = {Self-efficacy and digital literacy are key predictors to incarcerated people’s success in the modern workplace. While digitization in correctional facilities is expanding, few templates exist for how to design computing curricula that foster self-efficacy and digital literacy in carceral environments. As a result, formerly incarcerated people face increasing social and professional exclusion post-release. We report on a 12-week college-accredited web design class, taught virtually and synchronously, across 5 correctional facilities across the United States. The program brought together men and women from gender-segregated facilities into one classroom to learn fundamentals in HTML, CSS and Javascript, and create websites addressing social issues of their choosing. We conducted surveys with participating students, using dichotomous and open-ended questions, and performed thematic and quantitative analyses of their responses that suggest students’ increased self-efficacy. Our study discusses key design choices, needs, and recommendations for furthering computing curricula that foster self-efficacy and digital literacy in carceral settings.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {654},\nnumpages = {13},\nkeywords = {Prison education, digital literacy, self-efficacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641987,\nauthor = {Bustos, Alyshia and Chacon, Nanibah and Shaw, Mia and Bell, Fiona and Buechley, Leah},\ntitle = {Interactive Murals: New Opportunities for Collaborative STEAM Learning},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641987},\ndoi = {10.1145/3613904.3641987},\nabstract = {This paper introduces interactive murals—artworks that combine longstanding traditions in community mural painting with ubiquitous computing—as new sites for collaborative STEAM learning. Using research-through-design and participatory design methods, we conducted an intensive spring and summer workshop in which high school students were introduced to electronics and programming through the process of creating an interactive mural. We describe the workshop activities, the mural design process, and the data collection and analysis methods. Through documenting student learning in programming and electronics and the collaboration that occurred, we build an argument for the novel learning affordances of interactive murals, emphasizing the unique opportunities that they provide for collaborative STEAM learning.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {655},\nnumpages = {19},\nkeywords = {computing education, educational technology, interactive murals, participatory design, steam},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642097,\nauthor = {Kuang, Emily and Bellscheidt, Selah and Pham, Di and Shinohara, Kristen and Baker, Catherine M and Elglaly, Yasmine N.},\ntitle = {Mapping Accessibility Assignments into Core Computer Science Topics: An Empirical Study with Interviews and Surveys of Instructors and Students},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642097},\ndoi = {10.1145/3613904.3642097},\nabstract = {Incorporating accessibility education into undergraduate computer science (CS) programs is essential for preparing future technology professionals to create inclusive technology. However, many CS programs lack accessibility coverage, often confining it to human-computer interaction (HCI) courses. To address this gap, we developed accessibility assignments seamlessly integrated into core CS courses. We collaborated closely with ten instructors to select and customize these assignments to suit their needs. To evaluate the impact of these assignments, we conducted interviews with instructors and administered surveys and interviews with their students. Our findings indicate significant improvement in students' familiarity with accessibility concepts and confidence in implementation following completion of the assignments. However, their mindset and future interest in accessibility remained the same. Instructors found it straightforward to incorporate these assignments without compromising core computing concepts. In sum, we validated a foundation for effectively resourcing instructors with accessibility teaching materials and increasing their capacity in accessibility knowledge.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {656},\nnumpages = {16},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642475,\nauthor = {Garcia, Rosalinda and Morreale, Patricia and Verdi, Gail and Garcia, Heather and Noa, Geraldine Jimena and Madsen, Spencer P. and Alzugaray-Orellana, Maria Jesus and Li, Elizabeth and Burnett, Margaret},\ntitle = {The Matchmaker Inclusive Design Curriculum: A Faculty-Enabling Curriculum to Teach Inclusive Design Throughout Undergraduate CS},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642475},\ndoi = {10.1145/3613904.3642475},\nabstract = {Despite efforts to raise awareness of societal and ethical issues in CS education, research shows students often do not act upon their new awareness (Problem 1). One such issue, well-established by HCI research, is that much of technology contains barriers impacting numerous populations—such as minoritized genders, races, ethnicities, and more. HCI has inclusive design methods that help—but these skills are rarely taught, even in HCI classes (Problem 2). To address Problems 1 and 2, we created the Matchmaker Curriculum to pair CS faculty—including non-HCI faculty—with inclusive design elements to allow for inclusive design skill-building throughout their CS program. We present the curriculum and a field study, in which we followed 18 faculty along their journey. The results show how the Matchmaker Curriculum equipped 88\\% of these faculty with enough inclusive design teaching knowledge to successfully embed actionable inclusive design skill-building into 13 CS courses.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {657},\nnumpages = {22},\nkeywords = {GenderMag, HCI education, Inclusive Design, Responsible CS},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642423,\nauthor = {Fu, Shihan and Chen, Jianhao and Kuang, Emily and Fan, Mingming},\ntitle = {Bridging the Literacy Gap for Adults: Streaming and Engaging in Adult Literacy Education through Livestreaming},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642423},\ndoi = {10.1145/3613904.3642423},\nabstract = {Literacy—the ability to read, write, and comprehend text—is an important topic addressed by UNESCO. Despite global efforts to promote adult literacy education, rural areas with limited resources still lag behind. As livestreaming has gained popularity in China, many streamers leveraged its accessibility and affordability to reach low-literate adults. To gain a better understanding of the practices and challenges faced by adult literacy education through livestreaming, we conducted a mixed-methods study involving a 7-day observation of livestreaming sessions and an interview study with twelve streamers and ten viewers. We discovered streamers’ altruistic motives and unique interactive approaches. Viewers perceived livestreaming as a more engaging, community-supportive method than traditional approaches. We also identified both shared and unique challenges for streamers and viewers that limit its efficacy as a learning tool. Finally, we recognized opportunities to enhance educational equity, emphasizing design implications for advancing adult literacy education and promoting diversity in livestreaming.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {658},\nnumpages = {15},\nkeywords = {adult literacy, education, livestreaming},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642947,\nauthor = {Liu, Ziyi and Zhu, Zhengzhe and Zhu, Lijun and Jiang, Enze and Hu, Xiyun and Peppler, Kylie A and Ramani, Karthik},\ntitle = {ClassMeta: Designing Interactive Virtual Classmate to Promote VR Classroom Participation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642947},\ndoi = {10.1145/3613904.3642947},\nabstract = {Peer influence plays a crucial role in promoting classroom participation, where behaviors from active students can contribute to a collective classroom learning experience. However, the presence of these active students depends on several conditions and is not consistently available across all circumstances. Recently, Large Language Models (LLMs) such as GPT have demonstrated the ability to simulate diverse human behaviors convincingly due to their capacity to generate contextually coherent responses based on their role settings. Inspired by this advancement in technology, we designed ClassMeta, a GPT-4 powered agent to help promote classroom participation by playing the role of an active student. These agents, which are embodied as 3D avatars in virtual reality, interact with actual instructors and students with both spoken language and body gestures. We conducted a comparative study to investigate the potential of ClassMeta for improving the overall learning experience of the class.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {659},\nnumpages = {17},\nkeywords = {VR classroom, collaborative learning, large language Model, pedagogical agent},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642845,\nauthor = {Lee, Ha Yeon and Park, Seora and Kim, Esther Hehsun and Seo, Jiyeon and Lim, Hajin and Lee, Joonhwan},\ntitle = {Investigating the Effects of Real-time Student Monitoring Interface on Instructors’ Monitoring Practices in Online Teaching},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642845},\ndoi = {10.1145/3613904.3642845},\nabstract = {The shift to online education, accelerated by the COVID-19 pandemic, has introduced challenges in monitoring student engagement, an essential aspect of effective teaching. In response, real-time student monitoring interfaces have emerged as potential tools to aid instructors, yet their efficacy has not been thoroughly examined. Addressing this gap, we conducted a controlled experiment with 20 instructors examining the impact of engagement cues (presence versus absence) and student engagement levels (high versus low) on instructors’ monitoring effectiveness, teaching behavior adjustments, and cognitive load in online classes. Our findings underscored the fundamental benefits of student engagement monitoring interfaces for improving monitoring quality and effectiveness. Furthermore, our study highlighted the critical need for customizable interfaces that could balance the informational utility of engagement cues with the associated cognitive load and psychological stress on instructors. These insights may offer design implications for the design of future student engagement monitoring interfaces.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {660},\nnumpages = {11},\nkeywords = {Real-time student monitoring system, monitoring, online learning, online teaching, student engagement},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642914,\nauthor = {Kwapisz, Monika Blue and Kohli, Avanya and Rajivan, Prashanth},\ntitle = {Privacy Concerns of Student Data Shared with Instructors in an Online Learning Management System},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642914},\ndoi = {10.1145/3613904.3642914},\nabstract = {Learning management systems are used for facilitating communication between instructors and students, dissemination of lecture materials, and grading of assignments. They collect large amounts of student data, necessary or otherwise, with or without explicit consent from students. Furthermore, they make the data visible to instructors, which could have significant implications for students’ grades and experience in the classroom. In this study, we interviewed 31 students enrolled in a large public university about their privacy concerns towards different data sharing practices related to the learning management system used at their university – Canvas. Data from the study was analyzed by two researchers using inductive thematic analysis methods. The results show concerns about misrepresentation, the justification for information being visible, and discrimination. We present the implications of this study on instruction, design of learning management systems, and policy.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {661},\nnumpages = {16},\nkeywords = {Education technology, Higher education, Misrepresentation, Surveillance},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642060,\nauthor = {Kimura, Sharina and Zintl, Michael and Hammann, Claudius and Holzapfel, Florian},\ntitle = {Simulator-based Mixed Reality eVTOL Pilot Training: The Instructor Operator Station},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642060},\ndoi = {10.1145/3613904.3642060},\nabstract = {Advanced Air Mobility aircraft designs following the Simplified Vehicle Operations (SVO) concept require novel environments for practical and intuitive pilot training. Mixed Reality (MR) technologies can support immersive and interactive learning methods for operating several SVO aircraft, including electric Vertical Take-Off and Landing (eVTOL) systems. Despite this potential, regulatory guidelines for simulator-based eVTOL pilot training, especially concerning the Instructor Operator Station (IOS) design, are nascent and require substantive development. This paper investigates the feasibility of an MR eVTOL research simulator as a training tool for instructors. A user study forms the basis for a bottom-up categorization of the instructor’s performance shaping factors, which are pivotal for the design of an MR IOS. This paper contributes to the discourse on MR integration in pilot training by identifying key enhancements necessary for an IOS design.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {662},\nnumpages = {10},\nkeywords = {aviation, cockpit interaction, flight simulator, flight training, human performance shaping factors, mixed reality, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642487,\nauthor = {Ngoon, Tricia J. and Sushil, S and Stewart, Angela E.B. and Lee, Ung-Sang and Venkatraman, Saranya and Thawani, Neil and Mitra, Prasenjit and Clarke, Sherice and Zimmerman, John and Ogan, Amy},\ntitle = {ClassInSight: Designing Conversation Support Tools to Visualize Classroom Discussion for Personalized Teacher Professional Development},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642487},\ndoi = {10.1145/3613904.3642487},\nabstract = {Teaching is one of many professions for which personalized feedback and reflection can help improve dialogue and discussion between the professional and those they serve. However, professional development (PD) is often impersonal as human observation is labor-intensive. Data-driven PD tools in teaching are of growing interest, but open questions about how professionals engage with their data in practice remain. In this paper, we present ClassInSight, a tool that visualizes three levels of teachers’ discussion data and structures reflection. Through 22 reflection sessions and interviews with 5 high school science teachers, we found themes related to dissonance, contextualization, and sustainability in how teachers engaged with their data in the tool and in how their professional vision, the use of professional expertise to interpret events, shifted over time. We discuss guidelines for these conversational support tools to support personalized PD in professions beyond teaching where conversation and interaction are important.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {663},\nnumpages = {15},\nkeywords = {classroom discourse, conversation support, data visualizations, reflection, teacher professional development, teachers},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642221,\nauthor = {Kwon, Christine and Butler, Darren and Uchidiuno, Judith Odili and Stamper, John and Ogan, Amy},\ntitle = {Investigating Demographics and Motivation in Engineering Education Using Radio and Phone-Based Educational Technologies},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642221},\ndoi = {10.1145/3613904.3642221},\nabstract = {Despite the best intentions to support equity with educational technologies, they often lead to a “rich get richer” effect, in which communities of more advantaged learners gain greater benefit from these solutions. Effective design of these technologies necessitates a deeper understanding of learners in understudied contexts and their motivations to pursue an education. Consequently, we studied a 15-week remote course launched in 2021 with 17,896 learners that provided engineering education through a radio and phone-based system aimed for use in rural settings within Northern Uganda. We address shifts in learners’ motivations for course participation and investigate the impact of demographic features and motivations of students on persistence and performance. We found significant increases in student motivation to learn more about and pursue STEM. Importantly, the course was most successful for learners in demographics who typically experience fewer educational opportunities, showing promise for such technologies to close opportunity gaps.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {664},\nnumpages = {15},\nkeywords = {Engineering Education, Interactive Radio Education, Mobile Learning, Rural Learners},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642510,\nauthor = {Jin, Qiao and Liu, Yu and Yuan, Ye and Han, Bo and Qian, Feng and Yarosh, Svetlana},\ntitle = {Virtual Reality, Real Pedagogy: A Contextual Inquiry of Instructor Practices with VR Video},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642510},\ndoi = {10.1145/3613904.3642510},\nabstract = {Virtual reality (VR) offers promise in education given its immersive and socially engaging nature, but it can pose challenges for educators when creating VR-specific content. VR videos can function as a new educational tool for VR content creation due to their creation affordability and user-friendliness. However, little empirical research exists on how educators utilize VR videos and associated pedagogy in real classes. Our research employed a contextual inquiry, through in-person interviews and online surveys with 11 instructors to gain actionable insights from envisioned teaching scenarios for VR videos that are informed by actual instructional practices. Our study aims to understand the factors that motivate instructors’ adoption of VR videos, identify challenges educators face when incorporating VR videos into instructional units, and examine pedagogical adjustments when integrating VR videos into teaching. Through empirical evidence, we provide design implications for the development of VR-based learning experiences across diverse educational contexts. Our study also serves as a practical case of how VR can be adopted and integrated into education.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {665},\nnumpages = {21},\nkeywords = {360-degree videos, VR videos, Virtual Reality, educational VR, higher education},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642098,\nauthor = {Long, Duri and Yang, Jiaxi and Naomi, Cassandra and Magerko, Brian},\ntitle = {Xylocode: A Novel Approach to Fostering Interest in Computer Science via an Embodied Music Simulation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642098},\ndoi = {10.1145/3613904.3642098},\nabstract = {Fostering learners’ interest remains an important challenge in computer science (CS) education. In this paper, we explore how creative music-making, tangible interfaces, and embodiment can be used toward this end. The primary contribution of this paper is Xylocode, a novel exhibit that introduces middle school age learners to computing concepts and fosters interest in CS via a tangible playspace for making music using an embodied simulation. We additionally present an in-museum evaluation of Xylocode with 29 middle school age children. Our results indicate that the exhibit fosters situational interest in computer science and leads to recognition of certain computing concepts, including arrays and global variables. Future research is needed to assess whether the exhibit leads to longer-term learning and/or interest gains and to explore why other computing concepts were not recognized by as many learners. We identify several implications and directions for future work based on our findings.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {666},\nnumpages = {16},\nkeywords = {CS education, informal learning, interest development, museum exhibit, music and computing, tangible user interfaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642043,\nauthor = {Chen, John and Zhao, Lexie and Li, Yinmiao and Xie, Zhennian and Wilensky, Uri and Horn, Mike},\ntitle = {“Oh My God! It’s Recreating Our Room!” Understanding Children’s Experiences with A Room-Scale Augmented Reality Authoring Toolkit},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642043},\ndoi = {10.1145/3613904.3642043},\nabstract = {Human-Computer Interaction (HCI) and education researchers have applied Augmented Reality (AR) to support spatial thinking in K-12 education. However, fewer studies support spatial thinking through spatial exploration. Room-scale AR, a recent technology development, brings new opportunities not yet researched. We developed NetLogo AR, an AR authoring toolkit, that allows children to play with, design, and create room-scale AR experiences that combine AR with computational models. To acquire a deeper and more nuanced understanding of children’s interactions with this new technology, we conducted eight-week participatory design sessions with seven children aged 11-13. We analyzed 48 hours of video data, interview transcripts, and design artifacts. Children were enthusiastic and engaged in spatial thinking activities. We affirmed room-scale AR’s role in spatial exploration by comparing it with other supported modalities. Building on existing studies, we propose a new AR design framework around spatial movement and exploration that could help inform design decisions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {667},\nnumpages = {17},\nkeywords = {AR Authoring Toolkit, AR and Children, Augmented Reality, NetLogo AR, Participatory Design, Spatial AR},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642379,\nauthor = {Hedderich, Michael A. and Bazarova, Natalie N. and Zou, Wenting and Shim, Ryun and Ma, Xinda and Yang, Qian},\ntitle = {A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642379},\ndoi = {10.1145/3613904.3642379},\nabstract = {Cyberbullying harms teenagers’ mental health, and teaching them upstanding intervention is crucial. Wizard-of-Oz studies show chatbots can scale up personalized and interactive cyberbullying education, but implementing such chatbots is a challenging and delicate task. We created a no-code chatbot design tool for K-12 teachers. Using large language models and prompt chaining, our tool allows teachers to prototype bespoke dialogue flows and chatbot utterances. In offering this tool, we explore teachers’ distinctive needs when designing chatbots to assist their teaching, and how chatbot design tools might better support them. Our findings reveal that teachers welcome the tool enthusiastically. Moreover, they see themselves as playwrights guiding both the students’ and the chatbot’s behaviors, while allowing for some improvisation. Their goal is to enable students to rehearse both desirable and undesirable reactions to cyberbullying in a safe environment. We discuss the design opportunities LLM-Chains offer for empowering teachers and the research opportunities this work opens up.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {668},\nnumpages = {17},\nkeywords = {chatbot, cyberbullying, education, large language models, teachers},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642366,\nauthor = {Chung, Yu-Jung and Hsu, Chen-Wei and Chan, Meng-Hsun and Cherng, Fu-Yin},\ntitle = {Enhancing ESL Learners' Experience and Performance through Gradual Adjustment of Video Speed during Extensive Viewing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642366},\ndoi = {10.1145/3613904.3642366},\nabstract = {Adjusting video playback speed during extensive viewing is crucial for English-as-a-Second-Language (ESL) learners to enhance their learning experience. Since existing research suggests that abrupt speed changes might negatively impact the viewing experience, several novel speed-adjustment systems have been proposed to provide adaptive and optimal video playback speed for learners. However, empirical evidence is still sparse on whether gradual adjustments truly offer a superior experience compared to immediate changes. To delve into this, we conducted a study with 32 ESL participants, comparing direct and gradual adjustments on flow state, cognitive load, and behavioral measures. Employing both objective metrics, such as pupil diameter, and subjective feedback from surveys, our results strongly favor the gradual method. It not only enhanced flow state and video comprehension but was also less obtrusive to learners. These findings underscore the advantages of gradual speed adjustment for ESL learners, offering insights for the design of next-generation speed-adjustment systems.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {669},\nnumpages = {14},\nkeywords = {ESL Learner, Extensive Viewing, Flow State, Playback Speed, Pupillometry, Speed Adjustment},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641956,\nauthor = {Albaugh, Lea and Chen, Melinda and Liu, Sunniva and Jain, Harshika and Collins, Alisha and Yao, Lining},\ntitle = {Morphing Matter for Teens: Research Processes as a Template for Cross-Disciplinary Activities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641956},\ndoi = {10.1145/3613904.3641956},\nabstract = {We distilled a set of core practices within “morphing matter” research, derived a set of underlying skills and values, and developed these into a weekend workshop for high-school students. Participants in our workshop sampled a variety of research processes, including materials science and contextual design, incorporating curriculum-appropriate learning goals, toward an integrated pneumatic fashion project. We describe our approach, activity plan, and assessment as well as opportunities for research as an educational template to push beyond current “STEAM”-based educational practices for cross-disciplinary engagement.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {670},\nnumpages = {20},\nkeywords = {Cross-disciplinary learning, research practice},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642150,\nauthor = {Cai, Minghao and Rebolledo Mendez, Genaro and Arevalo, Gisele and Tang, Sin Sze and Abdullah, Yalmaz Ali and Demmans Epp, Carrie},\ntitle = {Toward Supporting Adaptation: Exploring Affect’s Role in Cognitive Load when Using a Literacy Game},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642150},\ndoi = {10.1145/3613904.3642150},\nabstract = {Educational technologies have been argued to enhance specific aspects of affect, such as motivation, and through that learner experiences and outcomes. Until recently, affect has been considered separately from cognition. In this study, we investigated how learner affect (valence and activation) was tied to learner cognitive load and behaviours during game-based literacy activities. We employed experience sampling as part of a lab-based case study where 35 English language learners used an adaptive educational game. The results indicated that both positive and negative affect predicted learner cognitive load, with negative affect predicting extraneous (unnecessary) load. These results and the newly identified interaction patterns that accompanied learner affect and cognitive-load trajectories provide insight into the role of affect during learning. They show a need for considering affect when studying cognitive load and have implications for how systems should adapt to learners.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {671},\nnumpages = {17},\nkeywords = {Affect, Cognitive load, Game-based learning, Literacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642907,\nauthor = {Lee, Hee Rin},\ntitle = {Contrasting Perspectives of Workers: Exploring Labor Relations in Workplace Automation and Potential Interventions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642907},\ndoi = {10.1145/3613904.3642907},\nabstract = {Many emerging technologies are expected to reconfigure workplaces, and serious concerns have already been raised about their impact on workers, especially those who are already precarious. This study explores what roles designers can play to address power issues regarding workplace automation. Following Marxist researchers addressing the importance of analyzing “struggle” as an event that reveals power relations in workplaces, this study examines conflicting views between stakeholders regarding the value of newly adopted robots, and the value of the human labor that the robots could displace. In this study, workers—even those who perform the same tasks—have conflicting views regarding how their work can be automated: the collective voice of workers is not naturally formed. This observation can be seen as closely related to the weakened solidarity among workers not only in the US but internationally, due to the neoliberal restructuring of labor market and corporations. Considering the unique countervailing power of worker solidarity, this study proposes a new role for designers: facilitator of “inclusive collective imaginaries” by bridging workers’ divided opinions, addressing the importance of inclusive solidarity, and mobilizing them to successfully contribute to shaping automation technologies as a way to intervene in automation-related issues.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {672},\nnumpages = {17},\nkeywords = {automation, capitalism, manufacturing, political economy, power issues, production workers, robots, solidarity, union, workplace},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642136,\nauthor = {Dhiman, Hitesh and Rovelo Ruiz, Gustavo Alberto and Ramakers, Raf and Leen, Danny and R\\\"{o}cker, Carsten},\ntitle = {Designing Instructions using Self-Determination Theory to Improve Motivation and Engagement for Learning Craft},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642136},\ndoi = {10.1145/3613904.3642136},\nabstract = {Recent HCI research has shown significant interest in investigating digital working instructions for guiding novices to perform manual tasks. While performance enhancement has been a primary focus, it is increasingly recognized that technology’s impact extends beyond objective metrics. Trainee motivation and engagement plays a pivotal role in enhancing learning outcomes and effectiveness. This paper investigates the utilization of principles from Self Determination Theory–clear attainable goals, meaningful rationale, and perspective taking–in designing multimedia instructions to enhance novice users’ indicators of psychological well-being. We present findings from an experiment involving real-world woodworking, where novice users, in a between-subjects study, followed interactive, in-situ projection-based guidance. Results demonstrate that adhering to SDT postulates can positively influence perceived competence, intrinsic motivation and task execution quality. These findings offer valuable insights for designing digital instructions to guide and train novices, emphasizing the importance of psychological well-being alongside task performance.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {673},\nnumpages = {16},\nkeywords = {augmented reality, instructions, intrinsic motivation, perceived self–competence, self–determination theory, well–being},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642205,\nauthor = {Liu, Guanhong and Shi, Qingyuan and Yao, Yuan and Feng, Yuan-Ling and Yu, Tianyu and Liu, Beituo and Ma, Zhijun and Huang, Li and Diao, Yuting},\ntitle = {Learning from Hybrid Craft: Investigating and Reflecting on Innovating and Enlivening Traditional Craft through Literature Review},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642205},\ndoi = {10.1145/3613904.3642205},\nabstract = {The key to preserving traditional crafts lies in living transmission, which is inseparable from sustaining artistic production, audience consumption, and progressive innovation with the physical media. As HCI researchers, we focus on the hybrid crafts field, which involves numerous cross-disciplinary integration cases between traditional craftsmanship and digital technology at the physical level, providing inspiration for innovating and enlivening traditional crafts. We conducted a multi-perspective review of 85 hybrid craft articles related to traditional crafts over the past decade, considering aspects such as craft categories, digital technology, target users, and research areas. Through reflection, we propose a design framework for fostering innovation and revitalizing traditional crafts. This paper aims to offer insight into the innovation and enlivenment of traditional crafts through a hybrid craft perspective while also serving as a first review of the hybrid craft field from the traditional craftsmanship perspective.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {674},\nnumpages = {19},\nkeywords = {culture heritage, hybrid craft, literature review, traditional craft innovation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642945,\nauthor = {Sakashita, Mose and Thoravi Kumaravel, Balasaravanan and Marquardt, Nicolai and Wilson, Andrew David},\ntitle = {SharedNeRF: Leveraging Photorealistic and View-dependent Rendering for Real-time and Remote Collaboration},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642945},\ndoi = {10.1145/3613904.3642945},\nabstract = {Collaborating around physical objects necessitates examining different aspects of design or hardware in detail when reviewing or inspecting physical artifacts or prototypes. When collaborators are remote, coordinating the sharing of views of their physical environment becomes challenging. Video-conferencing tools often do not provide the desired viewpoints for a remote viewer. While RGB-D cameras offer 3D views, they lack the necessary fidelity. We introduce SharedNeRF, designed to enhance synchronous remote collaboration by leveraging the photorealistic and view-dependent nature of Neural Radiance Field (NeRF). The system complements the higher visual quality of the NeRF rendering with the instantaneity of a point cloud and combines them through carefully accommodating the dynamic elements within the shared space, such as hand gestures and moving objects. The system employs a head-mounted camera for data collection, creating a volumetric task space on the fly and updating it as the task space changes. In our preliminary study, participants successfully completed a flower arrangement task, benefiting from SharedNeRF’s ability to render the space in high fidelity from various viewpoints.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {675},\nnumpages = {14},\nkeywords = {Collaboration, NeRF, Spatial Interfaces;},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642887,\nauthor = {Chen, Liuqing and Jiang, Zhaojun and Xia, Duowei and Cai, Zebin and Sun, Lingyun and Childs, Peter and Zuo, Haoyu},\ntitle = {BIDTrainer: An LLMs-driven Education Tool for Enhancing the Understanding and Reasoning in Bio-inspired Design},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642887},\ndoi = {10.1145/3613904.3642887},\nabstract = {Bio-inspired design (BID) fosters innovations in engineering. Learning BID is crucial for developing multidisciplinary innovation skills of designers and engineers. Current BID education aims to enhance learners’ understanding and analogical reasoning skills. However, it often heavily relies on the teachers’ expertise. When learners pursue independent learning using some educational tools, they face challenges in understanding and reasoning practice within this multidisciplinary field. Additionally, evaluating their learning outcomes comprehensively becomes problematic. Addressing these challenges, we introduce a LLMs-driven BID education method based on a structured ontology and three strategies: enhancing understanding through LLMs-enpowered \"learning by asking\", assisting reasoning by providing hints and feedback, and assessing learning outcomes through benchmarking against existing BID cases. Implementing the method, we developed BIDTrainer, a BID education tool. User studies indicate that learners using BIDTrainer understood BID knowledge better, reason faster with higher interactivity than the baseline, and BIDTrainer assessed the learning outcomes consistent with experts.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {676},\nnumpages = {20},\nkeywords = {Analogy training, Bio-inspired design, Design education, Design evaluation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642393,\nauthor = {Leong, Joanne and Pataranutaporn, Pat and Danry, Valdemar and Perteneder, Florian and Mao, Yaoli and Maes, Pattie},\ntitle = {Putting Things into Context: Generative AI-Enabled Context Personalization for Vocabulary Learning Improves Learning Motivation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642393},\ndoi = {10.1145/3613904.3642393},\nabstract = {Fostering students’ interests in learning is considered to have many positive downstream effects. Large language models have opened up new horizons for generating content tuned to one’s interests, yet it is unclear in what ways and to what extent this customization could have positive effects on learning. To explore this novel dimension, we conducted a between-subjects online study (n=272) featuring different variations of a generative AI vocabulary learning app that enables users to personalize their learning examples. Participants were randomly assigned to control (sentence sourced from pre-existing text) or experimental conditions (generated sentence or short story based on users’ text input). While we did not observe a difference in learning performance between the conditions, the analysis revealed that generative AI-driven context personalization positively affected learning motivation. We discuss how these results relate to previous findings and underscore their significance for the emerging field of using generative AI for personalized learning.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {677},\nnumpages = {15},\nkeywords = {education, generative artificial intelligence, learning, vocabulary},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642438,\nauthor = {Han, Ariel and Zhou, Xiaofei and Cai, Zhenyao and Han, Shenshen and Ko, Richard and Corrigan, Seth and Peppler, Kylie A},\ntitle = {Teachers, Parents, and Students' perspectives on Integrating Generative AI into Elementary Literacy Education},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642438},\ndoi = {10.1145/3613904.3642438},\nabstract = {The viral launch of new generative AI (GAI) systems, such as ChatGPT and Text-to-Image (TTL) generators, sparked questions about how they can be effectively incorporated into writing education. However, it is still unclear how teachers, parents, and students perceive and suspect GAI systems in elementary school settings. We conducted a workshop with twelve families (parent-child dyads) with children ages 8-12 and interviewed sixteen teachers in order to understand each stakeholder’s perspectives and opinions on GAI systems for learning and teaching writing. We found that the GAI systems could be beneficial in generating adaptable teaching materials for teachers, enhancing ideation, and providing students with personalized, timely feedback. However, there are concerns over authorship, students’ agency in learning, and uncertainty concerning bias and misinformation. In this article, we discuss design strategies to mitigate these constraints by implementing an adults-oversight system, balancing AI-role allocation, and facilitating customization to enhance students’ agency over writing projects.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {678},\nnumpages = {17},\nkeywords = {Artificial Intelligence, Generative AI, K-12 Education},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642460,\nauthor = {Khan, Sushmita and Iqbal, Mehtab and Osho, Oluwafemi and Singh, Khushbu and Derrick, Kyra and Nelson, Philip and Li, Lingyuan and Sidnam-Mauch, Emily and Bannister, Nicole and Caine, Kelly and Knijnenburg, Bart},\ntitle = {Teaching Middle Schoolers about the Privacy Threats of Tracking and Pervasive Personalization: A Classroom Intervention Using Design-Based Research},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642460},\ndoi = {10.1145/3613904.3642460},\nabstract = {With the pervasive and evolving use of tracking and AI to make inferences about online platform users, it has become imperative for adolescents—a key demographic using such platforms—to develop a deep understanding of these practices to protect their privacy. Traditionally, K-12 cybersecurity education has largely been confined to extracurricular activities, limiting underrepresented students’ access. To resolve this shortcoming, we partnered with a rural-identifying middle school to deliver AI-related privacy education in classrooms. Using Design-Based Research methodology, we identified students’ AI-related privacy learning needs and developed six education modules. This paper focuses on the design, classroom implementation, and evaluation of module #2, covering the privacy threats of Tracking and Pervasive Personalization (TaPP). Student assessment outcomes show they developed transferable foundational knowledge of the privacy implications of tracking and personalization after participating in the TaPP module. Our findings demonstrate the benefits of integrating AI-related privacy education into existing K-12 curricula.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {679},\nnumpages = {26},\nkeywords = {AI-related Privacy Education, Adolescent, Classroom intervention, Data Tracking, Design-Based Research, Personalization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642902,\nauthor = {Tankelevitch, Lev and Kewenig, Viktor and Simkute, Auste and Scott, Ava Elizabeth and Sarkar, Advait and Sellen, Abigail and Rintel, Sean},\ntitle = {The Metacognitive Demands and Opportunities of Generative AI},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642902},\ndoi = {10.1145/3613904.3642902},\nabstract = {Generative AI (GenAI) systems offer unprecedented opportunities for transforming professional and personal work, yet present challenges around prompting, evaluating and relying on outputs, and optimizing workflows. We argue that metacognition—the psychological ability to monitor and control one’s thoughts and behavior—offers a valuable lens to understand and design for these usability challenges. Drawing on research in psychology and cognitive science, and recent GenAI user studies, we illustrate how GenAI systems impose metacognitive demands on users, requiring a high degree of metacognitive monitoring and control. We propose these demands could be addressed by integrating metacognitive support strategies into GenAI systems, and by designing GenAI systems to reduce their metacognitive demand by targeting explainability and customizability. Metacognition offers a coherent framework for understanding the usability challenges posed by GenAI, and provides novel research and design directions to advance human-AI interaction.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {680},\nnumpages = {24},\nkeywords = {Generative AI, Human-AI interaction, Metacognition, System Usability, User Experience Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642879,\nauthor = {van Gemert, Thomas and Chew, Sean and Kalaitzoglou, Yiannis and Bergstr\\\"{o}m, Joanna},\ntitle = {Doorways Do Not Always Cause Forgetting: Studying the Effect of Locomotion Technique and Doorway Visualization in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642879},\ndoi = {10.1145/3613904.3642879},\nabstract = {The “doorway effect” predicts that crossing an environmental boundary affects memory negatively. In virtual reality (VR), we can design the crossing and the appearance of such boundaries in non-realistic ways. However, it is unclear whether locomotion techniques like teleportation, which avoid crossing the boundary altogether, still induce the effect. Furthermore, it is unclear how different appearances of a doorway act as a boundary and thus induce the effect. To address these questions, we conducted two lab studies. First, we conceptually replicated prior doorway effect studies in VR using natural walking and teleportation. Second, we investigated the effect of five doorway visualizations, ranging from doors to portals. The results show no difference in object recognition performance due to the presence of a doorway, locomotion technique, or doorway visualization. We discuss the implications of these findings on the role of boundaries in event-based memory and the design of boundary interactions in VR.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {681},\nnumpages = {13},\nkeywords = {boundary, doorway, effect, environment, event, forgetting, memory, object, reality, recognition, teleportation, virtual, vr, walking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641975,\nauthor = {Luo, Tianren and Lu, Fenglin and Lv, Jiafu and Tan, Xiaohui and Liu, Chang and Yan, Fangzhi and Huang, Jin and Yu, Chun and Han, Teng and Tian, Feng},\ntitle = {Exploring Experience Gaps Between Active and Passive Users During Multi-user Locomotion in VR},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641975},\ndoi = {10.1145/3613904.3641975},\nabstract = {Multi-user locomotion in VR has grown increasingly common, posing numerous challenges. A key factor contributing to these challenges is the gaps in experience between active and passive users during co-locomotion. Yet, there remains a limited understanding of how and to what extent these experiential gaps manifest in diverse multi-user co-locomotion scenarios. This paper systematically explores the gaps in physiological and psychological experience indicators between active and passive users across various locomotion situations. Such situations include when active users walk, fly by joystick, or teleport, and passive users stand still or look around. We also assess the impact of factors such as sub-locomotion type, speed/teleport-interval, motion sickness susceptibility, etc. Accordingly, we delineate acceptability disparities between active and passive users, offering insights into leveraging notable experimental findings to mitigate discomfort during co-locomotion through avoidance or intervention.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {682},\nnumpages = {19},\nkeywords = {experience gap, locomotion experience, multi-user VR, sensory conflict},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642088,\nauthor = {Ribeiro, Renato Alexandre and Gon\\c{c}alves, In\\^{e}s and Pi\\c{c}arra, Manuel and Seixas Pereira, Let\\'{\\i}cia and Duarte, Carlos and Rodrigues, Andr\\'{e} and Guerreiro, Jo\\~{a}o},\ntitle = {Investigating Virtual Reality Locomotion Techniques with Blind People},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642088},\ndoi = {10.1145/3613904.3642088},\nabstract = {Many Virtual Reality (VR) locomotion techniques have been proposed, but those explored for and with blind people are often custom-made or require specialized equipment. Consequently, it is unclear how popular techniques can support blind people’s VR locomotion, blocking access to most VR experiences. We implemented three popular techniques — Arm Swinging, Linear Movement (joystick-based steering), and Point \\& Teleport — with minor adaptations for accessibility. We conducted a study with 14 blind participants consisting of navigation tasks with these techniques and a semi-structured interview. We found no differences in overall performance (e.g., completion time), but contrasting preferences. Findings highlight the challenges and advantages of each technique and participants’ strategies. We discuss, among others, how augmenting the techniques enabled blind people to navigate in VR, the greater control of movement of Arm Swinging, the simplicity and familiarity of Linear Movement, and the potential for efficiency and for scanning the environment of Point \\& Teleport.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {683},\nnumpages = {17},\nkeywords = {Arm Swinging, Linear Movement, Navigation, Point and Teleport, Virtual Reality., Visual Impairment},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642228,\nauthor = {Clarence, Aldrich and Knibbe, Jarrod and Cordeil, Maxime and Wybrow, Michael},\ntitle = {Stacked Retargeting: Combining Redirected Walking and Hand Redirection to Expand Haptic Retargeting's Coverage},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642228},\ndoi = {10.1145/3613904.3642228},\nabstract = {We present Stacked Retargeting—combining haptic retargeting and redirected walking—to maximise the use of passive proxy objects for VR haptics. Haptic retargeting work to date has considered stationary reaching and grasping interactions, and this inherently limits a proxy object’s scope. We consider exactly where this reaching and grasping occurs from, to increase the potential of each proxy. We present (a) a staged approach to implementing Stacked Retargeting, (b) five redirected walking approaches that enable users to arrive anywhere at the site of interaction, and (c) a usability magnitude estimation evaluation of these techniques. We demonstrate how Stacked Retargeting can meaningfully increase the practical use of proxy objects for VR haptics without degrading the user experience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {684},\nnumpages = {13},\nkeywords = {Haptics, Redirection, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641919,\nauthor = {Gerritse, Maarten and Rietzler, Michael and Van Nimwegen, Christof and Frommel, Julian},\ntitle = {The Effect of Spatial Audio on Curvature Gains in VR Redirected Walking},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641919},\ndoi = {10.1145/3613904.3641919},\nabstract = {Redirected walking (RDW) is a technique that allows users to navigate larger physical spaces in virtual reality (VR) environments by manipulating the users’ view of the virtual world. In this study, we investigate the effect of adding spatial audio elements to curvature gains in RDW aiming to increase the perceptual threshold for the manipulation and allowing for higher levels of unnoticed redirection. We conducted a user study (n = 18), evaluating perceptual thresholds across conditions with and without spatial audio elements across different curvature gains. We found that spatial audio can significantly increase thresholds with a large effect size. This finding indicates the value of spatial audio for RDW. It could facilitate higher levels of redirection, while maintaining a convincing experience, leading to more freedom to navigate virtual environments in even smaller physical spaces.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {685},\nnumpages = {10},\nkeywords = {curvature gains, redirected walking, spatial audio, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642006,\nauthor = {Ibrahim, Zaidat and Panchpor, Pallavi and Nurain, Novia and Clawson, James},\ntitle = {\"Islamically, I am not on my period\": A Study of Menstrual Tracking in Muslim Women in the US},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642006},\ndoi = {10.1145/3613904.3642006},\nabstract = {The widespread adoption of menstrual tracking applications has garnered much attention with recent research focusing on inclusive design. However, existing literature has yet to explore the impact of religious practices on menstrual tracking behavior. We investigate the menstrual tracking practices of Muslim women of faith in the United States, a population whose personal reproductive health behaviors are deeply influenced by their faith, values, and religious laws We conducted a three-phase study consisting of preliminary surveys (N=133), semi-structured interviews (N=20), and a post-Roe v. Wade survey (N=77). We highlight motivations for tracking and uncover this overlooked population’s challenges as they engage with menstrual tracking technologies. We reveal an intimate connection between menstrual tracking and religious practices. We uncover challenges from engaging with existing menstrual tracking applications and contribute design recommendations for accommodating faith in the design of health tracking technologies. We amplify a call to action for the HCI community to reduce the \"othering\" of under-represented populations and to better support the inclusive design of technologies that center religious identities and values for individuals of faith.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {686},\nnumpages = {16},\nkeywords = {Menstruation, Muslim Women, Personal Informatics, Women’s Health},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642282,\nauthor = {Lin, Georgianna and Lessard, Pierre-William and Le, Minh Ngoc and Li, Brenna and Chevalier, Fanny and Truong, Khai N. and Mariakakis, Alex},\ntitle = {Functional Design Requirements to Facilitate Menstrual Health Data Exploration},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642282},\ndoi = {10.1145/3613904.3642282},\nabstract = {Menstrual trackers currently lack the affordances required to help individuals achieve their goals beyond menstrual event predictions and symptom logging. Taking an initial step towards this aspiration, we propose, validate, and refine five functional design requirements for future interface designs that facilitate menstrual data exploration. We interviewed 30 individuals who menstruate and collected their feedback on the practical application of these requirements. To elicit ideas and impressions, we designed two proof-of-concept interfaces to use as design probes with similar core functionalities but different presentations of phase timing predictions and signal arrangement. Our analysis revealed participants’ feedback regarding the presentation of predictions for menstrual-related events, the visualization of future signal patterns, personalization abilities for viewing signals relevant to their menstrual experience, the availability of resources to understand the underlying biological connections between signals, and the ability to compare multiple cycles side-by-side with context.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {687},\nnumpages = {15},\nkeywords = {gendered health, health informatics, health interface, menstrual tracking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642851,\nauthor = {Grimme, Sophie and Spoerl, Susanna Marie and Boll, Susanne and Koelle, Marion},\ntitle = {My Data, My Choice, My Insights: Women's Requirements when Collecting, Interpreting and Sharing their Personal Health Data},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642851},\ndoi = {10.1145/3613904.3642851},\nabstract = {HCI research has been instrumental in enabling self-directed health tracking. Despite a plethora of devices and data, however, users’ views of their own health are often fragmented. This is a problem for women’s health, where physical and mental observations and symptoms are strongly intertwined. An integrated view throughout different life stages could help to better understand these connections, facilitate symptom alleviation through life-style changes, and support timely diagnosis: currently, women’s health issues often go under-researched and under-diagnosed. To capture the needs and worries of self-directed tracking, interpreting and sharing women’s health data, we held workshops with 28 women. Drawing upon feminist methods, we conducted a Reflexive Thematic Analysis to identify six central themes that ground opportunities and challenges for life-long, self-directed tracking of intimate data. These themes inform the design of tools for data collection, analysis and sharing that empower women to better understand their bodies and demand adequate health services.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {688},\nnumpages = {18},\nkeywords = {Feminist HCI, Lifelong Health, Requirements, Women’s Health},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642374,\nauthor = {Ibrahim, Zaidat and Nurain, Novia and Clawson, James},\ntitle = {Tracking During Ramadan: Examining the Intersection of Menstrual and Religious Tracking Practices Among Muslim Women in the United States},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642374},\ndoi = {10.1145/3613904.3642374},\nabstract = {Personal Informatics (PI) tools are crucial in helping individuals monitor their physical health, mental health, and overall well-being. Many Muslim women use multiple PI tools to support their religious and spiritual well-being alongside their health. We investigate the religious and health-tracking practices of Muslim women living in the United States during the month of Ramadan. We conducted a month-long diary study and semi-structured interviews with nine (9) Muslim women observing Ramadan. Through this research, we uncover their motivations for tracking, discover the complex interplay between their social roles and religious practices, and identify conflicts arising from competing objectives (tracking their spiritual and physical health). Our findings contribute insights into the inclusive design of Personal Informatics tools tailored to the needs of Muslim women of faith and provide a call to the research community to expand tracking technologies to include aspects that support religious health and wellness. We discuss design considerations for supporting Muslim women during Ramadan and beyond.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {689},\nnumpages = {19},\nkeywords = {Faith, Muslim Women, Personal Informatics, Ramadan},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642207,\nauthor = {Al-Naimi, Latifa and Alistar, Mirela},\ntitle = {Understanding Cultural and Religious Values Relating to Awareness of Women’s Intimate Health among Arab Muslims},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642207},\ndoi = {10.1145/3613904.3642207},\nabstract = {Women’s intimate health is a historically stigmatized topic in many cultures. Arab and Muslim values such as privacy and modesty can influence the extent to which members of these communities seek information or help regarding their intimate health. However, Eurocentric approaches to design and research for these groups only yield resistance due to their challenging of core values. We explore prior work in women’s health in HCI and cultural models to design for an underrepresented group in HCI research. Through interviews conducted with 16 participants who identified as Arab Muslims, we investigated attitudes, cultural and religious values, and backgrounds relating to awareness of women’s intimate health issues. Our thematic analysis identified shared experiences in learning about women’s intimate health and ways in which Arab culture and Islam synchronize or diverge. We contribute cultural and religious elements to consider in research methodology and design for Arab and Muslim communities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {690},\nnumpages = {18},\nkeywords = {Arab, Islam, Women’s health, cross-cultural design, decolonization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642723,\nauthor = {Kitson, Alexandra and Antle, Alissa N. and Kenny, Sadhbh and Adhikari, Ashu and Karthik, Kenneth and Cimensel, Artun and Chan, Melissa},\ntitle = { 'I Call Upon a Friend': Virtual Reality-Based Supports for Cognitive Reappraisal Identified through Co-designing with Adolescents},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642723},\ndoi = {10.1145/3613904.3642723},\nabstract = {Virtual reality (VR) offers great promise to expand delivery models for therapeutic interventions to help adolescents develop adaptive emotion regulation skills. Cognitive reappraisal (CR) is an emotion regulation skill that involves changing your thinking to improve your emotional state. However, adolescents face developmental and implementation barriers to do CR successfully. To better understand adolescents’ (15-18 years) lived experience of CR challenges and how they envision VR could support their skills learning and transfer to everyday life, we ran three co-design workshops (N=69). Our research weaves together the workshop findings with prior literature to identify directions for future VR-based CR interventions. From our study results, we generated design strategies leveraging best practices of existing research: embedded and embodied scaffolds, providing different points of view, and externalizing the inner self. To illustrate these strategies in practice, we show how each would work in a challenging emotional scenario identified by adolescents.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {691},\nnumpages = {18},\nkeywords = {adolescents, co-design, cognitive reappraisal, emotion regulation, mental health, participatory design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642624,\nauthor = {Slovak, Petr and Munson, Sean A.},\ntitle = {HCI Contributions in Mental Health: A Modular Framework to Guide Psychosocial Intervention Design},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642624},\ndoi = {10.1145/3613904.3642624},\nabstract = {Many people prefer psychosocial interventions for mental health care or other concerns, but these interventions are often complex and unavailable in settings where people seek care. Intervention designers use technology to improve user experience or reach of interventions, and HCI researchers have made many contributions toward this goal. Both HCI and mental health researchers must navigate tensions between innovating on and adhering to the theories of change that guide intervention design. In this paper, we propose a framework that describes design briefs and evaluation approaches for HCI contributions at the scopes of capabilities, components, intervention systems, and intervention implementations. We show how theories of change (from mental health) can be translated into design briefs (in HCI), and that these translations can bridge and coordinate efforts across fields. It is our hope that this framework can support researchers in motivating, planning, conducting, and communicating work that advances psychosocial intervention design.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {692},\nnumpages = {21},\nkeywords = {behavioural science, complex interventions, mental health, psychosocial interventions, theory},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642604,\nauthor = {Kalanadhabhatta, Manasa and Mateo Santana, Adrelys and Mayorga, Lynnea and Rahman, Tauhidur and Ganesan, Deepak and Grabell, Adam},\ntitle = {Multi-stakeholder Perspectives on Mental Health Screening Tools for Children},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642604},\ndoi = {10.1145/3613904.3642604},\nabstract = {Pediatric mental health is a growing concern around the world, affecting children’s social-emotional development and increasing the risk of poor behavioral outcomes later in life. However, obtaining a behavioral diagnosis in early childhood is challenging due to lack of access to resources, low parental mental health literacy, and children’s dependence on several stakeholders to coordinate care for them. While app-based, at-home screening tools could offer a scalable and convenient diagnostic solution for families, stakeholder perspectives on their utility and usability remain to be examined. This work reports on a survey of child mental health practitioners and interviews with parents to illustrate existing barriers to care that stakeholders encounter, the perceived benefits of app-based screening tools in meeting their needs, and the challenges in scaling up these tools. We identify where stakeholders agree or disagree, delineate key design tensions, and provide recommendations for the development of future screening technologies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {693},\nnumpages = {15},\nkeywords = {caregiving, children, digital health, mental health, parenting},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642488,\nauthor = {Kitson, Alexandra and Slovak, Petr and Antle, Alissa N.},\ntitle = {Supporting Cognitive Reappraisal With Digital Technology: A Content Analysis and Scoping Review of Challenges, Interventions, and Future Directions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642488},\ndoi = {10.1145/3613904.3642488},\nabstract = {Cognitive reappraisal (CR) is a critical emotion regulation skill that is strongly associated with mental well-being outcomes. While CR has been well theorized psychologically and many therapeutic approaches exist, CR remains one of the toughest skills to learn and develop. We explore the design space of using technologically-mediated CR supports through a dual approach. First, we draw on a content analysis of 30 therapeutic manuals combined with five clinician interviews to understand existing CR processes and challenges in therapeutic settings. Second, we compare the identified challenges with a scoping review of 42 HCI papers on technologically-mediated CR interventions. This allowed us to identify trends and gaps in a field where digital health innovations are critically needed; and suggest four design opportunities that warrant further exploration. Together, our work contributes theoretically-derived future research opportunities, and provides researchers with concrete guidance to explore these important design spaces.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {694},\nnumpages = {17},\nkeywords = {Cognitive Reappraisal, Content Analysis, Emotion Regulation, Mental Health, Review, Technology-enabled Intervention},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3643054,\nauthor = {Staab, Phoebe A and Williams, A. Jess and Robertson, Mackenzie D. A. and Slovak, Petr},\ntitle = {“Can you be with that feeling?”: Extending Design Strategies for Interoceptive Awareness for the Context of Mental Health},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3643054},\ndoi = {10.1145/3613904.3643054},\nabstract = {Awareness of internal sensations, or interoceptive awareness (IA) is a topic of interest spanning multiple disciplines. In psychology, several therapeutic frameworks which cultivate IA have emerged. Meanwhile, HCI designers have developed novel approaches to IA across diverse contexts and design goals. These HCI strategies may hold value for mental health, however, it’s unclear to what degree designerly IA techniques match or contrast with those used in therapeutic settings. We seek to address this gap in two parts. First, we offer a set of design opportunities based on IA practices used in HCI and findings from interviews with 22 counselors. Second, we share context-specific insights from a 5-week probe study involving 24 young women with nonclinical disordered eating behaviors, which are linked to interoceptive deficits. Together, the design opportunities and probe study findings provide guidance and highlight open questions regarding the design of technology-mediated IA support for mental health.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {695},\nnumpages = {21},\nkeywords = {Disordered eating, Health, Interoceptive awareness, Probes},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642856,\nauthor = {Oewel, Bruna and Arean, Patricia Anne and Agapie, Elena},\ntitle = {Approaches for tailoring between-session mental health therapy activities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642856},\ndoi = {10.1145/3613904.3642856},\nabstract = {Mental health activities conducted by patients between therapy sessions (or \"therapy homework\") are a component of addressing anxiety and depression. However, to be effective, therapy homework must be tailored to the client’s needs to address the numerous barriers they encounter in everyday life. In this study, we analyze how therapists and clients tailor therapy homework to their client’s needs. We interviewed 13 therapists and 14 clients about their experiences tailoring and engaging in therapy homework. We identify criteria for tailoring homework, such as client skills, discomfort, and external barriers. We present how homework gets adapted, such as through changes in difficulty or by identifying alternatives. We discuss how technologies can better use client information for personalizing mental health interventions, such as adapting to client barriers, adjusting homework to these barriers, and creating a safer environment to support discomfort.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {696},\nnumpages = {19},\nkeywords = {behavior change, goal setting, mental health, personalization, tailoring},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642641,\nauthor = {Soubutts, Ewan and Shrestha, Pranita and Davidson, Brittany I and Qu, Chengcheng and Mindel, Charlotte and Sefi, Aaron and Marshall, Paul and Mcnaney, Roisin},\ntitle = {Challenges and Opportunities for the Design of Inclusive Digital Mental Health Tools: Understanding Culturally Diverse Young People's Experiences},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642641},\ndoi = {10.1145/3613904.3642641},\nabstract = {Mental health issues affect approximately 13\\% of people aged 10-24 years old worldwide. In Western countries (e.g. USA, UK, Australia), mental health issues are particularly prominent in Culturally and Linguistically Diverse (CALD) individuals, yet they are disproportionately affected in relation to service provision. Despite demand, there is a significant lack of literature explicitly exploring the design of digital mental health tools for CALD populations. Our study engaged five professionals working in CALD mental health, to gain insights into challenges for service access and provision, and then engaged 41 CALD young people to explore their experiences. We contribute a set of unique insights into the barriers that CALD young people face when seeking help, and their needs for future digital mental health tools. We also provide design recommendations for future researchers on how they might better support the inclusion of CALD communities in the design of digital health tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {697},\nnumpages = {16},\nkeywords = {Cultural and linguistic diversity, Interview study, Mental health, Survey, socio-cultural factors},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642649,\nauthor = {Feng, Yuan-Ling and Wang, Zhaoguo and Yao, Yuan and Li, Hanxuan and Diao, Yuting and Peng, Yu and Mi, Haipeng},\ntitle = {Co-designing the Collaborative Digital Musical Instruments for Group Music Therapy},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642649},\ndoi = {10.1145/3613904.3642649},\nabstract = {Digital Musical Instruments (DMIs) have been integrated into group music therapy, providing therapists with alternative ways to engage in musical dialogues with their clients. However, existing DMIs used in group settings are primarily designed for individual use and often overlook the social dynamics inherent in group therapy. Recognizing the crucial role of social interaction in the effectiveness of group therapy, we argue that Collaborative Digital Musical Instruments (CDMIs), seamlessly integrating social interaction with musical expression, hold significant potential to enhance group music therapy. To better tailor CDMIs for group music therapy, we engaged in a co-design process with music therapists, designing and practicing group therapy sessions involving the prototype ComString. In the end, we reflected on the co-design case to suggest future directions for designing CDMIs in group music therapy.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {698},\nnumpages = {18},\nkeywords = {Co-design, DMI, collocated social interaction, group music therapy, multi-user musical interface, stress-reducing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642846,\nauthor = {Lim, Jieun and Koh, Youngji and Kim, Auk and Lee, Uichin},\ntitle = {Exploring Context-Aware Mental Health Self-Tracking Using Multimodal Smart Speakers in Home Environments},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642846},\ndoi = {10.1145/3613904.3642846},\nabstract = {People with mental health issues often stay indoors, reducing their outdoor activities. This situation emphasizes the need for self-tracking technology in homes for mental health research, offering insights into their daily lives and potentially improving care. This study leverages a multimodal smart speaker to design a proactive self-tracking research system that delivers mental health surveys using an experience sampling method (ESM). Our system determines ESM delivery timing by detecting user context transitions and allowing users to answer surveys through voice dialogues or touch interactions. Furthermore, we explored the user experience of a proactive self-tracking system by conducting a four-week field study (n=20). Our results show that context transition-based ESM delivery can increase user compliance. Participants preferred touch interactions to voice commands, and the modality selection varied depending on the user’s immediate activity context. We explored the design implications for home-based, context-aware self-tracking with multimodal speakers, focusing on practical applications.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {699},\nnumpages = {18},\nkeywords = {Experience Sampling Method (ESM), Mental Health, Multimodal Smart Speakers, Self-tracking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642761,\nauthor = {Sharma, Ashish and Rushton, Kevin and Lin, Inna Wanyin and Nguyen, Theresa and Althoff, Tim},\ntitle = {Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642761},\ndoi = {10.1145/3613904.3642761},\nabstract = {Self-guided mental health interventions, such as “do-it-yourself” tools to learn and practice coping strategies, show great promise to improve access to mental health care. However, these interventions are often cognitively demanding and emotionally triggering, creating accessibility barriers that limit their wide-scale implementation and adoption. In this paper, we study how human-language model interaction can support self-guided mental health interventions. We take cognitive restructuring, an evidence-based therapeutic technique to overcome negative thinking, as a case study. In an IRB-approved randomized field study on a large mental health website with 15,531 participants, we design and evaluate a system that uses language models to support people through various steps of cognitive restructuring. Our findings reveal that our system positively impacts emotional intensity for 67\\% of participants and helps 65\\% overcome negative thoughts. Although adolescents report relatively worse outcomes, we find that tailored interventions that simplify language model generations improve overall effectiveness and equity.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {700},\nnumpages = {29},\nkeywords = {cognitive restructuring, field study, human-AI collaboration, language models, mental health, randomized trial},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642937,\nauthor = {Kim, Taewan and Bae, Seolyeong and Kim, Hyun Ah and Lee, Su-Woo and Hong, Hwajung and Yang, Chanmo and Kim, Young-Ho},\ntitle = {MindfulDiary: Harnessing Large Language Model to Support Psychiatric Patients' Journaling},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642937},\ndoi = {10.1145/3613904.3642937},\nabstract = {Large Language Models (LLMs) offer promising opportunities in mental health domains, although their inherent complexity and low controllability elicit concern regarding their applicability in clinical settings. We present MindfulDiary, an LLM-driven journaling app that helps psychiatric patients document daily experiences through conversation. Designed in collaboration with mental health professionals, MindfulDiary takes a state-based approach to safely comply with the experts’ guidelines while carrying on free-form conversations. Through a four-week field study involving 28 patients with major depressive disorder and five psychiatrists, we examined how MindfulDiary facilitates patients’ journaling practice and clinical care. The study revealed that MindfulDiary supported patients in consistently enriching their daily records and helped clinicians better empathize with their patients through an understanding of their thoughts and daily contexts. Drawing on these findings, we discuss the implications of leveraging LLMs in the mental health domain, bridging the technical feasibility and their integration into clinical settings.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {701},\nnumpages = {20},\nkeywords = {chatbot, clinical setting, journaling, large language models, mental health, psychiatric patient},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642369,\nauthor = {Yoo, Dong Whi and Woo, Hayoung and Nguyen, Viet Cuong and Birnbaum, Michael L. and Kruzan, Kaylee Payne and Kim, Jennifer G and Abowd, Gregory D. and De Choudhury, Munmun},\ntitle = {Patient Perspectives on AI-Driven Predictions of Schizophrenia Relapses: Understanding Concerns and Opportunities for Self-Care and Treatment},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642369},\ndoi = {10.1145/3613904.3642369},\nabstract = {Early detection and intervention for relapse is important in the treatment of schizophrenia spectrum disorders. Researchers have developed AI models to predict relapse from patient-contributed data like social media. However, these models face challenges, including misalignment with practice and ethical issues related to transparency, accountability, and potential harm. Furthermore, how patients who have recovered from schizophrenia view these AI models has been underexplored. To address this gap, we first conducted semi-structured interviews with 28 patients and reflexive thematic analysis, which revealed a disconnect between AI predictions and patient experience, and the importance of the social aspect of relapse detection. In response, we developed a prototype that used patients’ Facebook data to predict relapse. Feedback from seven patients highlighted the potential for AI to foster collaboration between patients and their support systems, and to encourage self-reflection. Our work provides insights into human-AI interaction and suggests ways to empower people with schizophrenia.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {702},\nnumpages = {20},\nkeywords = {artificial intelligence, mental health, patient perspectives, schizophrenia relapse},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641908,\nauthor = {Zhang, Jiayi Eurus and Hilpert, Bernhard and Broekens, Joost and Jokinen, Jussi P. P.},\ntitle = {Simulating Emotions With an Integrated Computational Model of Appraisal and Reinforcement Learning},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641908},\ndoi = {10.1145/3613904.3641908},\nabstract = {Predicting users’ emotional states during interaction is a long-standing goal of affective computing. However, traditional methods based on sensory data alone fall short due to the interplay between users’ latent cognitive states and emotional responses. To address this, we introduce a computational cognitive model that simulates emotion as a continuous process, rather than a static state, during interactive episodes. This model integrates cognitive-emotional appraisal mechanisms with computational rationality, utilizing value predictions from reinforcement learning. Experiments with human participants demonstrate the model’s ability to predict and explain the emergence of emotions such as happiness, boredom, and irritation during interactions. Our approach opens the possibility of designing interactive systems that adapt to users’ emotional states, thereby improving user experience and engagement. This work also deepens our understanding of the potential of modeling the relationship between reward processing, reinforcement learning, goal-directed behavior, and appraisal.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {703},\nnumpages = {12},\nkeywords = {Appraisal Theory, Computational Rationality, Emotion Modeling, Reinforcement Learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642764,\nauthor = {Sun, Jingjing and Yang, Jingyi and Zhou, Guyue and Jin, Yucheng and Gong, Jiangtao},\ntitle = {Understanding Human-AI Collaboration in Music Therapy Through Co-Design with Therapists},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642764},\ndoi = {10.1145/3613904.3642764},\nabstract = {The rapid development of musical AI technologies has expanded the creative potential of various musical activities, ranging from music style transformation to music generation. However, little research has investigated how musical AIs can support music therapists, who urgently need new technology support. This study used a mixed method, including semi-structured interviews and a participatory design approach. By collaborating with music therapists, we explored design opportunities for musical AIs in music therapy. We presented the co-design outcomes involving the integration of musical AIs into a music therapy process, which was developed from a theoretical framework rooted in emotion-focused therapy. After that, we concluded the benefits and concerns surrounding music AIs from the perspective of music therapists. Based on our findings, we discussed the opportunities and design implications for applying musical AIs to music therapy. Our work offers valuable insights for developing human-AI collaborative music systems in therapy involving complex procedures and specific requirements.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {704},\nnumpages = {21},\nkeywords = {co-design, human-AI collaboration, music therapy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642262,\nauthor = {Risseeuw, Clarice and Mcquillan, Holly and Martins, Joana and Karana, Elvin},\ntitle = {(Re)activate, (Re)direct, (Re)arrange: Exploring the Design Space of Direct Interactions with Flavobacteria},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642262},\ndoi = {10.1145/3613904.3642262},\nabstract = {HCI designers increasingly engage in the integration of microbes into artefacts, leveraging their distinct biological affordances for novel interactions. While in many explorations the interaction between humans and microbes is mediated, scholars also highlight the potential of direct interactions, such as visualising mechanical distortions or fostering a sense of relationality with nonhumans through eliciting intimate encounters. Seizing upon this potential, our study delves into the realm of direct interactions involving Flavobacteria, recently introduced as a colour-changing interactive medium in HCI. We present a design space for direct interactions where humans can (re)activate, (re)direct, and (re)arrange Flavobacteria’s colourations, thereby fostering a personal and dynamic interplay between humans and microbes. With our work, we aspire to provide pathways and ignite inspiration among HCI designers to create living artefacts that cultivate active engagement and heightened attentiveness towards microbial worlds and beyond.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {705},\nnumpages = {18},\nkeywords = {Biological-HCI, Direct Interactions, Flavobacteria, Human-Microbe Interactions, Living Aesthetics, Living Colour},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642742,\nauthor = {Pasumarthy, Nandini and Nisal, Shreyas and Danaher, Jessica and van den Hoven, Elise and Khot, Rohit Ashok},\ntitle = {Go-Go Biome: Evaluation of a Casual Game for Gut Health Engagement and Reflection},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642742},\ndoi = {10.1145/3613904.3642742},\nabstract = {Experts emphasise that maintaining a healthy gut microbial balance requires the public to understand factors beyond diet, such as physical activity, lifestyle, and other real-world influences. Games as experiential systems are known to foster playful engagement and reflection. We propose a novel approach to promoting activity engagement for gut health and its reflection through the design of the Go-Go Biome game. The game simulates the interplay between friendly and unfriendly gut microbes, encouraging real-world activity engagement for gut-microbial balance through interactive visuals, unstructured play mechanics, and reflective design principles. A field study with 14 participants revealed that important facets of our game design led to awareness, playful visualisation, and reflection on factors influencing gut health. Our findings suggest four design lenses– bio-temporality, visceral conversations, wellness comparison, and inner discovery, to aid future playful design explorations to foster gut health engagement and reflection.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {706},\nnumpages = {20},\nkeywords = {activity engagement, casual game, gut health, gut microbiome, health reflection, reflection},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641981,\nauthor = {Chen, Yuning and Pschetz, Larissa},\ntitle = {Microbial Revolt: Redefining biolab tools and practices for more-than-human care ecologies},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641981},\ndoi = {10.1145/3613904.3641981},\nabstract = {Recent work in HCI has called for deeper ethical considerations when engaging with more-than-human organisms in design. In this paper, we introduce Microbial Revolt, a provocative method to support reflection on the perspectives of organisms involved in HCI and design practice. By asking participants to consider the reality of a chosen organism in feral and lab environments and to redesign lab tools in order to account for their “non-participation”, we identified the manifestation of key epistemic differences between approaches to care and ecologies in typical design and biology research - as well as the potential for design and HCI to creatively redefine power dynamics in the lab. Further interviews revealed specific challenges and opportunities that designers and HCI researchers face in adapting practices to lab standards, and lab equipment to their practices, calling for a redefinition of tools, spaces and guidance to accommodate phenomenological perspectives and multiple modes of interaction with living organisms.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {707},\nnumpages = {16},\nkeywords = {Bio-HCI, Biodesign, Care Ecologies, DIY-Bio, Ethics of Care, Microbe-HCI, More-Than-Human, Multispecies Design, Other-Than-Human, Posthumanism},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642020,\nauthor = {Silva, Claudio and Piadyk, Yurii and Rulff, Jo\\~{a}o and Panozzo, Daniele and Silva, Maria Beatriz and Saraiva, Antonio Alamo Feitosa and Oliveira, Naiara Cipriano and Lima, Flaviana Jorge De and Bantim, Renan Alfredo Machado and Gomes, Otavio Jose De Freitas and Watanabe, Akinobu},\ntitle = {PaleoScan: Low-Cost Easy-to-use High-Volume Fossil Scanning},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642020},\ndoi = {10.1145/3613904.3642020},\nabstract = {Fossils are crucial for understanding our natural history and the digitalization of fossils has paved the way for paleontologists to share and study them in greater detail. Yet, many fossil-dense regions, in particular low- and middle-income countries, lack the resources to digitalize their vast collections. This project reports on a collaboration between paleontologists and computer scientists to design, build, and operate a device that can be deployed in the field for digitizing a collection of thousands of fossils. We introduce PaleoScan, a user-friendly, cost-effective, high-volume scanner designed to expedite the digitization of extensive fossil collections. PaleoScan is a self-contained 3D scanning system consisting of a light and compact mirrorless camera, a microcontroller, a ChArUco calibration board, and user-controlled LEDs. Software and data processing is cloud-based, where the user interacts with the system through a web application. We deployed PaleoScan in a museum in Brazil with a world-class fossil collection. Our early results reveal its potential to revolutionize the scanning process for fossils.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {708},\nnumpages = {16},\nkeywords = {3D reconstruction, 3D scanning, digitalization, paleontology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642439,\nauthor = {Shen, Zhouyang and Morgan, Zak and Vasudevan, Madhan Kumar and Obrist, Marianna and Martinez Plasencia, Diego},\ntitle = {Controlled-STM: A Two-stage Model to Predict User’s Perceived Intensity for Multi-point Spatiotemporal Modulation in Ultrasonic Mid-air Haptics},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642439},\ndoi = {10.1145/3613904.3642439},\nabstract = {Multi-point STM offers a great range of parameters (i.e., drawing frequency, number of points) to produce different tactile sensations. However, existing studies offer limited insight on the effects of these parameters, and ignore their effect on the physical stimuli delivered, limiting effective haptic design. We propose a two-stage model to predict response to multi-point STM. The first stage predicts physical stimulus properties with 7.8\\% error, while the second stage predicts mean and spread of perceived intensity with 8.0 \\% and 8.8\\% error. We report 3 studies conducted to derive this model: one to characterize physical stimuli, another one measuring user perceptual thresholds, and a third one measuring user’s perceptual response to multi-point STM. Besides, we characterize 4 effects that influence device performance, confirm if previous effects reported are due to physical or perceptual effects (or both) and derive recommendations for manufacturers, haptic designers and HCI researchers.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {709},\nnumpages = {12},\nkeywords = {mid-air haptics, perception, polynomial regression},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642522,\nauthor = {Lim, Chungman and Park, Gunhyuk and Seifi, Hasti},\ntitle = {Designing Distinguishable Mid-Air Ultrasound Tactons with Temporal Parameters},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642522},\ndoi = {10.1145/3613904.3642522},\nabstract = {Mid-air ultrasound technology offers new design opportunities for contactless tactile patterns (i.e., Tactons) in user applications. Yet, few guidelines exist for making ultrasound Tactons easy to distinguish for users. In this paper, we investigated the distinguishability of temporal parameters of ultrasound Tactons in five studies (n=72 participants). Study 1 established the discrimination thresholds for amplitude-modulated (AM) frequencies. In Studies 2–5, we investigated distinguishable ultrasound Tactons by creating four Tacton sets based on mechanical vibrations in the literature and collected similarity ratings for the ultrasound Tactons. We identified a subset of temporal parameters, such as rhythm and low envelope frequency, that could create distinguishable ultrasound Tactons. Also, a strong correlation (mean Spearman’s ρ =0.75) existed between similarity ratings for ultrasound Tactons and similarities of mechanical Tactons from the literature, suggesting vibrotactile designers can transfer their knowledge to ultrasound design. We present design guidelines and future directions for creating distinguishable mid-air ultrasound Tactons.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {710},\nnumpages = {18},\nkeywords = {Mid-Air Haptics, Perceptual Distinguishability, Temporal Parameters, Ultrasound Tacton Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642735,\nauthor = {Xu, Shan and Sykes, Sarah and Abtahi, Parastoo and Grossman, Tovi and Walden, Daylon and Glueck, Michael and Rognon, Carine},\ntitle = {Designing Haptic Feedback for Sequential Gestural Inputs},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642735},\ndoi = {10.1145/3613904.3642735},\nabstract = {This work seeks to design and evaluate haptic feedback for sequential gestural inputs, where mid-air hand gestures are used to express system commands. Nine haptic patterns are first designed leveraging metaphors. To pursue efficient interaction, we examine the trade-off between pattern duration and recognition accuracy and find that durations as short as 0.3s-0.5s achieve roughly 80\\%-90\\% accuracy. We then examine the haptic design for sequential inputs, where we vary when the feedback for each gesture is provided, along with pattern duration, gesture sequence length, and age. Results show that providing haptic patterns right after detected hand gestures leads to significantly more efficient interaction compared with concatenating all haptic patterns after the gesture sequence. Moreover, the number of gestures had little impact on performance, but age is a significant predictor. Our results suggest that immediate feedback with 0.3s and 0.5s pattern duration would be recommended for younger and older users respectively.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {711},\nnumpages = {17},\nkeywords = {gestural inputs, haptics, vibrotactile feedback},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642417,\nauthor = {Lan, Ruiheng and Sun, Xu and Wang, Qingfeng and Liu, Bingjian},\ntitle = {Ultrasonic Mid-Air Haptics on the Face: Effects of Lateral Modulation Frequency and Amplitude on Users’ Responses},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642417},\ndoi = {10.1145/3613904.3642417},\nabstract = {Ultrasonic mid-air haptics (UMH) has emerged as a promising technology for facial haptic applications, offering the advantage of contactless and high-resolution feedback. Despite this, previous studies have fallen short in thoroughly investigating individuals’ responses to UMH on the face. To bridge this gap, this study compares UMH feedback on various facial sites using the lateral modulation (LM) method. This method allows us to explore the impact of two LM parameters -frequency and amplitude - on both perceptual (intensity) and emotional (valence and arousal) responses. With 24 participants, positive relationships between LM amplitude and perceived intensity and arousal were observed, and the effect of LM frequency varied across facial sites. These findings not only contribute to the development of design guidelines and potential applications for UMH on the face, but also provide insights aimed to enhance the effectiveness and overall user experience in haptic interactions across diverse facial sites.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {712},\nnumpages = {12},\nkeywords = {emotion, lateral modulation, mid-air haptic on the face, perception},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642608,\nauthor = {Zheng, Jian and Gao, Ge},\ntitle = {Fragmented Moments, Balanced Choices: How Do People Make Use of Their Waiting Time?},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642608},\ndoi = {10.1145/3613904.3642608},\nabstract = {Everyone spends some time waiting every day. HCI research has developed tools for boosting productivity while waiting. However, little is known about how people naturally spend their waiting time. We conducted an experience sampling study with 21 working adults who used a mobile app to report their daily waiting time activities over two weeks. The aim of this study is to understand the activities people do while waiting and the effect of situational factors. We found that participants spent about 60\\% of their waiting time on leisure activities, 20\\% on productive activities, and 20\\% on maintenance activities. These choices are sensitive to situational factors, including accessible device, location, and certain routines of the day. Our study complements previous ones by demonstrating that people purpose waiting time for various goals beyond productivity and to maintain work-life balance. Our findings shed light on future empirical research and system design for time management.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {713},\nnumpages = {14},\nkeywords = {Experience sampling method, Micromoment, Productivity, Time management, Well-being, Work-life balance},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642430,\nauthor = {Sadprasid, Book and Mei, Anne and Mariakakis, Alex and Bateman, Scott and Chevalier, Fanny},\ntitle = {Leveraging Idle Games to Incentivize Intermittent and Frequent Practice of Deep Breathing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642430},\ndoi = {10.1145/3613904.3642430},\nabstract = {The need for frequent and brief practice in deep breathing presents challenges in maintaining motivation and consistency. While persuasive technologies have been shown to improve engagement in therapeutic exercises, there is a lack of insight into specific motivational strategies for such intermittent activities. We investigate how idle games can incentivize behaviors like deep breathing and identify specific mechanics for fostering an optimal practice cycle. We illustrate this approach in a game called BreathPurr-suade. After validating the physiological efficacy of the embedded breathing guide, our four-week study revealed idle games are more effective in maintaining deep breathing adherence than a standard breathing guide. Our work highlights the capacity of idle games to foster deep breathing, revealing their efficacy in subtle persuasive game designs that encourage intermittent therapeutic practices.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {714},\nnumpages = {17},\nkeywords = {Breathing Guide, Deep Breathing, Idle Games, Therapy Games},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642729,\nauthor = {Zassman, Saralin and Kaplan, Craig S. and Vogel, Daniel},\ntitle = {Mindful Scroll: An Infinite Scroll Abstract Colouring App for Mindfulness},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642729},\ndoi = {10.1145/3613904.3642729},\nabstract = {We design and evaluate Mindful Scroll, a mobile application for mindfulness that encourages a slow and deliberate approach to colouring. The app renders an infinite scroll of generated geometric tilings that reveal pseudo-random colour palettes and fill effects when coloured using a finger or pen. A five-day study (N=28) evaluated the efficacy of the app in reducing anxiety and enhancing mindfulness. The results indicate that the app is capable of promoting a greater sense of mindfulness over time and produced similar results across several measures compared to traditional structured colouring and existing mindfulness-based mobile applications. All participants expressed a desire to use the app again, with a majority stating they felt more mindful after the study.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {715},\nnumpages = {16},\nkeywords = {colouring, mindfulness, mobile applications, well-being},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641932,\nauthor = {Kim, Jieun and Song, Hayeon},\ntitle = {My Voice as a Daily Reminder: Self-Voice Alarm for Daily Goal Achievement},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641932},\ndoi = {10.1145/3613904.3641932},\nabstract = {Sticking to daily plans is essential for achieving life goals but challenging in reality. This study presents a self-voice alarm as a novel daily goal reminder. Based on the strong literature on the psychological effects of self-voice, we developed a voice alarm system that reminds users of daily tasks to support their consistent task completion. Over the course of 14 days, participants (N = 63) were asked to complete daily vocabulary tasks when reminded by an alarm (i.e., self-voice vs. other-voice vs. beep sound alarm). The self-voice alarm elicited higher alertness and uncomfortable feelings while fostering more days of task completion and repetition compared to the beep sound alarm. Both self-voice and other-voice alarms increased users’ perceived usefulness of the alarm system. Leveraging both quantitative and qualitative approaches, we provide a practical guideline for designing voice alarm systems that will foster users’ behavioral changes to achieve daily goals.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {716},\nnumpages = {16},\nkeywords = {Daily Reminder, Habit Formation, Language Learning, Mobile Alarm Design, Self-Voice},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641986,\nauthor = {Miner, Nathan and Abdollahi, Amir and Myers, Caleb and Kosa, Mehmet and Ghaednia, Hamid and Schwab, Joseph H. and Harteveld, Casper and Troiano, Giovanni Maria},\ntitle = {Stairway to Heaven: A Gamified VR Journey for Breath Awareness},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641986},\ndoi = {10.1145/3613904.3641986},\nabstract = {Gamification and virtual reality (VR) are increasingly being explored for their potential to enhance mindful practices and well-being. We further explore the potential of gamification and VR for breath awareness and mindfulness, and contribute Stairway to Heaven, a VR artifact that combines gamification with respiratory sensor biofeedback to cultivate mindful awareness of breathing. In our mixed-method study with 21 participants, we evaluated the usability and effectiveness of our artifact in promoting breathing frequencies between 4 and 10 breaths per minute (BPM). We integrate breath-driven teleportation as a virtual locomotion technique (VLT) using respiratory biofeedback to gamify progression through a virtual wilderness. Additionally, we supplement our design with a mindfulness audio guide. The results of our user study showcase the potential of combining actionable gamification and VR, guided mindfulness, and breath-driven VLT to foster slow breathing self-regulation successfully.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {717},\nnumpages = {19},\nkeywords = {Breathing Training, Gamification, Mindfulness, Respiration Sensor, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642859,\nauthor = {Bensch, Leonie and Nilsson, Tommy and Wulkop, Jan and Demedeiros, Paul and Herzberger, Nicolas Daniel and Preutenborbeck, Michael and Gerndt, Andreas and Flemisch, Frank and Dufresne, Florian and Albuquerque, Georgia and Cowley, Aidan},\ntitle = {Designing for Human Operations on the Moon: Challenges and Opportunities of Navigational HUD Interfaces},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642859},\ndoi = {10.1145/3613904.3642859},\nabstract = {Future crewed missions to the Moon will face significant environmental and operational challenges, posing risks to the safety and performance of astronauts navigating its inhospitable surface. Whilst head-up displays (HUDs) have proven effective in providing intuitive navigational support on Earth, the design of novel human-spaceflight solutions typically relies on costly and time-consuming analogue deployments, leaving the potential use of lunar HUDs largely under-explored. This paper explores an alternative approach by simulating navigational HUD concepts in a high-fidelity Virtual Reality (VR) representation of the lunar environment. In evaluating these concepts with astronauts and other aerospace experts (n=25), our mixed methods study demonstrates the efficacy of simulated analogues in facilitating rapid design assessments of early-stage HUD solutions. We illustrate this by elaborating key design challenges and guidelines for future lunar HUDs. In reflecting on the limitations of our approach, we propose directions for future design exploration of human-machine interfaces for the Moon.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {718},\nnumpages = {21},\nkeywords = {astronaut, augmented reality, head-up display, human factors, human space flight, human-system exploration, lunar exploration, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642131,\nauthor = {Moon, Hee-Seung and Liao, Yi-Chi and Li, Chenyu and Lee, Byungjoo and Oulasvirta, Antti},\ntitle = {Real-time 3D Target Inference via Biomechanical Simulation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642131},\ndoi = {10.1145/3613904.3642131},\nabstract = {Selecting a target in a 3D environment is often challenging, especially with small/distant targets or when sensor noise is high. To facilitate selection, target-inference methods must be accurate, fast, and account for noise and motor variability. However, traditional data-free approaches fall short in accuracy since they ignore variability. While data-driven solutions achieve higher accuracy, they rely on extensive human datasets so prove costly, time-consuming, and transfer poorly. In this paper, we propose a novel approach that leverages biomechanical simulation to produce synthetic motion data, capturing a variety of movement-related factors, such as limb configurations and motor noise. Then, an inference model is trained with only the simulated data. Our simulation-based approach improves transfer and lowers cost; variety-rich data can be produced in large quantities for different scenarios. We empirically demonstrate that our method matches the accuracy of human-data-driven approaches using data from seven users. When deployed, the method accurately infers intended targets in challenging 3D pointing conditions within 5–10 milliseconds, reducing users’ target-selection error by 71\\% and completion time by 35\\%.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {719},\nnumpages = {18},\nkeywords = {Target selection, amortized inference., biomechanical simulation, target inference},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642145,\nauthor = {Laattala, Markus and Piitulainen, Roosa and Ady, Nadia M. and Tamariz, Monica and H\\\"{a}m\\\"{a}l\\\"{a}inen, Perttu},\ntitle = {WAVE: Anticipatory Movement Visualization for VR Dancing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642145},\ndoi = {10.1145/3613904.3642145},\nabstract = {Dance games are one of the most popular game genres in Virtual Reality (VR), and active dance communities have emerged on social VR platforms such as VR Chat. However, effective instruction of dancing in VR or through other computerized means remains an unsolved human-computer interaction problem. Existing approaches either only instruct movements partially, abstracting away nuances, or require learning and memorizing symbolic notation. In contrast, we investigate how realistic, full-body movements designed by a professional choreographer can be instructed on the fly, without prior learning or memorization. Towards this end, we describe the design and evaluation of WAVE, a novel anticipatory movement visualization technique where the user joins a group of dancers performing the choreography with different time offsets, similar to spectators making waves in sports events. In our user study (N=36), the participants more accurately followed a choreography using WAVE, compared to following a single model dancer.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {720},\nnumpages = {9},\nkeywords = {VR, dance game, dance instruction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642550,\nauthor = {Fitton, Isabel Sophie and Dark, Elizabeth and Oliveira da Silva, Manoela Milena and Dalton, Jeremy and Proulx, Michael J and Clarke, Christopher and Lutteroth, Christof},\ntitle = {Watch This! Observational Learning in VR Promotes Better Far Transfer than Active Learning for a Fine Psychomotor Task},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642550},\ndoi = {10.1145/3613904.3642550},\nabstract = {Virtual Reality (VR) holds great potential for psychomotor training, with existing applications using almost exclusively a ‘learning-by-doing’ active learning approach, despite the possible benefits of incorporating observational learning. We compared active learning (n=26) with different variations of observational learning in VR for a manual assembly task. For observational learning, we considered three levels of visual similarity between the demonstrator avatar and the user, dissimilar (n=25), minimally similar (n=26), or a self-avatar (n=25), as similarity has been shown to improve learning. Our results suggest observational learning can be effective in VR when combined with ‘hands-on’ practice and can lead to better far skill transfer to real-world contexts that differ from the training context. Furthermore, we found self-similarity in observational learning can be counterproductive when focusing on a manual task, and skills decay quickly without further training. We discuss these findings and derive design recommendations for future VR training.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {721},\nnumpages = {19},\nkeywords = {Active, Avatar Similarity, Observational, Psychomotor, Skills Training, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642084,\nauthor = {Kasahara, Nobuhito and Oba, Yosuke and Yamanaka, Shota and Batmaz, Anil Ufuk and Stuerzlinger, Wolfgang and Miyashita, Homei},\ntitle = {Better Definition and Calculation of Throughput and Effective Parameters for Steering to Account for Subjective Speed-accuracy Tradeoffs},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642084},\ndoi = {10.1145/3613904.3642084},\nabstract = {In Fitts’ law studies to investigate pointing, throughput is used to characterize the performance of input devices and users, which is claimed to be independent of task difficulty or the user’s subjective speed-accuracy bias. While throughput has been recognized as a useful metric for target-pointing tasks, the corresponding formulation for path-steering tasks and its evaluation have not been thoroughly examined in the past. In this paper, we conducted three experiments using linear, circular, and sine-wave path shapes to propose and investigate a novel formulation for the effective parameters and the throughput of steering tasks. Our results show that the effective width substantially improves the fit to data with mixed speed-accuracy biases for all task shapes. Effective width also smoothed out the throughput across all biases, while the usefulness of the effective amplitude depended on the task shape. Our study thus advances the understanding of user performance in trajectory-based tasks.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {722},\nnumpages = {18},\nkeywords = {ISO9241-411, effective parameters, performance metric, throughput, trajectory-based interaction, understanding people},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642143,\nauthor = {Yu, Xingyao and Lee, Benjamin and Sedlmair, Michael},\ntitle = {Design Space of Visual Feedforward And Corrective Feedback in XR-Based Motion Guidance Systems},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642143},\ndoi = {10.1145/3613904.3642143},\nabstract = {Extended reality (XR) technologies are highly suited in assisting individuals in learning motor skills and movements—referred to as motion guidance. In motion guidance, the “feedforward’’ provides instructional cues of the motions that are to be performed, whereas the “feedback’’ provides cues which help correct mistakes and minimize errors. Designing synergistic feedforward and feedback is vital to providing an effective learning experience, but this interplay between the two has not yet been adequately explored. Based on a survey of the literature, we propose design space for both motion feedforward and corrective feedback in XR, and describe the interaction effects between them. We identify common design approaches of XR-based motion guidance found in our literature corpus, and discuss them through the lens of our design dimensions. We then discuss additional contextual factors and considerations that influence this design, together with future research opportunities for motion guidance in XR.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {723},\nnumpages = {15},\nkeywords = {Design Space, Extended Reality, Motion Guidance, Visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642354,\nauthor = {Yu, Difeng and Cibulskis, Mantas and Mortensen, Erik Skjoldan and Christensen, Mark Schram and Bergstr\\\"{o}m, Joanna},\ntitle = {Metrics of Motor Learning for Analyzing Movement Mapping in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642354},\ndoi = {10.1145/3613904.3642354},\nabstract = {Virtual reality (VR) techniques can modify how physical body movements are mapped to the virtual body. However, it is unclear how users learn such mappings and, therefore, how the learning process may impede interaction. To understand and quantify the learning of the techniques, we design new metrics explicitly for VR interactions based on the motor learning literature. We evaluate the metrics in three object selection and manipulation tasks, employing linear-translational and nonlinear-rotational gains and finger-to-arm mapping. The study shows that the metrics demonstrate known characteristics of motor learning similar to task completion time, typically with faster initial learning followed by more gradual improvements over time. More importantly, the metrics capture learning behaviors that task completion time does not. We discuss how the metrics can provide new insights into how users adapt to movement mappings and how they can help analyze and improve such techniques.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {724},\nnumpages = {18},\nkeywords = {Interaction techniques, beyond-real interactions, motor adaptation, visual-motor mismatches},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642047,\nauthor = {Tan, Xiaohui and He, Zhenxuan and Liu, Can and Fan, Mingming and Luo, Tianren and Liu, Zitao and Tian, Mi and Han, Teng and Tian, Feng},\ntitle = {WieldingCanvas: Interactive Sketch Canvases for Freehand Drawing in VR},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642047},\ndoi = {10.1145/3613904.3642047},\nabstract = {Sketching in Virtual Reality (VR) is challenging mainly due to the absence of physical surface support and virtual depth perception cues, which induce high cognitive and sensorimotor load. This paper presents WieldingCanvas, an interactive VR sketching platform that integrates canvas manipulations to draw lines and curves in 3D. Informed by real-life examples of two-handed creative activities, WieldingCanvas interprets users’ spatial gestures to move, swing, rotate, transform, or fold a virtual canvas, whereby users simply draw primitive strokes on the canvas, which are turned into finer and more sophisticated shapes via the manipulation of the canvas. We evaluated the capability and user experience of WieldingCanvas with two studies where participants were asked to sketch target shapes. A set of freehand sketches of high aesthetic qualities were created, and the results demonstrated that WieldingCanvas can assist users with creating 3D sketches.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {725},\nnumpages = {16},\nkeywords = {Virtual Reality, freehand drawing, interactive canvas, two-handed interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642665,\nauthor = {Choi, Youjin and Jeon, Junryeol and Lee, ChungHa and Noh, Yeo-Gyeong and Hong, Jin-Hyuk},\ntitle = {A Way for Deaf and Hard of Hearing People to Enjoy Music by Exploring and Customizing Cross-modal Music Concepts},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642665},\ndoi = {10.1145/3613904.3642665},\nabstract = {Deaf and hard of hearing (DHH) people enjoy music and access it using a music-sensory substitution system that delivers sound together with the corresponding visual and tactile feedback. However, it is often challenging for them to comprehend the colorful visuals and strong vibrations that are designed to represent music. We confirmed that it is necessary to conceptualize cross-modal mapping before experiencing music sensory substitution through focus group interviews with 24 DHH people. To improve the music appreciation experience, a cross-modal music conceptualization system was implemented herein, which is a prototype that allows DHH people to explore the visuals and vibrations associated with music to perceive and appreciate. An evaluation with 28 DHH individuals demonstrated the capability of the system to improve subjective music appreciation experience via music-sensory substitution. Eventually, DHH people with negative attitudes toward music became positive in the exploration and customization process with our system.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {726},\nnumpages = {17},\nkeywords = {Conceptualization, Deaf and Hard of Hearing (DHH), Music Appreciation, Music-Sensory Substitution System},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642153,\nauthor = {Hnatyshyn, Rostyslav and Hong, Jiayi and Maciejewski, Ross and Norby, Christopher and Maley, Carlo C.},\ntitle = {Capturing Cancer as Music: Cancer Mechanisms Expressed through Musification},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642153},\ndoi = {10.1145/3613904.3642153},\nabstract = {The development of cancer is difficult to express on a simple and intuitive level due to its complexity. Since cancer is so widespread, raising public awareness about its mechanisms can help those affected cope with its realities, as well as inspire others to make lifestyle adjustments and screen for the disease. Unfortunately, studies have shown that cancer literature is too technical for the general public to understand. We found that musification, the process of turning data into music, remains an unexplored avenue for conveying this information. We explore the pedagogical effectiveness of musification through the use of an algorithm that manipulates a piece of music in a manner analogous to the development of cancer. We conducted two lab studies and found that our approach is marginally more effective at promoting cancer literacy when accompanied by a text-based article than text-based articles alone.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {727},\nnumpages = {11},\nkeywords = {cancer, cancer evolution, cancer therapy, musification},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642079,\nauthor = {Cavez, Vincent and Letondal, Catherine and Pietriga, Emmanuel and Appert, Caroline},\ntitle = {Challenges of Music Score Writing and the Potentials of Interactive Surfaces},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642079},\ndoi = {10.1145/3613904.3642079},\nabstract = {Composers use music notation programs throughout their creative process. Those programs are essentially elaborate structured document editors that enable composers to create high-quality scores by enforcing musical notation rules. They effectively support music engraving, but impede the more creative stages in the composition process because of their lack of flexibility. Composers thus often combine these desktop tools with other mediums such as paper. Interactive surfaces that support pen and touch input have the potential to address the tension between the contradicting needs for structure and flexibility. We interview nine professional composers. We report insights about their thought process and creative intentions, and rely on the “Cognitive Dimensions of Notations” framework to capture the frictions they experience when materializing those intentions on a score. We then discuss how interactive surfaces could increase flexibility by temporarily breaking the structure when manipulating the notation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {728},\nnumpages = {16},\nkeywords = {digital pen, ink, multi-touch interaction, music engraving},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642376,\nauthor = {Wang, Alexander and Cheng, Yi Fei and Lindlbauer, David},\ntitle = {MARingBA: Music-Adaptive Ringtones for Blended Audio Notification Delivery},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642376},\ndoi = {10.1145/3613904.3642376},\nabstract = {Audio notifications provide users with an efficient way to access information beyond their current focus of attention. Current notification delivery methods, like phone ringtones, are primarily optimized for high noticeability, enhancing situational awareness in some scenarios but causing disruption and annoyance in others. In this work, we build on the observation that music listening is now a commonplace practice and present MARingBA, a novel approach that blends ringtones into background music to modulate their noticeability. We contribute a design space exploration of music-adaptive manipulation parameters, including beat matching, key matching, and timbre modifications, to tailor ringtones to different songs. Through two studies, we demonstrate that MARingBA supports content creators in authoring audio notifications that fit low, medium, and high levels of urgency and noticeability. Additionally, end users prefer music-adaptive audio notifications over conventional delivery methods, such as volume fading.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {729},\nnumpages = {15},\nkeywords = {Adaptive Interfaces, Audio, Music Computing, Notifications},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642040,\nauthor = {Kamath, Purnima and Morreale, Fabio and Bagaskara, Priambudi Lintang and Wei, Yize and Nanayakkara, Suranga},\ntitle = {Sound Designer-Generative AI Interactions: Towards Designing Creative Support Tools for Professional Sound Designers},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642040},\ndoi = {10.1145/3613904.3642040},\nabstract = {The practice of sound design involves creating and manipulating environmental sounds for music, films, or games. Recently, an increasing number of studies have adopted generative AI to assist in sound design co-creation. Most of these studies focus on the needs of novices, and less on the pragmatic needs of sound design practitioners. In this paper, we aim to understand how generative AI models might support sound designers in their practice. We designed two interactive generative AI models as Creative Support Tools (CSTs) and invited nine professional sound design practitioners to apply the CSTs in their practice. We conducted semi-structured interviews and reflected on the challenges and opportunities of using generative AI in mixed-initiative interfaces for sound design. We provide insights into sound designers’ expectations of generative AI and highlight opportunities to situate generative AI-based tools within the design process. Finally, we discuss design considerations for human-AI interaction researchers working with audio.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {730},\nnumpages = {17},\nkeywords = {Audio, Creative Support Tools, Generative AI, Mixed-Initiative Creative Interfaces, Sound design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642433,\nauthor = {Zannettou, Savvas and Nemes-Nemeth, Olivia and Ayalon, Oshrat and Goetzen, Angelica and Gummadi, Krishna P. and Redmiles, Elissa M. and Roesner, Franziska},\ntitle = {Analyzing User Engagement with TikTok's Short Format Video Recommendations using Data Donations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642433},\ndoi = {10.1145/3613904.3642433},\nabstract = {Short-format videos have exploded on platforms like TikTok, Instagram, and YouTube. Despite this, the research community lacks large-scale empirical studies into how people engage with short-format videos and the role of recommendation systems that offer endless streams of such content. In this work, we analyze user engagement on TikTok using data we collect via a data donation system that allows TikTok users to donate their data. We recruited 347 TikTok users and collected 9.2M TikTok video recommendations they received. By analyzing user engagement, we find that the average daily usage time increases over the users’ lifetime while the user attention remains stable at around 45\\%. We also find that users like more videos uploaded by people they follow than those recommended by people they do not follow. Our study offers valuable insights into how users engage with short-format videos on TikTok and lessons learned from designing a data donation system.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {731},\nnumpages = {16},\nkeywords = {Data Donation, Recommendation Algorithm, TikTok, User Engagement},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642269,\nauthor = {Kairam, Sanjay R and Foote, Jeremy},\ntitle = {How Founder Motivations, Goals, and Actions Influence Early Trajectories of Online Communities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642269},\ndoi = {10.1145/3613904.3642269},\nabstract = {Online communities offer their members various benefits, such as information access, social and emotional support, and entertainment. Despite the important role that founders play in shaping communities, prior research has focused primarily on what drives users to participate and contribute; the motivations and goals of founders remain underexplored. To uncover how and why online communities get started, we present findings from a survey of 951 recent founders of Reddit communities. We find that topical interest is the most common motivation for community creation, followed by motivations to exchange information, connect with others, and self-promote. Founders have heterogeneous goals for their nascent communities, but they tend to privilege community quality and engagement over sheer growth. Differences in founders’ early attitudes towards their communities help predict not only the community-building actions that they pursue, but also the ability of their communities to attract visitors, contributors, and subscribers over the first 28 days. We end with a discussion of the implications for researchers, designers, and founders of online communities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {732},\nnumpages = {11},\nkeywords = {Reddit, founders, moderators, motivations, online communities},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642120,\nauthor = {Feng, K. J. Kevin and Koo, Xander and Tan, Lawrence and Bruckman, Amy and McDonald, David W. and Zhang, Amy X.},\ntitle = {Mapping the Design Space of Teachable Social Media Feed Experiences},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642120},\ndoi = {10.1145/3613904.3642120},\nabstract = {Social media feeds are deeply personal spaces that reflect individual values and preferences. However, top-down, platform-wide content algorithms can reduce users’ sense of agency and fail to account for nuanced experiences and values. Drawing on the paradigm of interactive machine teaching (IMT), an interaction framework for non-expert algorithmic adaptation, we map out a design space for teachable social media feed experiences to empower agential, personalized feed curation. To do so, we conducted a think-aloud study (N = 24) featuring four social media platforms—Instagram, Mastodon, TikTok, and Twitter—to understand key signals users leveraged to determine the value of a post in their feed. We synthesized users’ signals into taxonomies that, when combined with user interviews, inform five design principles that extend IMT into the social media setting. We finally embodied our principles into three feed designs that we present as sensitizing concepts for teachable feed experiences moving forward.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {733},\nnumpages = {20},\nkeywords = {feed curation, interactive machine teaching, social media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642339,\nauthor = {Stamato, Lydia and Prottoy, Hasan Mahmud and Higgins, Erin and Scheifele, Lisa Z. and Hamidi, Foad},\ntitle = {Message in a Bottle: Investigating Bioart Installations as a Transdisciplinary Means of Community Engagement},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642339},\ndoi = {10.1145/3613904.3642339},\nabstract = {As exploration of living media, biology, and biotechnology advances HCI, researchers call attention to implications for ethics. We respond with a qualitative study of audience engagement with multimedia bioart installation. Bioart comprises a transdisciplinary practice that brings diverse perspectives in art, science, and technology into dialogue and engages audiences. Understanding a bioart exemplar, Raaz, as disrupting habitual modes of being, we investigate audience experiences in three contexts, elaborating transdisciplinary community engagement that takes seriously living media and biotechnology and informs HCI broadly through vital authenticity, performative reflection, empowered critique, distributed expertise, and revealed dynamics. We discuss how transdisciplinary community engagement functions as a mode of inquiry and design that supports inclusive liminal experiences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {734},\nnumpages = {17},\nkeywords = {DIYbio, Living media interfaces, bioart, community science, installations, synthetic biology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642520,\nauthor = {Lin, Fang-Yu and Tsai, Pei-Hua and Lee, Chia-Yi and Ho, Yi-Ting and Chen, Yao-Kuang and Yen, Yu-Chun and Chang, Yung-Ju},\ntitle = {“I Prefer Regular Visitors to Answer My Questions”: Users’ Desired Experiential Background of Contributors for Location-based Crowdsourcing Platform},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642520},\ndoi = {10.1145/3613904.3642520},\nabstract = {This three-phase study explores the experiential background of contributors to platforms that provide crowdsourced location-related information. Initially, we utilized interviews to understand users’ expectations for location-related information and the contributors’ experiential background they believe would enhance this information’s utility. We then deployed a survey to identify the top eight sought-after location-information types and their perceived characteristics. Then the concluding online scenario-based study provided quantitative evidence about the interrelationships of eight types of location-related information, ten crucial quality attributes, and aspects of the contributors’ experiential background believed to enhance the utility of the descriptions they provide. Notably, although certain experiential background aspects were deemed universally advantageous across all information types, unique connections were identified among specific information types and distinct experiential background aspects seen as augmenting the contributor’s descriptions’ utility. These insights underline the importance of location-based crowdsourcing platforms incorporating contributors’ experiential background when assigning tasks.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {735},\nnumpages = {18},\nkeywords = {information quality, information seeking, location-based, mobile crowdsourcing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642367,\nauthor = {Yoo, Suhyeon and Pu, Kevin and Truong, Khai N.},\ntitle = {Behind the Pup-ularity Curtain: Understanding the Motivations, Challenges, and Work Performed in Creating and Managing Pet Influencer Accounts},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642367},\ndoi = {10.1145/3613904.3642367},\nabstract = {Creating dedicated accounts to post users’ pet content is a growing trend on Instagram. While these account owners derive joy from this pursuit, they may also struggle with criticisms and challenges. Yet, there remains a knowledge gap on how pet account owners manage their pets’ online presence and navigate these obstacles successfully. Drawing from interviews with 21 Instagram pet account owners, we uncover the motivations behind pet account creation, spanning personal, altruistic, and commercial goals. We learn about the strategies employed for crafting their pets’ online identities and personas, as well as the challenges faced by both owners and their pets in navigating the complexities of digital identity management. We discuss the evolving dynamics between humans and their pets, positioning pet identity cultivation as a form of collaborative work, akin to the “third shift”, highlighting the need to design interfaces that support this unique identity management process.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {736},\nnumpages = {17},\nkeywords = {Instagram, human-animal interaction, pet influencers, social media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642869,\nauthor = {Hsu, Silas and Karahalios, Karrie},\ntitle = {Choosing What You Want Versus Getting What You Want: An Experiment with Choice in Video Ad Placement},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642869},\ndoi = {10.1145/3613904.3642869},\nabstract = {User agency and control serve as cornerstones of design in HCI, with numerous studies finding that choice improves user experiences. However, few studies examine how users benefit from the act of choosing, independent from the fulfillment of their chosen option; making this distinction is crucial for refining guidelines on when to provide user control. In our experiment on YouTube, participants randomly experienced either a pre-roll ad, a mid-roll ad, or a choice between the two. Participants then rated their subjective experiences. Mid-roll ads negatively affected experience ratings, but ratings between those choosing a pre-roll ad and those assigned a pre-roll ad were similar. That is, the right ad timing had a much larger impact than choosing an ad timing. The findings suggest that user interfaces should not offer choices solely for the sake of offering choices, and suggest scenarios where automation would be preferable to fine-grained user control.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {737},\nnumpages = {9},\nkeywords = {advertising, agency, autonomy, choice, control, marketing, user experience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642769,\nauthor = {Atcheson, Alex and Koshy, Vinay and Karahalios, Karrie},\ntitle = {Not What it Used to Be: Characterizing Content and User-base Changes in Newly Created Online Communities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642769},\ndoi = {10.1145/3613904.3642769},\nabstract = {Attracting new members is vital to the health of many online communities. Yet, prior qualitative work suggests that newcomers to online communities can be disruptive – either due to a lack of awareness around existing community norms or to differing expectations around how the community should operate. Consequently, communities may have to navigate a trade-off between growth and development of community identity. We evaluate the presence of this trade-off through a longitudinal analysis of two years of commenting data for each of 1,620 Reddit communities. We find that, on average, communities become less linguistically distinctive as they grow. These changes appear to be driven almost equally by newcomers and returning users. Surprisingly, neither heavily moderated communities nor communities undergoing major user-base diversification are any more or less likely to maintaining distinctiveness. Taken together, our results complicate the assumption that growth is inherently beneficial for online communities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {738},\nnumpages = {12},\nkeywords = {Computational Social Science, Content Moderation, Growth, Online communities},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642078,\nauthor = {Saha, Koustuv and Gupta, Pranshu and Mark, Gloria and Kiciman, Emre and De Choudhury, Munmun},\ntitle = {Observer Effect in Social Media Use},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642078},\ndoi = {10.1145/3613904.3642078},\nabstract = {While social media data is a valuable source for inferring human behavior, its in-practice utility hinges on extraneous factors. Notable is the “observer effect,” where awareness of being monitored can alter people’s social media use. We present a causal-inference study to examine this phenomenon on the longitudinal Facebook use of 300+ participants who voluntarily shared their data spanning an average of 82 months before and 5 months after study enrollment. We measured deviation from participants’ expected social media use through time series analyses. Individuals with high cognitive ability and low neuroticism decreased posting immediately after enrollment, and those with high openness increased posting. The sharing of self-focused content decreased, while diverse topics emerged. We situate the findings within theories of self-presentation and self-consciousness. We discuss the implications of correcting observer effect in social media data-driven measurements, and how this phenomenon shines light on the ethics of these measurements.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {739},\nnumpages = {20},\nkeywords = {causal-inference, hawthorne effect, human behavior, language, observer effect, self-presentation, social media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642779,\nauthor = {Islam, Md Adnanul and Richardson, Dan and Saha, Manika and Varghese, Delvin and Bartindale, Tom and Saha, Pratyasha and Saputra, Muhamad Risqi U. and Olivier, Patrick},\ntitle = {Recordkeeping in Voice-based Remote Community Engagement},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642779},\ndoi = {10.1145/3613904.3642779},\nabstract = {Driven by pragmatic, cost-related, and environmental factors, voice-based remote community engagement tools (such as Interactive Voice Response) are emerging as a key modality for engaging marginalized communities. These voice-based digital solutions offer new opportunities for distributed community engagement and empowerment, and the ability to capture, store, and access a wide range of different records (i.e., recordings, interactions and contextual metadata) associated with community engagements. This potential for large scale, distributed community record collection necessitates an understanding of inclusive and effective recordkeeping approaches for appraisal, documentation, preservation, and accessibility of different types of records (such as audio recordings, transcripts, reports, and observatory notes) related to voice-based community engagements. Through qualitative analysis of stakeholder focus group discussions with domestic workers (as marginalized community members) and NGOs working in the sector, we present valuable insights and recommendations for the development of recordkeeping approaches tailored to voice-based remote community engagement records.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {740},\nnumpages = {16},\nkeywords = {HCI4D, IVR, audio, community engagement, community voice},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642148,\nauthor = {Lyu, Yao and Cai, Jie and Callis, Anisa and Cotter, Kelley and Carroll, John M.},\ntitle = {\"I Got Flagged for Supposed Bullying, Even Though It Was in Response to Someone Harassing Me About My Disability.\": A Study of Blind TikTokers’ Content Moderation Experiences},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642148},\ndoi = {10.1145/3613904.3642148},\nabstract = {The Human-Computer Interaction (HCI) community has consistently focused on the experiences of users moderated by social media platforms. Recently, scholars have noticed that moderation practices could perpetuate biases, resulting in the marginalization of user groups undergoing moderation. However, most studies have primarily addressed marginalization related to issues such as racism or sexism, with little attention given to the experiences of people with disabilities. In this paper, we present a study on the moderation experiences of blind users on TikTok, also known as \"BlindToker,\" to address this gap. We conducted semi-structured interviews with 20 BlindTokers and used thematic analysis to analyze the data. Two main themes emerged: BlindTokers’ situated content moderation experiences and their reactions to content moderation. We reported on the lack of accessibility on TikTok’s platform, contributing to the moderation and marginalization of BlindTokers. Additionally, we discovered instances of harassment from trolls that prompted BlindTokers to respond with harsh language, triggering further moderation. We discussed these findings in the context of the literature on moderation, marginalization, and transformative justice, seeking solutions to address such issues.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {741},\nnumpages = {15},\nkeywords = {Accessibility, Blind and Low Vision, BlindTok, Content Moderation, Disability, Marginalization, Platform Governance, Short-Video Platform, TikTok, Transformative Justice, Visual Impairment},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642025,\nauthor = {Mun, Jimin and Buerger, Cathy and Liang, Jenny T and Garland, Joshua and Sap, Maarten},\ntitle = {Counterspeakers’ Perspectives: Unveiling Barriers and AI Needs in the Fight against Online Hate},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642025},\ndoi = {10.1145/3613904.3642025},\nabstract = {Counterspeech, i.e., direct responses against hate speech, has become an important tool to address the increasing amount of hate online while avoiding censorship. Although AI has been proposed to help scale up counterspeech efforts, this raises questions of how exactly AI could assist in this process, since counterspeech is a deeply empathetic and agentic process for those involved. In this work, we aim to answer this question, by conducting in-depth interviews with 10 extensively experienced counterspeakers and a large scale public survey with 342 everyday social media users. In participant responses, we identified four main types of barriers and AI needs related to resources, training, impact, and personal harms. However, our results also revealed overarching concerns of authenticity, agency, and functionality in using AI tools for counterspeech. To conclude, we discuss considerations for designing AI assistants that lower counterspeaking barriers without jeopardizing its meaning and purpose.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {742},\nnumpages = {22},\nkeywords = {AI-mediated communication, AI-supported counterspeech, counterspeech, hate speech, online activism},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642045,\nauthor = {Zheng, Wenqi and Walquist, Emma and Datey, Isha and Zhou, Xiangyu and Berishaj, Kelly and Mcdonald, Melissa and Parkhill, Michele and Zhu, Dongxiao and Zytko, Douglas},\ntitle = {“It’s Not What We Were Trying to Get At, but I Think Maybe It Should Be”: Learning How to Do Trauma-Informed Design with a Data Donation Platform for Online Dating Sexual Violence},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642045},\ndoi = {10.1145/3613904.3642045},\nabstract = {A majority of people experience trauma, spurring calls to incorporate trauma-informed approaches (TIA) from public health and social work into technology design. While technologies touted as trauma-informed are starting to propagate the literature, there persists a gap in knowledge around how design teams apply TIA and qualify their technology as adhering to trauma-informed principles. We address this through a 12-month development project with trauma and sexual violence experts to produce Ube, a data donation platform for collecting online dating sexual consent data to improve sexual risk detection AI. Through analysis of design documentation we retrospectively articulate a trauma-informed design process that evolved through the course of Ube’s development, comprising three elements for integrating trauma-informed principles: design goals that adapt the definition of TIA to the application domain, design activities that map to trauma-informed principles, and consequent design choices. We conclude with methodological recommendations to improve trauma-informed design processes.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {743},\nnumpages = {15},\nkeywords = {AI, Data Donation, Harm, Online Dating, Risk Detection, Sexual Violence, Trauma},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641949,\nauthor = {Heung, Sharon and Jiang, Lucy and Azenkot, Shiri and Vashistha, Aditya},\ntitle = {“Vulnerable, Victimized, and Objectified”: Understanding Ableist Hate and Harassment Experienced by Disabled Content Creators on Social Media},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641949},\ndoi = {10.1145/3613904.3641949},\nabstract = {Content creators (e.g., gamers, activists, vloggers) with marginalized identities are at-risk of experiencing hate and harassment. This paper examines the ableist hate and harassment that disabled content creators experience on social media. Through surveys (N=50) and interviews (N=20) with disabled creators, we developed a taxonomy of 11 types of ableist hate and harassment (e.g., eugenics-related speech, denial and stigmatization of accessibility) and outlined how ableism harms creators’ well-being and content creation practices. Using statistical modeling, we investigated differences in ableist experiences given creators’ intersecting identities such as race and sexuality. We found that LGBTQ disabled creators face significantly more ableist hate compared to non-LGBTQ disabled creators. Lastly, we discuss our findings through an infrastructure lens to highlight how disabled creators experience platform-enabled ableism, undergo labor to cope with hate, and develop strategies to safeguard against future hate.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {744},\nnumpages = {19},\nkeywords = {ableism, content creators, disability discrimination, hate and harassment, infrastructure lens, intersectionality, privacy, social media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642563,\nauthor = {Weitz, Katharina and Schlagowski, Ruben and Andr\\'{e}, Elisabeth and M\\\"{a}nniste, Maris and George, Ceenu},\ntitle = {Explaining It Your Way - Findings from a Co-Creative Design Workshop on Designing XAI Applications with AI End-Users from the Public Sector},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642563},\ndoi = {10.1145/3613904.3642563},\nabstract = {Human-Centered AI prioritizes end-users’ needs like transparency and usability. This is vital for applications that affect people’s everyday lives, such as social assessment tasks in the public sector. This paper discusses our pioneering effort to involve public sector AI users in XAI application design through a co-creative workshop with unemployment consultants from Estonia. The workshop’s objectives were identifying user needs and creating novel XAI interfaces for the used AI system. As a result of our user-centered design approach, consultants were able to develop AI interface prototypes that would support them in creating success stories for their clients by getting detailed feedback and suggestions. We present a discussion on the value of co-creative design methods with end-users working in the public sector to improve AI application design and provide a summary of recommendations for practitioners and researchers working on AI systems in the public sector.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {745},\nnumpages = {14},\nkeywords = {Co-Creation, Explainable AI, Focus Group, Human-Centered AI, Social Assessment, Unemployment Insurance, User-Centered Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642775,\nauthor = {Cooper, Ned and Zafiroglu, Alexandra},\ntitle = {From Fitting Participation to Forging Relationships: The Art of Participatory ML},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642775},\ndoi = {10.1145/3613904.3642775},\nabstract = {Participatory machine learning (ML) encourages the inclusion of end users and people affected by ML systems in design and development processes. We interviewed 18 participation brokers—individuals who facilitate such inclusion and transform the products of participants’ labour into inputs for an ML artefact or system—across a range of organisational settings and project locations. Our findings demonstrate the inherent challenges of integrating messy contextual information generated through participation with the structured data formats required by ML workflows and the uneven power dynamics in project contexts. We advocate for evolution in the role of brokers to more equitably balance value generated in Participatory ML projects for design and development teams with value created for participants. To move beyond ‘fitting’ participation to existing processes and empower participants to envision alternative futures through ML, brokers must become educators and advocates for end users, while attending to frustration and dissent from indirect stakeholders.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {746},\nnumpages = {9},\nkeywords = {artificial intelligence, design, machine learning, participatory methods},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642160,\nauthor = {Sun, Yuan and Jang, Eunchae and Ma, Fenglong and Wang, Ting},\ntitle = {Generative AI in the Wild: Prospects, Challenges, and Strategies},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642160},\ndoi = {10.1145/3613904.3642160},\nabstract = {Propelled by their remarkable capabilities to generate novel and engaging content, Generative Artificial Intelligence (GenAI) technologies are disrupting traditional workflows in many industries. While prior research has examined GenAI from a techno-centric perspective, there is still a lack of understanding about how users perceive and utilize GenAI in real-world scenarios. To bridge this gap, we conducted semi-structured interviews with (N = 18) GenAI users in creative industries, investigating the human-GenAI co-creation process within a holistic LUA (Learning, Using and Assessing) framework. Our study uncovered an intriguingly complex landscape: Prospects – GenAI greatly fosters the co-creation between human expertise and GenAI capabilities, profoundly transforming creative workflows; Challenges – Meanwhile, users face substantial uncertainties and complexities arising from resource availability, tool usability, and regulatory compliance; Strategies – In response, users actively devise various strategies to overcome many of such challenges. Our study reveals key implications for the design of future GenAI tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {747},\nnumpages = {16},\nkeywords = {Generative AI, Human-AI Collaboration, Transparency, User Agency},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642497,\nauthor = {Gu, Ken and Shang, Ruoxi and Althoff, Tim and Wang, Chenglong and Drucker, Steven M.},\ntitle = {How Do Analysts Understand and Verify AI-Assisted Data Analyses?},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642497},\ndoi = {10.1145/3613904.3642497},\nabstract = {Data analysis is challenging as it requires synthesizing domain knowledge, statistical expertise, and programming skills. Assistants powered by large language models (LLMs), such as ChatGPT, can assist analysts by translating natural language instructions into code. However, AI-assistant responses and analysis code can be misaligned with the analyst’s intent or be seemingly correct but lead to incorrect conclusions. Therefore, validating AI assistance is crucial and challenging. Here, we explore how analysts understand and verify the correctness of AI-generated analyses. To observe analysts in diverse verification approaches, we develop a design probe equipped with natural language explanations, code, visualizations, and interactive data tables with common data operations. Through a qualitative user study (n=22) using this probe, we uncover common behaviors within verification workflows and how analysts’ programming, analysis, and tool backgrounds reflect these behaviors. Additionally, we provide recommendations for analysts and highlight opportunities for designers to improve future AI-assistant experiences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {748},\nnumpages = {22},\nkeywords = {Auto Data Science, Copilot, Data Science Assistant, Design Probe, Explainable AI, Human-AI Interaction, Human-AI Verification},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642849,\nauthor = {Kawakami, Anna and Coston, Amanda and Zhu, Haiyi and Heidari, Hoda and Holstein, Kenneth},\ntitle = {The Situate AI Guidebook: Co-Designing a Toolkit to Support Multi-Stakeholder, Early-stage Deliberations Around Public Sector AI Proposals},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642849},\ndoi = {10.1145/3613904.3642849},\nabstract = {Public sector agencies are rapidly deploying AI systems to augment or automate critical decisions in real-world contexts like child welfare, criminal justice, and public health. A growing body of work documents how these AI systems often fail to improve services in practice. These failures can often be traced to decisions made during the early stages of AI ideation and design, such as problem formulation. However, today, we lack systematic processes to support effective, early-stage decision-making about whether and under what conditions to move forward with a proposed AI project. To understand how to scaffold such processes in real-world settings, we worked with public sector agency leaders, AI developers, frontline workers, and community advocates across four public sector agencies and three community advocacy groups in the United States. Through an iterative co-design process, we created the Situate AI Guidebook: a structured process centered around a set of deliberation questions to scaffold conversations around (1) goals and intended use for a proposed AI system, (2) societal and legal considerations, (3) data and modeling constraints, and (4) organizational governance factors. We discuss how the guidebook’s design is informed by participants’ challenges, needs, and desires for improved deliberation processes. We further elaborate on implications for designing responsible AI toolkits in collaboration with public sector agency stakeholders and opportunities for future work to expand upon the guidebook. This design approach can be more broadly adopted to support the co-creation of responsible AI toolkits that scaffold key decision-making processes surrounding the use of AI in the public sector and beyond.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {749},\nnumpages = {22},\nkeywords = {Participatory Approaches to Design, Public Sector AI, Responsible AI, Technology Governance and Policy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642073,\nauthor = {Zenner, Andr\\'{e} and Karr, Chiara and Feick, Martin and Ariza, Oscar and Kr\\\"{u}ger, Antonio},\ntitle = {Beyond the Blink: Investigating Combined Saccadic \\& Blink-Suppressed Hand Redirection in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642073},\ndoi = {10.1145/3613904.3642073},\nabstract = {In pursuit of hand redirection techniques that are ever more tailored to human perception, we propose the first algorithm for hand redirection in virtual reality that makes use of saccades, i.e., fast ballistic eye movements that are accompanied by the perceptual phenomenon of change blindness. Our technique combines the previously proposed approaches of gradual hand warping and blink-suppressed hand redirection with the novel approach of saccadic redirection in one unified yet simple algorithm. We compare three variants of the proposed Saccadic \\& Blink-Suppressed Hand Redirection (SBHR) technique with the conventional approach to redirection in a psychophysical study (N = 25). Our results highlight the great potential of our proposed technique for comfortable redirection by showing that SBHR allows for significantly greater magnitudes of unnoticeable redirection while being perceived as significantly less intrusive and less noticeable than commonly employed techniques that only use gradual hand warping.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {750},\nnumpages = {14},\nkeywords = {change blindness, detection thresholds, eye blinks, hand redirection, saccades, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642254,\nauthor = {Kim, Myung Jin and Ofek, Eyal and Pahud, Michel and Sinclair, Mike J and Bianchi, Andrea},\ntitle = {Big or Small, It’s All in Your Head: Visuo-Haptic Illusion of Size-Change Using Finger-Repositioning},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642254},\ndoi = {10.1145/3613904.3642254},\nabstract = {Haptic perception of physical sizes increases the realism and immersion in Virtual Reality (VR). Prior work rendered sizes by exerting pressure on the user’s fingertips or employing tangible, shape-changing devices. These interfaces are constrained by the physical shapes they can assume, making it challenging to simulate objects growing larger or smaller than the perceived size of the interface. Motivated by literature on pseudo-haptics describing the strong influence of visuals over haptic perception, this work investigates modulating the perception of size beyond this range. We developed a fixed-sized VR controller leveraging finger-repositioning to create a visuo-haptic illusion of dynamic size-change of handheld virtual objects. Through two user studies, we found that with an accompanying size-changing visual context, users can perceive virtual object sizes up to <Formula format=\"inline\"><TexMath><?TeX $44.2\\%$?></TexMath><AltText>Math 1</AltText><File name=\"chi24-364-inline1\" type=\"svg\"/></Formula> smaller to <Formula format=\"inline\"><TexMath><?TeX $160.4\\%$?></TexMath><AltText>Math 2</AltText><File name=\"chi24-364-inline2\" type=\"svg\"/></Formula> larger than the perceived size of the device. Without the accompanying visuals, a constant size (<Formula format=\"inline\"><TexMath><?TeX $141.4\\%$?></TexMath><AltText>Math 3</AltText><File name=\"chi24-364-inline3\" type=\"svg\"/></Formula> of device size) was perceived.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {751},\nnumpages = {15},\nkeywords = {cross-modal integration, perceptual illusion, pseudo-haptics, visuo-haptic perception},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642085,\nauthor = {Sutton, Jonathan and Langlotz, Tobias and Plopski, Alexander and Hornb\\ae{}k, Kasper},\ntitle = {Flicker Augmentations: Rapid Brightness Modulation for Real-World Visual Guidance using Augmented Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642085},\ndoi = {10.1145/3613904.3642085},\nabstract = {Providing attention guidance, such as assisting in search tasks, is a prominent use for Augmented Reality. Typically, this is achieved by graphically overlaying geometrical shapes such as arrows. However, providing visual guidance can cause side effects such as attention tunnelling or scene occlusions, and introduce additional visual clutter. Alternatively, visual guidance can adjust saliency but this comes with different challenges such as hardware requirements and environment dependent parameters. In this work we advocate for using flicker as an alternative for real-world guidance using Augmented Reality. We provide evidence for the effectiveness of flicker from two user studies. The first compared flicker against alternative approaches in a highly controlled setting, demonstrating efficacy (N = 28). The second investigated flicker in a practical task, demonstrating feasibility with higher ecological validity (N = 20). Finally, our discussion highlights the opportunities and challenges when using flicker to provide real-world visual guidance using Augmented Reality.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {752},\nnumpages = {19},\nkeywords = {augmented reality, eye tracking, flicker, gaze, visual guidance},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642702,\nauthor = {Kin, Kenrick and Wan, Chengde and Koh, Ken and Marin, Andrei and Camg\\\"{o}z, Necati Cihan and Zhang, Yubo and Cai, Yujun and Kovalev, Fedor and Ben-Zacharia, Moshe and Hoople, Shannon and Nunes-Ueno, Marcos and Sanchez-Rodriguez, Mariel and Bhargava, Ayush and Wang, Robert and Sauser, Eric and Ma, Shugao},\ntitle = {STMG: A Machine Learning Microgesture Recognition System for Supporting Thumb-Based VR/AR Input},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642702},\ndoi = {10.1145/3613904.3642702},\nabstract = {AR/VR devices have started to adopt hand tracking, in lieu of controllers, to support user interaction. However, today’s hand input rely primarily on one gesture: pinch. Moreover, current mappings of hand motion to use cases like VR locomotion and content scrolling involve more complex and larger arm motions than joystick or trackpad usage. STMG increases the gesture space by recognizing additional small thumb-based microgestures from skeletal tracking running on a headset. We take a machine learning approach and achieve a 95.1\\% recognition accuracy across seven thumb gestures performed on the index finger surface: four directional thumb swipes (left, right, forward, backward), thumb tap, and fingertip pinch start and pinch end. We detail the components to our machine learning pipeline and highlight our design decisions and lessons learned in producing a well generalized model. We then demonstrate how these microgestures simplify and reduce arm motions for hand-based locomotion and scrolling interactions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {753},\nnumpages = {15},\nkeywords = {augmented reality, machine learning, microgestures, neural networks, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642323,\nauthor = {Dupr\\'{e}, Camille and Appert, Caroline and Rey, St\\'{e}phanie and Saidi, Houssem and Pietriga, Emmanuel},\ntitle = {TriPad: Touch Input in AR on Ordinary Surfaces with Hand Tracking Only},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642323},\ndoi = {10.1145/3613904.3642323},\nabstract = {TriPad enables opportunistic touch interaction in Augmented Reality using hand tracking only. Users declare the surface they want to appropriate with a simple hand tap gesture. They can then use this surface at will for direct and indirect touch input. TriPad only involves analyzing hand movements and postures, without the need for additional instrumentation, scene understanding or machine learning. TriPad thus works on a variety of flat surfaces, including glass. It also ensures low computational overhead on devices that typically have a limited power budget. We describe the approach, and report on two user studies. The first study demonstrates the robustness of TriPad’s hand movement interpreter on different surface materials. The second study compares TriPad against direct mid-air AR input techniques on both discrete and continuous tasks and with different surface orientations. TriPad achieves a better speed-accuracy trade-off overall, improves comfort and minimizes fatigue.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {754},\nnumpages = {18},\nkeywords = {augmented reality, passive surfaces, touch input},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642786,\nauthor = {Kordyaka, Bastian and Laato, Samuli and Weber, Sebastian and Hamari, Juho and Niehaves, Bjoern},\ntitle = {Exploring the association between engagement with location-based game features and getting inspired about environmental issues and nature},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642786},\ndoi = {10.1145/3613904.3642786},\nabstract = {Today, millions worldwide play popular location-based games (LBGs) such as Pok\\'{e}mon GO. LBGs are designed to be played outdoors, and past research has shown that they can incentivize players to travel to nature. To further explore this nature-connection, we investigated via a mixed-methods approach the connections between engagement with LBGs, inspiration and environmental awareness as follows. First, we identified relevant gamification features in Study 1. Based on the insights, we built a survey that we sent to Pok\\'{e}mon GO players (N=311) in Study 2. The results showed that (a) social networking features, reminders, and virtual objects were the most relevant gamification features to explain inspired by playing Pok\\'{e}mon GO and that (b) inspired to outdoor engagement partially mediated the relationship between inspired by playing Pok\\'{e}mon GO and environmental awareness. These results warrant further investigations into whether LBGs could motivate pro-environment attitudes and inspire people to care for nature.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {755},\nnumpages = {15},\nkeywords = {Location based games, environmental awareness, gamification, inspiration, mixed-methods},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642797,\nauthor = {Kang, Donghyeon and Kim, Namsub and Kang, Daekaun and Yoon, June-Seop and Kim, Sunjun and Lee, Byungjoo},\ntitle = {Quantifying Wrist-Aiming Habits with A Dual-Sensor Mouse: Implications for Player Performance and Workload},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642797},\ndoi = {10.1145/3613904.3642797},\nabstract = {Computer mice are widely used today as the primary input device in competitive video games. If a player exhibits more wrist rotation than other players when moving the mouse laterally, the player is said to have stronger wrist-aiming habits. Despite strong public interest, there has been no affordable technique to quantify the extent of a player’s wrist-aiming habits and no scientific investigation into how the habits affect player performance and workload. We present a reliable and affordable technique to quantify the extent of a player’s wrist-aiming habits using a mouse equipped with two optical sensors (i.e., a dual-sensor mouse). In two user studies, we demonstrate the reliability of the technique and examine the relationship between wrist-aiming habits and player performance or workload. In summary, player expertise and mouse sensitivity significantly impacted wrist-aiming habits; the extent of wrist-aiming showed a positive correlation with upper limb workload.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {756},\nnumpages = {18},\nkeywords = {Computer Mouse, Esports, Fitts’ law, Pointing, Tunnel Syndrome},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642611,\nauthor = {Potts, Dominic and Broad, Zoe and Sehgal, Tarini and Hartley, Joseph and O'Neill, Eamonn and Jicol, Crescent and Clarke, Christopher and Lutteroth, Christof},\ntitle = {Sweating the Details: Emotion Recognition and the Influence of Physical Exertion in Virtual Reality Exergaming},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642611},\ndoi = {10.1145/3613904.3642611},\nabstract = {There is great potential for adapting Virtual Reality (VR) exergames based on a user’s affective state. However, physical activity and VR interfere with physiological sensors, making affect recognition challenging. We conducted a study (n=72) in which users experienced four emotion inducing VR exergaming environments (happiness, sadness, stress and calmness) at three different levels of exertion (low, medium, high). We collected physiological measures through pupillometry, electrodermal activity, heart rate, and facial tracking, as well as subjective affect ratings. Our validated virtual environments, data, and analyses are openly available. We found that the level of exertion influences the way affect can be recognised, as well as affect itself. Furthermore, our results highlight the importance of data cleaning to account for environmental and interpersonal factors interfering with physiological measures. The results shed light on the relationships between physiological measures and affective states and inform design choices about sensors and data cleaning approaches for affective VR.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {757},\nnumpages = {21},\nkeywords = {affect recognition, emotion recognition, exergaming, high-intensity exercise, physiological sensing, psychophysiological correlates, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642137,\nauthor = {Laato, Samuli and Kordyaka, Bastian and Hamari, Juho},\ntitle = {Traumatizing or Just Annoying? Unveiling the Spectrum of Gamer Toxicity in the StarCraft II Community},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642137},\ndoi = {10.1145/3613904.3642137},\nabstract = {The aim of this work is to explore the forms of toxic behaviour that players encounter in competitive multiplayer real-time strategy (RTS) games. To this end, we carried out ethnographic observations and player interviews within the popular RTS game StarCraft II, and approached the data inductively, leading us to discover ten categories of toxic behaviour. While the harmfulness of toxic actions can be obtained as a product of severity and frequency, players’ assessment of the severity of toxic behaviors was contextualized by, (1) directly observed; (2) background; and (3) extraneous factors. Following our empirical findings, we derive a conceptual model for differentiating toxicity from mildly annoying and more severe behaviors. The discovered view of toxicity challenges the prevailing paradigm of treating players’ toxic behavior as a monolithic construct with a linear intensity spectrum. Instead, we advocate for a more granular approach, encouraging an understanding of the underlying dynamics behind negative online behaviors.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {758},\nnumpages = {18},\nkeywords = {Gamer communities, Gamification, Online games, Player behavior, Toxicity},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642249,\nauthor = {Kleinman, Erica and Habibi, Reza and Powell, Garrett B and Reeves, Brent and Prather, James and Seif El-Nasr, Magy},\ntitle = {``Backseat Gaming\" A Study of Co-Regulated Learning within a Collegiate Male Esports Community},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642249},\ndoi = {10.1145/3613904.3642249},\nabstract = {Previous work demonstrated that esports players often leverage insights from other players and communities to learn and improve. However, little research examined social learning in esports, over time, in granular detail. Understanding the role of others in the esports learning process has implications for the design of computational support systems that can help esports players learn and make the games more accessible. Therefore, we perform an exploration of this topic using Co-Regulated Learning as a theoretical lens. In doing so, we hope to enrich existing knowledge on social learning in esports, provide insights for the future development of computational support, and a road-map for future work. Through an interview study of an esports community consisting of 14, college-aged, male players, we uncovered 10 themes regarding how Co-Regulated learning occurs within their teams. Based on these, we discuss three main takeaways and their implications for future research and development.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {759},\nnumpages = {14},\nkeywords = {Co-Regulated Learning, Esports, Esports Teams, Gaming, Learning, Social Gaming, Social Learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642874,\nauthor = {Kotut, Lindah and Alikhan, Hummd},\ntitle = {\"Things on the Ground are Different\": Utility, Survival and Ethics in Multi-Device Ownership and Smartphone Sharing Contexts},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642874},\ndoi = {10.1145/3613904.3642874},\nabstract = {As phones have become cheaper, there are still instances where people share them. Researchers have explored the sharing in the context of developing economies and brought to light the barriers to ownership and highlight the resulting power differentials. In this work, we explore the dynamics of single and multi-device ownership and sharing in Kenya. Through interviews with 34 participants, we seek to understand what these ownership patterns inform us about affordances and unstated needs–adding to our knowledge of device usage. We find that these dimensions of ownership raise new questions about ethics and survival, and we describe how they also serve as bellwethers to designing for a developing economy–especially in the context of access to money and other financial infrastructures. We discuss the impact and harms of unregulated policies and the influence of survival on peoples’ choices, the implications on ethics, and further explore strategies for identifying, auditing, and mitigating these risks.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {760},\nnumpages = {14},\nkeywords = {Device Sharing, HCI4D, Mobile-Based Transactions, Multiple-Device Ownership, Privacy Boundaries, SIM Ownership},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642749,\nauthor = {Precel, Heila and McDonald, Allison and Hecht, Brent and Vincent, Nicholas},\ntitle = {A Canary in the AI Coal Mine: American Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642749},\ndoi = {10.1145/3613904.3642749},\nabstract = {Systemic property dispossession from minority groups has often been carried out in the name of technological progress. In this paper, we identify evidence that the current paradigm of large language models (LLMs) likely continues this long history. Examining common LLM training datasets, we find that a disproportionate amount of content authored by Jewish Americans is used for training without their consent. The degree of over-representation ranges from around 2x to around 6.5x. Given that LLMs may substitute for the paid labor of those who produced their training data, they have the potential to cause even more substantial and disproportionate economic harm to Jewish Americans in the coming years. This paper focuses on Jewish Americans as a case study, but it is probable that other minority communities (e.g., Asian Americans, Hindu Americans) may be similarly affected and, most importantly, the results should likely be interpreted as a “canary in the coal mine” that highlights deep structural concerns about the current LLM paradigm whose harms could soon affect nearly everyone. We discuss the implications of these results for the policymakers thinking about how to regulate LLMs as well as for those in the AI field who are working to advance LLMs. Our findings stress the importance of working together towards alternative LLM paradigms that avoid both disparate impacts and widespread societal harms.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {761},\nnumpages = {17},\nkeywords = {dataset documentation, economic impacts, large language models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641890,\nauthor = {Scheuerman, Morgan Klaus and Brubaker, Jed R.},\ntitle = {Products of Positionality: How Tech Workers Shape Identity Concepts in Computer Vision},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641890},\ndoi = {10.1145/3613904.3641890},\nabstract = {There has been a great deal of scholarly attention on issues of identity-related bias in machine learning. Much of this attention has focused on data and data workers, workers who do annotation tasks. Yet tech workers—like engineers, data scientists, and researchers—introduce their own “biases” when defining “identity” concepts. More specifically, they instill their own positionalities, the way they understand and are shaped by the world around them. Through interviews with industry tech workers who focus on computer vision, we show how workers embed their own positional perspectives into products and how positional gaps can lead to unforeseen and undesirable outcomes. We discuss how worker positionality is mutually shaped by the contexts in which they are embedded. We provide implications for researchers and practitioners to engage with the positionalities of tech workers, as well as those in contexts outside of development that influence tech workers.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {762},\nnumpages = {18},\nkeywords = {Tech work, computer vision, identity, machine learning, positionality, work studies},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642942,\nauthor = {Wang, Yao and Wang, Weitian and Abdelhafez, Abdullah and Elfares, Mayar and Hu, Zhiming and B\\^{a}ce, Mihai and Bulling, Andreas},\ntitle = {SalChartQA: Question-driven Saliency on Information Visualisations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642942},\ndoi = {10.1145/3613904.3642942},\nabstract = {Understanding the link between visual attention and users’ information needs when visually exploring information visualisations is under-explored due to a lack of large and diverse datasets to facilitate these analyses. To fill this gap we introduce SalChartQA – a novel crowd-sourced dataset that uses the BubbleView interface to track user attention and a question-answering (QA) paradigm to induce different information needs in users. SalChartQA contains 74,340 answers to 6,000 questions on 3,000 visualisations. Informed by our analyses demonstrating the close correlation between information needs and visual saliency, we propose the first computational method to predict question-driven saliency on visualisations. Our method outperforms state-of-the-art saliency models for several metrics, such as the correlation coefficient and the Kullback-Leibler divergence. These results show the importance of information needs for shaping attentive behaviour and pave the way for new applications, such as task-driven optimisation of visualisations or explainable AI in chart question-answering.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {763},\nnumpages = {14},\nkeywords = {Information visualisation, deep learning, eye-tracking study, gaze bahaviour, visual saliency},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642056,\nauthor = {Correll, Michael and Garrison, Laura},\ntitle = {When the Body Became Data: Historical Data Cultures and Anatomical Illustration},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642056},\ndoi = {10.1145/3613904.3642056},\nabstract = {With changing attitudes around knowledge, medicine, art, and technology, the human body has become a source of information and, ultimately, shareable and analyzable data. Centuries of illustrations and visualizations of the body occur within particular historical, social, and political contexts. These contexts are enmeshed in different so-called data cultures: ways that data, knowledge, and information are conceptualized and collected, structured and shared. In this work, we explore how information about the body was collected as well as the circulation, impact, and persuasive force of the resulting images. We show how mindfulness of data cultural influences remain crucial for today’s designers, researchers, and consumers of visualizations. We conclude with a call for the field to reflect on how visualizations are not timeless and contextless mirrors on objective data, but as much a product of our time and place as the visualizations of the past.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {764},\nnumpages = {18},\nkeywords = {material culture, medical history, medical illustration, medical visualization, visualization rhetoric},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642014,\nauthor = {Gondimalla, Apoorva and Sreekanth, Varshinee and Joshi, Govind and Nelson, Whitney and Choi, Eunsol and Slota, Stephen C. and Greenberg, Sherri R. and Fleischmann, Kenneth R. and Lee, Min Kyung},\ntitle = {Aligning Data with the Goals of an Organization and Its Workers: Designing Data Labeling for Social Service Case Notes},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642014},\ndoi = {10.1145/3613904.3642014},\nabstract = {The challenges of data collection in nonprofits for performance and funding reports are well-established in HCI research. Few studies, however, delve into improving the data collection process. Our study proposes ideas to improve data collection by exploring challenges that social workers experience when labeling their case notes. Through collaboration with an organization that provides intensive case management to those experiencing homelessness in the U.S., we conducted interviews with caseworkers and held design sessions where caseworkers, managers, and program analysts examined storyboarded ideas to improve data labeling. Our findings suggest several design ideas on how data labeling practices can be improved: Aligning labeling with caseworker goals, enabling shared control on data label design for a comprehensive portrayal of caseworker contributions, improving the synthesis of qualitative and quantitative data, and making labeling user-friendly. We contribute design implications for data labeling to better support multiple stakeholder goals in social service contexts.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {765},\nnumpages = {21},\nkeywords = {Case Notes, Data Collection Practices, Data Labeling, Design Ideas, Nonprofits, Social Work},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642830,\nauthor = {Lam, Michelle S. and Teoh, Janice and Landay, James A. and Heer, Jeffrey and Bernstein, Michael S.},\ntitle = {Concept Induction: Analyzing Unstructured Text with High-Level Concepts Using LLooM},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642830},\ndoi = {10.1145/3613904.3642830},\nabstract = {Data analysts have long sought to turn unstructured text data into meaningful concepts. Though common, topic modeling and clustering focus on lower-level keywords and require significant interpretative work. We introduce concept induction, a computational process that instead produces high-level concepts, defined by explicit inclusion criteria, from unstructured text. For a dataset of toxic online comments, where a state-of-the-art BERTopic model outputs “women, power, female,” concept induction produces high-level concepts such as “Criticism of traditional gender roles” and “Dismissal of women’s concerns.” We present LLooM, a concept induction algorithm that leverages large language models to iteratively synthesize sampled text and propose human-interpretable concepts of increasing generality. We then instantiate LLooM in a mixed-initiative text analysis tool, enabling analysts to shift their attention from interpreting topics to engaging in theory-driven analysis. Through technical evaluations and four analysis scenarios ranging from literature review to content moderation, we find that LLooM’s concepts improve upon the prior art of topic models in terms of quality and data coverage. In expert case studies, LLooM helped researchers to uncover new insights even from familiar datasets, for example by suggesting a previously unnoticed concept of attacks on out-party stances in a political social media dataset.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {766},\nnumpages = {28},\nkeywords = {data visualization, human-AI interaction, large language models, topic modeling, unstructured text analysis},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642452,\nauthor = {Tran, Anh-Ton and Guo, Grace and Taylor, Jordan and Chan, Katsuki Andrew and Raymond, Elora Lee and Disalvo, Carl},\ntitle = {Situating Datasets: Making Public Eviction Data Actionable for Housing Justice},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642452},\ndoi = {10.1145/3613904.3642452},\nabstract = {Activists, governments, and academics regularly advocate for more open data. But how is data made open, and for whom is it made useful and usable? In this paper, we investigate and describe the work of making eviction data open to tenant organizers. We do this through an ethnographic description of ongoing work with a local housing activist organization. This work combines observation, direct participation in data work, and creating media artifacts, specifically digital maps. Our interpretation is grounded in D’Ignazio and Klein’s Data Feminism, emphasizing standpoint theory. Through our analysis and discussion, we highlight how shifting positionalities from data intermediaries to data accomplices affects the design of data sets and maps. We provide HCI scholars with three design implications when situating data for grassroots organizers: becoming a domain beginner, striving for data actionability, and evaluating our design artifacts by the social relations they sustain rather than just their technical efficacy.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {767},\nnumpages = {16},\nkeywords = {Counter-Mapping, Critical Visualization, Data Feminism, Eviction, Housing Justice, Open Data},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642004,\nauthor = {Harvey, Emma and Sandhaus, Hauke and Jacobs, Abigail Z. and Moss, Emanuel and Sloane, Mona},\ntitle = {The Cadaver in the Machine: The Social Practices of Measurement and Validation in Motion Capture Technology},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642004},\ndoi = {10.1145/3613904.3642004},\nabstract = {Motion capture systems, used across various domains, make body representations concrete through technical processes. We argue that the measurement of bodies and the validation of measurements for motion capture systems can be understood as social practices. By analyzing the findings of a systematic literature review (N=278) through the lens of social practice theory, we show how these practices, and their varying attention to errors, become ingrained in motion capture design and innovation over time. Moreover, we show how contemporary motion capture systems perpetuate assumptions about human bodies and their movements. We suggest that social practices of measurement and validation are ubiquitous in the development of data- and sensor-driven systems more broadly, and provide this work as a basis for investigating hidden design assumptions and their potential negative consequences in human-computer interaction.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {768},\nnumpages = {23},\nkeywords = {anthropometry, measurement, motion capture, social practices, validation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642669,\nauthor = {Das, Dipto and Guha, Shion and Brubaker, Jed R. and Semaan, Bryan},\ntitle = {The ``Colonial Impulse\" of Natural Language Processing: An Audit of Bengali Sentiment Analysis Tools and Their Identity-based Biases},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642669},\ndoi = {10.1145/3613904.3642669},\nabstract = {While colonization has sociohistorically impacted people’s identities across various dimensions, those colonial values and biases continue to be perpetuated by sociotechnical systems. One category of sociotechnical systems–sentiment analysis tools–can also perpetuate colonial values and bias, yet less attention has been paid to how such tools may be complicit in perpetuating coloniality, although they are often used to guide various practices (e.g., content moderation). In this paper, we explore potential bias in sentiment analysis tools in the context of Bengali communities who have experienced and continue to experience the impacts of colonialism. Drawing on identity categories most impacted by colonialism amongst local Bengali communities, we focused our analytic attention on gender, religion, and nationality. We conducted an algorithmic audit of all sentiment analysis tools for Bengali, available on the Python package index (PyPI) and GitHub. Despite similar semantic content and structure, our analyses showed that in addition to inconsistencies in output from different tools, Bengali sentiment analysis tools exhibit bias between different identity categories and respond differently to different ways of identity expression. Connecting our findings with colonially shaped sociocultural structures of Bengali communities, we discuss the implications of downstream bias of sentiment analysis tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {769},\nnumpages = {18},\nkeywords = {Algorithmic audit, Bias, Colonial, Identity, Sentiment analysis tools},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642897,\nauthor = {Zhou, Haozhe and Goel, Mayank and Agarwal, Yuvraj},\ntitle = {Bring Privacy To The Table: Interactive Negotiation for Privacy Settings of Shared Sensing Devices},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642897},\ndoi = {10.1145/3613904.3642897},\nabstract = {To address privacy concerns with the Internet of Things (IoT) devices, researchers have proposed enhancements in data collection transparency and user control. However, managing privacy preferences for shared devices with multiple stakeholders remains challenging. We introduced ThingPoll, a system that helps users negotiate privacy configurations for IoT devices in shared settings. We designed ThingPoll by observing twelve participants verbally negotiating privacy preferences, from which we identified potentially successful and inefficient negotiation patterns. ThingPoll bootstraps a preference model from a custom crowdsourced privacy preferences dataset. During negotiations, ThingPoll strategically scaffolds the process by eliciting users’ privacy preferences, providing helpful contexts, and suggesting feasible configuration options. We evaluated ThingPoll with 30 participants negotiating the privacy settings of 4 devices. Using ThingPoll, participants reached an agreement in 97.5\\% of scenarios within an average of 3.27 minutes. Participants reported high overall satisfaction of 83.3\\% with ThingPoll as compared to baseline approaches.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {770},\nnumpages = {22},\nkeywords = {Internet of Things, Negotiation Agent, Preference Elicitation, Privacy Enhancing Technology, Privacy Profiles, Usable Privacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641967,\nauthor = {Choksi, Madiha Zahrah and Aubin Le Qu\\'{e}r\\'{e}, Marianne and Lloyd, Travis and Tao, Ruojia and Grimmelmann, James and Naaman, Mor},\ntitle = {Under the (neighbor)hood: Hyperlocal Surveillance on Nextdoor},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641967},\ndoi = {10.1145/3613904.3641967},\nabstract = {This paper examines the tensions between neighborhood gentrification and community surveillance posts on Nextdoor, a hyperlocal social media platform for neighborhoods. We created a privacy-preserving pipeline to gather research data from public Nextdoor posts in Atlanta, Georgia and filtered these to a dataset of 1,537 community surveillance posts. We developed a qualitative codebook to label observed patterns of community surveillance, and deploy a large language model to tag these posts at scale. Ultimately, we present an extensible and empirically-tested typology of the modes of community surveillance that occur on hyperlocal platforms. We find a complex relationship between community surveillance posts and neighborhood gentrification, which indicates that publicly disclosing information about perceived outsiders, especially for petty crimes, is most prevalent in gentrifying neighborhoods. Our empirical evidence inform critical perspectives which posit that community surveillance on platforms like Nextdoor can exclude and marginalize minoritized populations, particularly in gentrifying neighborhoods. Our findings carry broader implications for hyperlocal social platforms and their potential to amplify and exacerbate social tensions and exclusion.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {771},\nnumpages = {22},\nkeywords = {City, Method, Online Communities, Privacy, Qualitative Methods, Quantitative Methods, Social Media, Theory},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642668,\nauthor = {Abraham, Melvin and Mcgill, Mark and Khamis, Mohamed},\ntitle = {What You Experience is What We Collect: User Experience Based Fine-Grained Permissions for Everyday Augmented Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642668},\ndoi = {10.1145/3613904.3642668},\nabstract = {Everyday Augmented Reality (AR) headsets pose significant privacy risks, potentially allowing prolonged sensitive data collection of both users and bystanders (e.g. members of the public). While users control data access through permissions, current AR systems inherit smartphone permission prompts, which may be less appropriate for all-day AR. This constrains informed choices and risks over-privileged access to sensors. We propose (N=20) a novel AR permission control system that allows better-informed privacy decisions and evaluate it using five mock application contexts. Our system’s novelty lies in enabling users to experience the varying impacts of permission levels on not only a) privacy, but also b) application functionality. This empowers users to better understand what data an application depends on and how its functionalities are impacted by limiting said data. Participants found that our method allows for making better informed privacy decisions, and deemed it more transparent and trustworthy than state-of-the-art AR and smartphone permission systems taken from Android and iOS. Our results offer insights into new and necessary AR permission systems, improving user understanding and control over data access.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {772},\nnumpages = {24},\nkeywords = {AR sensing, Augmented Reality, Privacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642770,\nauthor = {Razaq, Lubna and Ghoshal, Sucheta},\ntitle = {What to the Muslim is Internet search: Digital Borders as Barriers to Information},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642770},\ndoi = {10.1145/3613904.3642770},\nabstract = {In today’s digital age, searching for information online is considered a ubiquitous task that can be accomplished in just a few moments using various web-based technologies. Yet, information seeking has geopolitical burdens for users who are racialized and marginalized by the nation-state and other structures of power. In our paper, we conducted a qualitative interview study with 15 Muslim participants, mostly of South Asian origin, living in the US with varying citizenship or (non)immigration status about their information needs and concerns around privacy as a Muslim, and the resulting restrictive patterns of information seeking on various Internet platforms. We argue that our findings on the barriers faced and strategies employed by Muslim residents toward information access suggest a broader pattern of digital manifestations of border imperialism. We posit that HCI researchers should pay attention to how “digital borders\" have epistemic implications for people marginalized by geopolitical boundaries.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {773},\nnumpages = {17},\nkeywords = {border imperialism, immigration, information seeking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642164,\nauthor = {Mujirishvili, Tamara and Fedosov, Anton and Hashemifard, Kooshan and Climent-P\\'{e}rez, Pau and Florez-Revuelta, Francisco},\ntitle = {“I Don’t Want to Become a Number’’: Examining Different Stakeholder Perspectives on a Video-Based Monitoring System for Senior Care with Inherent Privacy Protection (by Design).},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642164},\ndoi = {10.1145/3613904.3642164},\nabstract = {Active and Assisted Living (AAL) technologies aim to enhance the quality of life of older adults and promote successful aging. While video-based AAL solutions offer rich capabilities for better healthcare management in older age, they pose significant privacy risks. To mitigate the risks, we developed a video-based monitoring system that incorporates different privacy-preserving filters. We deployed the system in one assistive technology center and conducted a qualitative study with older adults and other stakeholders involved in care provision. Our study demonstrates diverse users’ perceptions and experiences with video-monitoring technology and offers valuable insights for the system’s further development. The findings unpack the privacy-versus-safety trade-off inherent in video-based technologies and discuss how the privacy-preserving mechanisms within the system mitigate privacy-related concerns. The study also identifies varying stakeholder perspectives towards the system in general and highlights potential avenues for developing video-based monitoring technologies in the AAL context.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {774},\nnumpages = {19},\nkeywords = {Active and Assisted Living, Ageing, Older Adults, Privacy Concerns, Qualitative Study, Video Monitoring System},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642116,\nauthor = {Lee, Hao-Ping (Hank) and Yang, Yu-Ju and Von Davier, Thomas Serban and Forlizzi, Jodi and Das, Sauvik},\ntitle = {Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI Privacy Risks},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642116},\ndoi = {10.1145/3613904.3642116},\nabstract = {Privacy is a key principle for developing ethical AI technologies, but how does including AI technologies in products and services change privacy risks? We constructed a taxonomy of AI privacy risks by analyzing 321 documented AI privacy incidents. We codified how the unique capabilities and requirements of AI technologies described in those incidents generated new privacy risks, exacerbated known ones, or otherwise did not meaningfully alter the risk. We present 12 high-level privacy risks that AI technologies either newly created (e.g., exposure risks from deepfake pornography) or exacerbated (e.g., surveillance risks from collecting training data). One upshot of our work is that incorporating AI technologies into a product can alter the privacy risks it entails. Yet, current approaches to privacy-preserving AI/ML (e.g., federated learning, differential privacy, checklists) only address a subset of the privacy risks arising from the capabilities and data requirements of AI.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {775},\nnumpages = {19},\nkeywords = {AI incidents, Human-centered AI, Privacy, Privacy risks, Privacy taxonomy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641973,\nauthor = {Sohrawardi, Saniat Javid and Wu, Y. Kelly and Hickerson, Andrea and Wright, Matthew},\ntitle = {Dungeons \\& Deepfakes: Using scenario-based role-play to study journalists' behavior towards using AI-based verification tools for video content},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641973},\ndoi = {10.1145/3613904.3641973},\nabstract = {The evolving landscape of manipulated media, including the threat of deepfakes, has made information verification a daunting challenge for journalists. Technologists have developed tools to detect deepfakes, but these tools can sometimes yield inaccurate results, raising concerns about inadvertently disseminating manipulated content as authentic news. This study examines the impact of unreliable deepfake detection tools on information verification. We conducted role-playing exercises with 24 US journalists, immersing them in complex breaking-news scenarios where determining authenticity was challenging. Through these exercises, we explored questions regarding journalists’ investigative processes, use of a deepfake detection tool, and decisions on when and what to publish. Our findings reveal that journalists are diligent in verifying information, but sometimes rely too heavily on results from deepfake detection tools. We argue for more cautious release of such tools, accompanied by proper training for users to mitigate the risk of unintentionally propagating manipulated content as real news.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {776},\nnumpages = {17},\nkeywords = {deepfake, journalism, role-play, verification},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642103,\nauthor = {Xu, Anran and Fang, Shitao and Yang, Huan and Hosio, Simo and Yatani, Koji},\ntitle = {Examining Human Perception of Generative Content Replacement in Image Privacy Protection},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642103},\ndoi = {10.1145/3613904.3642103},\nabstract = {The richness of the information in photos can often threaten privacy, thus image editing methods are often employed for privacy protection. Existing image privacy protection techniques, like blurring, often struggle to maintain the balance between robust privacy protection and preserving image usability. To address this, we introduce a generative content replacement (GCR) method in image privacy protection, which seamlessly substitutes privacy-threatening contents with similar and realistic substitutes, using state-of-the-art generative techniques. Compared with four prevalent image protection methods, GCR consistently exhibited low detectability, making the detection of edits remarkably challenging. GCR also performed reasonably well in hindering the identification of specific content and managed to sustain the image’s narrative and visual harmony. This research serves as a pilot study and encourages further innovation on GCR and the development of tools that enable human-in-the-loop image privacy protection using approaches similar to GCR.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {777},\nnumpages = {16},\nkeywords = {diffusion models, generative artificial intelligence, image privacy, usable security},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641999,\nauthor = {Mink, Jaron and Wei, Miranda and Munyendo, Collins W. and Hugenberg, Kurt and Kohno, Tadayoshi and Redmiles, Elissa M. and Wang, Gang},\ntitle = {It's Trying Too Hard To Look Real: Deepfake Moderation Mistakes and Identity-Based Bias},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641999},\ndoi = {10.1145/3613904.3641999},\nabstract = {Online platforms employ manual human moderation to distinguish human-created social media profiles from deepfake-generated ones. Biased misclassification of real profiles as artificial can harm general users as well as specific identity groups; however, no work has yet systematically investigated such mistakes and biases. We conducted a user study (n=695) that investigates how 1) the identity of the profile, 2) whether the moderator shares that identity, and 3) components of a profile shown affect the perceived artificiality of the profile. We find statistically significant biases in people’s moderation of LinkedIn profiles based on all three factors. Further, upon examining how moderators make decisions, we find they rely on mental models of AI and attackers, as well as typicality expectations (how they think the world works). The latter includes reliance on race/gender stereotypes. Based on our findings, we synthesize recommendations for the design of moderation interfaces, moderation teams, and security training.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {778},\nnumpages = {20},\nkeywords = {Bias, Content Moderation, Deepfakes, Mental Models of AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642382,\nauthor = {Umbach, Rebecca and Henry, Nicola and Beard, Gemma Faye and Berryessa, Colleen M.},\ntitle = {Non-Consensual Synthetic Intimate Imagery: Prevalence, Attitudes, and Knowledge in 10 Countries},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642382},\ndoi = {10.1145/3613904.3642382},\nabstract = {Deepfake technologies have become ubiquitous, “democratizing” the ability to manipulate photos and videos. One popular use of deepfake technology is the creation of sexually explicit content, which can then be posted and shared widely on the internet. Drawing on a survey of over 16,000 respondents in 10 different countries, this article examines attitudes and behaviors related to “deepfake pornography” as a specific form of non-consensual synthetic intimate imagery (NSII). Our study found that deepfake pornography behaviors were considered harmful by respondents, despite nascent societal awareness. Regarding the prevalence of deepfake pornography victimization and perpetration, 2.2\\% of all respondents indicated personal victimization, and 1.8\\% all of respondents indicated perpetration behaviors. Respondents from countries with specific legislation still reported perpetration and victimization experiences, suggesting NSII laws are inadequate to deter perpetration. Approaches to prevent and reduce harms may include digital literacy education, as well as enforced platform policies, practices, and tools which better detect, prevent, and respond to NSII content.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {779},\nnumpages = {20},\nkeywords = {AI-generated images, deepfake pornography, deepfakes, image-based sexual abuse, involuntary synthetic pornographic imagery, non-consensual explicit imagery},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642005,\nauthor = {Saha, Pratyasha and Nowsher, Nadira and Baidya, Ayien Utshob and Mim, Nusrat Jahan and Ahmed, Syed Ishtiaque and Haque, S M Taiabul},\ntitle = {Computing and the Stigmatized: Trust, Surveillance, and Spatial Politics with the Sex Workers in Bangladesh},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642005},\ndoi = {10.1145/3613904.3642005},\nabstract = {The sex workers in the Global South represent a significant portion of the world sex industry. However, when compared to the relevant HCI literature on sex work and computing, there exists a noticeable gap in comprehending the experiences and circumstances of the sex workers in this region. This study fills the void by presenting the findings of a three-month-long ethnography with 25 legal sex workers in Daulatdia brothel, Bangladesh, revealing their struggles with stigma, low-tech literacy, and the emerging threats of online security, along with their skills and creativity to bypass those. Drawing on the previous literature on South Asian feminism, postcolonial computing, and critical urban studies, we demonstrate how these findings are deeply rooted in the country’s history and culture and propelled by a modernist vision of “development” that marginalizes such communities. Our discussion advances HCI’s discourse on sexuality, privacy, equity, and generates implications for design and policy changes.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {780},\nnumpages = {22},\nkeywords = {Global South, HCI4D, ICTD, feminism, gender, marginalized, patriarchy, qualitative, sex work, stigma, surveillance, technology-facilitated abuse, women},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642253,\nauthor = {Yin, Michael and Wang, Emi and Ng, Chuoxi and Xiao, Robert},\ntitle = {Lies, Deceit, and Hallucinations: Player Perception and Expectations Regarding Trust and Deception in Games},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642253},\ndoi = {10.1145/3613904.3642253},\nabstract = {Lying and deception are important parts of social interaction; when applied to storytelling mediums such as video games, such elements can add complexity and intrigue. We developed a game, “AlphaBetaCity”, in which non-playable characters (NPCs) made various false statements, and used this game to investigate perceptions of deceptive behaviour. We used a mix of human-written dialogue incorporating deliberate falsehoods and LLM-written scripts with (human-approved) hallucinated responses. The degree of falsehoods varied between believable but untrue statements to outright fabrications. 29 participants played the game and were interviewed about their experiences. Participants discussed methods for developing trust and gauging NPC truthfulness. Whereas perceived intentional false statements were often attributed towards narrative and gameplay effects, seemingly unintentional false statements generally mismatched participants’ mental models and lacked inherent meaning. We discuss how the perception of intentionality, the audience demographic, and the desire for meaning are major considerations when designing video games with falsehoods.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {781},\nnumpages = {15},\nkeywords = {LLM hallucinations, large language models, lying, player experience, video games},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642180,\nauthor = {Asthana, Sumit and Im, Jane and Chen, Zhe and Banovic, Nikola},\ntitle = {\"I know even if you don't tell me\": Understanding Users' Privacy Preferences Regarding AI-based Inferences of Sensitive Information for Personalization},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642180},\ndoi = {10.1145/3613904.3642180},\nabstract = {Personalization improves user experience by tailoring interactions relevant to each user’s background and preferences. However, personalization requires information about users that platforms often collect without their awareness or their enthusiastic consent. Here, we study how the transparency of AI inferences on users’ personal data affects their privacy decisions and sentiments when sharing data for personalization. We conducted two experiments where participants (N=877) answered questions about themselves for personalized public arts recommendations. Participants indicated their consent to let the system use their inferred data and explicitly provided data after awareness of inferences. Our results show that participants chose restrictive consent decisions for sensitive and incorrect inferences about them and for their answers that led to such inferences. Our findings expand existing privacy discourse to inferences and inform future directions for shaping existing consent mechanisms in light of increasingly pervasive AI inferences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {782},\nnumpages = {21},\nkeywords = {Personalization, consent., inference, privacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642471,\nauthor = {Liebers, Jonathan and Laskowski, Patrick and Rademaker, Florian and Sabel, Leon and Hoppen, Jordan and Gruenefeld, Uwe and Schneegass, Stefan},\ntitle = {Kinetic Signatures: A Systematic Investigation of Movement-Based User Identification in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642471},\ndoi = {10.1145/3613904.3642471},\nabstract = {Behavioral Biometrics in Virtual Reality (VR) enable implicit user identification by leveraging the motion data of users’ heads and hands from their interactions in VR. This spatiotemporal data forms a Kinetic Signature, which is a user-dependent behavioral biometric trait. Although kinetic signatures have been widely used in recent research, the factors contributing to their degree of identifiability remain mostly unexplored. Drawing from existing literature, this work systematically examines the influence of static and dynamic components in human motion. We conducted a user study (N = 24) with two sessions to reidentify users across different VR sports and exercises after one week. We found that the identifiability of a kinetic signature depends on its inherent static and dynamic factors, with the best combination allowing for 90.91\\% identification accuracy after one week had passed. Therefore, this work lays a foundation for designing and refining movement-based identification protocols in immersive environments.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {783},\nnumpages = {19},\nkeywords = {identification, kinetic signatures, task-driven biometrics, usable security, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642104,\nauthor = {Hadan, Hilda and Wang, Derrick M. and Nacke, Lennart E. and Zhang-Kennedy, Leah},\ntitle = {Privacy in Immersive Extended Reality: Exploring User Perceptions, Concerns, and Coping Strategies},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642104},\ndoi = {10.1145/3613904.3642104},\nabstract = {Extended Reality (XR) technology is changing online interactions, but its granular data collection sensors may be more invasive to user privacy than web, mobile, and the Internet of Things technologies. Despite an increased interest in studying developers’ concerns about XR device privacy, user perceptions have rarely been addressed. We surveyed 464 XR users to assess their awareness, concerns, and coping strategies around XR data in 18 scenarios. Our findings demonstrate that many factors, such as data types and sensitivity, affect users’ perceptions of privacy in XR. However, users’ limited awareness of XR sensors’ granular data collection capabilities, such as involuntary body signals of emotional responses, restricted the range of privacy-protective strategies they used. Our results highlight a need to enhance users’ awareness of data privacy threats in XR, design privacy-choice interfaces tailored to XR environments, and develop transparent XR data practices.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {784},\nnumpages = {24},\nkeywords = {Augmented Reality, Extended Reality, Mixed Reality, Privacy Perception, Privacy-Seeking Strategies, User privacy, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642597,\nauthor = {Tran, Van Hong and Mehrotra, Aarushi and Chetty, Marshini and Feamster, Nick and Frankenreiter, Jens and Strahilevitz, Lior},\ntitle = {Measuring Compliance with the California Consumer Privacy Act Over Space and Time},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642597},\ndoi = {10.1145/3613904.3642597},\nabstract = {The widespread sharing of consumers’ personal information with third parties raises significant privacy concerns. The California Consumer Privacy Act (CCPA) mandates that online businesses offer consumers the option to opt out of the sale and sharing of personal information. Our study automatically tracks the presence of the opt-out link longitudinally across multiple states after the California Privacy Rights Act (CPRA) went into effect. We categorize websites based on whether they are subject to CCPA and investigate cases of potential non-compliance. We find a number of websites that implement the opt-out link early and across all examined states but also find a significant number of CCPA-subject websites that fail to offer any opt-out methods even when CCPA is in effect. Our findings can shed light on how websites are reacting to the CCPA and identify potential gaps in compliance and opt-out method designs that hinder consumers from exercising CCPA opt-out rights.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {785},\nnumpages = {19},\nkeywords = {CCPA, Compliance, Opt-out, Privacy, Web Tracking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642831,\nauthor = {Bourdoucen, Amel and Lindqvist, Janne},\ntitle = {Privacy of Default Apps in Apple’s Mobile Ecosystem},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642831},\ndoi = {10.1145/3613904.3642831},\nabstract = {Users need to configure default apps when they first start using their devices. The privacy configurations of these apps do not always match what users think they have initially enabled. We first explored the privacy configurations of eight default apps Safari, Siri, Family Sharing, iMessage, FaceTime, Location Services, Find My, and Touch ID. We discovered serious issues with the documentation of these apps. Based on this, we studied users’ experiences with an interview study (N=15). We show that: the instructions for setting privacy configurations of default apps are vague and lack required steps; users were unable to disable default apps from accessing their personal information; users assumed they were being tracked by some default apps; default apps may cause tensions in family relationships because of information sharing. Our results illuminate on the privacy and security implications of configuring the privacy of default apps and how users understand the mobile ecosystem.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {786},\nnumpages = {32},\nkeywords = {Apps, Ecosystems., Mobile Devices, Privacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642815,\nauthor = {Lee, Hyunsoo and Jung, Yugyeong and Law, Hei Yiu and Bae, Seolyeong and Lee, Uichin},\ntitle = {PriviAware: Exploring Data Visualization and Dynamic Privacy Control Support for Data Collection in Mobile Sensing Research},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642815},\ndoi = {10.1145/3613904.3642815},\nabstract = {With increased interest in leveraging personal data collected from 24/7 mobile sensing for digital healthcare research, supporting user-friendly consent to data collection for user privacy has also become important. This work proposes PriviAware, a mobile app that promotes flexible user consent to data collection with data exploration and contextual filters that enable users to turn off data collection based on time and places that are considered privacy-sensitive. We conducted a user study (N = 58) to explore how users leverage data exploration and contextual filter functions to explore and manage their data and whether our system design helped users mitigate their privacy concerns. Our findings indicate that offering fine-grained control is a promising approach to raising users’ privacy awareness under the dynamic nature of the pervasive sensing context. We provide practical privacy-by-design guidelines for mobile sensing research.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {787},\nnumpages = {17},\nkeywords = {Mobile Sensing Research, Sensor Data Collection, Usable Privacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642252,\nauthor = {Harbach, Marian},\ntitle = {Websites Need Your Permission Too -- User Sentiment and Decision-Making on Web Permission Prompts in Desktop Chrome},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642252},\ndoi = {10.1145/3613904.3642252},\nabstract = {The web utilizes permission prompts to moderate access to certain capabilities. We present the first investigation of user behavior and sentiment of this security and privacy measure on the web, using 28 days of telemetry data from more than 100M Chrome installations on desktop platforms and experience sampling responses from 25,706 Chrome users. Based on this data, we find that ignoring and dismissing permission prompts are most common for geolocation and notifications. Permission prompts are perceived as more annoying and interrupting when they are not allowed, and most respondents cite a rational reason for the decision they took. Our data also supports that the perceived availability of contextual information from the requesting website is associated with allowing access to a requested capability. More usable permission controls could facilitate adoption of best practices that address several of the identified challenges; and ultimately could lead to better user experiences and a safer web.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {788},\nnumpages = {18},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642705,\nauthor = {Khoo, Yi Xuan and Kang, Rachael M. and Reynolds, Tera L. and Mentis, Helena M.},\ntitle = {“That’s Kind of Sus(picious)”: The Comprehensiveness of Mental Health Application Users’ Privacy and Security Concerns},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642705},\ndoi = {10.1145/3613904.3642705},\nabstract = {With the increasing usage of mental health applications (MHAs), there is growing concern regarding their data privacy practices. Analyzing 437 user reviews from 83 apps, we outline users’ predominant privacy and security concerns with currently available apps. We then compare those concerns to criteria from two prominent app evaluation websites – Privacy Not Included and One Mind PsyberGuide. Our findings show that MHA users have myriad data privacy and security concerns including a user’s control over their own data, but these concerns do not often overlap with those of experts from evaluation websites who focus more on issues such as required password strength. We highlight this disconnect and propose solutions in how the mental health care ecosystem can provide better guidance to MHA users and experts from the fields of privacy and security and mental health technology in choosing and evaluating, respectively, potentially useful mental health apps.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {789},\nnumpages = {16},\nkeywords = {apps, mental health, privacy, security},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642208,\nauthor = {Tazi, Faiza and Dykstra, Josiah and Rajivan, Prashanth and Das, Sanchari},\ntitle = {“We Have No Security Concerns”: Understanding the Privacy-Security Nexus in Telehealth for Audiologists and Speech-Language Pathologists},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642208},\ndoi = {10.1145/3613904.3642208},\nabstract = {The advent of telehealth revolutionizes healthcare by enabling remote consultations, yet poses complex security and privacy challenges. These are often acutely felt by lower-resourced, allied-healthcare practices. To address this, our study focuses on audiologists and speech-language pathologists (SLPs) in private practice settings, often characterized by limited information technology resources. Over the course of six months, we conducted semi-structured interviews with ten audiologists and ten SLPs to understand their telehealth experiences and concerns. Key findings reveal a diversity of opinions on technology trustworthiness, data security concerns, implemented security protocols, and patient behaviors. Given the nature of the medical practitioners’ primary work, participants expressed varied concerns about data breaches and platform vulnerabilities, yet trusted third-party services like Zoom due to inadequate expertise and time to evaluate security protocols. This work underscores the imperative of bridging the technology-healthcare gap to foster secure, patient/provider-centered telehealth as the prevailing practice. It also emphasizes the need to synergize security, privacy, and usability to securely deliver care through telehealth.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {790},\nnumpages = {20},\nkeywords = {Healthcare, Privacy, Security, Telehealth, User Study},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642066,\nauthor = {Radway, Sarah and Quintanilla, Katherine and Ludden, Cordelia and Votipka, Daniel},\ntitle = {An Investigation of US Universities' Implementation of FERPA Student Directory Policies and Student Privacy Preferences},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642066},\ndoi = {10.1145/3613904.3642066},\nabstract = {The Family Education Rights and Privacy Act (FERPA) is intended to protect student privacy, but has not adapted well to current technology. We consider a special class of student data: directory information. Unlike other FERPA-controlled data, directory information (e.g., student names, contact information, university affiliation) can be shared publicly online or by request without explicit permission. To understand this policy’s impact, we investigated 100 top-ranked US universities’ directory information sharing practices, finding they publish student contact information online, and provide PII offline by request to many parties, including data brokers. Universities provide limited opt out choices, and focus on negative effects when advising students about opting out. Lastly, we evaluate student preferences regarding the identified directory practices through a survey of 991 US university students. Based on these results, we provide recommendations to align directory practices with student privacy preferences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {791},\nnumpages = {35},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642500,\nauthor = {Yang, Yaqing and Li, Tony W and Jin, Haojian},\ntitle = {On the Feasibility of Predicting Users' Privacy Concerns using Contextual Labels and Personal Preferences},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642500},\ndoi = {10.1145/3613904.3642500},\nabstract = {Predicting users’ privacy concerns is challenging due to privacy’s subjective and complex nature. Previous research demonstrated that generic attitudes, such as those captured by Westin’s Privacy Segmentation Index, are inadequate predictors of context-specific attitudes. We introduce ContextLabel, a method enabling practitioners to capture users’ privacy profiles across domains and predict their privacy concerns towards unseen data practices. ContextLabel’s key innovations are (1) using non-mutually exclusive labels to capture more nuances of data practices, and (2) capturing users’ privacy profiles by asking them to express privacy concerns to a few data practices. To explore the feasibility of ContextLabel, we asked 38 participants to express their thoughts in free text towards 13 distinct data practices across five days. Our mixed-methods analysis shows that a preliminary version of ContextLabel can predict users’ privacy concerns towards unseen data practices with an accuracy (73\\%) surpassing Privacy Segmentation Index (56\\%) and methods using categorical factors (59\\%).},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {792},\nnumpages = {20},\nkeywords = {Empirical study that tells us about people, Privacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642623,\nauthor = {Farzand, Habiba and Marky, Karola and Khamis, Mohamed},\ntitle = {Out-of-Device Privacy Unveiled: Designing and Validating the Out-of-Device Privacy Scale (ODPS)},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642623},\ndoi = {10.1145/3613904.3642623},\nabstract = {This paper proposes an Out-of-Device Privacy Scale (ODPS) - a reliable, validated psychometric privacy scale that measures users’ importance of out-of-device privacy. In contrast to existing scales, ODPS is designed to capture the importance individuals attribute to protecting personal information from out-of-device threats in the physical world, which is essential when designing privacy protection mechanisms. We iteratively developed and refined ODPS in three high-level steps: item development, scale development, and scale validation, with a total of N=1378 participants. Our methodology included ensuring content validity by following various approaches to generate items. We collected insights from experts and target audiences to understand response variability. Next, we explored the underlying factor structure using multiple methods and performed dimensionality, reliability, and validity tests to finalise the scale. We discuss how ODPS can support future work predicting user behaviours and designing protection methods to mitigate privacy risks.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {793},\nnumpages = {15},\nkeywords = {out-of-device privacy, out-of-device threats, privacy, privacy in the physical world, scale development},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642244,\nauthor = {Wang, Portia and Miller, Mark R. and Queiroz, Anna C.M. and Bailenson, Jeremy N.},\ntitle = {Socially Late, Virtually Present: The Effects of Transforming Asynchronous Social Interactions in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642244},\ndoi = {10.1145/3613904.3642244},\nabstract = {Social Virtual Reality (VR) typically entails users interacting in real time. However, asynchronous Social VR presents the possibility of combining the convenience of asynchronous communication with the high presence of VR. Because the tools to easily record and replay VR social interactions are fairly new, scholars have not yet examined how users perceive asynchronous VR social interactions, and how nonverbal transformations of recorded interactions influence user behavior. In this work, we study nonverbal transformations of group interactions around proxemics and gaze and present results from an exploratory user study (N=128) investigating their effects. We found that the combination of spatial accommodation and added gaze increases social presence, perceived attention, and mutual gaze. Results also showed an inverse relationship between interpersonal distance and perceived levels of dominance and threat of the recorded group. Finally, we outline implications for educators and virtual meeting organizers to incorporate these transformations into real-world scenarios.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {794},\nnumpages = {19},\nkeywords = {Eye Gaze, Proxemics, Social Interaction, Transformed Social Interaction, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642072,\nauthor = {Joo, Dongyun and Kim, Hanseob and Kim, Gerard Jounghyun},\ntitle = {The Effects of False but Stable Heart Rate Feedback on Cybersickness and User Experience in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642072},\ndoi = {10.1145/3613904.3642072},\nabstract = {Virtual reality (VR) offers a compelling and immersive experience; however, cybersickness (or VR sickness) stands as a significant obstacle to its widespread adoption. When a user experiences cybersickness, one’s physical condition deteriorates with various symptoms, often accompanied by an increased and destabilized heart rate and even altered perception of one’s state. In this paper, we propose to provide “False but Stable Heart rate (FSH)” feedback through auditory and vibrotactile stimulation to reversely induce a stably perceived heart rate and, thereby, alleviate cybersickness while navigating a sickness-inducing VR content. The validation of the human experiment confirmed the intended effect in a statistically significant way. Furthermore, it was found that the lesser compatible FSH feedback had a more substantial sickness reduction effect but distracted the user with the reduced immersive experience. The compatible FSH feedback still showed moderate sickness reduction with the maintained sense of presence and immersion.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {795},\nnumpages = {15},\nkeywords = {Cognitive Distraction, Cybersickness, False Heart Rate},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641992,\nauthor = {Bonnail, Elise and Frommel, Julian and Lecolinet, Eric and Huron, Samuel and Gugenheimer, Jan},\ntitle = {Was it Real or Virtual? Confirming the Occurrence and Explaining Causes of Memory Source Confusion between Reality and Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641992},\ndoi = {10.1145/3613904.3641992},\nabstract = {Source confusion occurs when individuals attribute a memory to the wrong source (e.g., confusing a picture with an experienced event). Virtual Reality (VR) represents a new source of memories particularly prone to being confused with reality. While previous research identified causes of source confusion between reality and other sources (e.g., imagination, pictures), there is currently no understanding of what characteristics specific to VR (e.g., immersion, presence) could influence source confusion. Through a laboratory study (n=29), we 1) confirm the existence of VR source confusion with current technology, and 2) present a quantitative and qualitative exploration of factors influencing VR source confusion. Building on the Source Monitoring Framework, we identify VR characteristics and assumptions about VR capabilities (e.g., poor rendering) that are used to distinguish virtual from real memories. From these insights, we reflect on how the increasing realism of VR could leave users vulnerable to memory errors and perceptual manipulations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {796},\nnumpages = {17},\nkeywords = {Memory, Source Confusion, Source Misattribution, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642405,\nauthor = {Chen, Qijia and Bellucci, Andrea and Jacucci, Giulio},\ntitle = {“I’d rather drink in VRChat”: Understanding Drinking in Social Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642405},\ndoi = {10.1145/3613904.3642405},\nabstract = {Drinking in social VR has become popular, yet little is known about how users perceive and experience alcohol consumption while immersed in virtual spaces with others, as well as its potential harm and negative effects on their offline and online lives. To better understand this emerging phenomenon from the perspective of both drinkers and non-drinkers, we analyzed public discussions from the r/VRchat online community on users’ perceptions, and experiences with alcohol consumption in social VR. Heavy drinking is prevalent. We find that VR drinkers feel less intoxicated, which makes them drink more without being aware of it. Anti-cybersickness designs may affect users’ perception of vertigo, even if the vertigo is not caused by VR. We discuss how affordances that support meaningful activities (i.e., sense of presence, embodiment, and social interactions) exacerbate alcohol abuse. We propose implications for the design of safer social VR experiences for both drinkers and non-drinkers.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {797},\nnumpages = {16},\nkeywords = {Virtual Reality, alcohol intoxication, drinking, social VR, social interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642293,\nauthor = {Gr\\o{}nb\\ae{}k, Jens Emil Sloth and S\\'{a}nchez Esquivel, Juan and Leiva, Germ\\'{a}n and Velloso, Eduardo and Gellersen, Hans and Pfeuffer, Ken},\ntitle = {Blended Whiteboard: Physicality and Reconfigurability in Remote Mixed Reality Collaboration},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642293},\ndoi = {10.1145/3613904.3642293},\nabstract = {The whiteboard is essential for collaborative work. To preserve its physicality in remote collaboration, Mixed Reality (MR) can blend real whiteboards across distributed spaces. Going beyond reality, MR can further enable interactions like panning and zooming in a virtually reconfigurable infinite whiteboard. However, this reconfigurability conflicts with the sense of physicality. To address this tension, we introduce Blended Whiteboard, a remote collaborative MR system enabling reconfigurable surface blending across distributed physical whiteboards. Blended Whiteboard supports a unique collaboration style, where users can sketch on their local whiteboards but also reconfigure the blended space to facilitate transitions between loosely and tightly coupled work. We describe design principles inspired by proxemics; supporting users in changing between facing each other and being side-by-side, and switching between navigating the whiteboard synchronously and independently. Our work shows exciting benefits and challenges of combining physicality and reconfigurability in the design of distributed MR whiteboards.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {798},\nnumpages = {16},\nkeywords = {3C collaboration model, avatars, f-formations, mixed reality, proxemics, remote collaboration},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642844,\nauthor = {Pointecker, Fabian and Friedl-Knirsch, Judith and Jetter, Hans-Christian and Anthes, Christoph},\ntitle = {From Real to Virtual: Exploring Replica-Enhanced Environment Transitions along the Reality-Virtuality Continuum},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642844},\ndoi = {10.1145/3613904.3642844},\nabstract = {Recent Head-Mounted Displays enable users to perceive the real environment using a video-based see-through mode and the fully virtual environment within a single display. Leveraging these advancements, we present a generic concept to seamlessly transition between the real and virtual environment, with the goal of supporting users in engaging with and disengaging from any real environment into Virtual Reality. This transition process uses a digital replica of the real environment and incorporates various stages of Milgram’s Reality-Virtuality Continuum, along with visual transitions that facilitate gradual navigation between them. We implemented the overall transition concept and four object-based transition techniques. The overall transition concept and four techniques were evaluated in a qualitative user study, focusing on user experience, the use of the replica and visual coherence. The results of the user study show, that most participants stated that the replica facilitates the cognitive processing of the transition and supports spatial orientation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {799},\nnumpages = {13},\nkeywords = {Augmented Reality, Augmented Virtuality, Cross-Reality, Replica, Transitions, User Study, Virtual Reality, Visual Coherence},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642395,\nauthor = {Chan, Liwei and Mi, Tzu-Wei and Hsueh, Zhung Hao and Huang, Yi-Ci and Hsu, Ming Yun},\ntitle = {Seated-WIP: Enabling Walking-in-Place Locomotion for Stationary Chairs in Confined Spaces},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642395},\ndoi = {10.1145/3613904.3642395},\nabstract = {We introduce Seated-WIP, a footstep-based locomotion technique tailored for users seated in confined spaces such as on an airplane. It emulates real-world walking using forefoot or rearfoot in-place stepping, enhancing embodiment while reducing fatigue for prolonged interactions. Our footstep-locomotion maps users’ footstep motions to four locomotion actions: walking forward, turning-in-place, walking backward, and sidestepping. Our first study examined embodiment and fatigue levels across various sitting positions using forefoot, rearfoot, and fullfoot stepping methods. While all these methods effectively replicated walking, users favored the forefoot and rearfoot methods due to reduced fatigue. In our second study, we compared the footstep-locomotion to leaning- and controller-locomotion on a multitasking navigation task. Results indicate that footstep locomotion offers the best embodied sense of walking and has comparable fatigue levels to controller-locomotion, albeit with slightly reduced efficiency than controller-locomotion. In seated VR environments, footstep locomotion offers a harmonious blend of embodiment, fatigue mitigation, and efficiency.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {800},\nnumpages = {13},\nkeywords = {VR locomotion, foot-based interaction, seated locomotion, walking-in-place},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642358,\nauthor = {Wentzel, Johann and Anderson, Fraser and Fitzmaurice, George and Grossman, Tovi and Vogel, Daniel},\ntitle = {SwitchSpace: Understanding Context-Aware Peeking Between VR and Desktop Interfaces},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642358},\ndoi = {10.1145/3613904.3642358},\nabstract = {Cross-reality tasks, like creating or consuming virtual reality (VR) content, often involve inconvenient or distracting switches between desktop and VR. An initial formative study explores cross-reality switching habits, finding most switches are momentary “peeks” between interfaces, with specific habits determined by current context. The results inform a design space for context-aware “peeking” techniques that allow users to view or interact with desktop from VR, and vice versa, without fully switching. We implemented a set of peeking techniques and evaluated them in two levels of a cross-reality task: one requiring only viewing, and another requiring input and viewing. Peeking techniques made task completion faster, with increased input accuracy and reduced perceived workload.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {801},\nnumpages = {16},\nkeywords = {Virtual Reality, controlled experiments, interaction techniques, transitional interfaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642814,\nauthor = {Irlitti, Andrew and Latifoglu, Mesut and Hoang, Thuong and Syiem, Brandon Victor and Vetere, Frank},\ntitle = {Volumetric Hybrid Workspaces: Interactions with Objects in Remote and Co-located Telepresence},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642814},\ndoi = {10.1145/3613904.3642814},\nabstract = {Volumetric telepresence aims to create a shared space, allowing people in local and remote settings to collaborate seamlessly. Prior telepresence examples typically have asymmetrical designs, with volumetric capture in one location and objects in one format. In this paper, we present a volumetric telepresence mixed reality system that supports real-time, symmetrical, multi-user, partially distributed interactions, using objects in multiple formats, across multiple locations. We align two volumetric environments around a common spatial feature to create a shared workspace for remote and co-located people using objects in three formats: physical, virtual, and volumetric. We conducted a study with 18 participants over 6 sessions, evaluating how telepresence workspaces support spatial coordination and hybrid communication for co-located and remote users undertaking collaborative tasks. Our findings demonstrate the successful integration of remote spaces, effective use of proxemics and deixis to support negotiation, and strategies to manage interactivity in hybrid workspaces.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {802},\nnumpages = {16},\nkeywords = {augmented reality, collaboration, mixed reality, partially distributed teams, telepresence, volumetric capture, workspace awareness},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642322,\nauthor = {Govers, Jarod and Velloso, Eduardo and Kostakos, Vassilis and Goncalves, Jorge},\ntitle = {AI-Driven Mediation Strategies for Audience Depolarisation in Online Debates},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642322},\ndoi = {10.1145/3613904.3642322},\nabstract = {Online polarisation can tear the fabric of civility through reinforcing social media’s perceptions of division and discord. Social media platforms often rely on content-moderation to combat polarisation, contingent on the reactive removal or flagging of content. However, this approach often remains agnostic of the underlying debate’s ideas and stifles open discourse. In this study, we use prompt-tuned language models to mediate social media debates, applying the strategies of the Thomas-Kilmann Conflict Mode Instrument (TKI). We evaluate multiple mediation strategies in providing targeted responses to the debates, as shown to a debate audience. Our findings show that high-cooperativeness TKI strategies offered more persuasive arguments, while an accommodating argument strategy was the most successful at depolarising the audience’s opinion. Furthermore, high-cooperativeness strategies also increased the perception that the debaters will reach a consensus. Our work paves the way for scalable and personalised tools that mediate social media debates to encourage depolarisation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {803},\nnumpages = {18},\nkeywords = {Artificial Intelligence, chatbots, debates, depolarisation, generative AI, human-AI cooperation, mediation, psychology, social media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642284,\nauthor = {Odom, William and White, Jordan and Barnett, Samuel and Brand, Nico and Lin, Henry and Yoo, Minyoung and Amram, Tal},\ntitle = {Capra: Making Use of Multiple Perspectives for Capturing, Noticing and Revisiting Hiking Experiences Over Time},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642284},\ndoi = {10.1145/3613904.3642284},\nabstract = {As the practice of hiking becomes increasingly captured through personal data, it is timely to consider what kinds of alternative data encounters might support forms of noticing and connecting to nature as well as one's self and life history over time. To investigate this emerging design space, we designed Capra — a system that brings together the capture, storage, and exploration of personal hiking data with an emphasis on longer-term, occasional yet indefinite use. Over four years, our team adopted a designer-researcher approach where we progressively designed, built, refined, and tested Capra. This process produced frictions in terms of balancing unobtrusiveness, transforming hiking data into evolving interconnected elements in the archive, and managing the sheer quantity and diversity of information with our goal of supporting open-ended and ongoing engagements. It is these insights that emerged through the practice-based design research approach involved in creating Capra that we reflect on in this paper.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {804},\nnumpages = {27},\nkeywords = {Hiking, Personal Data, Personal History, Research through Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642513,\nauthor = {Tanprasert, Thitaree and Fels, Sidney S and Sinnamon, Luanne and Yoon, Dongwook},\ntitle = {Debate Chatbots to Facilitate Critical Thinking on YouTube: Social Identity and Conversational Style Make A Difference},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642513},\ndoi = {10.1145/3613904.3642513},\nabstract = {Exposure to diverse perspectives is helpful for bursting the filter bubble in online public video platforms. The recent advancement of Large Language Models (LLMs) illuminates the potential of creating a debate chatbot that prompts users to critically examine their stances on a topic formed by watching videos. However, whether the viewer is influenced by the chatbot may depend on its persona. In this paper, we investigated the effect of two relevant persona attributes - social identity and rhetorical styles - on critical thinking. In a mixed-methods study (n=36), we found that chatbots with outgroup (vs. ingroup) identity (t(33)=-2.33, p=0.03) and persuasive (vs. eristic) rhetoric (t(44)=1.98, p=0.05) induced critical thinking most effectively, making participants re-examine their arguments. However, participants’ stances remain largely unaffected, likely due to the chatbot’s lack of contextual knowledge and human touch. Our paper provides empirical groundwork for designing chatbot persona for remedying filter bubbles in online communities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {805},\nnumpages = {24},\nkeywords = {agent personas, conversational agents, critical thinking, filter bubble, online public videos},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642530,\nauthor = {Yeo, ShunYi and Lim, Gionnieve and Gao, Jie and Zhang, Weiyu and Perrault, Simon Tangi},\ntitle = {Help Me Reflect: Leveraging Self-Reflection Interface Nudges to Enhance Deliberativeness on Online Deliberation Platforms},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642530},\ndoi = {10.1145/3613904.3642530},\nabstract = {The deliberative potential of online platforms has been widely examined. However, little is known about how various interface-based reflection nudges impact the quality of deliberation. This paper presents two user studies with 12 and 120 participants, respectively, to investigate the impacts of different reflective nudges on the quality of deliberation. In the first study, we examined five distinct reflective nudges: persona, temporal prompts, analogies and metaphors, cultural prompts and storytelling. Persona, temporal prompts, and storytelling emerged as the preferred nudges for implementation on online deliberation platforms. In the second study, we assess the impacts of these preferred reflectors more thoroughly. Results revealed a significant positive impact of these reflectors on deliberative quality. Specifically, persona promotes a deliberative environment for balanced and opinionated viewpoints while temporal prompts promote more individualised viewpoints. Our findings suggest that the choice of reflectors can significantly influence the dynamics and shape the nature of online discussions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {806},\nnumpages = {32},\nkeywords = {civic engagement, deliberation, deliberative quality, deliberativeness, internal reflection, large language model, nudges, online deliberation, persona, public discussions, reflection, reflexivity, self-reflection, storytelling, temporal prompts},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642490,\nauthor = {Hughes, Emelia May and Wang, Renee and Juneja, Prerna and Li, Tony W and Mitra, Tanushree and Zhang, Amy X.},\ntitle = {Viblio: Introducing Credibility Signals and Citations to Video-Sharing Platforms},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642490},\ndoi = {10.1145/3613904.3642490},\nabstract = {As more users turn to video-sharing platforms like YouTube as an information source, they may consume misinformation despite their best efforts. In this work, we investigate ways that users can better assess the credibility of videos by first exploring how users currently determine credibility using existing signals on platforms and then by introducing and evaluating new credibility-based signals. We conducted 12 contextual inquiry interviews with YouTube users, determining that participants used a combination of existing signals, such as the channel name, the production quality, and prior knowledge, to evaluate credibility, yet sometimes stumbled in their efforts to do so. We then developed Viblio, a prototype system that enables YouTube users to view and add citations and related information while watching a video based on our participants’ needs. From an evaluation with 12 people, all participants found Viblio to be intuitive and useful in the process of evaluating a video’s credibility and could see themselves using Viblio in the future.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {807},\nnumpages = {20},\nkeywords = {Citations, Contextual Inquiry, Credibility Signals, Misinformation, Semi-Structured Interview, Social Platforms, YouTube},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642543,\nauthor = {V\\\"{a}kev\\\"{a}, Jaakko and Mekler, Elisa D. and Lindqvist, Janne},\ntitle = {From Disorientation to Harmony: Autoethnographic Insights into Transformative Videogame Experiences},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642543},\ndoi = {10.1145/3613904.3642543},\nabstract = {Videogames can transform the perspectives and attitudes of players. Prior discussion on this transformative potential has typically been limited to non-entertainment videogames with explicit transformational goals. However, recreational gaming appears to hold considerable potential for igniting deeply personal experiences of profound transformation in players. Towards understanding this phenomenon, we conducted an explorative autoethnographic study. For this, the first author played five narrative-driven videogames while collecting self-observational and self-reflective data of his experience during and outside gameplay. Our findings offer intimate insights into the trajectory and emotional qualities of personally meaningful and transformative videogame experiences. For example, we found that gameplay experiences that were initially perceived as bewildering or disorienting could evolve into more harmonious experiences laden with personal meaning. This shift in experience developed through different forms of subsequent re-engagement with initially discrepant game encounters.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {808},\nnumpages = {20},\nkeywords = {aesthetic experience, autoethnography, player experience, reflection, transformation, transformative experience, video games},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642662,\nauthor = {Neupane, Sameer and Saha, Mithun and Ali, Nasir and Hnat, Timothy and Samiei, Shahin Alan and Nandugudi, Anandatirtha and Almeida, David M. and Kumar, Santosh},\ntitle = {Momentary Stressor Logging and Reflective Visualizations: Implications for Stress Management with Wearables},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642662},\ndoi = {10.1145/3613904.3642662},\nabstract = {Commercial wearables from Fitbit, Garmin, and Whoop have recently introduced real-time notifications based on detecting changes in physiological responses indicating potential stress. In this paper, we investigate how these new capabilities can be leveraged to improve stress management. We developed a smartwatch app, a smartphone app, and a cloud service, and conducted a 100-day field study with 122 participants who received prompts triggered by physiological responses several times a day. They were asked whether they were stressed, and if so, to log the most likely stressor. Each week, participants received new visualizations of their data to self-reflect on patterns and trends. Participants reported better awareness of their stressors, and self-initiating fourteen kinds of behavioral changes to reduce stress in their daily lives. Repeated self-reports over 14 weeks showed reductions in both stress intensity (in 26,521 momentary ratings) and stress frequency (in 1,057 weekly surveys).},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {809},\nnumpages = {19},\nkeywords = {Affective Computing, Behavioral Change, Stress Intervention, Stress-tracking, Stressor-logging, Visualizations, Wearable Sensors},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641968,\nauthor = {LeFevre, Colin and Chung, Chia-Fang},\ntitle = {New Understandings of Loss: Examining the Role of Reflective Technology Within Bereavement and Meaning-Making},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641968},\ndoi = {10.1145/3613904.3641968},\nabstract = {Bereavement causes unique challenges, and bereaved individuals can benefit from support during their grieving process. Grief theory emphasizes the importance of reflection during bereavement, and HCI has established that reflective technology can support well-being. However, it remains unclear how to provide bereavement support with reflective technology. We build on constructivist grief psychotherapies to investigate bereavement meaning-making as a focus for reflective technology. We study meaning-making in the context of the digital game GRIS, due to digital games’ alignment with meaning-making. To understand the progression of meaning-making experiences, we conducted a qualitative diary and interview study: 11 bereaved individuals were interviewed on their bereavement experiences, played and completed diaries on GRIS, and were interviewed on their experiences engaging in meaning-making while playing. From these findings, we propose design recommendations for reflective technology to engage with individualized bereavement experiences, embed user agency within reflections, and focus on novel and anti-nihilistic reflections.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {810},\nnumpages = {15},\nkeywords = {Bereavement, Constructivism, Design, Grief, Health, Meaning-Making, Reflection},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642674,\nauthor = {Ibrahim, Zaidat and Caldeira, Clara and Chung, Chia-Fang},\ntitle = {Supporting Experiential Learning in People with Gestational Diabetes Mellitus},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642674},\ndoi = {10.1145/3613904.3642674},\nabstract = {Managing Gestational Diabetes Mellitus (GDM) is a significant challenge for pregnant individuals. Constant self-monitoring, emotional burden, and the short and long-term implications of GDM make the overall pregnancy experience challenging for these individuals, requiring action, learning, and lifestyle adjustment to manage the pregnancy properly. Prior literature on GDM mostly focuses on the medical and health management of the condition. However, pregnant individuals with GDM often must actively learn and adapt lifestyle strategies quickly without much support. Through semi-structured interviews with 13 pregnant individuals diagnosed with GDM, we investigate how these individuals experience, explore, learn, and reflect on ways to live with and manage GDM. Using Kolb’s Learning Theory to analyze and structure our findings, we built on pregnant individuals’ concrete lived experiences and uncovered the challenges as they navigate the GDM journey, managing their changing relationship with food and supporting emotional well-being while living with an often stigmatized condition in an at-risk pregnancy. Our study contributes to the discussion on the design opportunities to facilitate experiential learning of pregnant individuals’ journey.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {811},\nnumpages = {16},\nkeywords = {Experiential Learning, Gestational Diabetes Mellitus, Kolb’s Learning Cycle, Pregnancy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642696,\nauthor = {Dumaru, Prakriti and Hackler, Bryson D and Flood, Audrey and Al-Ameen, Mahdi Nasrullah},\ntitle = {“I feel like he’s looking in the computer world to be social, but I can’t trust his judgement”: Reimagining Parental Control for Children with ASD},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642696},\ndoi = {10.1145/3613904.3642696},\nabstract = {Children with Autism Spectrum Disorder (ASD) often seek comfort from devices (e.g., smartphones) to deal with social overstimulation. However, such reliance exposes them to inappropriate digital content and increases susceptibility to mimicry and social vulnerability. Thus, parents having children with ASD encounter unique challenges in regulating their device usage, which are little addressed in the existing literature on parental mediation. As we begin to address this gap, we designed low-fidelity prototypes centered around open communication and self-regulation, which we refined based on the feedback from six ASD experts in two focus groups. We evaluated updated designs (presented in form of storyboards) through semi-structured interviews with 25 parents whose children with ASD (aged below 14) are active Internet users. Our study joins the body of work on parental mediation; our findings provide insights into inclusive parental control tools for children with ASD, and offer guidelines for future research in these directions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {812},\nnumpages = {25},\nkeywords = {Autism Spectrum Disorder, Focus Group Discussion, Interviews, Low-fidelity Prototype, Open Communication, Parental Mediation, Self-Regulation, Storyboard},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642042,\nauthor = {Cao, Jiaxun and Laabadli, Hiba and Mathis, Chase H and Stern, Rebecca D and Emami-Naeini, Pardis},\ntitle = {\"I Deleted It After the Overturn of Roe v. Wade\": Understanding Women's Privacy Concerns Toward Period-Tracking Apps in the Post Roe v. Wade Era},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642042},\ndoi = {10.1145/3613904.3642042},\nabstract = {The overturn of Roe v. Wade has taken away the constitutional right to abortion. Prior work shows that period-tracking apps’ data practices can be used to detect pregnancy and abortion, hence putting women at risk of being prosecuted. It is unclear how much women know about the privacy practices of such apps and how concerned they are after the overturn. Such knowledge is critical to designing effective strategies for stakeholders to enhance women’s reproductive privacy. We conducted an online 183-participant vignette survey with US women from states with diverse policies on abortion. Participants were significantly concerned about the privacy practices of the period-tracking apps, such as data access by law enforcement and third parties. However, participants felt uninformed and powerless about risk mitigation practices. We provide several recommendations to enhance women’s privacy awareness toward their period-tracking practices.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {813},\nnumpages = {22},\nkeywords = {Period Trackers, Privacy, Roe v. Wade},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641934,\nauthor = {Dewan, Umama and Sula, Cora and Mcdonald, Nora},\ntitle = {Teen Reproductive Health Information Seeking and Sharing Post-Roe},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641934},\ndoi = {10.1145/3613904.3641934},\nabstract = {It has always been challenging for teens to access consistent and reliable information about their reproductive health. But we know little about the impact of recent changes to laws governing sex education and the right to abortion on teen reproductive health information seeking and sharing. We conducted interviews with 15 teens, finding that, post-Roe, teens are concerned about risks to their reproductive health data, particularly as it relates to what they search and share on social media and period tracking apps. Different assessments of risk related to sexual activity, state laws, social context, and cultural and family values dictate information practices. But social risk (like being harassed or doxxed) is the biggest driver of information seeking and sharing practices among mostly non-sexually active teens. We describe the complexities of teens navigating reproductive health post-Roe and offer some guidance about teen technology and privacy literacy.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {814},\nnumpages = {12},\nkeywords = {Teens, behavior change, privacy, reproductive health},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642652,\nauthor = {Lu, Xi and Powell, Jacquelyn E and Agapie, Elena and Chen, Yunan and Epstein, Daniel A.},\ntitle = {Unpacking the Lived Experience of Collaborative Pregnancy Tracking},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642652},\ndoi = {10.1145/3613904.3642652},\nabstract = {Pregnancy brings physical, emotional, and economic challenges for expectant parent(s), close relatives, and friends. Existing technology support, including tracking technology, largely targets pregnant people and ignores other stakeholders. We therefore lack an understanding of how to approach designing collaborative pregnancy tracking technology. To understand how people collaborate around pregnancy tracking and wish to do so, we interviewed 13 pregnant people and 11 non-pregnant stakeholders in the U.S., including partners, friends, and grandparents-to-be. We find that people collaborate for goals like social bonding and jointly managing various pregnancy data. Stakeholders collaborated by either dividing up data types or collectively monitoring the same information. We also identify tensions and challenges, such as pregnant people’s privacy concerns and stakeholders’ varied levels of interest in tracking. In light of socio-cultural norms and stakeholders’ distinctive roles around pregnancy, we point to opportunities for designing collaborative technology that aligns with as well as challenges socio-cultural practices around pregnancy tracking.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {815},\nnumpages = {17},\nkeywords = {Personal informatics, Pregnancy tracking, Self-tracking, Social tracking, Women’s health},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642384,\nauthor = {Song, Qiurong and Hernandez, Rie Helene (Lindy) and Kou, Yubo and Gui, Xinning},\ntitle = {“Our Users' Privacy is Paramount to Us”: A Discourse Analysis of How Period and Fertility Tracking App Companies Address the Roe v Wade Overturn},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642384},\ndoi = {10.1145/3613904.3642384},\nabstract = {After the overturn of Roe v. Wade gave states the license to ban abortion, numerous people in US have grown to worry about privacy in using period and fertility tracking apps. To address these concerns, some app companies have issued public statements to engage in privacy communication with their users. Prior literature has investigated period and fertility tracking apps’ data practices in their privacy policies. However, there remains a dearth of knowledge regarding how companies use privacy communication to address historic privacy-related events such as the overturn. To address the gap, this study investigated app companies’ public statements addressing the overturn of Roe using a combined approach of thematic and discourse analysis. Our findings revealed that companies strategically emphasize their commitment to privacy by demonstrating how their business practices and values are closely intertwined with their efforts to protect user data. We conclude by discussing translatable implications for privacy research.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {816},\nnumpages = {21},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642686,\nauthor = {Nurain, Novia and Chung, Chia-Fang and Caldeira, Clara and Connelly, Kay},\ntitle = {Designing a Card-Based Design Tool to Bridge Academic Research \\& Design Practice For Societal Resilience},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642686},\ndoi = {10.1145/3613904.3642686},\nabstract = {Professional designers often struggle to apply insights from HCI research in their work. To make academic knowledge more accessible to practitioners, HCI researchers have created translational design tools, such as design cards, that support the translation of research insights into design practice. Prior work explored design cards for behavior change, interaction design, personal health informatics, and the sharing economy. Our work complements prior research by exploring the design and use of translational design cards for social aspects of societal resilience through a two-stage study with 14 student designers and eight professional designers. Our findings provide an empirical understanding of the design cards’ generative value for incorporating research insights into the design process. Additionally, we discuss recommendations and highlight opportunities to enhance the design and use of the cards beyond societal resilience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {817},\nnumpages = {20},\nkeywords = {Translational design tools, design cards, design sprints, designers, societal resilience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642853,\nauthor = {Li, Junze and He, Changyang and Hu, Jiaxiong and Jia, Boyang and Halevy, Alon Y and Ma, Xiaojuan},\ntitle = {DiaryHelper: Exploring the Use of an Automatic Contextual Information Recording Agent for Elicitation Diary Study},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642853},\ndoi = {10.1145/3613904.3642853},\nabstract = {Elicitation diary studies, a type of qualitative, longitudinal research method, involve participants to self-report aspects of events of interest at their occurrences as memory cues for providing details and insights during post-study interviews. However, due to time constraints and lack of motivation, participants’ diary entries may be vague or incomplete, impairing their later recall. To address this challenge, we designed an automatic contextual information recording agent, DiaryHelper, based on the theory of episodic memory. DiaryHelper can predict five dimensions of contextual information and confirm with participants. We evaluated the use of DiaryHelper in both the recording period and the elicitation interview through a within-subject study (N=12) over a period of two weeks. Our results demonstrated that DiaryHelper can assist participants in capturing abundant and accurate contextual information without significant burden, leading to a more detailed recall of recorded events and providing greater insights.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {818},\nnumpages = {16},\nkeywords = {Diary Study Methods, Elicitation Diary Study, Episodic Memory, Generative AI Techniques},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642355,\nauthor = {Kaltenhauser, Annika and Stefanidi, Evropi and Sch\\\"{o}ning, Johannes},\ntitle = {Playing with Perspectives and Unveiling the Autoethnographic Kaleidoscope in HCI – A Literature Review of Autoethnographies},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642355},\ndoi = {10.1145/3613904.3642355},\nabstract = {Autoethnography is a valuable methodological approach bridging the gap between personal experiences and academic inquiry, enabling researchers to gain deep insights into various dimensions of technology use and design. While its adoption in Human-Computer Interaction (HCI) continues to grow, a comprehensive investigation of its function and role within HCI research is still lacking. This paper examines the evolving landscape of autoethnographies within HCI over the past two decades through a systematic literature review. We identify prevalent themes, methodologies, and contributions emerging from autoethnographies by analysing a corpus of 31 HCI publications. Furthermore, we detail data collection techniques and analysis methods and describe reporting standards. Our literature review aims to inform future (HCI) researchers, practitioners, and designers. It encourages them to embrace autoethnography’s rich opportunities by providing examples across domains (e.g., embodiment or health and wellbeing) to advance our understanding of the complex relationships between humans and technology.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {819},\nnumpages = {20},\nkeywords = {autoethnography, first-person method, literature review, literature survey, meta review, meta-analysis, qualitative methods},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642771,\nauthor = {Yang, Qian and Wong, Richmond Y. and Jackson, Steven and Junginger, Sabine and Hagan, Margaret D. and Gilbert, Thomas and Zimmerman, John},\ntitle = {The Future of HCI-Policy Collaboration},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642771},\ndoi = {10.1145/3613904.3642771},\nabstract = {Policies significantly shape computation’s societal impact, a crucial HCI concern. However, challenges persist when HCI professionals attempt to integrate policy into their work or affect policy outcomes. Prior research considered these challenges at the “border” of HCI and policy. This paper asks: What if HCI considers policy integral to its intellectual concerns, placing system-people-policy interaction not at the border but nearer the center of HCI research, practice, and education? What if HCI fosters a mosaic of methods and knowledge contributions that blend system, human, and policy expertise in various ways, just like HCI has done with blending system and human expertise? We present this re-imagined HCI-policy relationship as a provocation and highlight its usefulness: It spotlights previously overlooked system-people-policy interaction work in HCI. It unveils new opportunities for HCI’s futuring, empirical, and design projects. It allows HCI to coordinate its diverse policy engagements, enhancing its collective impact on policy outcomes.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {820},\nnumpages = {15},\nkeywords = {Policy, design, societal impact of technology.},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642295,\nauthor = {Brombacher, Hans and Dritsa, Dimitra and Vos, Steven and Houben, Steven},\ntitle = {\"To Click or not to Click\": Back to Basic for Experience Sampling for Office Well-being in Shared Office Spaces},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642295},\ndoi = {10.1145/3613904.3642295},\nabstract = {Sensors in offices mainly measure environmental data, missing qualitative insights into office workers’ perceptions. This opens the opportunity for active individual participation in data collection. To promote reflection on office well-being while overcoming experience sampling challenges in terms of privacy, notification, and display overload, and in-the-moment data collection, we developed Click-IO. Click-IO is a tangible, privacy-sensitive, mobile experience sampling tool that collects contextual information. We evaluated Click-IO for 20-days. The system enabled real-time reflections for office workers, promoting self-awareness of their environment and well-being. Its non-digital design ensured privacy-sensitive feedback collection, while its mobility facilitated in-the-moment feedback. Based on our findings, we identify design recommendations for the development of mobile experience sampling tools. Moreover, the integration of contextual data with environmental sensor data presented a more comprehensive understanding of individuals’ experiences. This research contributes to the development of experience sampling tools and sensor integration for understanding office well-being.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {821},\nnumpages = {18},\nkeywords = {Contextual data, Experience Sampling, Office well-being},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642397,\nauthor = {Pielot, Martin and Callegaro, Mario},\ntitle = {Did You Misclick? Reversing 5-Point Satisfaction Scales Causes Unintended Responses},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642397},\ndoi = {10.1145/3613904.3642397},\nabstract = {When fielding satisfaction questions, survey platforms offer the option to randomly reverse the response options. In this paper, we provide evidence that the use of this option leads to biased results. In Study 1, we show that reversing vertically oriented response options leads to significantly lower satisfaction ratings – from 90 to 82 percent in our case. Study 2 had survey respondents verify their response and found that on a reversed scale, the very-dissatisfied option was selected unintentionally in about half of the cases. The cause, shown by Study 3, is that survey respondents expect the positive option at the top and do not always pay sufficient attention to the question, combined with the similar spelling of satisfied and dissatisfied. To prevent unintentional responses from biasing the results, we recommend keeping the positive option at the top in vertically-oriented scales with visually-similar endpoint labels.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {822},\nnumpages = {7},\nkeywords = {response bias, satisfaction, satisfaction surveys, survey response option order, usability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642833,\nauthor = {Ghosh, Surjya and Mandi, Salma and Sen, Sougata and Mitra, Bivas and De, Pradipta},\ntitle = {Towards Estimating Missing Emotion Self-reports Leveraging User Similarity: A Multi-task Learning Approach},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642833},\ndoi = {10.1145/3613904.3642833},\nabstract = {The Experience Sampling Method (ESM) is widely used to collect emotion self-reports to train machine learning models for emotion inference. However, as ESM studies are time-consuming and burdensome, participants often withdraw in between. This unplanned withdrawal compels the researchers to discard the dropout participants’ data, significantly impacting the quality and quantity of the self-reports. To address this problem, we leverage only the self-reporting similarity across participants (unlike prior works that apply different machine learning approaches on additional modalities) for missing self-report estimation. In specific, we propose a Multi-task Learning (MTL) framework, MUSE, that constructs the missing self-reports of the dropout participants. We evaluate MUSE in two in-the-wild studies (N1=24, N2=30) of 6-week and 8-week duration, during which the participants reported four emotions (happy, sad, stressed, relaxed) using a smartphone application. The evaluation reveals that MUSE estimates the missing emotion self-reports with an average AUCROC of 84\\% (Study I) and 82\\% (Study II). A follow-up evaluation of MUSE for an emotion inference (downstream) task reveals no significant difference in emotion inference performance when estimated self-reports are used. These findings underscore the utility of MUSE in estimating missing self-reports in ESM studies and the applicability of MUSE for downstream tasks (e.g., emotion inference).},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {823},\nnumpages = {19},\nkeywords = {Emotion self-report, Experience Sampling Method (ESM), Multi-task learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642732,\nauthor = {Panicker, Aswati and Nurain, Novia and Ibrahim, Zaidat and Wang, Chun-Han (Ariel) and Ha, Seung Wan and Wu, Yuxing and Connelly, Kay and Siek, Katie A. and Chung, Chia-Fang},\ntitle = {Understanding fraudulence in online qualitative studies: From the researcher's perspective},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642732},\ndoi = {10.1145/3613904.3642732},\nabstract = {Researchers are increasingly facilitating qualitative research studies online. While this has made research more accessible for participation, there have been notable encounters with “fraudulent” participants. By fraudulent, we refer to individuals who are deceptive about meeting the inclusion criteria, their identity, or experiences. Fraudulent participants have generated new challenges for researchers who have to interact 1:1 with these individuals, face ethical dilemmas on appropriate next steps, diagnose and prevent the issue from happening again, and deal with their own identity as a scholar. In this study, we interview 16 HCI researchers to understand and learn from their experiences. We contribute: (1) an understanding of how HCI qualitative researchers deal with fraudulent participants; (2) a guide for qualitative HCI researchers on how to handle fraudulence; and (3) a reflection on how the HCI research community might better improve our science and training efforts.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {824},\nnumpages = {17},\nkeywords = {data integrity;, deception, ethics, fraudulence, human research participants, online studies, qualitative research},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642727,\nauthor = {Joshi, Tejaswini and Biggs, Heidi and Bardzell, Jeffrey and Bardzell, Shaowen},\ntitle = {Who is “I”?: Subjectivity and Ethnography in HCI},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642727},\ndoi = {10.1145/3613904.3642727},\nabstract = {HCI research applies ethnographic methods to understand and represent practices that involve the use of interactive systems. A subdomain of this work is interpretivist ethnography, which positions the researcher's perspectival view [37] as central to ethnographic research and its epistemic contribution. Given this we ask: How might ethnographic researchers in HCI surface the meaning-making role of their subjectivities in research? We reflect on our prior ethnographic fieldwork on small-scale sustainable farms in Indianapolis, Indiana to bring the ethnographic “I” into focus by articulating our reflections as “impressionist tales'' [64:101-124]. We ground this pursuit in sociologist Andrea Doucet's concept of “gossamer walls” to surface researcher's three reflexive relationships 1) with herself; 2) with participants; and 3) with her epistemic communities [34]. We build on and contribute to postmodern ethnography in HCI to clarify the epistemic virtues and methodological best practices of a more unapologetically subjective ethnographic practice in HCI.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {825},\nnumpages = {15},\nkeywords = {Ethnography, Gossamer Walls, Reflexivity, Subjectivity},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642843,\nauthor = {Berens, Benjamin Maximilian and Schaub, Florian and Mossano, Mattia and Volkamer, Melanie},\ntitle = {Better Together: The Interplay Between a Phishing Awareness Video and a Link-centric Phishing Support Tool},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642843},\ndoi = {10.1145/3613904.3642843},\nabstract = {Two popular approaches for helping consumers avoid phishing threats are phishing awareness videos and tools supporting users in identifying phishing emails. Awareness videos and tools have each been shown on their own to increase people’s phishing detection rate. Videos have been shown to be a particularly effective awareness measure; link-centric warnings have been shown to provide effective tool support. However, it is unclear how these two approaches compare to each other. We conducted a between-subjects online experiment (n=409) in which we compared the effectiveness of the NoPhish video and the TORPEDO tool and their combination. Our main findings suggest that the TORPEDO tool outperformed the NoPhish video and that the combination of both performs significantly better than just the tool. We discuss the implications of our findings for the design and deployment of phishing awareness measures and support tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {826},\nnumpages = {60},\nkeywords = {anti-phishing, online study, phishing awareness.},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642951,\nauthor = {Caven, Peter and Zhang, Zitao and Abbott, Jacob and Ma, Xinyao and Camp, Ljean},\ntitle = {Comparing the Use and Usefulness of Four IoT Security Labels},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642951},\ndoi = {10.1145/3613904.3642951},\nabstract = {There are currently multiple proposed security label designs for consumer products, with each prioritizing different security and privacy factors. These differences risk making product comparisons more confusing than informative. Standardized labels could potentially resolve this by informing consumers of a product’s security features at the point of purchase. But which standard? This survey, of 500 participants, studied four label designs and measured comprehension, response time, acceptability, and cognitive load. We gauged understanding of participant perception and preferences using three smart devices: light bulbs, cameras, and thermostats. We identified preferences and behaviors before, during, and after label use for product selection. At first, participants believed more information-dense labels would better support their purchasing behavior; however, after they evaluated and compared products, participants gravitated towards less cognitively demanding designs. We identified how participants utilized and prioritized label elements to provide recommendations for US label design efforts.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {827},\nnumpages = {31},\nkeywords = {IoT, icons, interaction, labels, privacy, security, trust},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642493,\nauthor = {Gallardo, Andrea and Erbes, Robert and Le Blanc, Katya and Bauer, Lujo and Cranor, Lorrie Faith},\ntitle = {Interdisciplinary Approaches to Cybervulnerability Impact Assessment for Energy Critical Infrastructure},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642493},\ndoi = {10.1145/3613904.3642493},\nabstract = {As energy infrastructure becomes more interconnected, understanding cybersecurity risks to production systems requires integrating operational and computer security knowledge. We interviewed 18 experts working in the field of energy critical infrastructure to compare what information they find necessary to assess the impact of computer vulnerabilities on energy operational technology. These experts came from two groups: 1) computer security experts and 2) energy sector operations experts. We find that both groups responded similarly for general categories of information and displayed knowledge about both domains, perhaps due to their interdisciplinary work at the same organization. Yet, we found notable differences in the details of their responses and in their stated perceptions of each group’s approaches to impact assessment. Their suggestions for collaboration across domains highlighted how these two groups can work together to help each other secure the energy grid. Our findings inform the development of interdisciplinary security approaches in critical-infrastructure contexts.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {828},\nnumpages = {24},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641943,\nauthor = {Chen, Xiaowei and Sacr\\'{e}, Margault and Lenzini, Gabriele and Greiff, Samuel and Distler, Verena and Sergeeva, Anastasia},\ntitle = {The Effects of Group Discussion and Role-playing Training on Self-efficacy, Support-seeking, and Reporting Phishing Emails: Evidence from a Mixed-design Experiment},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641943},\ndoi = {10.1145/3613904.3641943},\nabstract = {Organizations rely on phishing interventions to enhance employees’ vigilance and safe responses to phishing emails that bypass technical solutions. While various resources are available to counteract phishing, studies emphasize the need for interactive and practical training approaches. To investigate the effectiveness of such an approach, we developed and delivered two anti-phishing trainings, group discussion and role-playing, at a European university. We conducted a pre-registered1 experiment (N = 105), incorporating repeated measures at three time points, a control group, and three in-situ phishing tests. Both trainings enhanced employees’ anti-phishing self-efficacy and support-seeking intention in within-group analyses. Only the role-playing training significantly improved support-seeking intention when compared to the control group. Participants in both trainings reported more phishing tests and demonstrated heightened vigilance to phishing attacks compared to the control group. We discuss practical implications for evaluating and improving phishing interventions and promoting safe responses to phishing threats within organizations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {829},\nnumpages = {21},\nkeywords = {Anti-phishing training, Group discussion, Mixed-design experiment, Phishing intervention, Report phishing emails, Role-playing training, Self-efficacy, Support-seeking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642331,\nauthor = {Francis II, Errol and Barwulor, Catherine and Monroe, Ayana R and Morales, Kediel O and Potlapalli, Samya and Brown, Kimberly and Jose, Julia and Sidnam-Mauch, Emily and Mcgregor, Susan E and Caine, Kelly},\ntitle = {Usable News Authentication: How the Presentation and Location of Cryptographic Information Impacts the Usability of Provenance Information and Perceptions of News Articles},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642331},\ndoi = {10.1145/3613904.3642331},\nabstract = {Cryptographic tools for authenticating the provenance of web-based information are a promising approach to increasing trust in online news and information. However, making these tools’ technical assurances sufficiently usable for news consumers is essential to realizing their potential. We conduct an online study with 160 participants to investigate how the presentation (visual vs. textual) and location (on a news article page or a third-party site) of the provenance information affects news consumers’ perception of the content’s credibility and trustworthiness, as well as the usability of the tool itself. We find that although the visual presentation of provenance information is more challenging to adopt than its text-based counterpart, this approach leads its users to put more faith in the credibility and trustworthiness of digital news, especially when situated internally to the news article.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {830},\nnumpages = {20},\nkeywords = {Information Credibility, Usability and Human Interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642837,\nauthor = {Walendy, Ren\\'{e} and Weber, Markus and Li, Jingjie and Becker, Steffen and Wiesen, Carina and Elson, Malte and Kim, Younghyun and Fawaz, Kassem and Rummel, Nikol and Paar, Christof},\ntitle = {I see an IC: A Mixed-Methods Approach to Study Human Problem-Solving Processes in Hardware Reverse Engineering},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642837},\ndoi = {10.1145/3613904.3642837},\nabstract = {Trust in digital systems depends on secure hardware, often assured through HRE. This work develops methods for investigating human problem-solving processes in HRE, an underexplored yet critical aspect. Since reverse engineers rely heavily on visual information, eye tracking holds promise for studying their cognitive processes. To gain further insights, we additionally employ verbal thought protocols during and immediately after HRE tasks: Concurrent and Retrospective Think Aloud. We evaluate the combination of eye tracking and Think Aloud with 41 participants in an HRE simulation. Eye tracking accurately identifies fixations on individual circuit elements and highlights critical components. Based on two use cases, we demonstrate that eye tracking and Think Aloud can complement each other to improve data quality. Our methodological insights can inform future studies in HRE, a specific setting of human-computer interaction, and in other problem-solving settings involving misleading or missing information.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {831},\nnumpages = {20},\nkeywords = {eye tracking, hardware reverse engineering, integrated circuits, mixed-methods, problem solving, semiconductor industry, think aloud},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inbook{10.1145/3613904.3642011,\nauthor = {Chen, Claire C and Shu, Dillon and Ravishankar, Hamsini and Li, Xinran and Agarwal, Yuvraj and Cranor, Lorrie Faith},\ntitle = {Is a Trustmark and QR Code Enough? The Effect of IoT Security and Privacy Label Information Complexity on Consumer Comprehension and Behavior},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642011},\nabstract = {The U.S. Government is developing a package label to help consumers access reliable security and privacy information about Internet of Things (IoT) devices when making purchase decisions. The label will include the U.S. Cyber Trust Mark, a QR code to scan for more details, and potentially additional information. To examine how label information complexity and educational interventions affect comprehension of security and privacy attributes and label QR code use, we conducted an online survey with 518 IoT purchasers. We examined participants’ comprehension and preferences for three labels of varying complexities, with and without an educational intervention. Participants favored and correctly utilized the two higher-complexity labels, showing a special interest in the privacy-relevant content. Furthermore, while the educational intervention improved understanding of the QR code’s purpose, it had a modest effect on QR scanning behavior. We highlight clear design and policy directions for creating and deploying IoT security and privacy labels.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {832},\nnumpages = {32}\n}",
    "@inproceedings{10.1145/3613904.3642310,\nauthor = {Bhardwaj, Divyanshu and Guthoff, Carolyn and Dabrowski, Adrian and Fahl, Sascha and Krombholz, Katharina},\ntitle = {Mental Models, Expectations and Implications of Client-Side Scanning: An Interview Study with Experts},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642310},\ndoi = {10.1145/3613904.3642310},\nabstract = {Client-Side Scanning (CSS) is discussed as a potential solution to contain the dissemination of child sexual abuse material (CSAM). A significant challenge associated with this debate is that stakeholders have different interpretations of the capabilities and frontiers of the concept and its varying implementations. In this paper, we explore stakeholders’ understandings of the technology and the expectations and potential implications in the context of CSAM by conducting and analyzing 28 semi-structured interviews with a diverse sample of experts. We identified mental models of CSS and the expected challenges. Our results show that CSS is often a preferred solution in the child sexual abuse debate due to the lack of an alternative. Our findings illustrate the importance of further interdisciplinary discussions to define and comprehend the impact of CSS usage on society, particularly vulnerable groups such as children.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {833},\nnumpages = {24},\nkeywords = {child sexual abuse material, client-side scanning, crime prevention, surveillance},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642506,\nauthor = {Shin, Hyejin and Huh, Jun Ho and Kwon, Bum Jun and Kim, Iljoo and Cheon, Eunyong and Kim, HongMin and Lee, Choong-Hoon and Oakley, Ian},\ntitle = {SkullID: Through-Skull Sound Conduction based Authentication for Smartglasses},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642506},\ndoi = {10.1145/3613904.3642506},\nabstract = {This paper investigates the use of through-skull sound conduction to authenticate smartglass users. We mount a surface transducer on the right mastoid process to play cue signals and capture skull-transformed audio responses through contact microphones on various skull locations. We use the resultant bio-acoustic information as classification features. In an initial single-session study (N=25), we achieved mean Equal Error Rates (EERs) of 5.68\\% and 7.95\\% with microphones on the brow and left mastoid process. Combining the two signals substantially improves performance (to 2.35\\% EER). A subsequent multi-session study (N=30) demonstrates EERs are maintained over three recalls and, additionally, shows robustness to donning variations and background noise (achieving 2.72\\% EER). In a follow-up usability study over one week, participants report high levels of usability (as expressed by SUS scores) and that only modest workload is required to authenticate. Finally, a security analysis demonstrates the system’s robustness to spoofing and imitation attacks.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {834},\nnumpages = {19},\nkeywords = {Acoustic response, Biometric, Bone conduction, Smartglass authentication},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642027,\nauthor = {Wang, Chenkai and Jia, Zhuofan and Benkraouda, Hadjer and Zevnik, Cody and Heuermann, Nicholas and Foulger, Roopa and Handler, Jonathan A. and Wang, Gang},\ntitle = {VeriSMS: A Message Verification System for Inclusive Patient Outreach against Phishing Attacks},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642027},\ndoi = {10.1145/3613904.3642027},\nabstract = {Patient outreach enables timely communication between patients and healthcare providers but is vulnerable to phishing/spoofing attacks. In this paper, we work with a U.S.-based healthcare provider to design an inclusive method to address this threat. We present VeriSMS which allows patients to call a voice agent to verify whether the received (sensitive) messages are indeed sent by their healthcare provider. We design the system to be inclusive: it is accessible to patients who only have access to SMS and phone call capabilities. We perform a two-part user study to refine the system design (N=15) and confirm users can correctly understand the system and use it to identify spoofed/phishing messages (N=35). A key insight from our study is to not exclusively optimize for strong security but to tailor the designs based on user habits. Our result confirms the effectiveness and usability of VeriSMS and its ability to significantly increase adversaries’ costs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {835},\nnumpages = {17},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642621,\nauthor = {Schoeffer, Jakob and De-Arteaga, Maria and K\\\"{u}hl, Niklas},\ntitle = {Explanations, Fairness, and Appropriate Reliance in Human-AI Decision-Making},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642621},\ndoi = {10.1145/3613904.3642621},\nabstract = {In this work, we study the effects of feature-based explanations on distributive fairness of AI-assisted decisions, specifically focusing on the task of predicting occupations from short textual bios. We also investigate how any effects are mediated by humans’ fairness perceptions and their reliance on AI recommendations. Our findings show that explanations influence fairness perceptions, which, in turn, relate to humans’ tendency to adhere to AI recommendations. However, we see that such explanations do not enable humans to discern correct and incorrect AI recommendations. Instead, we show that they may affect reliance irrespective of the correctness of AI recommendations. Depending on which features an explanation highlights, this can foster or hinder distributive fairness: when explanations highlight features that are task-irrelevant and evidently associated with the sensitive attribute, this prompts overrides that counter AI recommendations that align with gender stereotypes. Meanwhile, if explanations appear task-relevant, this induces reliance behavior that reinforces stereotype-aligned errors. These results imply that feature-based explanations are not a reliable mechanism to improve distributive fairness.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {836},\nnumpages = {18},\nkeywords = {AI-informed decision-making, Human-AI interaction, algorithmic fairness, appropriate reliance, explainable AI, fairness perceptions},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642149,\nauthor = {Liu, Michael Xieyang and Wu, Tongshuang and Chen, Tianying and Li, Franklin Mingzhe and Kittur, Aniket and Myers, Brad A},\ntitle = {Selenite: Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642149},\ndoi = {10.1145/3613904.3642149},\nabstract = {Sensemaking in unfamiliar domains can be challenging, demanding considerable user effort to compare different options with respect to various criteria. Prior research and our formative study found that people would benefit from reading an overview of an information space upfront, including the criteria others previously found useful. However, existing sensemaking tools struggle with the “cold-start” problem — it not only requires significant input from previous users to generate and share these overviews, but such overviews may also turn out to be biased and incomplete. In this work, we introduce a novel system, Selenite, which leverages Large Language Models (LLMs) as reasoning machines and knowledge retrievers to automatically produce a comprehensive overview of options and criteria to jumpstart users’ sensemaking processes. Subsequently, Selenite also adapts as people use it, helping users find, read, and navigate unfamiliar information in a systematic yet personalized manner. Through three studies, we found that Selenite produced accurate and high-quality overviews reliably, significantly accelerated users’ information processing, and effectively improved their overall comprehension and sensemaking experience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {837},\nnumpages = {26},\nkeywords = {Human-AI Collaboration, Large Language Models, Natural Language Processing, Sensemaking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642139,\nauthor = {Gero, Katy Ilonka and Swoopes, Chelse and Gu, Ziwei and Kummerfeld, Jonathan K. and Glassman, Elena L.},\ntitle = {Supporting Sensemaking of Large Language Model Outputs at Scale},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642139},\ndoi = {10.1145/3613904.3642139},\nabstract = {Large language models (LLMs) are capable of generating multiple responses to a single prompt, yet little effort has been expended to help end-users or system designers make use of this capability. In this paper, we explore how to present many LLM responses at once. We design five features, which include both pre-existing and novel methods for computing similarities and differences across textual documents, as well as how to render their outputs. We report on a controlled user study (n=24) and eight case studies evaluating these features and how they support users in different tasks. We find that the features support a wide variety of sensemaking tasks and even make tasks tractable that our participants previously considered to be too difficult to attempt. Finally, we present design guidelines to inform future explorations of new LLM interfaces.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {838},\nnumpages = {21},\nkeywords = {analogical learning theory, foundation models, language models, large language models, reading, sensemaking, skimming, variation theory},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642934,\nauthor = {Pafla, Marvin and Larson, Kate and Hancock, Mark},\ntitle = {Unraveling the Dilemma of AI Errors: Exploring the Effectiveness of Human and Machine Explanations for Large Language Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642934},\ndoi = {10.1145/3613904.3642934},\nabstract = {The field of eXplainable artificial intelligence (XAI) has produced a plethora of methods (e.g., saliency-maps) to gain insight into artificial intelligence (AI) models, and has exploded with the rise of deep learning (DL). However, human-participant studies question the efficacy of these methods, particularly when the AI output is wrong. In this study, we collected and analyzed 156 human-generated text and saliency-based explanations collected in a question-answering task (N = 40) and compared them empirically to state-of-the-art XAI explanations (integrated gradients, conservative LRP, and ChatGPT) in a human-participant study (N = 136). Our findings show that participants found human saliency maps to be more helpful in explaining AI answers than machine saliency maps, but performance negatively correlated with trust in the AI model and explanations. This finding hints at the dilemma of AI errors in explanation, where helpful explanations can lead to lower task performance when they support wrong AI predictions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {839},\nnumpages = {20},\nkeywords = {Stanford Question Answering Dataset (SQuAD 1.1v), dilemma of AI errors, explainability, explainable artificial intelligence (XAI), explanation confirmation bias, explanation evaluation, human explanation, human-participant study, large language models (LLMs), local explanations, machine explanation, post-hoc explanations, question-answering task, saliency maps, text-explanations},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642671,\nauthor = {Ma, Shuai and Wang, Xinru and Lei, Ying and Shi, Chuhan and Yin, Ming and Ma, Xiaojuan},\ntitle = {“Are You Really Sure?” Understanding the Effects of Human Self-Confidence Calibration in AI-Assisted Decision Making},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642671},\ndoi = {10.1145/3613904.3642671},\nabstract = {In AI-assisted decision-making, it is crucial but challenging for humans to achieve appropriate reliance on AI. This paper approaches this problem from a human-centered perspective, “human self-confidence calibration”. We begin by proposing an analytical framework to highlight the importance of calibrated human self-confidence. In our first study, we explore the relationship between human self-confidence appropriateness and reliance appropriateness. Then in our second study, We propose three calibration mechanisms and compare their effects on humans’ self-confidence and user experience. Subsequently, our third study investigates the effects of self-confidence calibration on AI-assisted decision-making. Results show that calibrating human self-confidence enhances human-AI team performance and encourages more rational reliance on AI (in some aspects) compared to uncalibrated baselines. Finally, we discuss our main findings and provide implications for designing future AI-assisted decision-making interfaces.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {840},\nnumpages = {20},\nkeywords = {AI-Assisted Decision-making, Appropriate Reliance, Human-AI Collaboration, Reliance on AI systems, Trust Calibration},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642059,\nauthor = {Juneja, Prerna and Zhang, Wenjuan and Smith-Renner, Alison Marie and Lamba, Hemank and Tetreault, Joel and Jaimes, Alex},\ntitle = {Dissecting users' needs for search result explanations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642059},\ndoi = {10.1145/3613904.3642059},\nabstract = {There is a growing demand for transparency in search engines to understand how search results are curated and to enhance users’ trust. Prior research has introduced search result explanations with a focus on how to explain, assuming explanations are beneficial. Our study takes a step back to examine if search explanations are needed and when they are likely to provide benefits. Additionally, we summarize key characteristics of helpful explanations and share users’ perspectives on explanation features provided by Google and Bing. Interviews with non-technical individuals reveal that users do not always seek or understand search explanations and mostly desire them for complex and critical tasks. They find Google’s search explanations too obvious but appreciate the ability to contest search results. Based on our findings, we offer design recommendations for search engines and explanations to help users better evaluate search results and enhance their search experience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {841},\nnumpages = {17},\nkeywords = {explanations, search engines, search experience, search explanations, transparency},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641969,\nauthor = {Fok, Raymond and Lipka, Nedim and Sun, Tong and Siu, Alexa F},\ntitle = {Marco: Supporting Business Document Workflows via Collection-Centric Information Foraging with Large Language Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641969},\ndoi = {10.1145/3613904.3641969},\nabstract = {Knowledge workers often need to extract and analyze information from a collection of documents to solve complex information tasks in the workplace, e.g., hiring managers reviewing resumes or analysts assessing risk in contracts. However, foraging for relevant information can become tedious and repetitive over many documents and criteria of interest. We introduce Marco, a mixed-initiative workspace supporting sensemaking over diverse business document collections. Through collection-centric assistance, Marco reduces the cognitive costs of extracting and structuring information, allowing users to prioritize comparative synthesis and decision making processes. Users interactively communicate their information needs to an AI assistant using natural language and compose schemas that provide an overview of a document collection. Findings from a usability study (n=16) demonstrate that when using Marco, users complete sensemaking tasks 16\\% more quickly, with less effort, and without diminishing accuracy. A design probe with seven domain experts identifies how Marco can benefit various real-world workflows.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {842},\nnumpages = {20},\nkeywords = {business document workflows, document collections, large language models, mixed-initiative systems, sensemaking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642943,\nauthor = {Ko, Hyung-Kwon and Jeon, Hyeon and Park, Gwanmo and Kim, Dae Hyun and Kim, Nam Wook and Kim, Juho and Seo, Jinwook},\ntitle = {Natural Language Dataset Generation Framework for Visualizations Powered by Large Language Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642943},\ndoi = {10.1145/3613904.3642943},\nabstract = {We introduce VL2NL, a Large Language Model (LLM) framework that generates rich and diverse NL datasets using Vega-Lite specifications as input, thereby streamlining the development of Natural Language Interfaces (NLIs) for data visualization. To synthesize relevant chart semantics accurately and enhance syntactic diversity in each NL dataset, we leverage 1) a guided discovery incorporated into prompting so that LLMs can steer themselves to create faithful NL datasets in a self-directed manner; 2) a score-based paraphrasing to augment NL syntax along with four language axes. We also present a new collection of 1,981 real-world Vega-Lite specifications that have increased diversity and complexity than existing chart collections. When tested on our chart collection, VL2NL extracted chart semantics and generated L1/L2 captions with 89.4\\% and 76.0\\% accuracy, respectively. It also demonstrated generating and paraphrasing utterances and questions with greater diversity compared to the benchmarks. Last, we discuss how our NL datasets and framework can be utilized in real-world scenarios. The codes and chart collection are available at https://github.com/hyungkwonko/chart-llm.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {843},\nnumpages = {22},\nkeywords = {Vega-Lite, data visualization, framework, large language models, natural language datasets, natural language interfaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641971,\nauthor = {Rajcic, Nina and Llano Rodriguez, Maria Teresa and McCormack, Jon},\ntitle = {Towards a Diffractive Analysis of Prompt-Based Generative AI},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641971},\ndoi = {10.1145/3613904.3641971},\nabstract = {Recent developments in prompt-based generative AI has given rise to discourse surrounding the perceived ethical concerns, economic implications, and consequences for the future of cultural production. As generative imagery becomes pervasive in mainstream society, dominated primarily by emerging industry leaders, we encourage that the role of the CHI community be one of inquiry; to investigate the numerous ways in which generative AI has the potential to, and already is, augmenting human creativity. In this paper, we conducted a diffractive analysis exploring the potential role of prompt-based interfaces in artists’ creative practice. Over a two week period, seven visual artists were given access to a personalised instance of Stable Diffusion, fine-tuned on a dataset of their work. In the following diffractive analysis, we identified two dominant modes adopted by participants, AI for ideation, and AI for production. We furthermore present a number of ethical design considerations for the future development of generative AI interfaces.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {844},\nnumpages = {15},\nkeywords = {Creative AI, Diffractive Analysis, Diffusion, Generative AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642726,\nauthor = {Li, Haotian and Wang, Yun and Qu, Huamin},\ntitle = {Where Are We So Far? Understanding Data Storytelling Tools from the Perspective of Human-AI Collaboration},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642726},\ndoi = {10.1145/3613904.3642726},\nabstract = {Data storytelling is powerful for communicating data insights, but it requires diverse skills and considerable effort from human creators. Recent research has widely explored the potential for artificial intelligence (AI) to support and augment humans in data storytelling. However, there lacks a systematic review to understand data storytelling tools from the perspective of human-AI collaboration, which hinders researchers from reflecting on the existing collaborative tool designs that promote humans’ and AI’s advantages and mitigate their shortcomings. This paper investigated existing tools with a framework from two perspectives: the stages in the storytelling workflow where a tool serves, including analysis, planning, implementation, and communication, and the roles of humans and AI in each stage, such as creators, assistants, optimizers, and reviewers. Through our analysis, we recognize the common collaboration patterns in existing tools, summarize lessons learned from these patterns, and further illustrate research opportunities for human-AI collaboration in data storytelling.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {845},\nnumpages = {19},\nkeywords = {Data storytelling, human-AI collaboration},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642193,\nauthor = {Oh, Jeesun and Kim, Wooseok and Kim, Sungbae and Im, Hyeonjeong and Lee, Sangsu},\ntitle = {Better to Ask Than Assume: Proactive Voice Assistants’ Communication Strategies That Respect User Agency in a Smart Home Environment},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642193},\ndoi = {10.1145/3613904.3642193},\nabstract = {Proactive voice assistants (VAs) in smart homes predict users’ needs and autonomously take action by controlling smart devices and initiating voice-based features to support users’ various activities. Previous studies on proactive systems have primarily focused on determining action based on contextual information, such as user activities, physiological state, or mobile usage. However, there is a lack of research that considers user agency in VAs’ proactive actions, which empowers users to express their dynamic needs and preferences and promotes a sense of control. Thus, our study aims to explore verbal communication through which VAs can proactively take action while respecting user agency. To delve into communication between a proactive VA and a user, we used the Wizard of Oz method to set up a smart home environment, allowing controllable devices and unrestrained communication. This paper proposes design implications for the communication strategies of proactive VAs that respect user agency.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {846},\nnumpages = {17},\nkeywords = {Conversational User Interface, Proactive Voice Assistant, Proactivity, Proactivity Level, Smart Home, User agency, Voice Interaction, Voice User Interface, Voice-based Conversational Agent},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642862,\nauthor = {Schenkluhn, Marius and Knierim, Michael Thomas and Kiss, Francisco and Weinhardt, Christof},\ntitle = {Connecting Home: Human-Centric Setup Automation in the Augmented Smart Home},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642862},\ndoi = {10.1145/3613904.3642862},\nabstract = {Controlling smart homes via vendor-specific apps on smartphones is cumbersome. Augmented Reality (AR) offers a promising alternative by enabling direct interactions with Internet of Things (IoT) devices. However, using AR for smart home control requires knowledge of each device’s 3D position. In this paper, we introduce and evaluate three concepts for identifying IoT device positions with varying degrees of automation. Our mixed-methods laboratory study with 28 participants revealed that, despite being recognized as the most efficient option, the majority of participants opted against a fast, fully automated detection, favoring a balance between efficiency and perceived autonomy and control. We link this decision to psychological needs grounded in self-determination theory and discuss the strengths and weaknesses of each alternative, motivating a user-adaptive solution. Additionally, we observed a “wow-effect” in response to AR interaction for smart homes, suggesting potential benefits of a human-centric approach to the smart home of the future.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {847},\nnumpages = {16},\nkeywords = {Augmented Reality, Laboratory Experiment, Self-Determination Theory, Smart Home},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641991,\nauthor = {Lee, Sunjae and Jeong, Minwoo and Song, Daye and Choi, Junyoung and Son, Seoyun and Song, Jean Y and Shin, Insik},\ntitle = {FLUID-IoT : Flexible and Fine-Grained Access Control in Shared IoT Environments via Multi-user UI Distribution},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641991},\ndoi = {10.1145/3613904.3641991},\nabstract = {The rapid growth of the Internet of Things (IoT) in shared spaces has led to an increasing demand for sharing IoT devices among multiple users. Yet, existing IoT platforms often fall short by offering an all-or-nothing approach to access control, not only posing security risks but also inhibiting the growth of the shared IoT ecosystem. This paper introduces FLUID-IoT, a framework that enables flexible and granular multi-user access control, even down to the User Interface (UI) component level. Leveraging a multi-user UI distribution technique, FLUID-IoT transforms existing IoT apps into centralized hubs that selectively distribute UI components to users based on their permission levels. Our performance evaluation, encompassing coverage, latency, and memory consumption, affirm that FLUID-IoT can be seamlessly integrated with existing IoT platforms and offers adequate performance for daily IoT scenarios. An in-lab user study further supports that the framework is intuitive and user-friendly, requiring minimal training for efficient utilization.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {848},\nnumpages = {16},\nkeywords = {Access Control, IoT-Security, Multi-User IoT, UI distribution},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642288,\nauthor = {Chiang, Yi-Shyuan and Khan, Omar and Bates, Adam and Cobb, Camille},\ntitle = {More than just informed: The importance of consent facets in smart homes},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642288},\ndoi = {10.1145/3613904.3642288},\nabstract = {Data collection without proper consent is a growing concern as smart home devices gain prevalence. It is especially difficult to obtain consent from incidental users because they may be unaware or feel pressured to consent. To understand what appropriate consent means in smart homes, we conducted an online survey (N=360) covering 6 common consent facets: freely given, revertible, informed, enthusiastic, specific, and unburdensome. We study how these facets affect perceived acceptability of data collection and how users would allocate responsibility for obtaining consent. Our results show that all facets have meaningful impacts on perceived acceptability of data collection, and eroding freely given had the greatest impact. Device owners were considered the most responsible for obtaining consent. Based on these findings, we provide recommendations for users, device manufacturers, and policymakers to improve consent practices in smart homes, such as designing consent interfaces that prioritize multiple facets of consent.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {849},\nnumpages = {21},\nkeywords = {Consent, Data collection, Incidental users, Smart home},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641931,\nauthor = {Corbett, Eric and Dove, Graham},\ntitle = {Signs of the Smart City: Exploring the Limits and Opportunities of Transparency},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641931},\ndoi = {10.1145/3613904.3641931},\nabstract = {This paper reports on a research through design (RtD) inquiry into public perceptions of transparency of Internet of Things (IoT) sensors increasingly deployed within urban neighborhoods as part of smart city programs. In particular, we report on the results of three participatory design workshops during which 40 New York City residents used physical signage as a medium for materializing transparency concerns about several sensors. We found that people’s concerns went beyond making sensors more transparent but instead sought to reveal the technology’s interconnected social, political, and economic processes. Building from these findings, we highlight the opportunities to move from treating transparency as an object to treating it as an ongoing activity. We argue that this move opens opportunities for designers and policy-makers to provide meaningful and actionable transparency of smart cities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {850},\nnumpages = {14},\nkeywords = {Design, Internet of Things, Smart Cities, Transparency},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642591,\nauthor = {Marky, Karola and St\\\"{o}ver, Alina and Prange, Sarah and Bleck, Kira and Gerber, Paul and Zimmermann, Verena and M\\\"{u}ller, Florian and Alt, Florian and M\\\"{u}hlh\\\"{a}user, Max},\ntitle = {Decide Yourself or Delegate - User Preferences Regarding the Autonomy of Personal Privacy Assistants in Private IoT-Equipped Environments},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642591},\ndoi = {10.1145/3613904.3642591},\nabstract = {Personalized privacy assistants (PPAs) communicate privacy-related decisions of their users to Internet of Things (IoT) devices. There are different ways to implement PPAs by varying the degree of autonomy or decision model. This paper investigates user perceptions of PPA autonomy models and privacy profiles – archetypes of individual privacy needs – as a basis for PPA decisions in private environments (e.g., a friend’s home). We first explore how privacy profiles can be assigned to users and propose an assignment method. Next, we investigate user perceptions in 18 usage scenarios with varying contexts, data types and number of decisions in a study with 1126 participants. We found considerable differences between the profiles in settings with few decisions. If the number of decisions gets high (> 1/h), participants exclusively preferred fully autonomous PPAs. Finally, we discuss implications and recommendations for designing scalable PPAs that serve as privacy interfaces for future IoT devices.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {851},\nnumpages = {20},\nkeywords = {IoT, personal privacy assistance, privacy, privacy profiles},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642356,\nauthor = {Su, Yuning and Zhang, Tingyu and Feng, Jiuen and Shi, Yonghao and Yang, Xing-Dong and Wu, Te-Yen},\ntitle = {Tagnoo: Enabling Smart Room-Scale Environments with RFID-Augmented Plywood},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642356},\ndoi = {10.1145/3613904.3642356},\nabstract = {Tagnoo is a computational plywood augmented with RFID tags, aimed at empowering woodworkers to effortlessly create room-scale smart environments. Unlike existing solutions, Tagnoo does not necessitate technical expertise or disrupt established woodworking routines. This battery-free and cost-effective solution seamlessly integrates computation capabilities into plywood, while preserving its original appearance and functionality. In this paper, we explore various parameters that can influence Tagnoo’s sensing performance and woodworking compatibility through a series of experiments. Additionally, we demonstrate the construction of a small office environment, comprising a desk, chair, shelf, and floor, all crafted by an experienced woodworker using conventional tools such as a table saw and screws while adhering to established construction workflows. Our evaluation confirms that the smart environment can accurately recognize 18 daily objects and user activities, such as a user sitting on the floor or a glass lunchbox placed on the desk, with over 90\\% accuracy.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {852},\nnumpages = {18},\nkeywords = {RFID, computational material, smart environment},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642823,\nauthor = {Markert, Philipp and Lassak, Leona and Golla, Maximilian and D\\\"{u}rmuth, Markus},\ntitle = {Understanding Users' Interaction with Login Notifications},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642823},\ndoi = {10.1145/3613904.3642823},\nabstract = {Login notifications intend to inform users about sign-ins and help them protect their accounts from unauthorized access. Notifications are usually sent if a login deviates from previous ones, potentially indicating malicious activity. They contain information like the location, date, time, and device used to sign in. Users are challenged to verify whether they recognize the login (because it was them or someone they know) or to protect their account from unwanted access. In a user study, we explore users’ comprehension, reactions, and expectations of login notifications. We utilize two treatments to measure users’ behavior in response to notifications sent for a login they initiated or based on a malicious actor relying on statistical sign-in information. We find that users identify legitimate logins but need more support to halt malicious sign-ins. We discuss the identified problems and give recommendations for service providers to ensure usable and secure logins for everyone.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {853},\nnumpages = {17},\nkeywords = {authentication, email, notification, password change, risk-based authentication},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642866,\nauthor = {Xue, Xiao and Li, Xinyang and Jia, Boyang and Du, Jiachen and Fu, Xinyi},\ntitle = {Who Should Hold Control? Rethinking Empowerment in Home Automation among Cohabitants through the Lens of Co-Design},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642866},\ndoi = {10.1145/3613904.3642866},\nabstract = {Recent HCI research has highlighted home automation’s potential in providing residents with technology-enhanced domestic autonomy. However, in the cohabitation context, the prevalent solutionist paradigm of automated systems introduces challenges to non-experts, paradoxically marginalizing specific members. This paper reports a co-creation initiative involving cohabitants, exploring a new understanding of empowerment in home automation. Participants collaborated to construct Trigger-Action Program (TAP) schemes using card-based tools during workshops. Our findings showcase how cohabitants engaged in collective ideations and embodied different negotiation patterns, which reveals the significance of more perceptible and participatory design. We frame home automation as \"problematic co-design\", arguing the universal overlook of collaborative resources. Furthermore, we examine how automation systems act as obstacles and sources of empowerment through the co-design lens. The paper concludes with pragmatic recommendations for designers and researchers, emphasizing the need to foster contestability for cohabitants in the evolving home automation landscape.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {854},\nnumpages = {19},\nkeywords = {Card-Based Tool, Co-Design, Cohabitant, Empowerment, Home Automation, Smart Home},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642467,\nauthor = {Schmidt, Jacquelyn Q and Roy, Ariel and Kerkez, Branko},\ntitle = {“You can’t write down the logic”: Bringing smart technology into the water infrastructure control room},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642467},\ndoi = {10.1145/3613904.3642467},\nabstract = {Smart water systems, in which optimization algorithms autonomously control water infrastructure, are widely touted as a promising solution to urban water management and sanitation problems. Presently, however, adoption remains low. Even as automation and ubiquitous computing expand into consumer and civic applications, the operation of water infrastructure around the world remains stubbornly manual. We examine the barriers to adoption of smart water systems through a user study of control room operators managing a large urban sewer in Detroit, Michigan, USA. We find that limited operator trust, system complexity, high uncertainty in critical data sources, and structural barriers to regional cooperation are impeding smart technology adoption. In response, we introduce SewerTycoon, an interactive, model-based simulation tool, which we use as a scenario-based prototype. Discussion with operators on how the use of smart technologies can be expanded in the sewer system reveals opportunities for low risk testing of new technologies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {855},\nnumpages = {13},\nkeywords = {civic technology, control room, smart cities, user study, water infrastructure},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642387,\nauthor = {Lazaro Vasquez, Eldy S. and Alistar, Mirela and Devendorf, Laura and Rivera, Michael L.},\ntitle = {Desktop Biofibers Spinning: An Open-Source Machine for Exploring Biobased Fibers and Their Application Towards Sustainable Smart Textile Design},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642387},\ndoi = {10.1145/3613904.3642387},\nabstract = {Smart textiles combine electronics with traditional textile forms, showing great promise in creating soft and flexible interactive systems for human-computer interaction and robotics. However, they also present significant sustainability challenges as they merge two substantial waste streams: textiles and electronics. This paper contributes to sustainability efforts by focusing on the integration of biobased materials that are biodegradable, compostable, and recyclable in the design of smart textiles. We introduce a Desktop Biofibers Spinning Machine to enable smart textile innovators to explore biobased fibers (i.e., biofibers) and envision applications in sustainable smart textiles. We describe the machine’s design, a usage walkthrough, considerations for fiber spinning, and an exploration of various formulations to make gelatin biofibers. We provide several examples of biofibers integrated into smart textile applications. Finally, we discuss lessons learned from working with biofibers and the unique opportunities our machine brings to the fiber design space in HCI.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {856},\nnumpages = {18},\nkeywords = {biobased materials, digital fabrication, fibers, sustainability, sustainable smart textiles},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642718,\nauthor = {Zhu, Jingwen and Winagle, Lily and Kao, Hsin-Liu (Cindy)},\ntitle = {EcoThreads: Prototyping Biodegradable E-textiles Through Thread-based Fabrication},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642718},\ndoi = {10.1145/3613904.3642718},\nabstract = {We present EcoThreads, a sustainable e-textile prototyping approach for fabricating biodegradable functional threads. We synthesized two thread-based fabrication methods, wet spinning and thread coating, to fabricate functional threads from biomaterials or modify natural fiber to achieve conductive or interactive functionality. We built a wet spinning tool from a modified DIY syringe pump to spin biodegradable conductive threads. The conductive and interactive threads can be further integrated into textiles through weaving, knitting, embroidery, and braiding. We conducted a workshop study inviting e-textile practitioners to use the materials to fabricate e-textile swatches for transient use cases. The EcoThreads approach presents a path for individual creators to incorporate biodegradable material choices toward sustainable e-textile practices.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {857},\nnumpages = {17},\nkeywords = {bio-design, e-textiles, sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642338,\nauthor = {Lakhdhir, Sabrina and Perin, Charles and Somanath, Sowmya},\ntitle = {Expressive Clothing: Understanding Hobbyist-Sewers' Visions for Self-Expression Through Clothing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642338},\ndoi = {10.1145/3613904.3642338},\nabstract = {Researchers have found that hobbyist-sewers seek to create new or adapted clothing designs that foster self-expression through communicating ideas, opinions and emotions. Although existing sewing technologies enable designing new patterns, they focus only on the technical aspects of pattern drafting and not on how information can be expressed. To address this gap, we conducted a qualitative diary study with 12 hobbyist-sewers to better understand how they envision creating expressive clothing. From our analysis of the 24 expressive clothing sketches participants created and participant interviews, we identified i) five distinctive multifaceted approaches participants used for self-expression; and ii) four challenges participants identified from their design process. Informed by these insights, we present a set of implications for the design of future technologies that can better support hobbyist-sewers in designing and creating expressive clothing.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {858},\nnumpages = {17},\nkeywords = {communication, design, e-textiles, fashion and clothing, hobbyist-sewers, self-expression, storytelling, tools for storytelling},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642759,\nauthor = {Peng, Yuecheng and Yan, Danchang and Chen, Haotian and Yang, Yue and Tao, Ye and Song, Weitao and Sun, Lingyun and Wang, Guanyun},\ntitle = {IntelliTex: Fabricating Low-cost and Washable Functional Textiles using A Double-coating Process},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642759},\ndoi = {10.1145/3613904.3642759},\nabstract = {We present IntelliTex, a low-cost and highly accessible double-coating fabrication method for washable and reusable functional textiles with customized input functionalities. Specifically, off-the-shelf textiles are firstly coated with conductive carbon black using pen ink, which endows textiles with rich sensing capabilities, such as pressure, stretch, slide, and temperature. Secondly, textiles are coated with polyurethane to enhance the sensing stability over wash cycles for good reusability. To support user customization, we enrich the design space of double-coating by exploring various coating methods and diverse textiles to be coated. We further contribute a comprehensive library of input components and an online document to make our approach accessible to novice users. Finally, five application examples and a user study showcase the versatile functionalities and user accessibility of our method, with which we hope to support designers, makers, and researchers to easily create functional textiles ready to use in everyday life.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {859},\nnumpages = {18},\nkeywords = {E-Textile, Personal Fabrication, Textile Coatings, Textile Sensors, Wearables},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642799,\nauthor = {Twigg-Smith, Hannah and Whiting, Emily and Peek, Nadya},\ntitle = {KnitScape: Computational Design and Yarn-Level Simulation of Slip and Tuck Colorwork Knitting Patterns},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642799},\ndoi = {10.1145/3613904.3642799},\nabstract = {Slipped and tucked stitches introduce small areas of deformation that compound and result in emergent textures on knitted fabrics. When used together with color changes and ladders, these can also produce dramatic colorwork and openwork effects. However, designing slip and tuck colorwork patterns is challenging due to the complex interactions between operations, yarns, and deformations. We present KnitScape, a browser-based tool for design and simulation of stitch patterns for knitting. KnitScape provides a design interface to specify 1) operation repeats, 2) color changes, and 3) needle positions. These inputs are used to build a graph of yarn topology and run a yarn-level spring simulation. This enables visualization of the deformation that arises from slip and tuck operations. Through its design tool and simulation, KnitScape enables rapid exploration of a complex colorwork design space. We demonstrate KnitScape with a series of example swatches.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {860},\nnumpages = {20},\nkeywords = {Colorwork, Design Tools, Digital Fabrication, Knitting Patterns, Machine Knitting, Yarn Simulation, Yarn Topology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642920,\nauthor = {Chen, Weijen and Yang, Yang and Liu, Kao-Hua and Pai, Yun Suen and Yamaoka, Junichi and Minamizawa, Kouta},\ntitle = {Cymatics Cup: Shape-Changing Drinks by Leveraging Cymatics},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642920},\ndoi = {10.1145/3613904.3642920},\nabstract = {To enhance the dining experience, prior studies in Human-Computer Interaction (HCI) and gastrophysics have demonstrated that modifying the static shape of solid foods can amplify taste perception. However, the exploration of dynamic shape-changing mechanisms in liquid foods remains largely untapped. In the present study, we employ cymatics, a scientific discipline focused on utilizing sound frequencies to generate patterns in liquids and particles—to augment the drinking experience. Utilizing speakers, we dynamically reshaped liquids exhibiting five distinct taste profiles and evaluated resultant changes in taste perception and drinking experience. Our research objectives extend beyond merely augmenting taste from visual to tactile sensations; we also prioritize the experiential aspects of drinking. Through a series of experiments and workshops, we revealed a significant impact on taste perception and overall drinking experience when mediated by cymatics effects. Building upon these findings, we designed and developed tableware to integrate cymatics principles into gastronomic experiences.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {861},\nnumpages = {19},\nkeywords = {crossmodal correspondences, cymatics, food, gastrophysics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642528,\nauthor = {Aigner, Roland and Haberfellner, Mira Alida and Haller, Michael},\ntitle = {Loopsense: low-scale, unobtrusive, and minimally invasive knitted force sensors for multi-modal input, enabled by selective loop-meshing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642528},\ndoi = {10.1145/3613904.3642528},\nabstract = {Integrating sensors into knitted input devices traditionally comes with considerable constraints for textile and UI design freedom. In this work, we demonstrate a novel, minimally invasive method for fabricating knitted sensors that overcomes this limitation. We integrate copper wire with piezoresistive enamel directly into the fabric using weft knitting to establish strain and pressure sensing cells that consist only of single pairs of intermeshed loops. The result is unobtrusive and potentially invisible, which provides tremendous latitude for visual and haptic design. Furthermore, we present several variations of stitch compositions, resulting in loop meshes that feature distinct response with respect to direction of exerting force. Utilizing this property, we are able to infer actuation modalities and considerably expand the device’s input space. In particular, we discern strain directions and surface pressure. Moreover, we provide an in-depth description of our fabrication method, and demonstrate our solution’s versatility on three exemplary use cases.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {862},\nnumpages = {17},\nkeywords = {e-textiles, fabrication, force sensor, knitting, resistive sensor, smart textiles, textile interface},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642058,\nauthor = {Yasu, Kentaro},\ntitle = {MagneSwift: Low-Cost, Interactive Shape Display Leveraging Magnetic Materials},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642058},\ndoi = {10.1145/3613904.3642058},\nabstract = {Pin-based shape displays present shapes and motion by moving arrays of pins. However, using many linear actuators to achieve this inevitably increases the size and cost of the device. MagneShape instead uses magnetic force to control the levitation height of passive magnetic pins to display shape and motion. While it is simple and inexpensive, MagneShape offers only limited interactivity. Since a certain distance has to be maintained between the magnetic pins to avoid magnetic interference arising between them, MagneShape requires appropriate magnetic patterns and time-consuming magnetization processes to display characters properly. To address this limitation, we improved the configuration of the magnetic pins and developed MagneSwift, a magnetic belt conveyor system with a high-density pin array. When a hand-drawn magnetic pattern is conveyed under the high-density pin array, the drawn pattern is presented on the pin array. We also demonstrate several interactive applications and discuss future possibilities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {863},\nnumpages = {11},\nkeywords = {Pin-based shape display, magnet, shape display},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642246,\nauthor = {Bell, Fiona and Mcclure, Erin and Friedman-Gerlicz, Camila and Ta, Ruby and Buechley, Leah},\ntitle = {Shape-Changing Clay-Dough: Taking a Material-Oriented Approach to 3D Printing Ceramic Forms},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642246},\ndoi = {10.1145/3613904.3642246},\nabstract = {This paper presents clay-dough, a 3D printable ceramic material that is made from a mixture of stoneware clay and a biomaterial dough. While all clays shrink when they are fired at high temperatures, clay-dough enables more dramatic shrinkage due to the dough burning away. We developed three clay-dough recipes made from different ratios of clay-to-dough and characterized the properties of each recipe; ultimately correlating shrinkage, density, strength, and porosity to the amount of dough in the recipe. We then leveraged clay-dough’s shrinkage in our material-oriented approach to create ceramic forms, where form is dictated by the pattern we load the clay-dough materials in for 3D printing. To exemplify this approach, we built a design space around basic cylindrical forms that change shape during the firing process into more complex forms and explored a range of non-cylindrical applications. Lastly, we reflect on the limitations and opportunities for clay-dough and material-centered research.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {864},\nnumpages = {19},\nkeywords = {4D Printing, Biomaterials, Ceramics, Clay 3D Printing, Digital Fabrication, Material-Driven Design, Shape-Changing Interfaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642373,\nauthor = {Wu, Di and Guan, Emily and Zhang, Yunjia and Lai, Hsuanju and Lu, Qiuyu and Yao, Lining},\ntitle = {Waxpaper Actuator: Sequentially and Conditionally Programmable Wax Paper for Morphing Interfaces},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642373},\ndoi = {10.1145/3613904.3642373},\nabstract = {We print wax on the paper and turn the composite into a sequentially-controllable, moisture-triggered, rapidly-fabricated, and low-cost shape-changing interface. This technique relies on a sequential control method that harnesses two critical variables: gray levels and water amount. By integrating these variables within a bilayer structure, composed of a paper substrate and wax layer, we produce a diverse wax pattern using a solid inkjet printer. These patterns empower wax paper actuators with rapid control over sequential deformations, harnessing various bending degrees and response times, which helps to facilitate the potential of swift personal actuator customization. Our exploration encompasses the material mechanism, the sequential control method, fabrication procedures, primitive structures, and evaluations. Additionally, we introduce a user-friendly software tool for design and simulation. Lastly, we demonstrate our approach through applications across four domains: agricultural seeding, interactive toys and art, home decoration, and electrical control.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {865},\nnumpages = {16},\nkeywords = {morphing materials, programmable material, rapid fabrication., sequential control method, shape-changing interfaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642599,\nauthor = {Jonas, Rebecca M and De, Ankolika and Cotter, Kelley},\ntitle = {\"We happen to be different and different is not bad\": Designing for Intersectional Fat-Positive Information-Seeking},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642599},\ndoi = {10.1145/3613904.3642599},\nabstract = {Fat liberation is a social movement advocating for equal treatment of fat people, who are currently subjected to harmful stereotypes, harassment, discrimination at school and work, and medical mistreatment, and is an understudied movement in HCI research. Due to the social and legal acceptability of anti-fatness, many physical spaces, such as businesses and healthcare providers, are unsafe or inaccessible for fat people. We conducted three in-person and virtual participatory design workshops with fat liberationist organizers and community members (N = 15) to imagine fat positive technologies. Participants designed a system to help them find size-inclusive resources, services, and healthcare providers in the offline world with design features centered around intersectionality, and participants’ desire for technologies that recognized their diverse identities and characteristics. We present features and values for a fat-positive information-seeking system and synthesize present and historical HCI theories into a design framework for intersectional fat-positive HCI.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {866},\nnumpages = {17},\nkeywords = {Fat liberation, anti-fatness, intersectionality, participatory design, social justice, social movements},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642509,\nauthor = {Tanni, Tangila Islam and Akter, Mamtaj and Anderson, Joshua and Amon, Mary Jean and Wisniewski, Pamela J.},\ntitle = {Examining the Unique Online Risk Experiences and Mental Health Outcomes of LGBTQ+ versus Heterosexual Youth},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642509},\ndoi = {10.1145/3613904.3642509},\nabstract = {We collected and analyzed Instagram direct messages (DMs) from 173 youth aged 13–21 (including 86 LGBTQ+ youth). We examined youth’s risk-flagged social media trace data with their self-reported mental health outcomes to examine how the differing online experiences of LGBTQ+ youth compare with their heterosexual counterparts. We found that LGBTQ+ youth experienced significantly more high-risk online interactions compared to heterosexual youth. LGBTQ+ youth reported overall poorer mental health, with online harassment specifically amplifying Self-Harm and Injury. LGBTQ+ youth’s mental well-being linked positively to sexual messages, unlike heterosexual youth. Qualitatively, we found that most of the risk-flagged messages of LGBTQ+ youth were sexually motivated; however, a silver lining was that they sought support for their sexual identity from peers on the platform. The study highlights the importance of tailored online safety and inclusive design for LGBTQ+ youth, with implications for CHI community advancements in fostering a supportive online environments.Content Warning: This research discusses sensitive topics, including explicit sexual content, abusive language, and homophobic slurs, which may cause discomfort. Reader discretion is advised.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {867},\nnumpages = {21},\nkeywords = {Instagram, LGBTQ, Mental Health, Online Risks, Online Safefy, Self-Harm, Sexual Risk, Social Media, Youth},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642688,\nauthor = {Wu, Qunfang and Romero, Tayara and Semaan, Bryan},\ntitle = {Negotiating Sociotechnical Boundaries: Moderation Work to Counter Racist Attacks in Online Communities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642688},\ndoi = {10.1145/3613904.3642688},\nabstract = {Online communities are susceptible to racist attacks, even when community policies explicitly prohibit racism. Drawing on the concept of symbolic boundary, we explored how community members sustained their communities against the perpetuation of racist logics and practices on Reddit. We drew on trace ethnography to analyze conversations about crime in two city subreddits (i.e., r/baltimore and r/chicago). The findings illustrate that the fragility of community boundaries was contributed by race baiting posts, covert racism, and racist brigading. At the same time, our research highlights that moderation efforts maintained and established institutional, cultural, and geographical boundaries to combat racist attacks. We discuss boundary as a design technique for building safe spaces for community members. Content warning: This work contains racist quotes that can upset or harm some readers.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {868},\nnumpages = {15},\nkeywords = {Boundary, Reddit, crime, moderation, new racism, platformed racism, trace ethnography},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642893,\nauthor = {Park, Seora and Lim, Hajin and Lee, Joonhwan},\ntitle = {“Some Hope, Many Despair”: Experiences of the Normalization within Online Dating among Queer Women in a Closeted Society},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642893},\ndoi = {10.1145/3613904.3642893},\nabstract = {Online dating technology mediates various social interactions for LGBTQ+ communities, yet how such technology shapes queerness remains understudied, particularly within queer women’s communities in non-Western settings. To address this gap, we conducted a qualitative study with 17 queer women, aiming to uncover their experiences and challenges in online dating within the conservative context of South Korea. Contrary to their initial expectations of exploring open-ended forms of interaction, we found that dating applications tended to systematically normalize queerness in sexuality presentation, relationship building, and shared identities in the community. These mechanisms forced them to conform to the “normalized queerness,” thereby impeding non-normative and flexible aspects of queer interactions. Building upon these findings, we discuss how the technological affordances of online dating platforms facilitate the normalization of queerness under the influence of sociocultural contexts of South Korea.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {869},\nnumpages = {15},\nkeywords = {Normalization, Online dating, Queer women, Queerness, Technology-mediated queer interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642392,\nauthor = {Moon, Erina Seh-Young and Guha, Shion},\ntitle = {A Human-Centered Review of Algorithms in Homelessness Research},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642392},\ndoi = {10.1145/3613904.3642392},\nabstract = {Homelessness is a humanitarian challenge affecting an estimated 1.6 billion people worldwide. In the face of rising homeless populations in developed nations and a strain on social services, government agencies are increasingly adopting data-driven models to determine one’s risk of experiencing homelessness and assigning scarce resources to those in need. We conducted a systematic literature review of 57 papers to understand the evolution of these decision-making algorithms. We investigated trends in computational methods, predictor variables, and target outcomes used to develop the models using a human-centered lens and found that only 9 papers (15.7\\%) investigated model fairness and bias. We uncovered tensions between explainability and ecological validity wherein predictive risk models (53.4\\%) unduly focused on reductive explainability while resource allocation models (25.9\\%) were dependent on unrealistic assumptions and simulated data that are not useful in practice. Further, we discuss research challenges and opportunities for developing human-centered algorithms in this area.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {870},\nnumpages = {15},\nkeywords = {algorithmic bias, algorithmic decision-making, homelessness, public sector, risk assessments},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642895,\nauthor = {Martinez, Richard and Squire, Kurt},\ntitle = {Engaging recently incarcerated and gang affiliated Black and Latino/a young adults in designing social collocated applications for mixed reality smart glasses through community-based participatory design workshops.},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642895},\ndoi = {10.1145/3613904.3642895},\nabstract = {Involving Black and Latina/o communities early and often in emerging technology design can make innovation more democratic, address bias, and reduce harm against these marginalized groups. To the best of our knowledge, no work has examined how recently incarcerated and gang affiliated young adults conceptualize mixed reality (MR) use for social collocated scenarios based on their everyday interactions and meaning-making. To explore this topic, we used a design-based implementation research (DBIR) and community-based participatory design (CBPD) approach to elicit social-technical insights grounded in the personal and critical perspectives of these youth. We find participants frequently grounded design ideas as embodied design elements to surface intangible and invisible qualities such as emotions and reflections on lived experiences, namely criticizing institutional structures that have maintained exclusionary practices against them. We discuss how DBIR and CBPD can uncover larger societal issues impacting marginalized communities through emerging technology design, and we contribute design recommendations for social collocated interactions in MR.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {871},\nnumpages = {17},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642482,\nauthor = {Ma, Zilin and Mei, Yiyang and Long, Yinru and Su, Zhaoyuan and Gajos, Krzysztof Z.},\ntitle = {Evaluating the Experience of LGBTQ+ People Using Large Language Model Based Chatbots for Mental Health Support},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642482},\ndoi = {10.1145/3613904.3642482},\nabstract = {LGBTQ+ individuals are increasingly turning to chatbots powered by large language models (LLMs) to meet their mental health needs. However, little research has explored whether these chatbots can adequately and safely provide tailored support for this demographic. We interviewed 18 LGBTQ+ and 13 non-LGBTQ+ participants about their experiences with LLM-based chatbots for mental health needs. LGBTQ+ participants relied on these chatbots for mental health support, likely due to an absence of support in real life. Notably, while LLMs offer prompt support, they frequently fall short in grasping the nuances of LGBTQ-specific challenges. Although fine-tuning LLMs to address LGBTQ+ needs can be a step in the right direction, it isn’t the panacea. The deeper issue is entrenched in societal discrimination. Consequently, we call on future researchers and designers to look beyond mere technical refinements and advocate for holistic strategies that confront and counteract the societal biases burdening the LGBTQ+ community.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {872},\nnumpages = {15},\nkeywords = {Chatbot, Gender, Identity, LGBTQIA+ Health, Large Language Models, Mental health, Socio-technical AI, Stigma},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642789,\nauthor = {Kouaho, Whitney-Jocelyn and Epstein, Daniel A.},\ntitle = {Socioeconomic Class in Physical Activity Wearables Research and Design},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642789},\ndoi = {10.1145/3613904.3642789},\nabstract = {Wearable technology for physical activity promotion is a frequent research topic within HCI and health, and researchers have documented that much of our knowledge is sourced from understanding the needs of populations from college educated, racially privileged, Western backgrounds. However socioeconomic class, a core component for how people perceive physical activity, wearables, and even wearable studies, has not often been contended with. In this critical discussion of the literature, incorporating examples from over 30 deployment studies involving wearables and over 70 other related works, we investigate how socioeconomic class shows up in study design and identify how class cultures are embedded in the design of wearable technology. We hypothesize that common study components related to time and activity type engenders high SES class cultures and ultimately risk creating intervention generated inequalities. We discuss the implications of ignoring class such as further perpetuating inequities in subsequent waves of wearable device maturity.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {873},\nnumpages = {15},\nkeywords = {personal informatics, physical activity, socioeconomic class, wearables},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642962,\nauthor = {Haque, MD Romael and Franco, Zeno and Madiraju, Praveen and Baker, Natalie D and Ahamed, Sheikh Iqbal and Winstead, Otis and Curry, Robert and Rubya, Sabirat},\ntitle = {\"Butt call me once you get a chance to chat 🙂\" : Designing Persuasive Reminders for Veterans to Facilitate Peer-Mentor Support},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642962},\ndoi = {10.1145/3613904.3642962},\nabstract = {US military veterans (USMVs) are a vulnerable population with an elevated risk of mental health issues and suicide. Peer support, especially through mobile technology, has proven effective in addressing mental health related challenges, but ensuring long-term engagement remains a concern. This study explores the opportunity of designing persuasive technology, particularly persuasive reminders, to enhance engagement in peer support interventions for veterans. We followed community-based participatory research with ten veterans to identify specific peer support processes that can benefit from persuasive reminders and to uncover the underlying community values and needs to guide design. The findings emphasize the importance of designing reminders that focus on personalized strategies, effective delivery of success stories, understanding motivation levels, careful language selection, actionable reminders, and mutual accountability. The study advocates context-specific design and highlights the need for a broader user-centered persuasion design perspective to cater to veterans’ unique needs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {874},\nnumpages = {17},\nkeywords = {Mental health, Participatory design workshop, Peer-support, Persuasive technology, U.S. military veteran},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642034,\nauthor = {Syed, Sara and Iftikhar, Zainab and Xiao, Amy Wei and Huang, Jeff},\ntitle = {Machine and Human Understanding of Empathy in Online Peer Support: A Cognitive Behavioral Approach},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642034},\ndoi = {10.1145/3613904.3642034},\nabstract = {Online peer support provides space for individuals to connect with others and seek support. However, while empathy is critical for effective support, studies have found that highly empathetic support on these platforms can be rare. Using data from online peer support platforms, we conducted a mixed-methods analysis to study the factors that lead to support seekers’ perceived empathy. We found that CBT techniques like active listening and reflective restatements, along with fostering a space for exploration, increase perceived empathy, whereas rigid adherence to structure, misalignment of concerns, and lack of emotional validation can contribute to low perceived empathy. In addition, despite the high levels of empathy reported by most support seekers (<Formula format=\"inline\"><TexMath><?TeX $85\\%$?></TexMath><AltText>Math 1</AltText><File name=\"chi24-145-inline1\" type=\"svg\"/></Formula>), computational models reported low averaged empathy (1.69/6). Lastly, we propose that empathy is not a quantifiable metric and that future algorithmic empathy measurements require human perspectives.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {875},\nnumpages = {13},\nkeywords = {cognitive behavioral therapy, computational psychology, deep learning, mental health, peer support, social computing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642617,\nauthor = {Varanasi, Rama Adithya and Dell, Nicola and Vashistha, Aditya},\ntitle = {Saharaline: A Collective Social Support Intervention for Teachers in Low-Income Indian Schools},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642617},\ndoi = {10.1145/3613904.3642617},\nabstract = {This paper presents Saharaline, an intervention designed to provide collective social support for teachers in low-income schools. Implemented as a WhatsApp-based helpline, Saharaline enables teachers to reach out for personalized, long-term assistance with a wide range of problems and stressors, including pedagogical, emotional, and technological challenges. Depending on the support needed, teachers’ requests are routed to appropriate domain experts— staff employed by educational non-profit organizations who understand teachers’ on-the-ground realities—who offer localized and contextualized assistance. Via a three-month exploratory deployment with 28 teachers in India, we show how Saharaline’s design enabled a collective of diverse education experts to craft and deliver localized solutions that teachers could incorporate into their practice. We conclude by reflecting on the efficacy of our intervention in low-resource work contexts and provide recommendations to enhance collective social support interventions similar to Saharaline.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {876},\nnumpages = {20},\nkeywords = {HCI4D, WhatsApp, collective social support, education, emotional support, informational support, low-cost intervention, low-income schools, non-profits, online support, problem focused support, social support, teachers},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642202,\nauthor = {Hwang, Angel Hsing-Chi and Won, Andrea Stevenson},\ntitle = {The Sound of Support: Gendered Voice Agent as Support to Minority Teammates in Gender-Imbalanced Team},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642202},\ndoi = {10.1145/3613904.3642202},\nabstract = {The present work explores the potential of leveraging a teamwork agent’s identity – signaled through its gendered voice – to support marginalized individuals in gender-imbalanced teams. In a mixed design experiment (N = 178), participants were randomly assigned to work with a female and a male voice agent in either a female-dominated or male-dominated team. Results show the presence of a same-gender voice agent is particularly beneficial to the performance of minority female members, such that they would contribute more ideas and talk more when a female agent was present. Conversely, minority male members became more talkative but were less focused on the teamwork tasks at hand when working with a male-sounding agent. The findings of the present experiment support existing literature on the effect of social presence in gender-imbalanced teams, such that gendered agents serve similar benefits as human teammates of the same gender identities. However, the effect of agents’ presence remains limited when participants have experienced severe marginalization in the past. Based on findings from the present study, we discuss relevant design implications and avenues for future research.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {877},\nnumpages = {22},\nkeywords = {Human-Agent Teamwork, Majority, Marginalization, Minority, Voice Agent},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642247,\nauthor = {Reinmund, Tyler and Kunze, Lars and Jirotka, Marina Denise},\ntitle = {Transitioning Towards a Proactive Practice: A Longitudinal Field Study on the Implementation of a ML System in Adult Social Care},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642247},\ndoi = {10.1145/3613904.3642247},\nabstract = {Politicians and care associations advocate for the use of machine learning (ML) systems to improve the delivery of adult social services. Yet, guidance on how to implement ML systems remains limited and research indicates that future implementation efforts are likely to encounter difficulties. We aim to enhance the understanding of ML system implementations by conducting a longitudinal field study with a team responsible for deploying a ML system within an adult social services department. The ML system implementation represented a cross-organisational effort to facilitate the department’s transition to a proactive practice. Throughout this process, stakeholders adapted to numerous challenges in real-time. This study makes three contributions. First, we provide a description of how ML systems are implemented and highlight practical challenges. Second, we illustrate the utility of HCI knowledge in designing workflows for ML-assisted preventative care programmes. Finally, we provide recommendations for future deployments of ML systems in social care.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {878},\nnumpages = {19},\nkeywords = {adult social care, field study, implementation challenges, machine learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642900,\nauthor = {Wenzel, Kimi and Kaufman, Geoff},\ntitle = {Designing for Harm Reduction: Communication Repair for Multicultural Users' Voice Interactions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642900},\ndoi = {10.1145/3613904.3642900},\nabstract = {Voice assistants’ inability to serve people-of-color and non-native English speakers has largely been documented as a quality-of-service harm. However, little work has investigated what downstream harms propagate from this poor service. How does poor usability materially manifest and affect users’ lives? And what interaction designs might help users recover from these effects? We identify 6 downstream harms that propagate from quality-of-service harms in voice assistants. Through interviews and design activities with 16 multicultural participants, we unveil these 6 harms, outline how multicultural users uniquely personify their voice assistant, and suggest how these harms and personifications may affect their interactions. Lastly, we employ techniques from psychology on communication repair to contribute suggestions for harm-reducing repair that may be implemented in voice technologies. Our communication repair strategies include: identity affirmations (intermittent frequency), cultural sensitivity, and blame redirection. This work shows potential for a harm-repair framework to positively influence voice interactions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {879},\nnumpages = {17},\nkeywords = {Automated Speech Recognition, Communication Breakdown, Communication Repair, Conversational Repair, Conversational User Interface, Harm, Harm-Reduction, Language Technology, Multiculture, Multilingual, Voice Assistants},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642279,\nauthor = {Guntrum, Laura Gianna},\ntitle = {Keyboard Fighters: The Use of ICTs by Activists in Times of Military Coup in Myanmar},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642279},\ndoi = {10.1145/3613904.3642279},\nabstract = {Amidst the ongoing anti-military protests in Myanmar since 2021, there is a noticeable research gap on ICT-supported activism. Generally, ICTs play an important role during political crises in conjunction with activists’ practices on the ground. Inspired by Resource Mobilization Theory, I conducted qualitative interviews (N=16) and a qualitative online survey (N=34), which demonstrate the intersection between analog and digital domains, showcasing the ingenuity of the activists, and the rapid adoption of ICTs in a country that has experienced a digital revolution within the last few years. As not all people were able to protest on-the-ground, they acted as keyboard fighters to organize protests, to share information, and to support the civil disobedience movement in Myanmar. The study identifies, inter alia, the need for better offline applications with wider coverage in times of internet shutdowns, applications that cannot be easily identified during physical controls, and providing free and secure VPN access.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {880},\nnumpages = {19},\nkeywords = {ICT-enabled activism, Myanmar coup, digital rights, internet shutdown, protest participation, social media, social movement},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642194,\nauthor = {Deng, Yue and Chen, Zheng and He, Changyang and Lu, Zhicong and Li, Bo},\ntitle = {Persuasion or Insulting? Unpacking Discursive Strategies of Gender Debate in Everyday Feminism in China},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642194},\ndoi = {10.1145/3613904.3642194},\nabstract = {Speaking out for women’s daily needs on social media has become a crucial form of everyday feminism in China. Gender debate naturally intertwines with such feminist advocacy, where users in opposite stances discuss gender-related issues through intense discourse. The complexities of gender debate necessitate a systematic understanding of discursive strategies for achieving effective gender communication that balances civility and constructiveness. To address this problem, we adopted a mixed-methods study to navigate discursive strategies in gender debate, focusing on 38,636 posts and 187,539 comments from two representative cases in China. Through open coding, we identified a comprehensive taxonomy of linguistic strategies in gender debate, capturing five overarching themes including derogation, gender distinction, intensification, mitigation, and cognizance guidance. Further, we applied regression analysis to unveil these strategies’ correlations with user participation and response, illustrating the tension between debating tactics and public engagement. We discuss design implications to facilitate feminist advocacy on social media. Content Warning: This paper contains discussions on gender debate that may include swear words and sensitive topics, such as sex, potentially causing discomfort.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {881},\nnumpages = {19},\nkeywords = {discursive strategies, everyday feminism, feminist HCI, feminist advocacy, gender debate, social media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642337,\nauthor = {Rubambiza, Gloire and Sengers, Phoebe and Weatherspoon, Hakim and Liu, Jen},\ntitle = {Seam Work and Simulacra of Societal Impact in Networking Research: A Critical Technical Practice Approach},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642337},\ndoi = {10.1145/3613904.3642337},\nabstract = {This paper explores how conceptions of societal impact are produced and performed during academic computer science research, by leveraging critical technical practice while building a digital agriculture networking platform. Our findings reveal how everyday practices of envisioning and building infrastructure require working across disciplinary and institutional seams, leading us as computer scientists to continuously reconceptualize the intended societal impact. By self-reflectively analyzing how we accrue resources for projects, produce research systems, write about them, and maintain alignments with stakeholders, we demonstrate that this seam work produces shifting simulacra of societal impact around which the system’s success is narrated. HCI researchers frequently suggest that technical systems’ impact could be improved by motivating computer scientists to consider impact in system-building. Our findings show that institutional and disciplinary structures significantly shape how computer scientists can enact societal impact in their work. This work suggests opportunities for structural interventions to shape the impact of computing systems.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {882},\nnumpages = {19},\nkeywords = {anticipation, critical technical practice, design for societal impact, networking, seamfulness, systems research, values in design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642892,\nauthor = {Noh, Hayoun and Yoon, Soohyun and Jo, Hyunah and Van Kleek, Max and Kang, Younah},\ntitle = {Starting a New Life after Crossing the Tumen River: How North Korean Defectors Use Digital Technology in Transition},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642892},\ndoi = {10.1145/3613904.3642892},\nabstract = {In a world where digital technology is omnipresent, North Korea stands as an outlier, with most citizens uninformed about its existence. This study explores the experiences of North Korean defectors as they transition to a highly digitally connected society—South Korea. Through 21 semi-structured interviews, we initially investigate the critical needs and challenges they encounter during their transitions. Then, we examine the role of digital technology as they adapt to the highly connected digital environment of South Korea. Our findings highlight that social media serves as a double-edged sword, providing the freedom to construct a desired identity while accentuating the gap between their real and ideal selves. This empirical research offers insights into how an underrepresented population navigates the digital landscape during life transitions, shedding light on the drawbacks and ways to better address their needs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {883},\nnumpages = {26},\nkeywords = {Empirical study that tells us about people, HCI for Development, Interview, Social Media/Online Communities},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642062,\nauthor = {Huang, Jeremy Zhengqi and Wood, Reyna and Chhabria, Hriday and Jain, Dhruv},\ntitle = {A Human-AI Collaborative Approach for Designing Sound Awareness Systems},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642062},\ndoi = {10.1145/3613904.3642062},\nabstract = {Current sound recognition systems for deaf and hard of hearing (DHH) people identify sound sources or discrete events. However, these systems do not distinguish similar sounding events (e.g., a patient monitor beep vs. a microwave beep). In this paper, we introduce HACS, a novel futuristic approach to designing human-AI sound awareness systems. HACS assigns AI models to identify sounds based on their characteristics (e.g., a beep) and prompts DHH users to use this information and their contextual knowledge (e.g., “I am in a kitchen”) to recognize sound events (e.g., a microwave). As a first step for implementing HACS, we articulated a sound taxonomy that classifies sounds based on sound characteristics using insights from a multi-phased research process with people of mixed hearing abilities. We then performed a qualitative (with 9 DHH people) and a quantitative (with a sound recognition model) evaluation. Findings demonstrate the initial promise of HACS for designing accurate and reliable human-AI systems.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {884},\nnumpages = {11},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642257,\nauthor = {Sch\\\"{u}tz, Laura and El Chemaly, Trishia and Weber, Emmanuelle and Doan, Anh Thien and Tsai, Jacqueline and Leuze, Christoph and Daniel, Bruce and Navab, Nassir},\ntitle = {Interactive Shape Sonification for Tumor Localization in Breast Cancer Surgery},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642257},\ndoi = {10.1145/3613904.3642257},\nabstract = {About 20 percent of patients undergoing breast-conserving surgery require reoperation due to cancerous tissue remaining inside the breast. Breast cancer localization systems utilize auditory feedback to convey the distance between a localization probe and a small marker (seed) implanted into the breast tumor prior to surgery. However, no information on the location of the tumor margin is provided. To reduce the reoperation rate by improving the usability and accuracy of the surgical task, we developed an auditory display using shape sonification to assist with tumor margin localization. Accuracy and usability of the interactive shape sonification were determined on models of the female breast in three user studies with both breast surgeons and non-clinical participants. The comparative studies showed a significant increase in usability (p<0.05) and localization accuracy (p<0.001) of the shape sonification over the auditory feedback currently used in surgery.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {885},\nnumpages = {15},\nkeywords = {Auditory display, Augmented reality, Breast cancer localization, Breast cancer surgery, Computer assisted interventions, Lumpectomy, Shape sonification, Sonification, Surgical sonification},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641940,\nauthor = {Yoo, MinYoung and Odom, William and Berger, Arne and Barnett, Samuel and Kenny, Sadhbh and Lo, Priscilla and Shamsher, Samein and Russell, Gillian and Knight, Lauren},\ntitle = {Remembering through Sound: Co-creating Sound-based Mementos together with People with Blindness},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641940},\ndoi = {10.1145/3613904.3641940},\nabstract = {Sound is a preferred and dominant medium that people with blindness use to capture, share and reflect on meaningful moments in their lives. Within the timeframe of 12 months, we worked with seven people with blindness and two of their sighted loved ones to engage in a multi-stage co-creative design process involving multiple steps building toward the final co-design workshop. We report three types of sonic mementos, designed together with the participants, that Encapsulate, Augment and Re-imagine personal audio recordings into more interesting and meaningful sonic memories. Building on these sonic mementos, we critically reflect and describe insights into designing sound that supports personal and social experiences of reminiscence for people with blindness through sound. We propose design opportunities to promote collective remembering between people with blindness and their sighted loved ones and design recommendations for remembering through sound.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {886},\nnumpages = {19},\nkeywords = {Co-creation, Co-design, People with Blindness, Reminiscence, Research through Design, Sonic memory, Sound},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642311,\nauthor = {Hassan, Waseem and Marzo, Asier and Hornb\\ae{}k, Kasper},\ntitle = {Using Low-frequency Sound to Create Non-contact Sensations On and In the Body},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642311},\ndoi = {10.1145/3613904.3642311},\nabstract = {This paper proposes a method for generating non-contact sensations using low-frequency sound waves without requiring user instrumentation. This method leverages the fundamental acoustic response of a confined space to produce predictable pressure spatial distributions at low frequencies, called modes. These modes can be used to produce sensations either throughout the body, in localized areas of the body, or within the body. We first validate the location and strength of the modes simulated by acoustic modeling. Next, a perceptual study is conducted to show how different frequencies produce qualitatively different sensations across and within the participants’ bodies. The low-frequency sound offers a new way of delivering non-contact sensations throughout the body. The results indicate a high accuracy for predicting sensations at specific body locations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {887},\nnumpages = {22},\nkeywords = {Vibrotactile feedback, low frequency sounds, midair, non-contact haptics, psychophysics, room modes.},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642710,\nauthor = {Hassan, Saad and De Lacerda Pataca, Calu\\~{a} and Nourian, Laleh and Tigwell, Garreth W. and Davis, Briana and Silver Wagman, Will Zhenya},\ntitle = {Designing and Evaluating an Advanced Dance Video Comprehension Tool with In-situ Move Identification Capabilities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642710},\ndoi = {10.1145/3613904.3642710},\nabstract = {Analyzing dance moves and routines is a foundational step in learning dance. Videos are often utilized at this step, and advancements in machine learning, particularly in human-movement recognition, could further assist dance learners. We developed and evaluated a Wizard-of-Oz prototype of a video comprehension tool that offers automatic in-situ dance move identification functionality. Our system design was informed by an interview study involving 12 dancers to understand the challenges they face when trying to comprehend complex dance videos and taking notes. Subsequently, we conducted a within-subject study with 8 Cuban salsa dancers to identify the benefits of our system compared to an existing traditional feature-based search system. We found that the quality of notes taken by participants improved when using our tool, and they reported a lower workload. Based on participants’ interactions with our system, we offer recommendations on how an AI-powered span-search feature can enhance dance video comprehension tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {888},\nnumpages = {19},\nkeywords = {Choreography, Dance, Dance Learning, Human Movement Analysis, Search Systems},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642346,\nauthor = {Liu, Shuqi and Bu, Jia and Ye, Huayuan and Chen, Juntong and Jiang, Shiqi and Tao, Mingtian and Guo, Liping and Wang, Changbo and Li, Chenhui},\ntitle = {DoodleTunes: Interactive Visual Analysis of Music-Inspired Children Doodles with Automated Feature Annotation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642346},\ndoi = {10.1145/3613904.3642346},\nabstract = {Music and visual arts are essential in children’s arts education, and their integration has garnered significant attention. Existing data analysis methods for exploring audio-visual correlations are limited. Yet, relevant research is necessary for innovating and promoting arts integration courses. In our work, we collected substantial volumes of music-inspired doodles created by children and interviewed education experts to comprehend the challenges they encountered in the relevant analysis. Based on the insights we obtained, we designed and constructed an interactive visualization system DoodleTunes. DoodleTunes integrates deep learning-driven methods for automatically annotating several types of data features. The visual designs of the system are based on a four-level analysis structure to construct a progressive workflow, facilitating data exploration and insight discovery between doodle images and corresponding music pieces. We evaluated the accuracy of our feature prediction results and collected usage feedback on DoodleTunes from five domain experts.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {889},\nnumpages = {19},\nkeywords = {Multimodal data visualization, audio data, data annotation, painting, user interface, visual analysis},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642677,\nauthor = {Trajkova, Milka and Long, Duri and Deshpande, Manoj and Knowlton, Andrea and Magerko, Brian},\ntitle = {Exploring Collaborative Movement Improvisation Towards the Design of LuminAI—a Co-Creative AI Dance Partner},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642677},\ndoi = {10.1145/3613904.3642677},\nabstract = {Co-creation in embodied contexts is central to the human experience but is often lacking in our interactions with computers. We seek to develop a better understanding of embodied human co-creativity to inform the human-centered design of machines that can co-create with us. In this paper, we ask: What characterizes dancers’ experiences of embodied dyadic interaction in movement improvisation? To answer this, we ran focus groups with 24 university dance students and conducted a thematic analysis of their responses. We synthesize our findings in an Interconnected Model of Improvisational Dance Inputs, where movement choices are shaped by the interplay between in-the-moment influences between the self, partner, and the environment, a set of generative strategies, and heuristics for a successful collaboration. We present a set of design recommendations for LuminAI, a co-creative AI dance partner. Our contributions can inform the design of AI in embodied co-creative domains.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {890},\nnumpages = {22},\nkeywords = {AI agents, co-creative agents, co-creativity, computational creativity, dance improvisation, movement improvisation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642533,\nauthor = {Xue, Jing and Montano Murillo, Roberto and Dawes, Christopher and Frier, William and Cornelio, Patricia and Obrist, Marianna},\ntitle = {FabSound: Audio-Tactile and Affective Fabric Experiences Through Mid-air Haptics},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642533},\ndoi = {10.1145/3613904.3642533},\nabstract = {The sound produced when touching fabrics, like a blanket, often provides information regarding the fabric’s texture properties (e.g., its roughness). Fabric roughness is one of the most important aspects of assessing fabric tactile properties. Prior research has demonstrated that touch-related sounds can alter the perception of textures. However, understanding touch-related sound of digital fabric textures, and how they could convey affective responses remain a challenge. In this study, we mapped digital fabric textures using mid-air haptics stimuli and examined how auditory manipulation influences people’s roughness perception. Through qualitative interviews, participants detailed that while rubbing sounds smoothen fabric texture perception, pure tone sounds of 450Hz and 900Hz accent roughness perception. The rubbing sound of fabric evoked associations with soft-materials and led to more calming experiences. In addition, we discussed how haptic interaction can be extended to multisensory modes, revealing a new perspective of mapping multisensory experiences for digital fabrics.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {891},\nnumpages = {17},\nkeywords = {Affective Haptic, Audio-tactile, Digital touch, Fabrics, Mid-air Haptic, Tactile experiences, Textile, Texture perception},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642434,\nauthor = {Portugal da Fonseca, Leonor and Nunes, Francisco and Silva, Paula Alexandra},\ntitle = {Understanding Feedback in Rhythmic Gymnastics Training: An Ethnographic-Informed Study of a Competition Class},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642434},\ndoi = {10.1145/3613904.3642434},\nabstract = {Rhythmic Gymnastics is an Olympic sport that demands an exceptional level of expertise. From early age, athletes relentlessly practise exercises until they can flawlessly perform them before an audience and a panel of judges. Technology can potentially support rhythmic gymnasts’ training by monitoring gymnasts’ exercises and providing feedback on their execution. However, the limited understanding of the training nuances in Rhythmic Gymnastics restricts the development of technologies to support training. Drawing on the observation of training sessions and on interviews with athletes and coaches, this paper uncovers how coaches personalise feedback timing, type, form, format, and quantity, to adapt it to the gymnasts’ skill level and type of exercise. Taking stock of our findings, we draw out five implications that can inform the design of systems to support feedback in Rhythmic Gymnastics training.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {892},\nnumpages = {16},\nkeywords = {Dance, Ethnographic, Feedback, Fieldwork, Rhythmic Gymnastics, Sports, Training},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642314,\nauthor = {Joshi, Nikhita and Vogel, Daniel},\ntitle = {Constrained Highlighting in a Document Reader can Improve Reading Comprehension},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642314},\ndoi = {10.1145/3613904.3642314},\nabstract = {Highlighting text in a document is a common active reading strategy to remember information from documents. Learning theory suggests that for highlights to be effective, readers must be selective with what they choose to highlight. We investigate if an imposed user interface constraint limiting the number of highlighted words in a document reader can improve reading comprehension. A large-scale between-subjects experiment shows that constraining the number of words that can be highlighted leads to higher reading comprehension scores than highlighting nothing or highlighting an unlimited number of words. Our work empirically validates theories in psychology, which in turn enables several new research directions within HCI.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {893},\nnumpages = {10},\nkeywords = {constraints, controlled experiments, highlighting, reading},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642878,\nauthor = {Wang, Ru and Potter, Zach and Ho, Yun and Killough, Daniel and Zeng, Linxiu and Mondal, Sanbrita and Zhao, Yuhang},\ntitle = {GazePrompt: Enhancing Low Vision People's Reading Experience with Gaze-Aware Augmentations},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642878},\ndoi = {10.1145/3613904.3642878},\nabstract = {Reading is a challenging task for low vision people. While conventional low vision aids (e.g., magnification) offer certain support, they cannot fully address the difficulties faced by low vision users, such as locating the next line and distinguishing similar words. To fill this gap, we present GazePrompt, a gaze-aware reading aid that provides timely and targeted visual and audio augmentations based on users’ gaze behaviors. GazePrompt includes two key features: (1) a Line-Switching support that highlights the line a reader intends to read; and (2) a Difficult-Word support that magnifies or reads aloud a word that the reader hesitates with. Through a study with 13 low vision participants who performed well-controlled reading-aloud tasks with and without GazePrompt, we found that GazePrompt significantly reduced participants’ line switching time, reduced word recognition errors, and improved their subjective reading experiences. A follow-up silent-reading study showed that GazePrompt can enhance users’ concentration and perceived comprehension of the reading contents. We further derive design considerations for future gaze-based low vision aids.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {894},\nnumpages = {17},\nkeywords = {Accessibility, eye tracking, low vision, reading, visual augmentation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642839,\nauthor = {Van Daele, Tess and Iyer, Akhil and Zhang, Yuning and Derry, Jalyn C and Huh, Mina and Pavel, Amy},\ntitle = {Making Short-Form Videos Accessible with Hierarchical Video Summaries},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642839},\ndoi = {10.1145/3613904.3642839},\nabstract = {Short videos on platforms such as TikTok, Instagram Reels, and YouTube Shorts (i.e. short-form videos) have become a primary source of information and entertainment. Many short-form videos are inaccessible to blind and low vision (BLV) viewers due to their rapid visual changes, on-screen text, and music or meme-audio overlays. In our formative study, 7 BLV viewers who regularly watched short-form videos reported frequently skipping such inaccessible content. We present  ShortScribe, a system that provides hierarchical visual summaries of short-form videos at three levels of detail to support BLV viewers in selecting and understanding short-form videos. ShortScribe allows BLV users to navigate between video descriptions based on their level of interest. To evaluate  ShortScribe, we assessed description accuracy and conducted a user study with 10 BLV participants comparing  ShortScribe to a baseline interface. When using ShortScribe, participants reported higher comprehension and provided more accurate summaries of video content.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {895},\nnumpages = {17},\nkeywords = {Accessibility, Short-Form Video, Summaries, Video Description},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642319,\nauthor = {Barnaby, Celeste and Chen, Qiaochu and Wang, Chenglong and Dillig, Isil},\ntitle = {PhotoScout: Synthesis-Powered Multi-Modal Image Search},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642319},\ndoi = {10.1145/3613904.3642319},\nabstract = {Due to the availability of increasingly large amounts of visual data, there is a growing need for tools that can help users find relevant images. While existing tools can perform image retrieval based on similarity or metadata, they fall short in scenarios that necessitate semantic reasoning about the content of the image. This paper explores a new multi-modal image search approach that allows users to conveniently specify and perform semantic image search tasks. With our tool, PhotoScout, the user interactively provides natural language descriptions, positive and negative examples, and object tags to specify their search tasks. Under the hood, PhotoScout is powered by a program synthesis engine that generates visual queries in a domain-specific language and executes the synthesized program to retrieve the desired images. In a study with 25 participants, we observed that PhotoScout allows users to perform image retrieval tasks more accurately and with less manual effort.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {896},\nnumpages = {15},\nkeywords = {Interface design, multi-modal interfaces, program synthesis},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642238,\nauthor = {Jiang, Lucy and Jung, Crescentia and Phutane, Mahika and Stangl, Abigale and Azenkot, Shiri},\ntitle = {“It’s Kind of Context Dependent”: Understanding Blind and Low Vision People’s Video Accessibility Preferences Across Viewing Scenarios},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642238},\ndoi = {10.1145/3613904.3642238},\nabstract = {While audio description (AD) is the standard approach for making videos accessible to blind and low vision (BLV) people, existing AD guidelines do not consider BLV users’ varied preferences across viewing scenarios. These scenarios range from how-to videos on YouTube, where users seek to learn new skills, to historical dramas on Netflix, where a user’s goal is entertainment. Additionally, the increase in video watching on mobile devices provides an opportunity to integrate nonverbal output modalities (e.g., audio cues, tactile elements, and visual enhancements). Through a formative survey and 15 semi-structured interviews, we identified BLV people’s video accessibility preferences across diverse scenarios. For example, participants valued action and equipment details for how-to videos, tactile graphics for learning scenarios, and 3D models for fantastical content. We define a six-dimensional video accessibility design space to guide future innovation and discuss how to move from “one-size-fits-all” paradigms to scenario-specific approaches.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {897},\nnumpages = {20},\nkeywords = {audio cue, audio description, blind, context-aware, low vision, scenario-based design, scenarios, tactile feedback, video accessibility},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642699,\nauthor = {Gu, Ziwei and Arawjo, Ian and Li, Kenneth and Kummerfeld, Jonathan K. and Glassman, Elena L.},\ntitle = {An AI-Resilient Text Rendering Technique for Reading and Skimming Documents},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642699},\ndoi = {10.1145/3613904.3642699},\nabstract = {Readers find text difficult to consume for many reasons. Summarization can address some of these difficulties, but introduce others, such as omitting, misrepresenting, or hallucinating information, which can be hard for a reader to notice. One approach to addressing this problem is to instead modify how the original text is rendered to make important information more salient. We introduce Grammar-Preserving Text Saliency Modulation (GP-TSM), a text rendering method with a novel means of identifying what to de-emphasize. Specifically, GP-TSM uses a recursive sentence compression method to identify successive levels of detail beyond the core meaning of a passage, which are de-emphasized by rendering words in successively lighter but still legible gray text. In a lab study (n=18), participants preferred GP-TSM over pre-existing word-level text rendering methods and were able to answer GRE reading comprehension questions more efficiently.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {898},\nnumpages = {22},\nkeywords = {human-AI interaction, natural language processing, text visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642258,\nauthor = {de Lacerda Pataca, Calu\\~{a} and Hassan, Saad and Tinker, Nathan and Peiris, Roshan Lalintha and Huenerfauth, Matt},\ntitle = {Caption Royale: Exploring the Design Space of Affective Captions from the Perspective of Deaf and Hard-of-Hearing Individuals},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642258},\ndoi = {10.1145/3613904.3642258},\nabstract = {Affective captions employ visual typographic modulations to convey a speaker’s emotions, improving speech accessibility for Deaf and Hard-of-Hearing (dhh) individuals. However, the most effective visual modulations for expressing emotions remain uncertain. Bridging this gap, we ran three studies with 39 dhh  participants, exploring the design space of affective captions, which include parameters like text color, boldness, size, and so on. Study 1 assessed preferences for nine of these styles, each conveying either valence or arousal separately. Study 2 combined Study 1’s top-performing styles and measured preferences for captions depicting both valence and arousal simultaneously. Participants outlined readability, minimal distraction, intuitiveness, and emotional clarity as key factors behind their choices. In Study 3, these factors and an emotion-recognition task were used to compare how Study 2’s winning styles performed versus a non-styled baseline. Based on our findings, we present the two best-performing styles as design recommendations for applications employing affective captions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {899},\nnumpages = {17},\nkeywords = {Accessibility, Accessibility for people who are Deaf and Hard-of-Hearing, Caption, Emotion},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642325,\nauthor = {Das, Maitraye and Fiannaca, Alexander J. and Morris, Meredith Ringel and Kane, Shaun K. and Bennett, Cynthia L.},\ntitle = {From Provenance to Aberrations: Image Creator and Screen Reader User Perspectives on Alt Text for AI-Generated Images},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642325},\ndoi = {10.1145/3613904.3642325},\nabstract = {AI-generated images are proliferating as a new visual medium. However, state-of-the-art image generation models do not output alternative (alt) text with their images, rendering them largely inaccessible to screen reader users (SRUs). Moreover, less is known about what information would be most desirable to SRUs in this new medium. To address this, we invited AI image creators and SRUs to evaluate alt text prepared from various sources and write their own alt text for AI images. Our mixed-methods analysis makes three contributions. First, we highlight creators’ perspectives on alt text, as creators are well-positioned to write descriptions of their images. Second, we illustrate SRUs’ alt text needs particular to the emerging medium of AI images. Finally, we discuss the promises and pitfalls of utilizing text prompts written as input for AI models in alt text generation, and areas where broader digital accessibility guidelines could expand to account for AI images.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {900},\nnumpages = {21},\nkeywords = {AI art, Accessibility, Alt text, Blind, Screen reader users, Text-to-Image},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642211,\nauthor = {Gonzalez Penuela, Ricardo E and Collins, Jazmin and Bennett, Cynthia and Azenkot, Shiri},\ntitle = {Investigating Use Cases of AI-Powered Scene Description Applications for Blind and Low Vision People},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642211},\ndoi = {10.1145/3613904.3642211},\nabstract = {\"Scene description\" applications that describe visual content in a photo are useful daily tools for blind and low vision (BLV) people. Researchers have studied their use, but they have only explored those that leverage remote sighted assistants; little is known about applications that use AI to generate their descriptions. Thus, to investigate their use cases, we conducted a two-week diary study where 16 BLV participants used an AI-powered scene description application we designed. Through their diary entries and follow-up interviews, users shared their information goals and assessments of the visual descriptions they received. We analyzed the entries and found frequent use cases, such as identifying visual features of known objects, and surprising ones, such as avoiding contact with dangerous objects. We also found users scored the descriptions relatively low on average, 2.76 out of 5 (SD=1.49) for satisfaction and 2.43 out of 4 (SD=1.16) for trust, showing that descriptions still need significant improvements to deliver satisfying and trustworthy experiences. We discuss future opportunities for AI as it becomes a more powerful accessibility tool for BLV users.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {901},\nnumpages = {21},\nkeywords = {AI, Assistive Technology, Blind and Low Vision People, Computer Vision, Diary Study, Scene Description, Use cases},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642632,\nauthor = {Ning, Zheng and Wimer, Brianna L and Jiang, Kaiwen and Chen, Keyi and Ban, Jerrick and Tian, Yapeng and Zhao, Yuhang and Li, Toby Jia-Jun},\ntitle = {SPICA: Interactive Video Content Exploration through Augmented Audio Descriptions for Blind or Low-Vision Viewers},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642632},\ndoi = {10.1145/3613904.3642632},\nabstract = {Blind or Low-Vision (BLV) users often rely on audio descriptions (AD) to access video content. However, conventional static ADs can leave out detailed information in videos, impose a high mental load, neglect the diverse needs and preferences of BLV users, and lack immersion. To tackle these challenges, we introduce Spica, an AI-powered system that enables BLV users to interactively explore video content. Informed by prior empirical studies on BLV video consumption, Spica offers interactive mechanisms for supporting temporal navigation of frame captions and spatial exploration of objects within key frames. Leveraging an audio-visual machine learning pipeline, Spica augments existing ADs by adding interactivity, spatial sound effects, and individual object descriptions without requiring additional human annotation. Through a user study with 14 BLV participants, we evaluated the usability and usefulness of Spica and explored user behaviors, preferences, and mental models when interacting with augmented ADs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {902},\nnumpages = {18},\nkeywords = {accessibility, audio description, video consumption},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642152,\nauthor = {Seo, Woosuk and Yang, Chanmo and Kim, Young-Ho},\ntitle = {ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642152},\ndoi = {10.1145/3613904.3642152},\nabstract = {Children typically learn to identify and express their emotions by sharing stories and feelings with others, particularly family members. However, it is challenging for parents or siblings to have effective emotion communication with children since children are still developing their communication skills. We present ChaCha, a chatbot that encourages and guides children to share personal events and associated emotions. ChaCha combines a state machine and large language models (LLMs) to keep the dialogue on track while carrying on free-form conversations. Through an exploratory study with 20 children (aged 8–12), we examine how ChaCha prompts children to share personal events and guides them to describe associated emotions. Participants perceived ChaCha as a close friend and shared their stories on various topics, such as family trips and personal achievements. Based on the findings, we discuss opportunities for leveraging LLMs to design child-friendly chatbots to support children in sharing emotions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {903},\nnumpages = {20},\nkeywords = {Chatbots, Children, Conversational Agents, Large Language Models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642297,\nauthor = {Mcdonald, Nora and Seberger, John S. and Razi, Afsaneh},\ntitle = {For Me or Not for Me? The Ease With Which Teens Navigate Accurate and Inaccurate Personalized Social Media Content},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642297},\ndoi = {10.1145/3613904.3642297},\nabstract = {Social media apps present personalized content to users. Such content is often described as “for you,” raising questions about the relationship between users’ sense of “self” and the “you” that is represented. Answering such questions is pressing in the case of teen users whose identities are still forming. Thus we ask, “What do teens think about the relationship between personalized content and their sense of self?” We interviewed teens aged 13 to 17 (n = 15) about their experiences with personalized content on social media. Participants so routinely saw themselves accurately reflected in personalized content that they noted the occasional inaccuracy with surprise, while simply scrolling past it. Our findings point to: the normalization of data doubles in the form of personalized content; and teens’ indifference to inaccuracies presented by such data doubles.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {904},\nnumpages = {7},\nkeywords = {Teens, algorithms, data doubles, identity, personalized content, privacy, social media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642264,\nauthor = {Hartwig, Katrin and Biselli, Tom and Schneider, Franziska and Reuter, Christian},\ntitle = {From Adolescents' Eyes: Assessing an Indicator-Based Intervention to Combat Misinformation on TikTok},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642264},\ndoi = {10.1145/3613904.3642264},\nabstract = {Misinformation poses a recurrent challenge for video-sharing platforms (VSPs) like TikTok. Obtaining user perspectives on digital interventions addressing the need for transparency (e.g., through indicators) is essential. This article offers a thorough examination of the comprehensibility, usefulness, and limitations of an indicator-based intervention from an adolescents’ perspective. This study (N = 39; aged 13-16 years) comprised two qualitative steps: (1) focus group discussions and (2) think-aloud sessions, where participants engaged with a smartphone-app for TikTok. The results offer new insights into how video-based indicators can assist adolescents’ assessments. The intervention received positive feedback, especially for its transparency, and could be applicable to new content. This paper sheds light on how adolescents are expected to be experts while also being prone to video-based misinformation, with limited understanding of an intervention’s limitations. By adopting teenagers’ perspectives, we contribute to HCI research and provide new insights into the chances and limitations of interventions for VSPs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {905},\nnumpages = {20},\nkeywords = {adolescents, disinformation, fake news, misinformation, social media, teenagers, user intervention, video-sharing platforms},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642044,\nauthor = {Liu, Lanjing and Zhang, Chao and Lu, Zhicong},\ntitle = {Wrist-bound Guanxi, Jiazu, and Kuolie: Unpacking Chinese Adolescent Smartwatch-Mediated Socialization},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642044},\ndoi = {10.1145/3613904.3642044},\nabstract = {Adolescent peer relationships, essential for their development, are increasingly mediated by digital technologies. As this trend continues, wearable devices, especially smartwatches tailored for adolescents, is reshaping their socialization. In China, smartwatches like XTC have gained wide popularity, introducing unique features such as “Bump-to-Connect” and exclusive social platforms. Nonetheless, how these devices influence adolescents’ peer experience remains unknown. Addressing this, we interviewed 18 Chinese adolescents (age: 11—16), discovering a smartwatch-mediated social ecosystem. Our findings highlight the ice-breaking role of smartwatches in friendship initiation and their use for secret messaging with local peers. Within the online smartwatch community, peer status is determined by likes and visibility, leading to diverse pursuit activities (i.e., chu guanxi, jiazu, kuolie) and negative social dynamics. We discuss the core affordances of smartwatches and Chinese cultural factors that influence adolescent social behavior, and offer implications for designing future wearables that responsibly and safely support adolescent socialization.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {906},\nnumpages = {21},\nkeywords = {Smartwatches, adolescents, computer-mediated communication, peer relations, social behaviors, social interaction, social networking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642690,\nauthor = {Reddy, Ananya and Kumar, Priya C.},\ntitle = {‘A Teaspoon of Authenticity’: Exploring How Young Adults BeReal on Social Media},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642690},\ndoi = {10.1145/3613904.3642690},\nabstract = {BeReal is the latest social media platform to tout itself as a more authentic space for connection. The app notifies users at a random time each day and gives users two minutes to post an image from their smartphone's front- and back-facing cameras. While prior work has theorized authenticity on social media and studied how various user populations enact authenticity, more research is needed to understand whether and how specific design features afford authenticity. We conducted a walkthrough of the BeReal app and interviewed 31 young adults about their experiences on BeReal. We found that participants approached authenticity in two ways—as extemporaneous interaction and as comprehensive self-presentation—and that BeReal harnesses the affordances of visibility, editability, availability, and persistence in a way that enables the former more than the latter. Based on our findings, we offer four recommendations for designers and researchers who seek to support authenticity online.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {907},\nnumpages = {14},\nkeywords = {BeReal, Social media, affordances, authenticity, college students, self-presentation, young adults},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642108,\nauthor = {Cai, Tianyuan and Niklaus, Aleena Gertrudes and Kerr, Bernard and Kraley, Michael and Bylinskii, Zoya},\ntitle = {COR Themes for Readability from Iterative Feedback},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642108},\ndoi = {10.1145/3613904.3642108},\nabstract = {Digital reading applications give readers the ability to customize fonts, sizes, and spacings, all of which have been shown to improve the reading experience for readers from different demographics. However, tweaking these text features can be challenging, especially given their interactions on the final look and feel of the text. Our solution is to offer readers preset combinations of font, character, word and line spacing, which we bundle together into reading themes. We identify a recommended set of reading themes through data-driven design iterations with the crowd and experts. We show that after four design iterations, we converge on a set of three COR themes (Compact, Open, and Relaxed) that meet diverse readers’ preferences, when evaluating the reading speeds, comprehension scores, and preferences of hundreds of readers with and without dyslexia, using crowdsourced experiments.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {908},\nnumpages = {23},\nkeywords = {crowdsourcing, human-in-the-loop, presets, readability, reading},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642110,\nauthor = {Qian, Xun and Tan, Feitong and Zhang, Yinda and Collins, Brian Moreno and Kim, David and Olwal, Alex and Ramani, Karthik and Du, Ruofei},\ntitle = {ChatDirector: Enhancing Video Conferencing with Space-Aware Scene Rendering and Speech-Driven Layout Transition},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642110},\ndoi = {10.1145/3613904.3642110},\nabstract = {Remote video conferencing systems (RVCS) are widely adopted in personal and professional communication. However, they often lack the co-presence experience of in-person meetings. This is largely due to the absence of intuitive visual cues and clear spatial relationships among remote participants, which can lead to speech interruptions and loss of attention. This paper presents ChatDirector, a novel RVCS that overcomes these limitations by incorporating space-aware visual presence and speech-aware attention transition assistance. ChatDirector employs a real-time pipeline that converts participants’ RGB video streams into 3D portrait avatars and renders them in a virtual 3D scene. We also contribute a decision tree algorithm that directs the avatar layouts and behaviors based on participants’ speech states. We report on results from a user study (N=16) where we evaluated ChatDirector. The satisfactory algorithm performance and complimentary subject user feedback imply that ChatDirector significantly enhances communication efficacy and user engagement.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {909},\nnumpages = {16},\nkeywords = {3D portrait avatar, attention transition, augmented communication, collaborative work, depth estimation, depth map, machine learning, tele-presence, video conferencing, video-mediated communication},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642080,\nauthor = {Fontana De Vargas, Mauricio and Yu, Christina and Shane, Howard C. and Moffatt, Karyn},\ntitle = {Co-Designing QuickPic: Automated Topic-Specific Communication Boards from Photographs for AAC-Based Language Instruction},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642080},\ndoi = {10.1145/3613904.3642080},\nabstract = {Traditional topic-specific communication boards for Augmentative and Alternative Communication (AAC) require manual programming of relevant symbolic vocabulary, which is time-consuming and often impractical even for experienced Speech-Language Pathologists (SLPs). While recent research has demonstrated the potential to automatically generate these boards from photographs using artificial intelligence, there has been no exploration on how to design such tools to support the specific needs of AAC-based language instruction. This paper introduces QuickPic, a mobile AAC application co-designed with SLPs and special educators, aimed at enhancing language learning for non-speaking individuals, such as autistic children. Through a 17-month design process, we uncover the unique design features required to provide timely language support in therapy and special education contexts. We present emerging evidence on the overall satisfaction of SLPs using QuickPic, and on the advantages of large language model-based generation compared to the existing technique for automated vocabulary from photographs for AAC.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {910},\nnumpages = {16},\nkeywords = {Augmentative and Alternative Communication, LLM, assistive technology, autism, just-in-time},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641911,\nauthor = {Sieghart, Sabina and Rohles, Bj\\\"{o}rn and Bessemans, Ann},\ntitle = {Empowering Independence Through Design: Investigating Standard Digital Design Patterns For Easy-to-Read Users.},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641911},\ndoi = {10.1145/3613904.3641911},\nabstract = {As designers and researchers, it is our duty to ensure information accessibility for all, irrespective of cognitive abilities. Currently, Easy-to-Read (ETR) is commonly used to simplify text for individuals with cognitive impairments. Although design aspects of text comprehensibility have recently gained attention, digital design patterns remain relatively unexplored. Our understanding of how ETR users interact with digital media, and how to design specifically for their needs, is still limited. Our study involved observing 20 German ETR users engaging with a digital PDF and a website designed in a participatory process. We collected data on their access to digital media, personal use and workarounds, and their interaction with digital design patterns. Tasks on the smartphone were completed mostly successfully, while only 50\\% could navigate a digital PDF. In both cases, visual cues played a significant role. Our findings contribute recommendations for beneficial digital design patterns and future research.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {911},\nnumpages = {18},\nkeywords = {Easy-to-Read (ETR), accessibility, digital design patterns, inclusion, interaction design, reading, typography, usability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641893,\nauthor = {Nevsky, Alexandre and Neate, Timothy and Simperl, Elena and Cruice, Madeline N},\ntitle = {Lights, Camera, Access: A Closeup on Audiovisual Media Accessibility and Aphasia},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641893},\ndoi = {10.1145/3613904.3641893},\nabstract = {The presence of audiovisual media is a mainstay in the lives of many, increasingly so with technological progress. Accessing video and audio content, however, can be challenging for people with diverse needs. Existing research has explored a wide range of accessibility challenges and worked with disabled communities to design technologies that help bridge the access gap. Despite this work, our understanding of the challenges faced by communities with complex communication needs (CCNs) remains poor. To address this shortcoming, we present the first study that investigates the viewing experience of people with the communication impairment aphasia through an online survey (N=41) and two focus group sessions (N=10), with the aim of understanding their specific access challenges. We find that aphasia significantly impact viewing experience and present a taxonomy of access barriers and facilitators, with suggestions for future research.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {912},\nnumpages = {17},\nkeywords = {Accessibility, aphasia, audiovisual, barriers, challenges, media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642950,\nauthor = {Johnson, Hailey Lynn and Sterling, Audra and Mutlu, Bilge},\ntitle = {\"It Is Easy Using My Apps:\" Understanding Technology Use and Needs of Adults with Down Syndrome},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642950},\ndoi = {10.1145/3613904.3642950},\nabstract = {Assistive technologies for adults with Down syndrome (DS) need designs tailored to their specific technology requirements. While prior research has explored technology design for individuals with intellectual disabilities, little is understood about the needs and expectations of adults with DS. Assistive technologies should leverage the abilities and interests of the population, while incorporating age- and context-considerate content. In this work, we interviewed six adults with DS, seven parents of adults with DS, and three experts in speech-language pathology, special education, and occupational therapy to determine how technology could support adults with DS. In our thematic analysis, four main themes emerged, including (1) community vs. home social involvement; (2) misalignment of skill expectations between adults with DS and parents; (3) family limitations in technology support; and (4) considerations for technology development. Our findings extend prior literature by including the voices of adults with DS in how and when they use technology.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {913},\nnumpages = {17},\nkeywords = {Adults with Down syndrome, Assistive technologies, Down syndrome, Technology Needs, Technology usage},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642327,\nauthor = {Curtis, Humphrey and Lau, Ying Hei and Neate, Timothy},\ntitle = {Breaking Badge: Augmenting Communication with Wearable AAC Smartbadges and Displays},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642327},\ndoi = {10.1145/3613904.3642327},\nabstract = {People living with complex communication needs employ multimodal pathways to communicate including: limited speech, paralinguistics, non-verbal communication and leveraging low-tech devices. However, most augmentative and alternative communication (AAC) interventions undermine end-users’ agency by obstructing these intuitive communication pathways. In this paper, we collaborate with 19 people living with the language impairment aphasia exploring contextual communication challenges, before low-fidelity prototyping and wireframing wearable AAC displays. These activities culminated in two low-input wearable AAC prototypes that instead, scaffold users’ pre-existing communication abilities. Firstly, the InkTalker is a low-power and affordable eInk AAC smartbadge designed to discreetly reveal invisible disabilities and usable as a communication prop. Secondly, WalkieTalkie is a scalable AAC app that converts smartphones into a feature-rich public display operable via multimodal input/outputs. We offer results from communication interactions with both devices, discussions and feedback responses. Participants used both AAC devices to interdependently socialise with others and augment pre-existing communication abilities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {914},\nnumpages = {25},\nkeywords = {AAC, Accessibility, Alternative and Augmentative Communication, Discreet and Wearable Devices, Smart badges},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642762,\nauthor = {Valencia, Stephanie and Huynh, Jessica and Jiang, Emma Y and Wu, Yufei and Wan, Teresa and Zheng, Zixuan and Admoni, Henny and Bigham, Jeffrey P and Pavel, Amy},\ntitle = {COMPA: Using Conversation Context to Achieve Common Ground in AAC},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642762},\ndoi = {10.1145/3613904.3642762},\nabstract = {Group conversations often shift quickly from topic to topic, leaving a small window of time for participants to contribute. AAC users often miss this window due to the speed asymmetry between using speech and using AAC devices. AAC users may take over a minute longer to contribute, and this speed difference can cause mismatches between the ongoing conversation and the AAC user’s response. This results in misunderstandings and missed opportunities to participate. We present COMPA, an add-on tool for online group conversations that seeks to support conversation partners in achieving common ground. COMPA uses a conversation’s live transcription to enable AAC users to mark conversation segments they intend to address (Context Marking) and generate contextual starter phrases related to the marked conversation segment (Phrase Assistance) and a selected user intent. We study COMPA in 5 different triadic group conversations, each composed by a researcher, an AAC user and a conversation partner (n=10) and share findings on how conversational context supports conversation partners in achieving common ground.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {915},\nnumpages = {18},\nkeywords = {AAC, accessibility, communication, language models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642746,\nauthor = {Wu, Shaomei and Li, Jingjin and Leshed, Gilly},\ntitle = {Finding My Voice over Zoom: An Autoethnography of Videoconferencing Experience for a Person Who Stutters},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642746},\ndoi = {10.1145/3613904.3642746},\nabstract = {Existing videoconferencing (VC) technologies are often optimized for productivity and efficiency, with little support for the “soft side” of VC meetings such as empathy, authenticity, belonging, and emotional connections. This paper presents findings from a 15-month long autoethnographic study of VC experiences by the first author, a person who stutters (PWS). Our research shed light on the hidden costs of VC for PWS, uncovering the substantial emotional and cognitive efforts that other meeting attendants are often unaware of. Recognizing the disproportionate burden on PWS to be heard in VC, we propose a set of design implications for a more inclusive communication environment, advocating for shared responsibility among all, including communication technologies, to ensure the inclusion and respect of every voice.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {916},\nnumpages = {16},\nkeywords = {Zoom, accessibility, autoethnography, computer-mediated communication, mindfulness, stuttering, videoconferencing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642788,\nauthor = {Du, Yao and O'Connor, Claire and Byun, Ginna and Kim, Lauren H and Amrgousian, Siona and Vora, Priyal},\ntitle = {Voice Assistive Technology for Activities of Daily Living: Developing an Alexa Telehealth Training for Adults with Cognitive-Communication Disorders},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642788},\ndoi = {10.1145/3613904.3642788},\nabstract = {Individuals with cognitive-communication disorders (CCDs) due to neurological conditions, such as traumatic brain injury and aphasia, experience difficulties in communication and cognition that impact their ability to perform activities of daily living, or ADLs (e.g., self-care, meal preparation, scheduling). Voice assistive technology (VAT) can support the independent performance of ADLs; however, there are limited VAT training programs that teach individuals with CCDs how to properly implement and use VAT for ADLs. The present study examined the implementation of an online training program using Alexa voice commands for five ADL domains (scheduling, entertainment, self-care, news \\& facts, and meal preparation). Using video analysis with seven adults with CCDs between ages 25 and 82 and interviews with five participants and three caregivers, we synthesized five weeks of training performance, analyzed participants' perceived benefits and challenges, and discussed challenges and opportunities for implementing VAT training for ADLs skills for adults with CCDs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {917},\nnumpages = {15},\nkeywords = {Activities of daily living, Individuals with cognitive-communication disorders, Telehealth, Voice assistive technology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642722,\nauthor = {Coduto, Kathryn D and McDonald, Allison},\ntitle = {\"Delete it and Move On\": Digital Management of Shared Sexual Content after a Breakup},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642722},\ndoi = {10.1145/3613904.3642722},\nabstract = {Sexting is a common and healthy behavior in romantic and sexual relationships. However, not every relationship lasts. When a relationship ends, the fate of sexual content that was previously shared can be a source of discomfort, anxiety, or fear for individuals who may no longer trust their former partners. In extreme cases, intimate content may be leaked or misused by its recipient. To investigate opportunities for building safer sexting tools with breakups in mind, we conducted a survey with 310 U.S. adults who have sexted in the last year. We asked about their sexting practices, communication practices within their relationship about sexting, and preferences for their own sexting content after a breakup. We find that most people save sexts in some form, either actively (e.g., via screenshots) or passively (e.g., in chat history). There is no consensus around what one should do with an ex’s content: although most (55\\%) want their content to be deleted at the end of a relationship, many others don’t care (25\\%) or even hope their ex keeps the material (11\\%). However, most have never spoken to their partner about this preference. We end with design recommendations that support sexting while keeping the entire relationship lifecycle in mind.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {918},\nnumpages = {16},\nkeywords = {breakups, online safety, relationships, sexting},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642146,\nauthor = {Yong, Seraphina and Cui, Leo and Suma Rosenberg, Evan and Yarosh, Svetlana},\ntitle = {A Change of Scenery: Transformative Insights from Retrospective VR Embodied Perspective-Taking of Conflict With a Close Other},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642146},\ndoi = {10.1145/3613904.3642146},\nabstract = {Close relationships are irreplaceable social resources, yet prone to high-risk conflict. Building on findings from the fields of HCI, virtual reality, and behavioral therapy, we evaluate the unexplored potential of retrospective VR-embodied perspective-taking to fundamentally influence conflict resolution in close others. We develop a biographically-accurate Retrospective Embodied Perspective-Taking system (REPT) and conduct a mixed-methods evaluation of its influence on close others’ reflection and communication, compared to video-based reflection methods currently used in therapy (treatment as usual, or TAU). Our key findings provide evidence that REPT was able to significantly improve communication skills and positive sentiment of both partners during conflict, over TAU. The qualitative data also indicated that REPT surpassed basic perspective-taking by exclusively stimulating users to embody and reflect on both their own and their partner’s experiences at the same level. In light of these findings, we provide implications and an agenda for social embodiment in HCI design: conceptualizing the use of ‘embodied social cognition,’ and envisioning socially-embodied experiences as an interactive context.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {919},\nnumpages = {18},\nkeywords = {behavior change, close relationships, communication, conflict, embodiment, empathy, mixed-methods, perspective-taking, reflection, social cognition, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642159,\nauthor = {Shaikh, Omar and Chai, Valentino Emil and Gelfand, Michele and Yang, Diyi and Bernstein, Michael S.},\ntitle = {Rehearsal: Simulating Conflict to Teach Conflict Resolution},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642159},\ndoi = {10.1145/3613904.3642159},\nabstract = {Interpersonal conflict is an uncomfortable but unavoidable fact of life. Navigating conflict successfully is a skill—one that can be learned through deliberate practice—but few have access to effective training or feedback. To expand this access, we introduce Rehearsal, a system that allows users to rehearse conflicts with a believable simulated interlocutor, explore counterfactual “what if?” scenarios to identify alternative conversational paths, and learn through feedback on how and when to apply specific conflict strategies. Users can utilize Rehearsal to practice handling a variety of predefined conflict scenarios, from office disputes to relationship issues, or they can choose to create their own setting. To enable Rehearsal, we develop IRP prompting, a method of conditioning output of a large language model on the influential Interest-Rights-Power (IRP) theory from conflict resolution. Rehearsal uses IRP to generate utterances grounded in conflict resolution theory, guiding users towards counterfactual conflict resolution strategies that help de-escalate difficult conversations. In a between-subjects evaluation, 40 participants engaged in an actual conflict with a confederate after training. Compared to a control group with lecture material covering the same IRP theory, participants with simulated training from Rehearsal significantly improved their performance in the unaided conflict: they reduced their use of escalating competitive strategies by an average of 67\\%, while doubling their use of cooperative strategies. Overall, Rehearsal highlights the potential effectiveness of language models as tools for learning and practicing interpersonal skills.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {920},\nnumpages = {20},\nkeywords = {conflict resolution, interests-rights-power, large language models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642860,\nauthor = {Shen, Chenxinran and Xu, Yan and Lc, Ray and Lu, Zhicong},\ntitle = {Seeking Soulmate via Voice: Understanding Promises and Challenges of Online Synchronized Voice-Based Mobile Dating},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642860},\ndoi = {10.1145/3613904.3642860},\nabstract = {Online dating has become a popular way for individuals to connect with potential romantic partners. Many dating apps use personal profiles that include a headshot and self-description, allowing users to present themselves and search for compatible matches. However, this traditional model often has limitations. In this study, we explore a non-traditional voice-based dating app called “Soul”. Unlike traditional platforms that rely heavily on profile information, Soul facilitates user interactions through voice-based communication. We conducted semi-structured interviews with 18 dedicated Soul users to investigate how they engage with the platform and perceive themselves and others in this unique dating environment. Our findings indicate that the role of voice as a moderator influences impression management and shapes perceptions between the sender and the receiver of the voice. Additionally, the synchronous voice-based and community-based dating model offers benefits to users in the Chinese cultural context. Our study contributes to understanding the affordances introduced by voice-based interactions in online dating in China.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {921},\nnumpages = {14},\nkeywords = {Online dating, affordance, online communities, social media, voice},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642818,\nauthor = {Huang, Zeyu and Cao, Xinyi and Zhang, Yuanhao and Ma, Xiaojuan},\ntitle = {Sharing Frissons among Online Video Viewers: Exploring the Design of Affective Communication for Aesthetic Chills},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642818},\ndoi = {10.1145/3613904.3642818},\nabstract = {On online video platforms, viewers often lack a channel to sense others’ and express their affective state on the fly compared to co-located group-viewing. This study explored the design of complementary affective communication specifically for effortless, spontaneous sharing of frissons during video watching. Also known as aesthetic chills, frissons are instant psycho-physiological reactions like goosebumps and shivers to arousing stimuli. We proposed an approach that unobtrusively detects viewers’ frissons using skin electrodermal activity sensors and presents the aggregated data alongside online videos. Following a design process of brainstorming, focus group interview (N=7), and design iterations, we proposed three different designs to encode viewers’ frisson experiences, namely, ambient light, icon, and vibration. A mixed-methods within-subject study (N=48) suggested that our approach offers a non-intrusive and efficient way to share viewers’ frisson moments, increases the social presence of others as if watching together, and can create affective contagion among viewers.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {922},\nnumpages = {19},\nkeywords = {affective communication, asynchronous communication, biosignal, frissons, online video platforms},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641980,\nauthor = {Chakraborti, Mahasweta and Atkisson, Curtis and St\\u{a}nciulescu, \\c{S}tefan and Filkov, Vladimir and Frey, Seth},\ntitle = {Do We Run How We Say We Run? Formalization and Practice of Governance in OSS Communities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641980},\ndoi = {10.1145/3613904.3641980},\nabstract = {Open Source Software (OSS) communities often resist regulation typical of traditional organizations. Yet formal governance systems are being increasingly adopted among communities, particularly through non-profit project-sponsoring foundations. Our study looks at the Apache Software Foundation Incubator program and 208 of the projects it has supported. We assemble a scalable, semantic pipeline to discover and analyze the governance behavior of projects from their mailing lists. We then investigate the relationship of such behavior to what the formal policies prescribe, through their own governance priorities and how their members internalize them. Our findings indicate that a greater amount of policy over a governed topic doesn’t elicit more governed activity on that topic, but does predict greater internalization by community members. Moreover, alignment of community operations with foundation governance, be it dedicating their governance focus or adopting policy along topics seeing greater policy-making, has limited association with project outcomes.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {923},\nnumpages = {26},\nkeywords = {Collective Action, OSS Governance, Online Communities, Open Source Software, Peer Production},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642541,\nauthor = {Jahn, Leonie and Engelbutzeder, Philip and Randall, Dave and Bollmann, Yannick and Ntouros, Vasilis and Michel, Lea Katharina and Wulf, Volker},\ntitle = {In Between Users and Developers: Serendipitous Connections and Intermediaries in Volunteer-Driven Open-Source Software Development},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642541},\ndoi = {10.1145/3613904.3642541},\nabstract = {Technology plays a pivotal role in driving transformation through grassroots movements, which operate on a local scale while embracing a global perspective on sustainability. Consequently, research emerged within Sustainable HCI, aiming to derive design principles that can empower these movements to scale their impact. However, a notable gap exists in contributions when addressing scalability of large free and open-source software (FOSS) projects.This paper aims to present our endeavors as action-oriented researchers with the voluntary-driven Foodsharing.de movement, focusing on a local community, the open-source developers and their connections. Within a community of 585,000 users and only a few developers that is dedicated to save and share surplus food, we explore the concepts of ‘intermediary experience’. We also introduce the notion of ‘serendipitous connections’, highlighting the unintentional yet beneficial associations that can arise from the collaboration between developers and users.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {924},\nnumpages = {15},\nkeywords = {Grassroots community, Intermediary, Open-Source Software, Sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642012,\nauthor = {Wang, Leijie and Vincent, Nicholas and Rukanskaitundefined, Julija and Zhang, Amy Xian},\ntitle = {Pika: Empowering Non-Programmers to Author Executable Governance Policies in Online Communities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642012},\ndoi = {10.1145/3613904.3642012},\nabstract = {Internet users have formed a wide array of online communities with diverse community goals and nuanced norms. However, most online platforms only offer a limited set of governance models in their software infrastructure and leave little room for customization. Consequently, technical proficiency becomes a prerequisite for online communities to build governance policies in code, excluding non-programmers from participation in designing community governance. In this paper, we present Pika, a system that empowers non-programmers to author a wide range of executable governance policies. At its core, Pika incorporates a declarative language that decomposes governance policies into modular components, thereby facilitating expressive policy authoring through a user-friendly, form-based web interface. Our user studies with 10 non-programmers and 7 programmers show that Pika can empower non-programmers to author policies approximately 2.5 times faster than programmers who author in code. We also provide insights about Pika’s expressivity in supporting diverse policies online communities want.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {925},\nnumpages = {18},\nkeywords = {Community Governance, Declarative Language, End-user Programming, Online Communities},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642787,\nauthor = {Cai, Jie and Lin, Ya-Fang and Zhang, He and Carroll, John M.},\ntitle = {Third-Party Developers and Tool Development For Community Management on Live Streaming Platform Twitch},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642787},\ndoi = {10.1145/3613904.3642787},\nabstract = {Community management is critical for stakeholders to collaboratively build and sustain communities with socio-technical support. However, most of the existing research has mainly focused on the community members and the platform, with little attention given to the developers who act as intermediaries between the platform and community members and develop tools to support community management. This study focuses on third-party developers (TPDs) for the live streaming platform Twitch and explores their tool development practices. Using a mixed method with in-depth qualitative analysis, we found that TPDs maintain complex relationships with different stakeholders (streamers, viewers, platform, professional developers), and the multi-layered policy restricts their agency regarding idea innovation and tool development. We argue that HCI research should shift its focus from tool users to tool developers with regard to community management. We propose designs to support closer collaboration between TPDS and the platform and professional developers and streamline TPDs’ development process with unified toolkits and policy documentation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {926},\nnumpages = {18},\nkeywords = {Community Management, Community moderation, Discord, Extension and Bot Development, Live Streaming, Moderation Tools, Platform Governance, Third-Party Developers, Twitch},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642209,\nauthor = {Hui, Julie and Seefeldt, Kristin and Sanifu, Lutalo and Baer, Christie and Szomstein, Jeanette and Dillahunt, Tawanna R},\ntitle = {“I was able to give her the confidence”: Reciprocal Capacity Building in a Community-based Program for Digital Engagement},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642209},\ndoi = {10.1145/3613904.3642209},\nabstract = {Assets-based approaches emphasize the importance of leveraging and building upon community strengths. We describe how a community-based digital capacity building program, Community Tech Workers (CTW), addresses the goals of assets-based development by hiring local residents and students to serve as tech support personnel for underserved minority small business owners in Detroit. Through interviews and observations, we examine how reciprocal relationships between tech workers and small business owners are critical to the success and sustainability of the program. We find that tech workers and business owners mutually benefit by 1) building confidence in technology together, 2) having business owners provide reciprocal guidance in professional development, and 3) fostering mutual appreciation and commitment to community development. We conclude by introducing the concept of reciprocal capacity building to HCI and discussing how it provides a potentially more equitable approach to community-based interventions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {927},\nnumpages = {13},\nkeywords = {Black-owned businesses, Small businesses, assets-based community development, community-based research, digital engagement, entrepreneurship, underserved},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642752,\nauthor = {Yang, Saelyne and Vermeulen, Jo and Fitzmaurice, George and Matejka, Justin},\ntitle = {AQuA: Automated Question-Answering in Software Tutorial Videos with Visual Anchors},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642752},\ndoi = {10.1145/3613904.3642752},\nabstract = {Tutorial videos are a popular help source for learning feature-rich software. However, getting quick answers to questions about tutorial videos is difficult. We present an automated approach for responding to tutorial questions. By analyzing 633 questions found in 5,944 video comments, we identified different question types and observed that users frequently described parts of the video in questions. We then asked participants (N=24) to watch tutorial videos and ask questions while annotating the video with relevant visual anchors. Most visual anchors referred to UI elements and the application workspace. Based on these insights, we built AQuA, a pipeline that generates useful answers to questions with visual anchors. We demonstrate this for Fusion 360, showing that we can recognize UI elements in visual anchors and generate answers using GPT-4 augmented with that visual information and software documentation. An evaluation study (N=16) demonstrates that our approach provides better answers than baseline methods.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {928},\nnumpages = {19},\nkeywords = {generative AI, large language models, question answering, software learning, tutorial videos},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642676,\nauthor = {Horvath, Amber and Macvean, Andrew and Myers, Brad A},\ntitle = {Meta-Manager: A Tool for Collecting and Exploring Meta Information about Code},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642676},\ndoi = {10.1145/3613904.3642676},\nabstract = {Modern software engineering is in a state of flux. With more development utilizing AI code generation tools and the continued reliance on online programming resources, understanding code and the original intent behind it is becoming more important than it ever has been. To this end, we have developed the “Meta-Manager”, a Visual Studio Code extension, with a supplementary browser extension, that automatically collects and organizes changes made to code while keeping track of the provenance of each part of the code, including code that has been AI-generated or copy-pasted from popular programming resources online. These sources and subsequent changes are represented in the editor and may be explored using searching and filtering mechanisms to help developers answer historically hard-to-answer questions about code, its provenance, and its design rationale. In our evaluation of Meta-Manager, we found developers were successfully able to use it to answer otherwise unanswerable questions about an unfamiliar code base.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {929},\nnumpages = {17},\nkeywords = {Code history, code comprehension, code provenance, design rationale, meta-information, sensemaking, software engineering},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641910,\nauthor = {Wu, Zihan and Ericson, Barbara J.},\ntitle = {SQL Puzzles: Evaluating Micro Parsons Problems With Different Feedbacks as Practice for Novices},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641910},\ndoi = {10.1145/3613904.3641910},\nabstract = {This paper investigates using micro Parsons problems as a novel practice approach for learning Structured Query Language (SQL). In micro Parsons problems learners arrange predefined code fragments to form a SQL statement instead of typing the code. SQL is a standard language for working with relational databases. Targeting beginner-level SQL statements, we evaluated the efficacy of micro Parsons problems with block-based feedback and execution-based feedback compared to traditional text-entry problems. To delve into learners’ experiences and preferences for the three problem types, we conducted a within-subjects think-aloud study with 12 participants. We found that learners reported very different preferences. Factors they considered included perceived learning, task authenticity, and prior knowledge. Next, we conducted two between-subjects classroom studies to evaluate the effectiveness of micro Parsons problems with different feedback types versus text-entry problems for SQL practice. We found that learners who practiced by solving Parsons problems with block-based feedback had a significantly higher learning gain than those who practiced with traditional text-entry problems.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {930},\nnumpages = {15},\nkeywords = {SQL education, empirical study, learning, programming puzzle},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642683,\nauthor = {Hayatpur, Devamardeep and Hempel, Brian and Chen, Kathy and Duan, William and Guo, Philip and Xia, Haijun},\ntitle = {Taking ASCII Drawings Seriously: How Programmers Diagram Code},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642683},\ndoi = {10.1145/3613904.3642683},\nabstract = {Documentation in codebases facilitates knowledge transfer. But tools for programming are largely text-based, and so developers resort to creating ASCII diagrams—graphical artifacts approximated with text—to show visual ideas within their code. Despite real-world use, little is known about these diagrams. We interviewed nine authors of ASCII diagrams, learning why they use ASCII and what roles the diagrams play. We also compile and analyze a corpus of 507 ASCII diagrams from four open source projects, deriving a design space with seven dimensions that classify what these diagrams show, how they show it, and ways they connect to code. These investigations reveal that ASCII diagrams are professional artifacts used across many steps in the development lifecycle, diverse in role and content, and used because they visualize ideas within the variety of programming tools in use. Our findings highlight the importance of visualization within code and lay a foundation for future programming tools that tightly couple text and graphics.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {931},\nnumpages = {16},\nkeywords = {graphical representations, programming},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642576,\nauthor = {Xia, Chengshuo and Min, Tian and Zhang, Daxing and Wang, Congsi},\ntitle = {Understanding the Needs of Novice Developers in Creating Self-Powered IoT},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642576},\ndoi = {10.1145/3613904.3642576},\nabstract = {The rise of the Internet of Things (IoT) has given birth to transformative and massively deployed computing applications that raise the significant issue of energy sources. It is impractical and irresponsible to rely on wires and batteries to power trillion-level devices. One promising prediction is that energy harvesting technologies will serve as alternative power sources for IoT devices. However, we might be losing this prophecy for lack of understanding of how novice developers comprehend energy in developing IoT. In response, we conducted a mentored physical prototyping study with a two-day workshop involving eight novice developers. The study consisted of qualitative and quantitative analyses, the artifacts, interviews with both novice developers and an expert, and implications of designs for future tools. The findings reveal informational gaps that demand educational efforts and assistive features to facilitate novice developers. We present major findings from the study and implications for the design of future tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {932},\nnumpages = {17},\nkeywords = {Battery-free computing, Developer Supports, Energy Harvesting, Mentored Physical Prototyping, Novice Developers, Sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642130,\nauthor = {Cha, Yoonha and Figueira, Isabela and Ayala, Jessy and Edwards, Emory James and Garcia, Joshua and van der Hoek, Andr\\'{e} and Branham, Stacy Marie},\ntitle = {\"Do You Want Me to Participate or Not?\": Investigating the Accessibility of Software Development Meetings for Blind and Low Vision Professionals},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642130},\ndoi = {10.1145/3613904.3642130},\nabstract = {Scholars have investigated numerous barriers to accessible software development tools and processes for Blind and Low Vision (BLV) developers. However, the research community has yet to study the accessibility of software development meetings, which are known to play a crucial role in software development practice. We conducted semi-structured interviews with 26 BLV software professionals about software development meeting accessibility. We found four key themes related to in-person and remote software development meetings: (1) participants observed that certain meeting activities and software tools used in meetings were inaccessible, (2) participants performed additional labor in order to make meetings accessible, (3) participants avoided disclosing their disability during meetings due to fear of career repercussions, (4) participants suggested technical, social and organizational solutions for accessible meetings, including developing their own solutions. We suggest recommendations and design implications for future accessible software development meetings including technical and policy-driven solutions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {933},\nnumpages = {17},\nkeywords = {accessibility, collaboration, meetings, software development, work},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642212,\nauthor = {Feng, Li and Yen, Ryan and You, Yuzhe and Fan, Mingming and Zhao, Jian and Lu, Zhicong},\ntitle = {CoPrompt: Supporting Prompt Sharing and Referring in Collaborative Natural Language Programming},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642212},\ndoi = {10.1145/3613904.3642212},\nabstract = {Natural language (NL) programming has become more approachable due to the powerful code-generation capability of large language models (LLMs). This shift to using NL to program enhances collaborative programming by reducing communication barriers and context-switching among programmers from varying backgrounds. However, programmers may face challenges during prompt engineering in a collaborative setting as they need to actively keep aware of their collaborators’ progress and intents. In this paper, we aim to investigate ways to assist programmers’ prompt engineering in a collaborative context. We first conducted a formative study to understand the workflows and challenges of programmers when using NL for collaborative programming. Based on our findings, we implemented a prototype, CoPrompt, to support collaborative prompt engineering by providing referring, requesting, sharing, and linking mechanisms. Our user study indicates that CoPrompt assists programmers in comprehending collaborators’ prompts and building on their collaborators’ work, reducing repetitive updates and communication costs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {934},\nnumpages = {21},\nkeywords = {collaborative programming, large language model, natural language interface, natural language programming, prompt engineering},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642596,\nauthor = {Kabir, Samia and Udo-Imeh, David N. and Kou, Bonan and Zhang, Tianyi},\ntitle = {Is Stack Overflow Obsolete? An Empirical Study of the Characteristics of ChatGPT Answers to Stack Overflow Questions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642596},\ndoi = {10.1145/3613904.3642596},\nabstract = {Q&A platforms have been crucial for the online help-seeking behavior of programmers. However, the recent popularity of ChatGPT is altering this trend. Despite this popularity, no comprehensive study has been conducted to evaluate the characteristics of ChatGPT’s answers to programming questions. To bridge the gap, we conducted the first in-depth analysis of ChatGPT answers to 517 programming questions on Stack Overflow and examined the correctness, consistency, comprehensiveness, and conciseness of ChatGPT answers. Furthermore, we conducted a large-scale linguistic analysis, as well as a user study, to understand the characteristics of ChatGPT answers from linguistic and human aspects. Our analysis shows that 52\\% of ChatGPT answers contain incorrect information and 77\\% are verbose. Nonetheless, our user study participants still preferred ChatGPT answers 35\\% of the time due to their comprehensiveness and well-articulated language style. However, they also overlooked the misinformation in the ChatGPT answers 39\\% of the time. This implies the need to counter misinformation in ChatGPT answers to programming questions and raise awareness of the risks associated with seemingly correct answers.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {935},\nnumpages = {17},\nkeywords = {chatgpt, large language model, misinformation, q&a, stack overflow},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642186,\nauthor = {Methfessel, Paul and Beckmann, Tom and Rein, Patrick and Ramson, Stefan and Hirschfeld, Robert},\ntitle = {MµSE: Supporting Exploration of Software-Hardware Interactions Through Examples},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642186},\ndoi = {10.1145/3613904.3642186},\nabstract = {Programmers regularly explore the execution of code examples to verify assumptions by adding print statements or commenting in and out setup code in their implementation to isolate code paths of interest. In our formative study on developing embedded programs, where proximity to hardware dictates low abstraction levels, we observed that wrong assumptions occur frequently. However, traditional editors for embedded programs lack support for such explorations. Consequently, programmers have to re-create and clean up setup and print statements in their code for each example. MµSE supports isolated explorations of code examples by promoting examples to first-class entities that allow for the mocking of side effects from code and hardware, which could interfere with examples, and automatically showing values of expressions, replacing print statements for debugging. Our exploratory study found that MµSE supports participants in developing an understanding of software and hardware components and identifying false assumptions from observation of incorrect behavior.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {936},\nnumpages = {16},\nkeywords = {embedded systems, examples, live programming},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642721,\nauthor = {Nam, Daye and Macvean, Andrew and Myers, Brad A and Vasilescu, Bogdan},\ntitle = {Understanding Documentation Use Through Log Analysis: A Case Study of Four Cloud Services},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642721},\ndoi = {10.1145/3613904.3642721},\nabstract = {Almost no modern software system is written from scratch, and developers are required to effectively learn to use third-party libraries and software services. Thus, many practitioners and researchers have looked for ways to create effective documentation that supports developers’ learning. However, few efforts have focused on how people actually use the documentation. In this paper, we report on an exploratory, multi-phase, mixed methods empirical study of documentation page-view logs from four cloud-based industrial services. By analyzing page-view logs for over 100,000 users, we find diverse patterns of documentation page visits. Moreover, we show statistically that which documentation pages people visit often correlates with user characteristics such as past experience with the specific product, on the one hand, and with future adoption of the API on the other hand. We discuss the implications of these results on documentation design and propose documentation page-view log analysis as a feasible technique for design audits of documentation, from ones written for software developers to ones designed to support end users (e.g., Adobe Photoshop).},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {937},\nnumpages = {17},\nkeywords = {Design review, Documentation, Empirical study, Log analysis},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642347,\nauthor = {Teng, Songyan and D'Alfonso, Simon and Kostakos, Vassilis},\ntitle = {A Tool for Capturing Smartphone Screen Text},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642347},\ndoi = {10.1145/3613904.3642347},\nabstract = {Context sensing on smartphones is often used to understand user behaviour. Amongst the many available sensors, the collection of text is crucial due to its richness. However, previous work has been limited to collecting text only from keyboard input, or intermittently collecting screen text indirectly by taking screenshots and applying optical character recognition. Here, we present a novel software sensor that unobtrusively and continuously captures all screen text on smartphones. We conducted a validation study with 21 participants over a two-week period, where they used our software on their personal smartphones. Our findings demonstrate how data from our sensor can be used to understand user behaviour and categorise mobile apps. We also show how smartphone sensing can be enhanced by using our sensor in conjunction with other sensors. We discuss the strengths and limitations of our sensor, highlighting potential areas for improvement and providing recommendations for its use.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {938},\nnumpages = {24},\nkeywords = {Context-Awareness, Screen Text, Smartphone Sensing, User Behaviour},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642772,\nauthor = {Wu, Guande and Qian, Jing and Castelo Quispe, Sonia and Chen, Shaoyu and Rulff, Jo\\~{a}o and Silva, Claudio},\ntitle = {ARTiST: Automated Text Simplification for Task Guidance in Augmented Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642772},\ndoi = {10.1145/3613904.3642772},\nabstract = {Text presented in augmented reality provides in-situ, real-time information for users. However, this content can be challenging to apprehend quickly when engaging in cognitively demanding AR tasks, especially when it is presented on a head-mounted display. We propose ARTiST, an automatic text simplification system that uses a few-shot prompt and GPT-3 models to specifically optimize the text length and semantic content for augmented reality. Developed out of a formative study that included seven users and three experts, our system combines a customized error calibration model with a few-shot prompt to integrate the syntactic, lexical, elaborative, and content simplification techniques, and generate simplified AR text for head-worn displays. Results from a 16-user empirical study showed that ARTiST lightens the cognitive load and improves performance significantly over both unmodified text and text modified via traditional methods. Our work constitutes a step towards automating the optimization of batch text data for readability and performance in augmented reality.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {939},\nnumpages = {24},\nkeywords = {augmented reality, large language model, text simplification},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642757,\nauthor = {Wan, Tingjie and Zhang, Liangyuting and Yang, Hongyu and Irani, Pourang and Yu, Lingyun and Liang, Hai-Ning},\ntitle = {Exploration of Foot-based Text Entry Techniques for Virtual Reality Environments},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642757},\ndoi = {10.1145/3613904.3642757},\nabstract = {Foot-based input can serve as a supplementary or alternative approach to text entry in virtual reality (VR). This work explores the feasibility and design of foot-based techniques that are hands-free. We first conducted a preliminary study to assess foot-based text entry in standing and seated positions with tap and swipe input approaches. The findings showed that foot-based text input was feasible, with the possibility for performance and usability improvements. We then developed three foot-based techniques, including two tap-based techniques (FeetSymTap and FeetAsymTap) and one swipe-based technique (FeetGestureTap), and evaluated their performance via another user study. The results show that the two tap-based techniques supported entry rates of 11.12 WPM and 10.80 WPM, while the swipe-based technique led to 9.16 WPM. Our findings provide a solid foundation for the future design and implementation of foot-based text entry in VR and have the potential to be extended to MR and AR.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {940},\nnumpages = {17},\nkeywords = {foot-based interaction, hands-free interaction, text entry, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642569,\nauthor = {Akamine, Kai and Tsuchida, Ryotaro and Kato, Tsuneo and Tamura, Akihiro},\ntitle = {PonDeFlick: A Japanese Text Entry on Smartwatch Commonalizing Flick Operation with Smartphone Interface},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642569},\ndoi = {10.1145/3613904.3642569},\nabstract = {While the QWERTY keyboard is a standard text entry for Latin script languages on smart devices, it is not always true for non-Latin script languages. In Japanese, the most popular text entry on smartphones is a flick-based interface that systematically assigns more than fifty kana characters to twelve keys of a numeric keypad in combination with flick directions. Under these circumstances, studies on Japanese text entry on smartwatches have focused on an efficient interface design that takes advantage of the regularity of the kana consonant and vowel structure, but overlooked commonality with familiar interfaces. Thus, we propose PonDeFlick, a Japanese text entry that commonalizes the flick directions with the familiar smartphone interface while providing the entire touchscreen for gestural operation. A ten-day user study showed that PonDeFlick reached a text-entry speed of 57.7 characters per minute, significantly faster than the numeric-keypad-based interface and a modification of PonDeFlick without the commonality.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {941},\nnumpages = {11},\nkeywords = {Japanese kana, PonDeFlick, smartwatch, software keyboard, text entry},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642918,\nauthor = {Shi, Danqing and Zhu, Yujun and Jokinen, Jussi P. P. and Acharya, Aditya and Putkonen, Aini and Zhai, Shumin and Oulasvirta, Antti},\ntitle = {CRTypist: Simulating Touchscreen Typing Behavior via Computational Rationality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642918},\ndoi = {10.1145/3613904.3642918},\nabstract = {Touchscreen typing requires coordinating the fingers and visual attention for button-pressing, proofreading, and error correction. Computational models need to account for the associated fast pace, coordination issues, and closed-loop nature of this control problem, which is further complicated by the immense variety of keyboards and users. The paper introduces CRTypist, which generates human-like typing behavior. Its key feature is a reformulation of the supervisory control problem, with the visual attention and motor system being controlled with reference to a working memory representation tracking the text typed thus far. Movement policy is assumed to asymptotically approach optimal performance in line with cognitive and design-related bounds. This flexible model works directly from pixels, without requiring hand-crafted feature engineering for keyboards. It aligns with human data in terms of movements and performance, covers individual differences, and can generalize to diverse keyboard designs. Though limited to skilled typists, the model generates useful estimates of the typing performance achievable under various conditions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {942},\nnumpages = {17},\nkeywords = {Computational modeling, Reinforcement learning, Simulation models, Touchscreen typing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642657,\nauthor = {Krauter, Christian and Angerbauer, Katrin and Sousa Calepso, Aim\\'{e}e and Achberger, Alexander and Mayer, Sven and Sedlmair, Michael},\ntitle = {Sitting Posture Recognition and Feedback: A Literature Review},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642657},\ndoi = {10.1145/3613904.3642657},\nabstract = {Extensive sitting is unhealthy; thus, countermeasures are needed to react to the ongoing trend toward more prolonged sitting. A variety of studies and guidelines have long addressed the question of how we can improve our sitting habits. Nevertheless, sitting time is still increasing. Here, smart devices can provide a general overview of sitting habits for more nuanced feedback on the user’s sitting posture. Based on a literature review (N=223), including publications from engineering, computer science, medical sciences, electronics, and more, our work guides developers of posture systems. There is a large variety of approaches, with pressure-sensing hardware and visual feedback being the most prominent. We found factors like environment, cost, privacy concerns, portability, and accuracy important for deciding hardware and feedback types. Further, one should consider the user’s capabilities, preferences, and tasks. Regarding user studies for sitting posture feedback, there is a need for better comparability and for investigating long-term effects.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {943},\nnumpages = {20},\nkeywords = {Literature review, chair, posture, sitting},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642555,\nauthor = {Huang, William and Ghahremani, Sam and Pei, Siyou and Zhang, Yang},\ntitle = {WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642555},\ndoi = {10.1145/3613904.3642555},\nabstract = {Existing pose estimation models perform poorly on wheelchair users due to a lack of representation in training data. We present a data synthesis pipeline to address this disparity in data collection and subsequently improve pose estimation performance for wheelchair users. Our configurable pipeline generates synthetic data of wheelchair users using motion capture data and motion generation outputs simulated in the Unity game engine. We validated our pipeline by conducting a human evaluation, investigating perceived realism, diversity, and an AI performance evaluation on a set of synthetic datasets from our pipeline that synthesized different backgrounds, models, and postures. We found our generated datasets were perceived as realistic by human evaluators, had more diversity than existing image datasets, and had improved person detection and pose estimation performance when fine-tuned on existing pose estimation models. Through this work, we hope to create a foothold for future efforts in tackling the inclusiveness of AI in a data-centric and human-centric manner with the data synthesis techniques demonstrated in this work. Finally, for future works to extend upon, we open source all code in this research and provide a fully configurable Unity Environment used to generate our datasets. In the case of any models we are unable to share due to redistribution and licensing policies, we provide detailed instructions on how to source and replace said models. All materials can be found at https://github.com/hilab-open-source/wheelpose.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {944},\nnumpages = {25},\nkeywords = {Accessibility, Data Synthesis, Pose Estimation, Wheelchair Users},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641944,\nauthor = {Liu, Jingyuan and Wei, Li-Yi and Shamir, Ariel and Igarashi, Takeo},\ntitle = {iPose: Interactive Human Pose Reconstruction from Video},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641944},\ndoi = {10.1145/3613904.3641944},\nabstract = {Reconstructing 3D human poses from video has wide applications, such as character animation and sports analysis. Automatic 3D pose reconstruction methods have demonstrated promising results, but failure cases can still appear due to the diversity of human actions, capturing conditions, and depth ambiguities. Thus, manual intervention remains indispensable, which can be time-consuming and require professional skills. We thus present iPose, an interactive tool that facilitates intuitive human pose reconstruction from a given video. Our tool incorporates both human perception in specifying pose appearance to achieve controllability, and video frame processing algorithms to achieve precision and automation. A user manipulates the projection of a 3D pose via 2D operations on top of video frames, and the 3D poses are updated correspondingly while satisfying both kinematic and video frame constraints. The pose updates are propagated temporally to reduce user workload. We evaluate the effectiveness of iPose with a user study on the 3DPW dataset and expert interviews.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {945},\nnumpages = {14},\nkeywords = {Monocular reconstruction, human pose estimation, user interface., video processing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642473,\nauthor = {Jahanbakhsh, Farnaz and Karger, David R},\ntitle = {A Browser Extension for in-place Signaling and Assessment of Misinformation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642473},\ndoi = {10.1145/3613904.3642473},\nabstract = {The status-quo of misinformation moderation is a central authority, usually social platforms, deciding what content constitutes misinformation and how it should be handled. However, to preserve users’ autonomy, researchers have explored democratized misinformation moderation. One proposition is to enable users to assess content accuracy and specify whose assessments they trust. We explore how these affordances can be provided on the web, without cooperation from the platforms where users consume content. We present a browser extension that empowers users to assess the accuracy of any content on the web and shows the user assessments from their trusted sources in-situ. Through a two-week user study, we report on how users perceive such a tool, the kind of content users want to assess, and the rationales they use in their assessments. We identify implications for designing tools that enable users to moderate content for themselves with the help of those they trust.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {946},\nnumpages = {21},\nkeywords = {democratized content moderation, fact-checking, misinformation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642927,\nauthor = {Zhang, Yixuan and Wang, Yimeng and Yongsatianchot, Nutchanon and Gaggiano, Joseph D and Suhaimi, Nurul M and Okrah, Anne and Kim, Miso and Griffin, Jacqueline and Parker, Andrea G},\ntitle = {Profiling the Dynamics of Trust \\& Distrust in Social Media: A Survey Study},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642927},\ndoi = {10.1145/3613904.3642927},\nabstract = {In the era of digital communication, misinformation on social media threatens the foundational trust in these platforms. While myriad measures have been implemented to counteract misinformation, the complex relationship between these interventions and the multifaceted dynamics of trust and distrust on social media remains underexplored. To bridge this gap, we surveyed 1,769 participants in the U.S. to gauge their trust and distrust in social media and examine their experiences with anti-misinformation features. Our research demonstrates how trust and distrust in social media are not simply two ends of a spectrum; but can also co-exist, enriching the theoretical understanding of these constructs. Furthermore, participants exhibited varying patterns of trust and distrust across demographic characteristics and platforms. Our results also show that current misinformation interventions helped heighten awareness of misinformation and bolstered trust in social media, but did not alleviate underlying distrust. We discuss theoretical and practical implications for future research.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {947},\nnumpages = {24},\nkeywords = {distrust, misinformation, social media, survey, trust},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641966,\nauthor = {Mokhberi, Azadeh and Huang, Yue and Humbert, Guillaume and Obada-Obieh, Borke and Mehrabi Koushki, Masoud and Beznosov, Konstantin},\ntitle = {Trust, Privacy, and Safety Factors Associated with Decision Making in P2P Markets Based on Social Networks: A Case Study of Facebook Marketplace in USA and Canada},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641966},\ndoi = {10.1145/3613904.3641966},\nabstract = {As peer-to-peer (P2P) marketplaces have grown rapidly, concerns related to trust, privacy, and safety (TPS) have also increased. While previous studies have explored these aspects in various P2P marketplaces, there has been limited research on Facebook Marketplace (FM), which is distinguished by dramatic growth and intricate entanglement with the Facebook social networking site (SNS). To address this knowledge gap, we conducted interviews with 42 FM users in the US and Canada, investigating TPS factors associated with trading decisions. We identified four categories of factors: pre-existing concerns, signals, interactions, and perceived benefits. We uncover the challenges arising from the interplay of these factors, offer design recommendations for SNS–based marketplaces like FM, and suggest directions for future research. Our study advances the understanding of decision-making processes in SNS–based marketplaces, informs future design improvements for such platforms, and ultimately contributes to a better user experience related to trust, privacy, and safety.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {948},\nnumpages = {25},\nkeywords = {decision making, e-commerce, peer-to-peer (P2P) marketplaces, privacy, qualitative research, s-commerce, safety, trust},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642817,\nauthor = {Han, Chaeeun and Mitra, Prasenjit and Billah, Syed Masum},\ntitle = {Uncovering Human Traits in Determining Real and Spoofed Audio: Insights from Blind and Sighted Individuals},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642817},\ndoi = {10.1145/3613904.3642817},\nabstract = {This paper explores how blind and sighted individuals perceive real and spoofed audio, highlighting differences and similarities between the groups. Through two studies, we find that both groups focus on specific human traits in audio–such as accents, vocal inflections, breathing patterns, and emotions–to assess audio authenticity. We further reveal that humans, irrespective of visual ability, can still outperform current state-of-the-art machine learning models in discerning audio authenticity; however, the task proves psychologically demanding. Moreover, detection accuracy scores between blind and sighted individuals are comparable, but each group exhibits unique strengths: the sighted group excels at detecting deepfake-generated audio, while the blind group excels at detecting text-to-speech (TTS) generated audio. These findings not only deepen our understanding of machine-manipulated and neural-renderer audio but also have implications for developing countermeasures, such as perceptible watermarks and human-AI collaboration strategies for spoofing detection.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {949},\nnumpages = {14},\nkeywords = {Audio perception, and audio watermarking, and human-AI collaboration., audio, blind, bona fide audio, deep fake audio, generative AI, neural speech, replay attack, sighted, speech, spoofed audio, text-to-speech (TTS), vision impairments, voice},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642342,\nauthor = {Oak, Rajvardhan and Shafiq, Zubair},\ntitle = {Understanding Underground Incentivized Review Services},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642342},\ndoi = {10.1145/3613904.3642342},\nabstract = {While human factors in fraud have been studied by the HCI and security communities, most research has been directed to understanding either the victims’ perspectives or prevention strategies, and not on fraudsters, their motivations and operation techniques. Additionally, the focus has been on a narrow set of problems: phishing, spam and bullying. In this work, we seek to understand review fraud on e-commerce platforms through an HCI lens. Through surveys with real fraudsters (N=36 agents and N=38 reviewers), we uncover sophisticated recruitment, execution, and reporting mechanisms fraudsters use to scale their operation while resisting takedown attempts, including the use of AI tools like ChatGPT. We find that countermeasures that crack down on communication channels through which these services operate are effective in combating incentivized reviews. This research sheds light on the complex landscape of incentivized reviews, providing insights into the mechanics of underground services and their resilience to removal efforts.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {950},\nnumpages = {18},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642158,\nauthor = {Qian, Xun and Wang, Tianyi and Xu, Xuhai and Jonker, Tanya R. and Todi, Kashyap},\ntitle = {Fast-Forward Reality: Authoring Error-Free Context-Aware Policies with Real-Time Unit Tests in Extended Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642158},\ndoi = {10.1145/3613904.3642158},\nabstract = {Advances in ubiquitous computing have enabled end-user authoring of context-aware policies (CAPs) that control smart devices based on specific contexts of the user and environment. However, authoring CAPs accurately and avoiding run-time errors is challenging for end-users as it is difficult to foresee CAP behaviors under complex real-world conditions. We propose Fast-Forward Reality, an Extended Reality (XR) based authoring workflow that enables end-users to iteratively author and refine CAPs by validating their behaviors via simulated unit test cases. We develop a computational approach to automatically generate test cases based on the authored CAP and the user’s context history. Our system delivers each test case with immersive visualizations in XR, facilitating users to verify the CAP behavior and identify necessary refinements. We evaluated Fast-Forward Reality in a user study (N=12). Our authoring and validation process improved the accuracy of CAPs and the users provided positive feedback on the system usability.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {951},\nnumpages = {17},\nkeywords = {Context-Aware Policy, Extended Reality, Unit Test, Validation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642864,\nauthor = {Rasch, Julian and Perzl, Florian and Weiss, Yannick and M\\\"{u}ller, Florian},\ntitle = {Just Undo It: Exploring Undo Mechanics in Multi-User Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642864},\ndoi = {10.1145/3613904.3642864},\nabstract = {With the proliferation of VR and a metaverse on the horizon, many multi-user activities are migrating to the VR world, calling for effective collaboration support. As one key feature, traditional collaborative systems provide users with undo mechanics to reverse errors and other unwanted changes. While undo has been extensively researched in this domain and is now considered industry standard, it is strikingly absent for VR systems in research and industry. This work addresses this research gap by exploring different undo techniques for basic object manipulation in different collaboration modes in VR. We conducted a study involving 32 participants organized in teams of two. Here, we studied users’ performance and preferences in a tower stacking task, varying the available undo techniques and their mode of collaboration. The results suggest that users desire and use undo in VR and that the choice of the undo technique impacts users’ performance and social connection.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {952},\nnumpages = {14},\nkeywords = {CSCW, Connectedness, Multi-User, SocialVR, Undo, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642925,\nauthor = {Li, Jingyi and Kristensson, Per Ola},\ntitle = {On the Benefits of Image-Schematic Metaphors when Designing Mixed Reality Systems},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642925},\ndoi = {10.1145/3613904.3642925},\nabstract = {A Mixed Reality (MR) system encompasses various aspects, such as visualization and spatial registration of user interface elements, user interactions and interaction feedback. Image-schematic metaphors (ISMs) are universal knowledge structures shared by a wide range of users. They hold a theoretical promise of facilitating greater ease of learning and use for interactive systems without costly adaptations. This paper investigates whether image-schematic metaphors (ISMs) can improve user learning, by comparing an existing MR instruction authoring system with or without ISM enhancements. In a user study with 32 participants, we found that the ISM-enhanced system significantly improved task performance, learnability and mental efficiency compared to the baseline. Participants also rated the ISM-enhanced system significantly higher in terms of perspicuity, efficiency, and novelty. These results empirically demonstrate multiple benefits of ISMs when integrated into the design of this MR system and encourage further studies to explore the wider applicability of ISMs in user interface design.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {953},\nnumpages = {20},\nkeywords = {Image Schema, Instruction Authoring, Mixed Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642399,\nauthor = {Li, Zhipeng and Cheng, Yi Fei and Yan, Yukang and Lindlbauer, David},\ntitle = {Predicting the Noticeability of Dynamic Virtual Elements in Virtual Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642399},\ndoi = {10.1145/3613904.3642399},\nabstract = {While Virtual Reality (VR) systems can present virtual elements such as notifications anywhere, designing them so they are not missed by or distracting to users is highly challenging for content creators. To address this challenge, we introduce a novel approach to predict the noticeability of virtual elements. It computes the visual saliency distribution of what users see, and analyzes the temporal changes of the distribution with respect to the dynamic virtual elements that are animated. The computed features serve as input for a long short-term memory (LSTM) model that predicts whether a virtual element will be noticed. Our approach is based on data collected from 24 users in different VR environments performing tasks such as watching a video or typing. We evaluate our approach (n = 12), and show that it can predict the timing of when users notice a change to a virtual element within 2.56 sec compared to a ground truth, and demonstrate the versatility of our approach with a set of applications. We believe that our predictive approach opens the path for computational design tools that assist VR content creators in creating interfaces that automatically adapt virtual elements based on noticeability.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {954},\nnumpages = {17},\nkeywords = {Computational Interaction, Mixed Reality, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642360,\nauthor = {He, Ziyao and Li, Shiyuan and Song, Yunpeng and Cai, Zhongmin},\ntitle = {Towards Building Condition-Based Cross-Modality Intention-Aware Human-AI Cooperation under VR Environment},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642360},\ndoi = {10.1145/3613904.3642360},\nabstract = {To address critical challenges in effectively identifying user intent and forming relevant information presentations and recommendations in VR environments, we propose an innovative condition-based multi-modal human-AI cooperation framework. It highlights the intent tuples (intent, condition, intent prompt, action prompt) and 2-Large-Language-Models (2-LLMs) architecture. This design, utilizes “condition” as the core to describe tasks, dynamically match user interactions with intentions, and empower generations of various tailored multi-modal AI responses. The architecture of 2-LLMs separates the roles of intent detection and action generation, decreasing the prompt length and helping with generating appropriate responses. We implemented a VR-based intelligent furniture purchasing system based on the proposed framework and conducted a three-phase comparative user study. The results conclusively demonstrate the system’s superiority in time efficiency and accuracy, intention conveyance improvements, effective product acquisitions, and user satisfaction and cooperation preference. Our framework provides a promising approach towards personalized and efficient user experiences in VR.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {955},\nnumpages = {13},\nkeywords = {Action Generation, Human-AI Cooperation, Intention Detection, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642426,\nauthor = {Zargham, Nima and Fetni, Mohamed Lamine and Spillner, Laura and Muender, Thomas and Malaka, Rainer},\ntitle = {\"I Know What You Mean\": Context-Aware Recognition to Enhance Speech-Based Games},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642426},\ndoi = {10.1145/3613904.3642426},\nabstract = {Recent advances in language processing and speech recognition open up a large opportunity for video game companies to embrace voice interaction as an intuitive feature and appealing game mechanics. However, speech-based systems still remain liable to recognition errors. These add a layer of challenge on top of the game’s existing obstacles, preventing players from reaching their goals and thus often resulting in player frustration. This work investigates a novel method called context-aware speech recognition, where the game environment and actions are used as supplementary information to enhance recognition in a speech-based game. In a between-subject user study (<Formula format=\"inline\"><TexMath><?TeX $N~{=}~40$?></TexMath><AltText>Math 1</AltText><File name=\"chi24-534-inline1\" type=\"svg\"/></Formula>), we compared our proposed method with a standard method in which recognition is based only on the voice input without taking context into account. Our results indicate that our proposed method could improve the player experience and the usability of the speech system.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {956},\nnumpages = {18},\nkeywords = {Game Design, Speech Recognition, Speech-Based Systems, Voice Interaction, Voice-Controlled Game},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642588,\nauthor = {Lee, Hanbyeol and Lee, Seyeon and Nallapati, Rohan and Uh, Youngjung and Lee, Byungjoo},\ntitle = {Characterizing and Quantifying Expert Input Behavior in League of Legends},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642588},\ndoi = {10.1145/3613904.3642588},\nabstract = {To achieve high performance in esports, players must be able to effectively and efficiently control input devices such as a computer mouse and keyboard (i.e., input skills). Characterizing and quantifying a player’s input skills can provide useful insights, but collecting and analyzing sufficient amounts of data in ecologically valid settings remains a challenge. Targeting the popular esports game, League of Legends, we go beyond the limitations of previous studies and demonstrate a holistic pipeline of input behavior analysis: from quantifying the quality of players’ input behavior (i.e., input skill) to training players based on the analysis. Based on interviews with five top-tier professionals and analysis of input behavior logs from 4,835 matches played freely at home collected from 193 players (including 18 professionals), we confirmed that players with higher ranks in the game implement eight different input skills with higher quality. In a three-week follow-up study using a training aid that visualizes a player’s input skill levels, we found that the analysis provided players with actionable lessons, potentially leading to meaningful changes in their input behavior.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {957},\nnumpages = {21},\nkeywords = {Esports, Input Skill, League of Legends, Mechanical Skill},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642654,\nauthor = {Adamkiewicz, Krzysztof and Dominiak, Julia and Walczak, Anna and Romanowski, Andrzej and Wo\\'{z}niak, Pawe\\l{} W.},\ntitle = {Screenless Interactive Tabletop Gaming with Capacitive Surface Sensing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642654},\ndoi = {10.1145/3613904.3642654},\nabstract = {Many interactive systems that support tabletop games either augment the experience with additional elements or transform game components into digital counterparts, e.g., using mixed reality. However, as many users prefer tangible game elements, digital augmentations can disrupt the immersion they seek to enhance, often due to the complexity of the hardware used. Responding to this challenge, we designed a screenless interactive tabletop system with capacitive sensing. The system is suitable for novice players and provides automatic score-keeping. Our method eliminates the need for external sensors and retains all original game pieces intact. We evaluated our system in a study with a forest planting game (n = 20). Gameplay with our system exhibited shorter turn duration, and participants adopted more effective strategies than in traditional gameplay. These results underscore the potential of screenless interactive tabletops to amplify the gaming experience without causing distractions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {958},\nnumpages = {10},\nkeywords = {3D Printing, Board Games, Capacitive Sensing, Machine Learning, Tangibles, Touchscreen},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642523,\nauthor = {Frommel, Julian and Mandryk, Regan L},\ntitle = {Toxicity in Online Games: The Prevalence and Efficacy of Coping Strategies},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642523},\ndoi = {10.1145/3613904.3642523},\nabstract = {Toxicity is pervasive in online multiplayer games, exposing players to disruptive and harmful behaviours. Players employ various approaches to cope with exposure to toxicity; however, game designers and researchers lack guidance on how to implement coping support within games. In this paper, we first conduct a formative study to collect a comprehensive list of coping approaches from toxicity literature and use affinity mapping to identify overarching game-based coping strategies. Then, we report findings from a survey (n = 85) on players’ experiences with toxicity, how they employ the identified coping strategies, how games support coping, and their general coping styles. Our paper contributes a framework for coping strategies to deal with game-based toxicity and provides insights into the prevalence of these strategies among players and factors that affect their usage and effectiveness. These findings can be used to guide better in-game tools that help players mitigate the harm caused by toxicity.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {959},\nnumpages = {12},\nkeywords = {coping, esports, online games, toxicity},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642418,\nauthor = {Markovitch, Benny and Markopoulos, Panos and Birk, Max V.},\ntitle = {Tunnel Runner: a Proof-of-principle for the Feasibility and Benefits of Facilitating Players' Sense of Control in Cognitive Assessment Games},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642418},\ndoi = {10.1145/3613904.3642418},\nabstract = {Cognitive assessment games attempt to improve cognitive assessment’s experience and data quality by implementing game-like features, e.g., points and narratives. However, cognitive games maintain the repetitiveness and restricted control common in traditional cognitive assessment tasks, which thwart players’ sense of control and impair their motivation and experience. Leading to only modest improvements over traditional tasks. To demonstrate the value of designing cognitive games that facilitate a sense of control, we created and evaluated the infinite runner game Tunnel Runner. In two studies (n1=117, n2=121), we assessed the validity of the game’s cognitive measurements (inhibitory control, decision-making) against traditional cognitive tasks. Our results demonstrate Tunnel Runner’s valid and reliable cognitive measurements alongside substantial improvements to players’ experience and sense of control compared to the cognitive tasks, showcasing the feasibility and benefits of cognitive games designed to facilitate players’ sense of control.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {960},\nnumpages = {18},\nkeywords = {autonomy, cognitive assessment, decision-making under uncertainty, game-based measurement, inhibitory control, player experience, sense of control},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641930,\nauthor = {Xiao, Lan and Bandukda, Maryam and Angerbauer, Katrin and Lin, Weiyue and Bhatnagar, Tigmanshu and Sedlmair, Michael and Holloway, Catherine},\ntitle = {A Systematic Review of Ability-diverse Collaboration through Ability-based Lens in HCI},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641930},\ndoi = {10.1145/3613904.3641930},\nabstract = {In a world where diversity is increasingly recognised and celebrated, it is important for HCI to embrace the evolving methods and theories for technologies to reflect the diversity of its users and be ability-centric. Interdependence Theory, an example of this evolution, highlights the interpersonal relationships between humans and technologies and how technologies should be designed to meet shared goals and outcomes for people, regardless of their abilities. This necessitates a contemporary understanding of \"ability-diverse collaboration,\" which motivated this review. In this review, we offer an analysis of 117 papers sourced from the ACM Digital Library spanning the last two decades. We contribute (1) a unified taxonomy and the Ability-Diverse Collaboration Framework, (2) a reflective discussion and mapping of the current design space, and (3) future research opportunities and challenges. Finally, we have released our data and analysis tool to encourage the HCI research community to contribute to this ongoing effort.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {961},\nnumpages = {21},\nkeywords = {Interdependence, ability-based method, accessibility, collaboration},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642777,\nauthor = {Taeb, Maryam and Swearngin, Amanda and Schoop, Eldon and Cheng, Ruijia and Jiang, Yue and Nichols, Jeffrey},\ntitle = {AXNav: Replaying Accessibility Tests from Natural Language},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642777},\ndoi = {10.1145/3613904.3642777},\nabstract = {Developers and quality assurance testers often rely on manual testing to test accessibility features throughout the product lifecycle. Unfortunately, manual testing can be tedious, often has an overwhelming scope, and can be difficult to schedule amongst other development milestones. Recently, Large Language Models (LLMs) have been used for a variety of tasks including automation of UIs. However, to our knowledge, no one has yet explored the use of LLMs in controlling assistive technologies for the purposes of supporting accessibility testing. In this paper, we explore the requirements of a natural language based accessibility testing workflow, starting with a formative study. From this we build a system that takes a manual accessibility test instruction in natural language (e.g., “Search for a show in VoiceOver”) as input and uses an LLM combined with pixel-based UI Understanding models to execute the test and produce a chaptered, navigable video. In each video, to help QA testers, we apply heuristics to detect and flag accessibility issues (e.g., Text size not increasing with Large Text enabled, VoiceOver navigation loops). We evaluate this system through a 10-participant user study with accessibility QA professionals who indicated that the tool would be very useful in their current work and performed tests similarly to how they would manually test the features. The study also reveals insights for future work on using LLMs for accessibility testing.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {962},\nnumpages = {16},\nkeywords = {Accessibility, Large language models, UI testing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642767,\nauthor = {Kwon, Nahyun and Lu, Qian and Qazi, Muhammad Hasham and Liu, Joanne and Oh, Changhoon and Kong, Shu and Kim, Jeeeun},\ntitle = {AccessLens: Auto-detecting Inaccessibility of Everyday Objects},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642767},\ndoi = {10.1145/3613904.3642767},\nabstract = {In our increasingly diverse society, everyday physical interfaces often present barriers, impacting individuals across various contexts. This oversight, from small cabinet knobs to identical wall switches that can pose different contextual challenges, highlights an imperative need for solutions. Leveraging low-cost 3D-printed augmentations such as knob magnifiers and tactile labels seems promising, yet the process of discovering unrecognized barriers remains challenging because disability is context-dependent. We introduce AccessLens, an end-to-end system designed to identify inaccessible interfaces in daily objects, and recommend 3D-printable augmentations for accessibility enhancement. Our approach involves training a detector using the novel AccessDB dataset designed to automatically recognize 21 distinct Inaccessibility Classes (e.g., bar-small and round-rotate) within 6 common object categories (e.g., handle and knob). AccessMeta serves as a robust way to build a comprehensive dictionary linking these accessibility classes to open-source 3D augmentation designs. Experiments demonstrate our detector’s performance in detecting inaccessible objects.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {963},\nnumpages = {17},\nkeywords = {3D assistive design, end-user interface, object detection},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642526,\nauthor = {Seixas Pereira, Let\\'{\\i}cia and Matos, Maria and Duarte, Carlos},\ntitle = {Exploring Mobile Device Accessibility: Challenges, Insights, and Recommendations for Evaluation Methodologies},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642526},\ndoi = {10.1145/3613904.3642526},\nabstract = {With the ubiquitous use of mobile applications, it is paramount that they are accessible, so they can empower all users, including those with different needs. Determining if an app is accessible implies conducting an accessibility evaluation. While accessibility evaluations have been thoroughly studied in the web domain, there are still many open questions when evaluating mobile applications. This paper investigates mobile accessibility evaluation methodologies. We conducted four studies, including an examination of accessibility reports from European Member-states, interviews with accessibility experts, manual evaluations, and usability tests involving users. Our investigations have uncovered significant limitations in current evaluation methods, suggesting that the absence of authoritative guidelines and standards, similar to what exists for the web, but tailored specifically to mobile devices, hampers the effectiveness of accessibility evaluation and monitoring activities. Based on our findings, we present a set of recommendations aimed at improving the evaluation methodologies for assessing mobile applications’ accessibility.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {964},\nnumpages = {17},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642065,\nauthor = {Liu, Xingyu Bruce and Li, Jiahao Nick and Kim, David and Chen, Xiang 'Anthony' and Du, Ruofei},\ntitle = {Human I/O: Towards a Unified Approach to Detecting Situational Impairments},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642065},\ndoi = {10.1145/3613904.3642065},\nabstract = {Situationally Induced Impairments and Disabilities (SIIDs) can significantly hinder user experience in contexts such as poor lighting, noise, and multi-tasking. While prior research has introduced algorithms and systems to address these impairments, they predominantly cater to specific tasks or environments and fail to accommodate the diverse and dynamic nature of SIIDs. We introduce Human I/O, a unified approach to detecting a wide range of SIIDs by gauging the availability of human input/output channels. Leveraging egocentric vision, multimodal sensing and reasoning with large language models, Human I/O achieves a 0.22 mean absolute error and a 82\\% accuracy in availability prediction across 60 in-the-wild egocentric video recordings in 32 different scenarios. Furthermore, while the core focus of our work is on the detection of SIIDs rather than the creation of adaptive user interfaces, we showcase the efficacy of our prototype via a user study with 10 participants. Findings suggest that Human I/O significantly reduces effort and improves user experience in the presence of SIIDs, paving the way for more adaptive and accessible interactive systems in the future.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {965},\nnumpages = {18},\nkeywords = {augmented reality, context awareness, large language models, multimodal sensing, situational impairments},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642380,\nauthor = {Langlotz, Tobias and Sutton, Jonathan and Regenbrecht, Holger},\ntitle = {A Design Space for Vision Augmentations and Augmented Human Perception using Digital Eyewear},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642380},\ndoi = {10.1145/3613904.3642380},\nabstract = {Head-mounted displays were originally introduced to directly present computer-generated information to the human eye. More recently, the potential to use this kind of technology to support human vision and augment human perception has become actively pursued with applications such as compensating for visual impairments or aiding unimpaired vision. Unfortunately, a systematic analysis of the field is missing. Within this work, we close that gap by presenting a design space for vision augmentations that allows research to systematically explore the field of digital eyewear for vision aid and how it can augment the human visual system. We test our design space against currently available solutions and conceptually develop new solutions. The design space and findings can guide future development and can lead to a consistent categorisation of the many existing approaches.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {966},\nnumpages = {16},\nkeywords = {accessibility, augmented human, augmented reality, design space, human augmentation, mixed reality, sensory augmentation, vision augmentation, visual impairments},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642508,\nauthor = {Aljedaani, Wajdi and Mkaouer, Mohamed Wiem and Eler, Marcelo Medeiros and Kessentini, Marouane},\ntitle = {Empirical Investigation of Accessibility Bug Reports in Mobile Platforms: A Chromium Case Study},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642508},\ndoi = {10.1145/3613904.3642508},\nabstract = {Accessibility is an important quality factor of mobile applications. Many studies have shown that, despite the availability of many resources to guide the development of accessible software, most apps and web applications contain many accessibility issues. Some researchers surveyed professionals and organizations to understand the lack of accessibility during software development, but few studies have investigated how developers and organizations respond to accessibility bug reports. Therefore, this paper analyzes accessibility bug reports posted in the Chromium repository to understand how developers and organizations handle them. More specifically, we want to determine the frequency of accessibility bug reports over time, the time-to-fix compared to traditional bug reports (e.g., functional bugs), and the types of accessibility barriers reported. Results show that the frequency of accessibility reports has increased over the years, and accessibility bugs take longer to be fixed, as they tend to be given low priority.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {967},\nnumpages = {17},\nkeywords = {Accessibility, Bug Report, Bug Repository, Empirical Studies., Google Chromium, Mobile applications, Open Source},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642140,\nauthor = {Su, Xia and Zhang, Han and Cheng, Kaiming and Lee, Jaewook and Liu, Qiaochu and Olson, Wyatt and Froehlich, Jon E.},\ntitle = {RASSAR: Room Accessibility and Safety Scanning in Augmented Reality},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642140},\ndoi = {10.1145/3613904.3642140},\nabstract = {The safety and accessibility of our homes is critical to quality of life and evolves as we age, become ill, host guests, or experience life events such as having children. Researchers and health professionals have created assessment instruments such as checklists that enable homeowners and trained experts to identify and mitigate safety and access issues. With advances in computer vision, augmented reality (AR), and mobile sensors, new approaches are now possible. We introduce RASSAR, a mobile AR application for semi-automatically identifying, localizing, and visualizing indoor accessibility and safety issues such as an inaccessible table height or unsafe loose rugs using LiDAR and real-time computer vision. We present findings from three studies: a formative study with 18 participants across five stakeholder groups to inform the design of RASSAR, a technical performance evaluation across ten homes demonstrating state-of-the-art performance, and a user study with six stakeholders. We close with a discussion of future AI-based indoor accessibility assessment tools, RASSAR’s extensibility, and key application scenarios.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {968},\nnumpages = {17},\nkeywords = {Accessibility, Augmented Reality, Computer Vision, Indoor Accessibility Auditing, Object Detection},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642003,\nauthor = {Li, Chu and Ma, Katrina Oi Yau and Saugstad, Michael and Fujii, Kie and Delaney, Molly and Eisenberg, Yochai and Labb\\'{e}, Delphine and Shanley, Judy L and Snyder, Devon and Thomas, Florian P P and Froehlich, Jon E.},\ntitle = {“I never realized sidewalks were a big deal”: A Case Study of a Community-Driven Sidewalk Accessibility Assessment using Project Sidewalk},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642003},\ndoi = {10.1145/3613904.3642003},\nabstract = {Despite decades of effort, pedestrian infrastructure in cities continues to be unsafe or inaccessible to people with disabilities. In this paper, we examine the potential of community-driven digital civics to assess sidewalk accessibility through a deployment study of an open-source crowdsourcing tool called Project Sidewalk. We explore Project Sidewalk’s potential as a platform for civic learning and service. Specifically, we assess its effectiveness as a tool for community members to learn about human mobility, urban planning, and accessibility advocacy. Our findings demonstrate that community-driven digital civics can support accessibility advocacy and education, raise community awareness, and drive pro-social behavioral change. We also outline key considerations for deploying digital civic tools in future community-led accessibility initiatives.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {969},\nnumpages = {18},\nkeywords = {accessibility, community science, crowdsourcing, digital civics, service learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642889,\nauthor = {Lassak, Leona and Markert, Philipp and Golla, Maximilian and Stobert, Elizabeth and D\\\"{u}rmuth, Markus},\ntitle = {A Comparative Long-Term Study of Fallback Authentication Schemes},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642889},\ndoi = {10.1145/3613904.3642889},\nabstract = {Fallback authentication, the process of re-establishing access to an account when the primary authenticator is unavailable, holds critical significance. Approaches range from secondary channels like email and SMS to personal knowledge questions (PKQs) and social authentication. A key difference to primary authentication is that the duration between enrollment and authentication can be much longer, typically months or years. However, few systems have been studied over extended timeframes, making it difficult to know how well these systems truly help users recover their accounts. We also lack meaningful comparisons of schemes as most prior work examined two mechanisms at most. We report the results of a long-term user study of the usability of fallback authentication over 18 months to provide a fair comparison of the four most commonly used fallback authentication methods. We show that users prefer email and SMS-based methods, while mechanisms based on PKQs and trustees lag regarding successful resets and convenience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {970},\nnumpages = {19},\nkeywords = {SMS, email, fallback authentication, personal knowledge questions},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642301,\nauthor = {Shrestha, Ankit and Flood, Audrey and Sohrawardi, Saniat and Wright, Matthew and Al-Ameen, Mahdi Nasrullah},\ntitle = {A First Look into Targeted Clickbait and its Countermeasures: The Power of Storytelling},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642301},\ndoi = {10.1145/3613904.3642301},\nabstract = {Clickbait headlines work through superlatives and intensifiers, creating information gaps to increase the relevance of their associated links that direct users to time-wasting and sometimes even malicious websites. This approach can be amplified using targeted clickbait that takes publicly available information from social media to align clickbait to users’ preferences and beliefs. In this work, we first conducted preliminary studies to understand the influence of targeted clickbait on users’ clicking behavior. Based on our findings, we involved 24 users in the participatory design of story-based warnings against targeted clickbait. Our analysis of user-created warnings led to four design variations, which we evaluated through an online survey over Amazon Mechanical Turk. Our findings show the significance of integrating information with persuasive narratives to create effective warnings against targeted clickbait. Overall, our studies provide valuable insights into understanding users’ perceptions and behaviors towards targeted clickbait, and the efficacy of story-based interventions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {971},\nnumpages = {23},\nkeywords = {interventions, qualitative study, quantitative study, storytelling, targeted clickbait},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642456,\nauthor = {Jenkins, Adam D G and Liu, Linsen and Wolters, Maria K and Vaniea, Kami},\ntitle = {Not as easy as just update: Survey of System Administrators and Patching Behaviours},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642456},\ndoi = {10.1145/3613904.3642456},\nabstract = {Patching software theoretically leads to improvements including security critical changes, but it can also lead to new issues. For System Administrators (sysadmins) new issues can negatively impact operations at their organization. While mitigation options like test environments exist, little is known about their prevalence or how contextual factors like size of organization impact the practice of Patch Management. We surveyed 220 sysadmins engaged in Patch Management to investigate self-reported behaviors. We found that dedicated testing environments are not as prevalent as previously assumed. We also expand on known behaviours that sysadmins perform when facing a troublesome patch, such as employing a range of problem solving behaviours to inform their patching decisions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {972},\nnumpages = {17},\nkeywords = {patch management, software updates, survey, system administrators},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642432,\nauthor = {Borgert, Nele and Jansen, Luisa and B\\\"{o}se, Imke and Friedauer, Jennifer and Sasse, M. Angela and Elson, Malte},\ntitle = {Self-Efficacy and Security Behavior: Results from a Systematic Review of Research Methods},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642432},\ndoi = {10.1145/3613904.3642432},\nabstract = {Amidst growing IT security challenges, psychological underpinnings of security behaviors have received considerable interest, e.g. cybersecurity Self-Efficacy (SE), the belief in one’s own ability to enact cybersecurity-related skills. Due to diverging definitions and proposed mechanisms, research methods in this field vary considerably, potentially impeding replicable evidence and meaningful research synthesis. We report a preregistered systematic literature review investigating (a) cybersecurity SE measures, (b) SE’s proposed roles, and (c) intervention approaches. We minimized selection bias by detailed exclusion criteria, interdisciplinary search strategy, and double coding. Among 174 cybersecurity SE studies (2010-2021) from 18 databases with 55,758 subjects, we identified 173 different SE measures with considerable differences in psychometric quality and validity evidence. We found 276 variables as assumed causes/outcomes of cybersecurity SE and identified 13 intervention designs. This review demonstrates the extent of methodological and conceptual fragmentation in cybersecurity SE research. We offer recommendations to inspire our research community toward standardization.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {973},\nnumpages = {32},\nkeywords = {Cybersecurity, Research Methods, Self-Efficacy, Systematic Review},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642291,\nauthor = {Si, Janice Jianing and Sharma, Tanusree and Wang, Kanye Ye},\ntitle = {Understanding User-Perceived Security Risks and Mitigation Strategies in the Web3 Ecosystem},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642291},\ndoi = {10.1145/3613904.3642291},\nabstract = {The advent of Web3 technologies promises unprecedented levels of user control and autonomy. However, this decentralization shifts the burden of security onto the users, making it crucial to understand their security behaviors and perceptions. To address this, our study introduces a comprehensive framework that identifies four core components of user interaction within the Web3 ecosystem: blockchain infrastructures, Web3-based Decentralized Applications (DApps), online communities, and off-chain cryptocurrency platforms. We delve into the security concerns perceived by users in each of these components and analyze the mitigation strategies they employ, ranging from risk assessment and aversion to diversification and acceptance. We further discuss the landscape of both technical and human-induced security risks in the Web3 ecosystem, identify the unique security differences between Web2 and Web3, and highlight key challenges that render users vulnerable, to provide implications for security design in Web3.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {974},\nnumpages = {22},\nkeywords = {Web3 ecosystem, mitigation strategy, security risk, user perception},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642462,\nauthor = {Masson, Damien and Malacria, Sylvain and Casiez, G\\'{e}ry and Vogel, Daniel},\ntitle = {DirectGPT: A Direct Manipulation Interface to Interact with Large Language Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642462},\ndoi = {10.1145/3613904.3642462},\nabstract = {We characterize and demonstrate how the principles of direct manipulation can improve interaction with large language models. This includes: continuous representation of generated objects of interest; reuse of prompt syntax in a toolbar of commands; manipulable outputs to compose or control the effect of prompts; and undo mechanisms. This idea is exemplified in DirectGPT, a user interface layer on top of ChatGPT that works by transforming direct manipulation actions to engineered prompts. A study shows participants were 50\\% faster and relied on 50\\% fewer and 72\\% shorter prompts to edit text, code, and vector images compared to baseline ChatGPT. Our work contributes a validated approach to integrate LLMs into traditional software using direct manipulation. Data, code, and demo available at https://osf.io/3wt6s.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {975},\nnumpages = {16},\nkeywords = {direct manipulation, large language models, prompt engineering},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642335,\nauthor = {Wang, Zijie J. and Kulkarni, Chinmay and Wilcox, Lauren and Terry, Michael and Madaio, Michael},\ntitle = {Farsight: Fostering Responsible AI Awareness During AI Application Prototyping},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642335},\ndoi = {10.1145/3613904.3642335},\nabstract = {Prompt-based interfaces for Large Language Models (LLMs) have made prototyping and building AI-powered applications easier than ever before. However, identifying potential harms that may arise from AI applications remains a challenge, particularly during prompt-based prototyping. To address this, we present Farsight, a novel in situ interactive tool that helps people identify potential harms from the AI applications they are prototyping. Based on a user’s prompt, Farsight highlights news articles about relevant AI incidents and allows users to explore and edit LLM-generated use cases, stakeholders, and harms. We report design insights from a co-design study with 10 AI prototypers and findings from a user study with 42 AI prototypers. After using Farsight, AI prototypers in our user study are better able to independently identify potential harms associated with a prompt and find our tool more useful and usable than existing resources. Their qualitative feedback also highlights that Farsight encourages them to focus on end-users and think beyond immediate harms. We discuss these findings and reflect on their implications for designing AI prototyping experiences that meaningfully engage with AI harms. Farsight is publicly accessible at: https://pair-code.github.io/farsight.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {976},\nnumpages = {40},\nkeywords = {Human-AI Collaboration, Large Language Models, Responsible AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641955,\nauthor = {Fu, Yue and Foell, Sami and Xu, Xuhai and Hiniker, Alexis},\ntitle = {From Text to Self: Users’ Perception of AIMC Tools on Interpersonal Communication and Self},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641955},\ndoi = {10.1145/3613904.3641955},\nabstract = {In the rapidly evolving landscape of AI-mediated communication (AIMC), tools powered by Large Language Models (LLMs) are becoming integral to interpersonal communication. Employing a mixed-methods approach, we conducted a one-week diary and interview study to explore users’ perceptions of these tools’ ability to: 1) support interpersonal communication in the short-term, and 2) lead to potential long-term effects. Our findings indicate that participants view AIMC support favorably, citing benefits such as increased communication confidence, finding precise language to express their thoughts, and navigating linguistic and cultural barriers. However, our findings also show current limitations of AIMC tools, including verbosity, unnatural responses, and excessive emotional intensity. These shortcomings are further exacerbated by user concerns about inauthenticity and potential overreliance on the technology. We identify four key communication spaces delineated by communication stakes (high or low) and relationship dynamics (formal or informal) that differentially predict users’ attitudes toward AIMC tools. Specifically, participants report that these tools are more suitable for communicating in formal relationships than informal ones and more beneficial in high-stakes than low-stakes communication.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {977},\nnumpages = {17},\nkeywords = {computer mediated communication, diary study},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642076,\nauthor = {Rae, Irene},\ntitle = {The Effects of Perceived AI Use On Content Perceptions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642076},\ndoi = {10.1145/3613904.3642076},\nabstract = {There is a potential future where the content created by a human and an AI are indistinguishable. In this future, if you can’t tell the difference, does it matter? We conducted a 3 (Assigned creator: human, human with AI assistance, AI) by 4 (Context: news, travel, health, and jokes) mixed-design experiment where participants evaluated human-written content that was presented as created by a human, a human with AI assistance, or an AI. We found that participants felt more negatively about the content creator and were less satisfied when they thought AI was used, but assigned creator had no effect on content judgments. We also identified five interpretations for how participants thought AI use affected the content creation process. Our work suggests that informing users about AI use may not have the intended effect of helping consumers make content judgments and may instead damage the relationship between creators and followers.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {978},\nnumpages = {14},\nkeywords = {ai generated content, artificial intelligence, credibility, human-AI interaction, information literacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642135,\nauthor = {Wester, Joel and Schrills, Tim and Pohl, Henning and van Berkel, Niels},\ntitle = {“As an AI language model, I cannot”: Investigating LLM Denials of User Requests},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642135},\ndoi = {10.1145/3613904.3642135},\nabstract = {Users ask large language models (LLMs) to help with their homework, for lifestyle advice, or for support in making challenging decisions. Yet LLMs are often unable to fulfil these requests, either as a result of their technical inabilities or policies restricting their responses. To investigate the effect of LLMs denying user requests, we evaluate participants’ perceptions of different denial styles. We compare specific denial styles (baseline, factual, diverting, and opinionated) across two studies, respectively focusing on LLM’s technical limitations and their social policy restrictions. Our results indicate significant differences in users’ perceptions of the denials between the denial styles. The baseline denial, which provided participants with brief denials without any motivation, was rated significantly higher on frustration and significantly lower on usefulness, appropriateness, and relevance. In contrast, we found that participants generally appreciated the diverting denial style. We provide design recommendations for LLM denials that better meet peoples’ denial expectations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {979},\nnumpages = {14},\nkeywords = {Breakdowns, Denials, Errors, GPT-4, Large Language Models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642174,\nauthor = {Kim, Injung and Lee, Adam J.},\ntitle = {\"I know what you did last semester\": Understanding Privacy Expectations and Preferences in the Smart Campus},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642174},\ndoi = {10.1145/3613904.3642174},\nabstract = {Sensing technologies in smart campuses help make them sustainable and well-connected environments. However, as with other smart environments, smart campuses can cause privacy concerns during and after deployment. We present the results of a 14-day in-situ study designed to understand peoples’ sentiments about sensing capabilities in smart campuses and how they would specify privacy preferences. In contrast to prior work, which reported the importance of sensing modality and purpose, our findings indicate that indoor location type and recipient are primary determinants for comfort, surprise, notification preferences, and allowance of data collection. Further, we observed that indoor location type influences privacy control willingness and how users specify sensor controlling rule. For example, our participants allowed policy-controlled data collection in group areas while denying it in learning areas. Finally, we suggest that academic environments are unique, possibly due to the complex relationships between students, staff, and faculty.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {980},\nnumpages = {15},\nkeywords = {experience sampling, privacy, sensing data collection, smart campus},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642863,\nauthor = {Delgado Rodriguez, Sarah and Chatterjee, Priyasha and Dao Phuong, Anh and Alt, Florian and Marky, Karola},\ntitle = {Do You Need to Touch? Exploring Correlations between Personal Attributes and Preferences for Tangible Privacy Mechanisms},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642863},\ndoi = {10.1145/3613904.3642863},\nabstract = {This paper explores how personal attributes, such as age, gender, technological expertise, or “need for touch”, correlate with people’s preferences for properties of tangible privacy protection mechanisms, for example, physically covering a camera. For this, we conducted an online survey (N = 444) where we captured participants’ preferences of eight established tangible privacy mechanisms well-known in daily life, their perceptions of effective privacy protection, and personal attributes. We found that the attributes that correlated most strongly with participants’ perceptions of the established tangible privacy mechanisms were their “need for touch” and previous experiences with the mechanisms. We use our findings to identify desirable characteristics of tangible mechanisms to better inform future tangible, digital, and mixed privacy protections. We also show which individuals benefit most from tangibles, ultimately motivating a more individual and effective approach to privacy protection in the future.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {981},\nnumpages = {23},\nkeywords = {internet of things, privacy, tangible, tangible privacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642640,\nauthor = {Ghaiumy Anaraky, Reza and Li, Yao and Cho, Hichang and Huang, Danny Yuxing and Byrne, Kaileigh Angela and Knijnenburg, Bart and Nov, Oded},\ntitle = {Personalizing Privacy Protection With Individuals' Regulatory Focus: Would You Preserve or Enhance Your Information Privacy?},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642640},\ndoi = {10.1145/3613904.3642640},\nabstract = {In this study, we explore the effectiveness of persuasive messages endorsing the adoption of a privacy protection technology (IoT Inspector) tailored to individuals’ regulatory focus (promotion or prevention). We explore if and how regulatory fit (i.e., tuning the goal-pursuit mechanism to individuals’ internal regulatory focus) can increase persuasion and adoption. We conducted a between-subject experiment (N = 236) presenting participants with the IoT Inspector in gain (\"Privacy Enhancing Technology\"—PET) or loss (\"Privacy Preserving Technology\"—PPT) framing. Results show that the effect of regulatory fit on adoption is mediated by trust and privacy calculus processes: prevention-focused users who read the PPT message trust the tool more. Furthermore, privacy calculus favors using the tool when promotion-focused individuals read the PET message. We discuss the contribution of understanding the cognitive mechanisms behind regulatory fit in privacy decision-making to support privacy protection.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {982},\nnumpages = {17},\nkeywords = {Framing effect, Personalized persuasion, Privacy, Regulatory fit},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641962,\nauthor = {Alghythee, Kenan Kamel A and Hrncic, Adel and Singh, Karthik and Kunisetty, Sumanth and Yao, Yaxing and Soni, Nikita},\ntitle = {Towards Understanding Family Privacy and Security Literacy Conversations at Home: Design Implications for Privacy Literacy Interfaces},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641962},\ndoi = {10.1145/3613904.3641962},\nabstract = {Policymakers and researchers have emphasized the crucial role of parent-child conversations in shaping children’s digital privacy and security literacy. Despite this emphasis, little is known about the current nature of these parent-child conversations, including their content, structure, and children’s engagement during these conversations. This paper presents the findings of an interview study involving 13 parents of children ages under 13 reflecting on their privacy literacy practices at home. Through qualitative thematic analysis, we identify five categories of parent-child privacy and security conversations and examine parents’ perceptions of their children’s engagement during these discussions. Our findings show that although parents used different conversation approaches, rule-based conversations were one of the most common approaches taken by our participants, with example-based conversations perceived to be effective by parents. We propose important design implications for developing effective privacy educational technologies for families to support parent-child conversations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {983},\nnumpages = {12},\nkeywords = {Children, Education Technology, Family, Literacy, Parents, Privacy, Security},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642260,\nauthor = {Kyi, Lin and Mhaidli, Abraham and Santos, Cristiana Teixeira and Roesner, Franziska and Biega, Asia J.},\ntitle = {“It doesn’t tell me anything about how my data is used”: User Perceptions of Data Collection Purposes},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642260},\ndoi = {10.1145/3613904.3642260},\nabstract = {Data collection purposes and their descriptions are presented on almost all privacy notices under the GDPR, yet there is a lack of research focusing on how effective they are at informing users about data practices. We fill this gap by investigating users’ perceptions of data collection purposes and their descriptions, a crucial aspect of informed consent. We conducted 23 semi-structured interviews with European users to investigate user perceptions of six common purposes (Strictly Necessary, Statistics and Analytics, Performance and Functionality, Marketing and Advertising, Personalized Advertising, and Personalized Content) and identified elements of an effective purpose name and description. We found that most purpose descriptions do not contain the information users wish to know, and that participants preferred some purpose names over others due to their perceived transparency or ease of understanding. Based on these findings, we suggest how the framing of purposes can be improved toward meaningful informed consent.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {984},\nnumpages = {12},\nkeywords = {GDPR, personal data, privacy, purposes, qualitative methods, tracking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642639,\nauthor = {Vaithilingam, Priyan and Glassman, Elena L. and Inala, Jeevana Priya and Wang, Chenglong},\ntitle = {DynaVis: Dynamically Synthesized UI Widgets for Visualization Editing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642639},\ndoi = {10.1145/3613904.3642639},\nabstract = {Users often rely on GUIs to edit and interact with visualizations — a daunting task due to the large space of editing options. As a result, users are either overwhelmed by a complex UI or constrained by a custom UI with a tailored, fixed subset of options with limited editing flexibility. Natural Language Interfaces (NLIs) are emerging as a feasible alternative for users to specify edits. However, NLIs forgo the advantages of traditional GUI: the ability to explore and repeat edits and see instant visual feedback. We introduce DynaVis, which blends natural language and dynamically synthesized UI widgets. As the user describes an editing task in natural language, DynaVis performs the edit and synthesizes a persistent widget that the user can interact with to make further modifications. Study participants (n=24) preferred DynaVis over the NLI-only interface citing ease of further edits and editing confidence due to immediate visual feedback.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {985},\nnumpages = {17},\nkeywords = {Usability Study, User Experience Design, Visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642442,\nauthor = {Kim, Hyeok and Kim, Yea-Seul and Hullman, Jessica},\ntitle = {Erie: A Declarative Grammar for Data Sonification},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642442},\ndoi = {10.1145/3613904.3642442},\nabstract = {Data sonification—mapping data variables to auditory variables, such as pitch or volume—is used for data accessibility, scientific exploration, and data-driven art (e.g.,  museum exhibitions) among others. While a substantial amount of research has been made on effective and intuitive sonification design, software support is not commensurate, limiting researchers from fully exploring its capabilities. We contribute Erie, a declarative grammar for data sonification, that enables abstractly expressing auditory mappings. Erie supports specifying extensible tone designs (e.g.,  periodic wave, sampling, frequency/amplitude modulation synthesizers), various encoding channels, auditory legends, and composition options like sequencing and overlaying. Using standard Web Audio and Web Speech APIs, we provide an Erie compiler for web environments. We demonstrate the expressiveness and feasibility of Erie by replicating research prototypes presented by prior work and provide a sonification design gallery. We discuss future steps to extend Erie toward other audio computing environments and support interactive data sonification.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {986},\nnumpages = {19},\nkeywords = {Data sonification, data accessibility, declarative grammar},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642776,\nauthor = {While, Zack and Blascheck, Tanja and Gong, Yujie and Isenberg, Petra and Sarvghad, Ali},\ntitle = {Glanceable Data Visualizations for Older Adults: Establishing Thresholds and Examining Disparities Between Age Groups},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642776},\ndoi = {10.1145/3613904.3642776},\nabstract = {We present results of a replication study on smartwatch visualizations with adults aged 65 and older. The older adult population is rising globally, coinciding with their increasing interest in using small wearable devices, such as smartwatches, to track and view data. Smartwatches, however, pose challenges to this population: fonts and visualizations are often small and meant to be seen at a glance. How concise design on smartwatches interacts with aging-related changes in perception and cognition, however, is not well understood. We replicate a study that investigated how visualization type and number of data points affect glanceable perception. We observe strong evidence of differences for participants aged 75 and older, sparking interesting questions regarding the study of visualization and older adults. We discuss first steps toward better understanding and supporting an older population of smartwatch wearers and reflect on our experiences working with this population. Supplementary materials are available at https://osf.io/7x4hq/.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {987},\nnumpages = {17},\nkeywords = {glanceable visualization, mobile visualization, older adults},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642822,\nauthor = {Jiang, Yue and Zhou, Changkong and Garg, Vikas and Oulasvirta, Antti},\ntitle = {Graph4GUI: Graph Neural Networks for Representing Graphical User Interfaces},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642822},\ndoi = {10.1145/3613904.3642822},\nabstract = {Present-day graphical user interfaces (GUIs) exhibit diverse arrangements of text, graphics, and interactive elements such as buttons and menus, but representations of GUIs have not kept up. They do not encapsulate both semantic and visuo-spatial relationships among elements. To seize machine learning’s potential for GUIs more efficiently, Graph4GUI  exploits graph neural networks to capture individual elements’ properties and their semantic—visuo-spatial constraints in a layout. The learned representation demonstrated its effectiveness in multiple tasks, especially generating designs in a challenging GUI autocompletion task, which involved predicting the positions of remaining unplaced elements in a partially completed GUI. The new model’s suggestions showed alignment and visual appeal superior to the baseline method and received higher subjective ratings for preference. Furthermore, we demonstrate the practical benefits and efficiency advantages designers perceive when utilizing our model as an autocompletion plug-in.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {988},\nnumpages = {18},\nkeywords = {Constraint-based Layout, Graph Neural Networks, Graphical User Interface, User Interface Representation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642261,\nauthor = {Xiong, Zeyu and Fu, Shihan and Zhu, Yanying and Zhu, Chenqing and Ma, Xiaojuan and Fan, Mingming},\ntitle = {“It is hard to remove from my eye”: Design Makeup Residue Visualization System for Chinese Traditional Opera (Xiqu) Performers},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642261},\ndoi = {10.1145/3613904.3642261},\nabstract = {Chinese traditional opera (Xiqu) performers often experience skin problems due to the long-term use of heavy-metal-laden face paints. To explore the current skincare challenges encountered by Xiqu performers, we conducted an online survey (N=136) and semi-structured interviews (N=15) as a formative study. We found that incomplete makeup removal is the leading cause of human-induced skin problems, especially the difficulty in removing eye makeup. Therefore, we proposed EyeVis, a prototype that can visualize the residual eye makeup and record the time make-up was worn by Xiqu performers. We conducted a 7-day deployment study (N=12) to evaluate EyeVis. Results indicate that EyeVis helps to increase Xiqu performers’ awareness about removing makeup, as well as boosting their confidence and security in skincare. Overall, this work also provides implications for studying the work of people who wear makeup on a daily basis, and helps to promote and preserve the intangible cultural heritage of practitioners.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {989},\nnumpages = {16},\nkeywords = {Chinese traditional opera, computer vision, intangible cultural heritage, interactive design, makeup, mobile computing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642672,\nauthor = {Wimer, Brianna L and Szymanski, Annalisa and Metoyer, Ronald A},\ntitle = {Beyond Static Labels: Unpacking Nutrition Comprehension in the Digital Age},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642672},\ndoi = {10.1145/3613904.3642672},\nabstract = {Understanding nutrition labels remains challenging for consumers; however, digital shopping environments offer opportunities to explore how interactive nutrition labels may be used to enhance comprehension. We conducted an A/B study with 24 participants, comparing their ability to interpret and apply nutrition information using conventional, static labels versus interactive labels. We evaluated interactive nutrition labels’ impact through quantitative metrics and qualitative insights from interviews and think-aloud sessions. Our findings reveal a statistically significant improvement in assessing nutrient amounts and interpreting numerical information when users engage with interactive labels. These results underscore the potential interactivity has on promoting public understanding of nutritional content and highlight opportunities for refinement. Based on our findings, we propose new design directions and discuss technology’s role in making nutrition labels more effective for decision-making and nutrition education.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {990},\nnumpages = {15},\nkeywords = {Health management, interactive label, nutrition label},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641984,\nauthor = {Choi, Ryuhaerang and Park, Subin and Han, Sujin and Lee, Sung-Ju},\ntitle = {FoodCensor: Promoting Mindful Digital Food Content Consumption for People with Eating Disorders},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641984},\ndoi = {10.1145/3613904.3641984},\nabstract = {Digital food content’s popularity is underscored by recent studies revealing its addictive nature and association with disordered eating. Notably, individuals with eating disorders exhibit a positive correlation between their digital food content consumption and disordered eating behaviors. Based on these findings, we introduce FoodCensor, an intervention designed to empower individuals with eating disorders to make informed, conscious, and health-oriented digital food content consumption decisions. FoodCensor (i) monitors and hides passively exposed food content on smartphones and personal computers, and (ii) prompts reflective questions for users when they spontaneously search for food content. We deployed FoodCensor to people with binge eating disorder or bulimia (n=22) for three weeks. Our user study reveals that FoodCensor fostered self-awareness and self-reflection about unconscious digital food content consumption habits, enabling them to adopt healthier behaviors consciously. Furthermore, we discuss design implications for promoting healthier digital content consumption practices for vulnerable populations to specific content types.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {991},\nnumpages = {18},\nkeywords = {binge eating disorder, bulimia, eating disorder, food content, intervention},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641924,\nauthor = {Szymanski, Annalisa and Wimer, Brianna L and Anuyah, Oghenemaro and Eicher-Miller, Heather A and Metoyer, Ronald A},\ntitle = {Integrating Expertise in LLMs: Crafting a Customized Nutrition Assistant with Refined Template Instructions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641924},\ndoi = {10.1145/3613904.3641924},\nabstract = {Large Language Models (LLMs) have the potential to contribute to the fields of nutrition and dietetics in generating food product explanations that facilitate informed food selections. However, the extent to which these models offer effective and accurate information remains unverified. In collaboration with registered dietitians (RDs), we evaluate the strengths and weaknesses of LLMs in providing accurate and personalized nutrition information. Through a mixed-methods approach, RDs validated GPT-4 outputs at various levels of prompt specificity, which led to the development of design guidelines used to prompt LLMs for nutrition information. We tested these guidelines by creating a GPT prototype, The Food Product Nutrition Assistant, tailored for food product explanations. This prototype was refined and evaluated in focus groups with RDs. We find that the implementation of these dietitian-reviewed template instructions enhance the generation of detailed food product descriptions and tailored nutrition information.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {992},\nnumpages = {22},\nkeywords = {Artificial Intelligence, Food Recommendations, Large Language Models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641993,\nauthor = {Chen, Yu-Peng and Woodward, Julia and Bista, Dinank and Zhang, Xuanpu and Singh, Ishvina and Obajemu, Oluwatomisin and Shankar, Meena N. and Ross, Kathryn M. and Ruiz, Jaime and Anthony, Lisa},\ntitle = {Investigating Contextual Notifications to Drive Self-Monitoring in mHealth Apps for Weight Maintenance},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641993},\ndoi = {10.1145/3613904.3641993},\nabstract = {Mobile health applications for weight maintenance offer self-monitoring as a tool to empower users to achieve health goals (e.g., losing weight); yet maintaining consistent self-monitoring over time proves challenging for users. These apps use push notifications to help increase users’ app engagement and reduce long-term attrition, but they are often ignored by users due to appearing at inopportune moments. Therefore, we analyzed whether delivering push notifications based on time alone or also considering user context (e.g., current activity) affected users’ engagement in a weight maintenance app, in a 4-week in-the-wild study with 30 participants. We found no difference in participants’ overall (across the day) self-monitoring frequency between the two conditions, but in the context-based condition, participants responded faster and more frequently to notifications, and logged their data more timely (as eating/exercising occurs). Our work informs the design of notifications in weight maintenance apps to improve their efficacy in promoting self-monitoring.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {993},\nnumpages = {21},\nkeywords = {Health Behavior Change, Mobile Health, Notifications},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642321,\nauthor = {Jakob, Robert and Lepper, Nils and Fleisch, Elgar and Kowatsch, Tobias},\ntitle = {Predicting early user churn in a public digital weight loss intervention},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642321},\ndoi = {10.1145/3613904.3642321},\nabstract = {Digital health interventions (DHIs) offer promising solutions to the rising global challenges of noncommunicable diseases by promoting behavior change, improving health outcomes, and reducing healthcare costs. However, high churn rates are a concern with DHIs, with many users disengaging before achieving desired outcomes. Churn prediction can help DHI providers identify and retain at-risk users, enhancing the efficacy of DHIs. We analyzed churn prediction models for a weight loss app using various machine learning algorithms on data from 1,283 users and 310,845 event logs. The best-performing model, a random forest model that only used daily login counts, achieved an F1 score of 0.87 on day 7 and identified an average of 93\\% of churned users during the week-long trial. Notably, higher-dimensional models performed better at low false positive rate thresholds. Our findings suggest that user churn can be forecasted using engagement data, aiding in timely personalized strategies and better health results.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {994},\nnumpages = {16},\nkeywords = {attrition, churn, digital health, dropout, mHealth, machine learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642736,\nauthor = {Zhang, Hui and Zheng, Ruixiao and Yang, Shirao and Wei, Wanyi and Shan, Huafeng and Zhang, Jianwei},\ntitle = {\"Waves Push Me to Slumberland\": Reducing Pre-Sleep Stress through Spatio-Temporal Tactile Displaying of Music.},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642736},\ndoi = {10.1145/3613904.3642736},\nabstract = {Despite the fact that spatio-temporal patterns of vibration, characterized as rhythmic compositions of tactile content, have exhibited an ability to elicit specific emotional responses and enhance the emotion conveyed by music, limited research has explored their underlying mechanism in regulating emotional states within the pre-sleep context. Aiming to investigate whether synergistic spatio-temporal tactile displaying of music can facilitate relaxation before sleep, we developed 16 vibration patterns and an audio-tactile prototype for presenting an ambient experience in a pre-sleep scenario. The stress-reducing effects were further evaluated and compared via a user experiment. The results showed that the spatio-temporal tactile display of music significantly reduced stress and positively influenced users’ emotional states before sleep. Furthermore, our study highlights the therapeutic potential of incorporating quantitative and adjustable spatio-temporal parameters correlated with subjective psychophysical perceptions in the audio-tactile experience for stress management.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {995},\nnumpages = {15},\nkeywords = {Audio-Tactile Display, Emotion Regulation, Multisensory Experience, Spatio-Temporal Vibration Pattern, Stress Reduction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642680,\nauthor = {Nepal, Subigya and Pillai, Arvind and Wang, Weichen and Griffin, Tess and Collins, Amanda C and Heinz, Michael and Lekkas, Damien and Mirjafari, Shayan and Nemesure, Matthew and Price, George and Jacobson, Nicholas and Campbell, Andrew},\ntitle = {MoodCapture: Depression Detection using In-the-Wild Smartphone Images},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642680},\ndoi = {10.1145/3613904.3642680},\nabstract = {MoodCapture presents a novel approach that assesses depression based on images automatically captured from the front-facing camera of smartphones as people go about their daily lives. We collect over 125,000 photos in the wild from N=177 participants diagnosed with major depressive disorder for 90 days. Images are captured naturalistically while participants respond to the PHQ-8 depression survey question: “I have felt down, depressed, or hopeless”. Our analysis explores important image attributes, such as angle, dominant colors, location, objects, and lighting. We show that a random forest trained with face landmarks can classify samples as depressed or non-depressed and predict raw PHQ-8 scores effectively. Our post-hoc analysis provides several insights through an ablation study, feature importance analysis, and bias assessment. Importantly, we evaluate user concerns about using MoodCapture to detect depression based on sharing photos, providing critical insights into privacy concerns that inform the future design of in-the-wild image-based mental health assessment tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {996},\nnumpages = {18},\nkeywords = {Depression, Face, Facial Expressions, In-the-wild, Machine Learning, Mental Health, Mood, PHQ, Passive Sensing, Smartphones},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3643473,\nauthor = {Khan, Yasser and Mauriello, Matthew Louis and Nowruzi, Parsa and Motani, Akshara and Hon, Grace and Vitale, Nicholas and Li, Jinxing and Kim, Jayoung and Foudeh, Amir and Duvio, Dalton and Shols, Erika and Chesnut, Megan and Landay, James A. and Liphardt, Jan and Williams, Leanne and Sudheimer, Keith D. and Murmann, Boris and Bao, Zhenan and Paredes, Pablo E},\ntitle = {On Stress: Combining Human Factors and Biosignals to Inform the Placement and Design of a Skin-like Stress Sensor},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3643473},\ndoi = {10.1145/3613904.3643473},\nabstract = {With advances in electronic-skin and wearable technologies, it is possible to continuously measure stress markers from the skin and sweat to monitor and improve wellbeing and health. Understandably, the sensor’s engineering and resolution are important towards its function. However, we find that people looking for an e-skin stress sensor may look beyond measurement precision, demanding a private and stealth design to reduce, for example, social stigmatization. We introduce the idea of a stress sensing \"wear index,\" created from the combination of human-centered design (n=24), physiological (n=10), and biochemical (n=16) data. This wear index can inform the design of stress wearables to fit specific applications, e.g., human factors may be relevant for a wellbeing application, versus a relapse prevention application that may require more sensing precision. Our wear index idea can be further generalized as a method to close gaps between design and engineering practices.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {997},\nnumpages = {13},\nkeywords = {bioelectronics, cortisol, electronic-skin, heart rate variability, mental health, mental wellness, precision psychiatry, skin conductance, wearable electronics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642478,\nauthor = {O'Dea, Bridianne and Braund, Taylor A and Batterham, Philip J and Larsen, Mark E and Glozier, Nick and Whitton, Alexis E},\ntitle = {Reading Between the Lines: Identifying the Linguistic Markers of Anhedonia for the Stratification of Depression},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642478},\ndoi = {10.1145/3613904.3642478},\nabstract = {Stratifying depressed individuals may help to improve recovery rates by identifying the subgroups who would benefit from targeted treatments. Detecting depressed individuals with prominent anhedonia (i.e. lack of pleasure) may be one effective approach, given these individuals experience poorer treatment outcomes. This paper explores the linguistic features associated with anhedonia among depressed adults. Over 9 weeks, 218 individuals with depressive symptoms completed a fortnightly psychometric measure of depression (PHQ-9) and provided text data (SMS, social media posts, expressive essays, emotion diaries, personal letters). Linguistic features were examined using LIWC-22. Greater use of discrepancy words was significantly associated with higher anhedonia, but in SMS data only. Machine learning showed some utility for predicting increased anhedonia, with discrepancy words the most important linguistic feature in the model. Discrepancy words were not found to be associated with overall depression scores. These results suggest that this linguistic feature may show some promise for the stratification of anhedonic depression.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {998},\nnumpages = {13},\nkeywords = {Anhedonia, Depression, Digital phenotype, Linguistics, Stratification},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642489,\nauthor = {Ali, Naima Samreen and Qadir, Sarvech and Alsoubai, Ashwaq and De Choudhury, Munmun and Razi, Afsaneh and Wisniewski, Pamela J.},\ntitle = {\"I'm gonna KMS\": From Imminent Risk to Youth Joking about Suicide and Self-Harm via Social Media},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642489},\ndoi = {10.1145/3613904.3642489},\nabstract = {Recent increases in self-harm and suicide rates among youth have coincided with prevalent social media use; therefore, making these sensitive topics of critical importance to the HCI research community. We analyzed 1,224 direct message conversations (DMs) from 151 young Instagram users (ages 13-21), who engaged in private conversations using self-harm and suicide-related language. We found that youth discussed their personal experiences, including imminent thoughts of suicide and/or self-harm, as well as their past attempts and recovery. They gossiped about others, including complaining about triggering content and coercive threats of self-harm and suicide but also tried to intervene when a friend was in danger. Most of the conversations involved suicide or self-harm language that did not indicate the intent to harm but instead used hyperbolical language or humor. Our results shed light on youth perceptions, norms, and experiences of self-harm and suicide to inform future efforts towards risk detection and prevention. Content Warning: This paper discusses the sensitive topics of self-harm and suicide. Reader discretion is advised.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {999},\nnumpages = {18},\nkeywords = {Self-harm, Social Media, Suicide, Youth},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642766,\nauthor = {Jung, Gyuwon and Park, Sangjun and Lee, Uichin},\ntitle = {DeepStress: Supporting Stressful Context Sensemaking in Personal Informatics Systems Using a Quasi-experimental Approach},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642766},\ndoi = {10.1145/3613904.3642766},\nabstract = {Personal informatics (PI) systems are widely used in various domains such as mental health to provide insights from self-tracking data for behavior change. Users are highly interested in examining relationships from the self-tracking data, but identifying causality is still considered challenging. In this study, we design DeepStress, a PI system that helps users analyze contextual factors causally related to stress. DeepStress leverages a quasi-experimental approach to address potential biases related to confounding factors. To explore the user experience of DeepStress, we conducted a user study and a follow-up diary study using participants’ own self-tracking data collected for 6 weeks. Our results show that DeepStress helps users consider multiple contexts when investigating causalities and use the results to manage their stress in everyday life. We discuss design implications for causality support in PI systems.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1000},\nnumpages = {18},\nkeywords = {Causal Relationship, Mental Health, Personal Informatics, Quasi-experimental Approach},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642899,\nauthor = {Tang, Yilin and Chen, Liuqing and Chen, Ziyu and Chen, Wenkai and Cai, Yu and Du, Yao and Yang, Fan and Sun, Lingyun},\ntitle = {EmoEden: Applying Generative Artificial Intelligence to Emotional Learning for Children with High-Function Autism},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642899},\ndoi = {10.1145/3613904.3642899},\nabstract = {Children with high-functioning autism (HFA) often face challenges in emotional recognition and expression, leading to emotional distress and social difficulties. Conversational agents developed for HFA children in previous studies show limitations in children's learning effectiveness due to the conversational agents’ inability to dynamically generate personalized and contextual content. Recent advanced generative Artificial Intelligence techniques, with the capability to generate substantial diverse and high-quality texts and visual content, offer an opportunity for personalized assistance in emotional learning for HFA children. Based on the findings of our formative study, we integrated large language models and text-to-image models to develop a tool named EmoEden supporting children with HFA. Over a 22-day study involving six HFA children, it is observed that EmoEden effectively engaged children and improved their emotional recognition and expression abilities. Additionally, we identified the advantages and potential risks of applying generative AI to assist HFA children in emotional learning.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1001},\nnumpages = {20},\nkeywords = {Conversational agents, Emotional learning, Generative AI, High-functioning autism},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642386,\nauthor = {Kim, Jieun and Uriu, Daisuke and Barbareschi, Giulia and Kamiyama, Youichi and Minamizawa, Kouta},\ntitle = {Maintaining Continuing Bonds in Bereavement: A Participatory Design Process of Be.side},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642386},\ndoi = {10.1145/3613904.3642386},\nabstract = {During the grieving process, physical objects often serve as catalysts for remembering and honouring the relationship with departed loved ones. Leveraging a participatory design approach, we created Be.side, a fully customisable multi-modal artefact that incorporates scent, sound, and heartbeat stimulation and acts as a touch-point between the deceased and the bereaved. We conducted a four-week study with three participants to understand how the artefact, continuously attuned to each participant, helped to continue bonds with the deceased. Our results show that Be.side’s bespoke elements helped participants to evoke memories of the deceased. Participants created personalised rituals for remembrance. They sustained bonds by not only interacting with Be.side but also participating in the research. Finally, highlighting that remembrance can both provide comfort and deepen sadness, we discuss future design considerations.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1002},\nnumpages = {15},\nkeywords = {bereavement, continuing bonds, memory, participatory design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642178,\nauthor = {Kang, Rachael M. and Reynolds, Tera L.},\ntitle = {“This app said I had severe depression, and now I don’t know what to do”: the unintentional harms of mental health applications},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642178},\ndoi = {10.1145/3613904.3642178},\nabstract = {A growing market for mental health applications and increasing evidence for the efficacy of these applications have made apps a popular mode of mental healthcare delivery. However, given the gravity of mental illnesses, the potential harms of using these applications must be continually investigated. In this study, we conducted a thematic analysis using user-comments left on depression self-management applications. We analyzed 6,253 reviews from thirty-six, systematically selected apps from the Google Play and Apple App stores. We identified four themes regarding the potential, unintentional harms caused by these applications. This study uniquely contributes to the literature by examining the reported harms to users caused by depression self-management apps and contextualizing them in an ethical framework. We provide recommendations to developers for creating ethical depression self-management apps and resources for practitioners and consumers to aid in screening apps.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1003},\nnumpages = {17},\nkeywords = {depression apps, mental health apps, thematic analysis, user reviews},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642285,\nauthor = {Montoya, Maria F. and Qiao, Hannah and Sasikumar, Prasanth and Elvitigala, Don Samitha and Pell, Sarah Jane and Nanayakkara, Suranga and Mueller, Florian ‘Floyd’},\ntitle = {Exploring an Extended Reality Floatation Tank Experience to Reduce the Fear of Being in Water},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642285},\ndoi = {10.1145/3613904.3642285},\nabstract = {People with a fear of being in water rarely engage in water activities and hence miss out on the associated health benefits. Prior research suggested virtual exposure to treat fears. However, when it comes to a fear of being in water, virtual water might not capture water’s immersive qualities, while real water can pose safety risks. We propose extended reality to combine both advantages: We conducted a study (N=12) where participants with a fear of being in water interacted with playful water-inspired virtual reality worlds while floating inside a floatation tank. Our findings, supported quantitatively by heart rate variability and qualitatively by interviews, suggest that playful extended reality could mitigate fear responses in an entertaining way. We also present insights for the design of future systems that aim to help people with a fear of being in water and other phobias by using the best of the virtual and physical worlds.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1004},\nnumpages = {17},\nkeywords = {exposure therapy, extended reality, fear of being in water, floatation tank, flotation pod, phobia, virtual reality, water},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642832,\nauthor = {Lee, Hansoo and Kim, Auk and Bae, SangWon and Lee, Uichin},\ntitle = {S-ADL: Exploring Smartphone-based Activities of Daily Living to Detect Blood Alcohol Concentration in a Controlled Environment},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642832},\ndoi = {10.1145/3613904.3642832},\nabstract = {In public health and safety, precise detection of blood alcohol concentration (BAC) plays a critical role in implementing responsive interventions that can save lives. While previous research has primarily focused on computer-based or neuropsychological tests for BAC identification, the potential use of daily smartphone activities for BAC detection in real-life scenarios remains largely unexplored. Drawing inspiration from Instrumental Activities of Daily Living (I-ADL), our hypothesis suggests that Smartphone-based Activities of Daily Living (S-ADL) can serve as a viable method for identifying BAC. In our proof-of-concept study, we propose, design, and assess the feasibility of using S-ADLs to detect BAC in a scenario-based controlled laboratory experiment involving 40 young adults. In this study, we identify key S-ADL metrics, such as delayed texting in SMS, site searching, and finance management, that significantly contribute to BAC detection (with an AUC-ROC and accuracy of 81\\%). We further discuss potential real-life applications of the proposed BAC model.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1005},\nnumpages = {25},\nkeywords = {Activities of daily living, Alcohol drinking detection, Functional assessment, Machine learning, Smartphone app usage},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642574,\nauthor = {Young, Jordyn and Jawara, Laala M and Nguyen, Diep N and Daly, Brian and Huh-Yoo, Jina and Razi, Afsaneh},\ntitle = {The Role of AI in Peer Support for Young People: A Study of Preferences for Human- and AI-Generated Responses},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642574},\ndoi = {10.1145/3613904.3642574},\nabstract = {Generative Artificial Intelligence (AI) is integrated into everyday technology, including news, education, and social media. AI has further pervaded private conversations as conversational partners, auto-completion, and response suggestions. As social media becomes young people’s main method of peer support exchange, we need to understand when and how AI can facilitate and assist in such exchanges in a beneficial, safe, and socially appropriate way. We asked 622 young people to complete an online survey and evaluate blinded human- and AI-generated responses to help-seeking messages. We found that participants preferred the AI-generated response to situations about relationships, self-expression, and physical health. However, when addressing a sensitive topic, like suicidal thoughts, young people preferred the human response. We also discuss the role of training in online peer support exchange and its implications for supporting young people’s well-being. Disclaimer: This paper includes sensitive topics, including suicide ideation. Reader discretion is advised.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1006},\nnumpages = {18},\nkeywords = {AI-Mediated Communication (AI-MC), Artificial Intelligence (AI), Chatbot, Human-AI Interaction (HAII), LLM, Mental Health, Peer Support, Social Support, Youth},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642411,\nauthor = {Sakel, Sophia and Blenk, Tabea and Schmidt, Albrecht and Haliburton, Luke},\ntitle = {The Social Journal: Investigating Technology to Support and Reflect on Social Interactions},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642411},\ndoi = {10.1145/3613904.3642411},\nabstract = {Social interaction is a crucial part of what it means to be human. Maintaining a healthy social life is strongly tied to positive outcomes for both physical and mental health. While we use personal informatics data to reflect on many aspects of our lives, technology-supported reflection for social interactions is currently under-explored. To address this, we first conducted an online survey (N=124) to understand how users want to be supported in their social interactions. Based on this, we designed and developed an app for users to track and reflect on their social interactions and deployed it in the wild for two weeks (N=25). Our results show that users are interested in tracking meaningful in-person interactions that are currently untraced and that an app can effectively support self-reflection on social interaction frequency and social load. We contribute insights and concrete design recommendations for technology-supported reflection for social interaction.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1007},\nnumpages = {18},\nkeywords = {Reflection, Social Interaction, Well-being},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642642,\nauthor = {Aldaweesh, Sarah and Alateeq, Deemah and Van Kleek, Max and Shadbolt, Nigel},\ntitle = {“If Someone Walks In On Us Talking, Pretend to be My Friend, Not My Therapist\": Challenges and Opportunities for Digital Mental Health Support in Saudi Arabia},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642642},\ndoi = {10.1145/3613904.3642642},\nabstract = {Mental health disorders are prevalent worldwide, yet they remain stigmatized, especially in the Middle East. While mHealth has the potential to circumvent traditional barriers, research on its application remains scarce in Arab countries. To address this gap, we conducted a mixed-methods study of mental health apps availability, adoption, and perceptions in the Kingdom of Saudi Arabia (KSA) where digital health transformation is rapidly progressing. We interviewed twelve psychiatrists and psychologists to elicit their views on local barriers and opportunities for digital mental health support. We further systematically reviewed the Saudi app market, analysing 110 Arabic mental health apps. Our findings indicate that whilst fear of stigma and cultural factors hindered help-seeking, the privacy and anonymity enabled by technology created new opportunities for accessing mental support in the KSA. We revealed tensions between experts’ professional and practical perspectives, explored technology-exacerbated challenges and provided considerations for improving Saudi digital mental healthcare experience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1008},\nnumpages = {19},\nkeywords = {app review, content analysis, interview, mHealth, mental health, mobile apps, psychiatrist, psychologist, qualitative study, well-being},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642595,\nauthor = {Ullal, Akshith and Tauseef, Mahrukh and Watkins, Alexandra and Juckett, Lisa and Maxwell, Cathy A. and Tate, Judith and Mion, Lorraine and Sarkar, Nilanjan},\ntitle = {An Iterative Participatory Design Approach to Develop Collaborative Augmented Reality Activities for Older Adults in Long-Term Care Facilities},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642595},\ndoi = {10.1145/3613904.3642595},\nabstract = {Over four million older adults living in long-term care (LTC) communities experience loneliness, adversely impacting their health. Increased contact with friends and family is an evidence-based intervention to reduce loneliness, but in-person visits are not always possible. Augmented Reality (AR)-based telepresence activities can offer viable alternatives with increased immersion and presence compared to video calls. However, its feasibility as an interaction technology for older adults is not known. In this paper, we detail the design of two dyadic collaborative AR activities that accommodate diminished physical and cognitive abilities of older adults. The findings include a general design framework based on an iterative participatory design focusing on preferred activities, modes of interaction, and overall AR experience of eight older adults, two family members, and five LTC staff. Results demonstrate the potential of collaborative AR as an effective means of interaction for older adults with their family, if designed to cater to their needs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1009},\nnumpages = {21},\nkeywords = {accessibility, collaborative augmented reality, iterative participatory design, long term care settings, older adults},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642728,\nauthor = {Hsu, Long-Jing and Chung, Chia-Fang},\ntitle = {Dancing with the Roles: Towards Designing Technology that Supports the Multifaceted Roles of Caregivers for Older Adults},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642728},\ndoi = {10.1145/3613904.3642728},\nabstract = {Caregivers of older adults often undertake their caregiving journey driven by filial obligation, facing inherent expectations and multifaceted roles. While Human-Computer Interaction (HCI) research has explored these roles, some invisible work in managing them remains under-examined. To address this gap, we interviewed 19 informal caregivers of older adults to uncover their invisible work and the potential role of technology in supporting these complex responsibilities. Our findings detail the caregivers’ lived experiences, highlighting the challenges and strategies they employ in managing multiple roles. We discuss design opportunities that include facilitating the identification and reflection on existing roles, leveraging this understanding for coordination, aiding in role-based scheduling with acknowledgment, and providing support for the dynamic roles transitioning between various responsibilities. These insights could inform future caregiving technology design, enhancing support for caregivers in their multifaceted roles.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1010},\nnumpages = {12},\nkeywords = {caregiver, caregiving ecosystem, filial obligations, informal caregivers, invisible work, older adults, role dynamics, roles},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641892,\nauthor = {Xing, Yushan and Kelly, Ryan M. and Rogerson, Melissa J. and Waycott, Jenny and Aslam, Kashifa},\ntitle = {Designing for Inclusive Experiences: Investigating Opportunities for Supporting Older Adults in Community-based Social Programs},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641892},\ndoi = {10.1145/3613904.3641892},\nabstract = {Community-based social programs, such as interest groups and outings, provide valuable ways for older adults to maintain social connectedness. To understand how technology can be designed to support older adults in such programs, we conducted a four-month field study with a local community centre, involving: (1) observations of social program sessions, (2) interviews with staff, and (3) co-design workshops with staff and program participants. We found that staff used technologies in a situated way to make social programs more inclusive for older adults. Technology promoted incidental social interactions and group learning, but also excluded participants from some activities. Participants believed that future technologies to support community-based social programs should be designed to enable efficient communication, promote flexible interactions, and maintain the flow of social activities. We argue that technology interventions in this setting should not become the focus of an activity but instead support social interactions triggered by existing activities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1011},\nnumpages = {20},\nkeywords = {community organization, design recommendations, field study, older adults, social connectedness},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642800,\nauthor = {Jin, Yucheng and Cai, Wanling and Chen, Li and Zhang, Yizhe and Doherty, Gavin and Jiang, Tonglin},\ntitle = {Exploring the Design of Generative AI in Supporting Music-based Reminiscence for Older Adults},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642800},\ndoi = {10.1145/3613904.3642800},\nabstract = {Music-based reminiscence has the potential to positively impact the psychological well-being of older adults. However, the aging process and physiological changes, such as memory decline and limited verbal communication, may impede the ability of older adults to recall their memories and life experiences. Given the advanced capabilities of generative artificial intelligence (AI) systems, such as generated conversations and images, and their potential to facilitate the reminiscing process, this study aims to explore the design of generative AI to support music-based reminiscence in older adults. This study follows a user-centered design approach incorporating various stages, including detailed interviews with two social workers and two design workshops (involving ten older adults). Our work contributes to an in-depth understanding of older adults’ attitudes toward utilizing generative AI for supporting music-based reminiscence and identifies concrete design considerations for the future design of generative AI to enhance the reminiscence experience of older adults.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1012},\nnumpages = {17},\nkeywords = {Generative AI, Human-AI Interaction, Music-based Reminiscence, Older Adults, Reminiscence},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642318,\nauthor = {Chen, Baihui and Li, Xueliang},\ntitle = {Understanding Socio-technical Opportunities for Enhancing Communication Between Older Adults and their Remote Family},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642318},\ndoi = {10.1145/3613904.3642318},\nabstract = {With the digitalization and mobilization of the society, older people face the challenge of maintaining high-quality communication with their younger family members who move out and lead separate lives at a distance. In HCI, little work is done to understand the social dynamics between distributed families and their remote communication mediated by the technologies. To identify design opportunities to support their remote communication, we conducted interviews with nine family pairs composed of distributed intergenerational family members. In addition, we interviewed eight community volunteers to formulate a perspective of social service providers. Our paper contributes to the HCI community by providing an account of the social dynamics mediated by communication technologies between older adults and their remote families, and opportunities to promote their social connections from a multi-stakeholder perspective. This paper presents valuable insights for designers aiming to enhance wellbeing of older adults within the context of distributed families.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1013},\nnumpages = {16},\nkeywords = {Computer-mediated Communication, Distributed Families, Intergenerational Communication, Non-Western, Older Adults},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642191,\nauthor = {Kotturi, Yasmine and Anderson, Angel and Ford, Glenn and Skirpan, Michael and Bigham, Jeffrey P},\ntitle = {Deconstructing the Veneer of Simplicity: Co-Designing Introductory Generative AI Workshops with Local Entrepreneurs},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642191},\ndoi = {10.1145/3613904.3642191},\nabstract = {Generative AI platforms and features are permeating many aspects of work. Entrepreneurs from lean economies in particular are well positioned to outsource tasks to generative AI given limited resources. In this paper, we work to address a growing disparity in use of these technologies by building on a four-year partnership with a local entrepreneurial hub dedicated to equity in tech and entrepreneurship. Together, we co-designed an interactive workshops series aimed to onboard local entrepreneurs to generative AI platforms. Alongside four community-driven and iterative workshops with entrepreneurs across five months, we conducted interviews with 15 local entrepreneurs and community providers. We detail the importance of communal and supportive exposure to generative AI tools for local entrepreneurs, scaffolding actionable use (and supporting non-use), demystifying generative AI technologies by emphasizing entrepreneurial power, while simultaneously deconstructing the veneer of simplicity to address the many operational skills needed for successful application.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1014},\nnumpages = {16},\nkeywords = {community-based research, entrepreneurship, generative artificial intelligence},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641891,\nauthor = {Gu, Ken and Grunde-McLaughlin, Madeleine and McNutt, Andrew and Heer, Jeffrey and Althoff, Tim},\ntitle = {How Do Data Analysts Respond to AI Assistance? A Wizard-of-Oz Study},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641891},\ndoi = {10.1145/3613904.3641891},\nabstract = {Data analysis is challenging as analysts must navigate nuanced decisions that may yield divergent conclusions. AI assistants have the potential to support analysts in planning their analyses, enabling more robust decision making. Though AI-based assistants that target code execution (e.g., Github Copilot) have received significant attention, limited research addresses assistance for both analysis execution and planning. In this work, we characterize helpful planning suggestions and their impacts on analysts’ workflows. We first review the analysis planning literature and crowd-sourced analysis studies to categorize suggestion content. We then conduct a Wizard-of-Oz study (n=13) to observe analysts’ preferences and reactions to planning assistance in a realistic scenario. Our findings highlight subtleties in contextual factors that impact suggestion helpfulness, emphasizing design implications for supporting different abstractions of assistance, forms of initiative, increased engagement, and alignment of goals between analysts and assistants.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1015},\nnumpages = {22},\nkeywords = {Analysis Planning, Analysis Tools, Artificial Intelligence, Code Assistant, Computational Notebooks, Copilot, Data Analysis, Data Science Assistant, Human-AI Collaboration, Human-AI Interaction, Human-Centered Data Science, Human-LLM Interaction, Statistical Analysis, Wizard of Oz},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642650,\nauthor = {Kadoma, Kowe and Aubin Le Quere, Marianne and Fu, Xiyu Jenny and Munsch, Christin and Metaxa, Dana\\\"{e} and Naaman, Mor},\ntitle = {The Role of Inclusion, Control, and Ownership in Workplace AI-Mediated Communication},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642650},\ndoi = {10.1145/3613904.3642650},\nabstract = {Given large language models’ (LLMs) increasing integration into workplace software, it is important to examine how biases in the models may impact workers. For example, stylistic biases in the language suggested by LLMs may cause feelings of alienation and result in increased labor for individuals or groups whose style does not match. We examine how such writer-style bias impacts inclusion, control, and ownership over the work when co-writing with LLMs. In an online experiment, participants wrote hypothetical job promotion requests using either hesitant or self-assured auto-complete suggestions from an LLM and reported their subsequent perceptions. We found that the style of the AI model did not impact perceived inclusion. However, individuals with higher perceived inclusion did perceive greater agency and ownership, an effect more strongly impacting participants of minoritized genders. Feelings of inclusion mitigated a loss of control and agency when accepting more AI suggestions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1016},\nnumpages = {10},\nkeywords = {Co-writing, autocomplete, large language models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642959,\nauthor = {Wang, Piaohong and Hu, Siying and Wen, Bo and Lu, Zhicong},\ntitle = {“There is a Job Prepared for Me Here”: Understanding How Short Video and Live-streaming Platforms Empower Ageing Job Seekers in China},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642959},\ndoi = {10.1145/3613904.3642959},\nabstract = {In recent years, the global unemployment rate has remained persistently high. Compounding this issue, the ageing population in China often encounters additional challenges in finding employment due to prevalent age discrimination in daily life. However, with the advent of social media, there has been a rise in the popularity of short videos and live-streams for recruiting ageing workers. To better understand the motivations of ageing job seekers to engage with these video-based recruitment methods and to explore the extent to which such platforms can empower them, we conducted an interview-based study with ageing job seekers who have had exposure to these short recruitment videos and live-streaming channels. Our findings reveal that these platforms can provide a job-seeking choice that is particularly friendly to ageing job seekers, effectively improving their disadvantaged situation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1017},\nnumpages = {14},\nkeywords = {Ageing People, Employment, Job-Seeking, Live-streaming, Short Videos, Social Media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641964,\nauthor = {Kobiella, Charlotte and Flores L\\'{o}pez, Yarhy Said and Waltenberger, Franz and Draxler, Fiona and Schmidt, Albrecht},\ntitle = {\"If the Machine Is As Good As Me, Then What Use Am I?\" – How the Use of ChatGPT Changes Young Professionals' Perception of Productivity and Accomplishment},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641964},\ndoi = {10.1145/3613904.3641964},\nabstract = {Large language models (LLMs) like ChatGPT have been widely adopted in work contexts. We explore the impact of ChatGPT on young professionals’ perception of productivity and sense of accomplishment. We collected LLMs’ main use cases in knowledge work through a preliminary study, which served as the basis for a two-week diary study with 21 young professionals reflecting on their ChatGPT use. Findings indicate that ChatGPT enhanced some participants’ perceptions of productivity and accomplishment by enabling greater creative output and satisfaction from efficient tool utilization. Others experienced decreased perceived productivity and accomplishment, driven by a diminished sense of ownership, perceived lack of challenge, and mediocre results. We found that the suitability of task delegation to ChatGPT varies strongly depending on the task nature. It’s especially suitable for comprehending broad subject domains, generating creative solutions, and uncovering new information. It’s less suitable for research tasks due to hallucinations, which necessitate extensive validation.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1018},\nnumpages = {16},\nkeywords = {Generative AI, knowledge work, productivity, self-efficacy, sense of accomplishment},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642107,\nauthor = {Merino, \\'{A}ngel and Gonz\\'{a}lez-Caba\\~{n}as, Jos\\'{e} and Cuevas, \\'{A}ngel and Cuevas, Rub\\'{e}n},\ntitle = {Analysis and Implementation of Nanotargeting on LinkedIn Based on Publicly Available Non-PII},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642107},\ndoi = {10.1145/3613904.3642107},\nabstract = {The literature has shown that combining a few non-Personal Identifiable Information (non-PII) is enough to make a user unique in a dataset including millions of users. This work demonstrates that a combination of a few non-PII items can be activated to nanotarget users. We demonstrate that the combination of the location and 5 rare (13 random) skills in a LinkedIn profile is enough to become unique in a user base of ∼ 970M users with a probability of 75\\%. The novelty is that these attributes are publicly accessible to anyone registered on LinkedIn and can be activated through advertising campaigns. We ran an experiment configuring ad campaigns using the location and skills of three of the paper’s authors, demonstrating how all the ads using ≥ 13 skills were delivered exclusively to the targeted user. We reported this vulnerability to LinkedIn, which initially ignored the problem, but fixed it as of November 2023.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1019},\nnumpages = {22},\nkeywords = {LinkedIn, Nanotargeting., Online advertising, User privacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642667,\nauthor = {Leake, Mackenzie and Li, Wilmot},\ntitle = {ChunkyEdit: Text-first video interview editing via chunking},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642667},\ndoi = {10.1145/3613904.3642667},\nabstract = {The early stages of video editing present many cognitively demanding tasks that require editors to remember and structure large amounts of video. In our formative work we learned that editors break down the editing process into smaller parts by labeling and organizing footage around central themes. Using current video editing tools, this process is slow and largely manual. We present a system called ChunkyEdit for helping editors group video interview clips into thematically coherent chunks, which can then be exported to existing video editing tools and composed into an edited narrative. By focusing on this intermediate step, we leverage computation to do tedious organizational tasks, while preserving the editor’s ability to control the primary storytelling decisions. We explore four different topic modeling approaches to creating video chunks. We then evaluate our tool with eight professional video editors to learn how a chunking-based approach could be incorporated into video editing workflows.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1020},\nnumpages = {16},\nkeywords = {Chunking, Topic modeling applications, Video editing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641972,\nauthor = {Park, Hyanghee and Ahn, Daehwan and Lee, Joonhwan},\ntitle = {Lessons From Working in the Metaverse: Challenges, Choices, and Implications from a Case Study},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641972},\ndoi = {10.1145/3613904.3641972},\nabstract = {Although the metaverse workspace has the potential to solve some of the drawbacks of remote work while maintaining its benefits, there are few real-world cases of adopting the metaverse as a legitimate workspace and fewer subsequent studies on how to design and operate the metaverse workspace. Thus, questions exist about the organizational or sociotechnical challenges that may emerge and how decisions are made when adopting and operating the metaverse workspace in a real-world setting. To answer such questions, we scrutinized the startup company Zigbang, which has completely replaced their physical office with Soma— a metaverse platform they developed where thousands of people work and other cooperative companies have moved in as tenants. By conducting field observations and semi-structured interviews with various workers and Zigbang's stakeholders, we identify essential design challenges and decisions when adopting a metaverse workspace and highlight the key takeaways learned from the company's trials and errors.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1021},\nnumpages = {16},\nkeywords = {Case Study, Future of Work, Metaverse, Metaverse Workspace, Remote Work, Virtual Environment},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642913,\nauthor = {Lee, Chia Hsin (Josette) and Yuan, Chien Wen (Tina)},\ntitle = {The Hidden Toll of Instant Messaging Use in Remote Work: Interaction Dynamics Between Subordinates and Supervisors},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642913},\ndoi = {10.1145/3613904.3642913},\nabstract = {The research centers on examining the utilization of instant messaging (IM) in remote work interactions between supervisors and subordinates. Through a series of one-on-one in-depth interviews (n = 12), our findings unveil distinct nuances in how subordinates and supervisors perceive and engage with IM, encompassing aspects such as response strategies, interaction frequency, and interaction nature. Notably, a significant finding emerged concerning the practice of self-censorship in IM, acting as a strategy for impression management, predominantly adopted by subordinates in their interactions with supervisors. Additionally, our investigation sheds light on a contrasting viewpoint regarding the social use of IM. While supervisor participants acknowledged its potential to bolster work-related collaboration, this perspective appeared less pronounced among subordinate participants. Our study concludes by delving into the design implications for IM application design, offering insights that can shape collaborative dynamics within remote work environments.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1022},\nnumpages = {15},\nkeywords = {Computer-mediated communication, image management, instant messaging, remote workers, self-censorship},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642175,\nauthor = {Ferguson, Sharon A and Massimi, Michael},\ntitle = {Circle Back Next Week: The Effect of Meeting-Free Weeks on Distributed Workers’ Unstructured Time and Attention Negotiation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642175},\ndoi = {10.1145/3613904.3642175},\nabstract = {While distributed workers rely on scheduled meetings for coordination and collaboration, these meetings can also challenge their ability to focus. Protecting worker focus has been addressed from a technical perspective, but companies are now attempting organizational interventions, such as meeting-free weeks. Recognizing distributed collaboration as a sociotechnical challenge, we first present an interview study with distributed workers participating in meeting-free weeks at an enterprise software company. We identify three orientations workers exhibit during these weeks: Focus, Collaborative, and Time-Bound, each with varying levels and use of unstructured time. These different orientations result in challenges in attention negotiation, which may be suited for technical interventions. This motivated a follow-up study investigating attention negotiation and the compensating mechanisms workers developed during meeting-free weeks. Our framework identified tensions between the attention-getting and attention-delegation strategies. We extend past work to show how workers adapt their virtual collaboration mechanisms in response to organizational interventions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1023},\nnumpages = {17},\nkeywords = {Attention Negotiation, Computer Supported-Cooperative Work, Focus, Meetings, Workplace},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642774,\nauthor = {Chen, Jonathan and Yoon, Dongwook},\ntitle = {Exploring the Diminishing Allure of Paper and Low-Fidelity Prototyping Among Designers in the Software Industry: Impacts of Hybrid Work, Digital Tools, and Corporate Culture},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642774},\ndoi = {10.1145/3613904.3642774},\nabstract = {In a rapidly evolving UX/UI design landscape marked by technological advancements and shifts toward hybrid work, understanding the implications of these changes on software prototyping practices is crucial. This study investigates the influence of evolving work practices, tool advancements, and designers’ attitudes on prototyping practices and design processes in the contemporary software industry. Based on in-depth interviews with 10 practitioners and educators, we explore the factors contributing to the preference for digital-first prototypes and the diminishing appeal of low-fidelity prototyping methods. Our findings reveal how digital prototypes outshine physical counterparts in hybrid work, the role of all-in-one digital tools in centralizing designers’ workflows and encouraging high-fidelity prototyping, corporate preferences for visually appealing prototypes, and the impact of designers’ educational backgrounds, generational differences, and professional maturity. This research offers valuable insights to inform decision-making and strategies for design practitioners, educators, and organizations in adapting to current and future prototyping practices.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1024},\nnumpages = {14},\nkeywords = {design practices, digital prototyping tools, expert interviews, hybrid and remote work, industry, lo-fi prototyping, paper prototyping},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642670,\nauthor = {Scott, Ava Elizabeth and Tankelevitch, Lev and Rintel, Sean},\ntitle = {Mental Models of Meeting Goals: Supporting Intentionality in Meeting Technologies},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642670},\ndoi = {10.1145/3613904.3642670},\nabstract = {Ineffective meetings due to unclear goals are major obstacles to productivity, yet support for intentionality is surprisingly scant in our meeting and allied workflow technologies. To design for intentionality, we need to understand workers’ attitudes and practices around goals. We interviewed 21 employees of a global technology company and identified contrasting mental models of meeting goals: meetings as a means to an end, and meetings as an end in themselves. We explore how these mental models impact how meeting goals arise, goal prioritization, obstacles to considering goals, and how lack of alignment around goals may create tension between organizers and attendees. We highlight the challenges in balancing preparation, constraining scope, and clear outcomes, with the need for intentional adaptability and discovery in meetings. Our findings have implications for designing systems which increase effectiveness in meetings by catalyzing intentionality and reducing tension in the organisation of meetings.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1025},\nnumpages = {17},\nkeywords = {agenda, calendar, goal, intentionality, meeting, teams, videoconferencing, workflows},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642381,\nauthor = {Cho, Janghee and Choi, Dasom and Yu, Junnan and Voida, Stephen},\ntitle = {Reinforcing and Reclaiming The Home: Co-speculating Future Technologies to Support Remote and Hybrid Work},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642381},\ndoi = {10.1145/3613904.3642381},\nabstract = {With the rise of remote and hybrid work after COVID-19, there is growing interest in understanding remote workers’ experiences and designing digital technology for the future of work within the field of HCI. To gain a holistic understanding of how remote workers navigate the blurred boundary between work and home and how designers can better support their boundary work, we employ humanistic geography as a lens. We engaged in co-speculative design practices with 11 remote workers in the US, exploring how future technologies might sustainably enhance participants’ work and home lives in remote/hybrid arrangements. We present the imagined technologies that resulted from this process, which both reinforce remote workers’ existing boundary work practices through everyday routines/rituals and reclaim the notion of home by fostering independence, joy, and healthy relationships. Our discussions with participants inform implications for designing digital technologies that promote sustainability in the future remote/hybrid work landscape.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1026},\nnumpages = {28},\nkeywords = {boundary work, future of work, work-from-home},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641928,\nauthor = {Akahori, Wataru and Yamashita, Naomi and Jamieson, Jack and Nakatani, Momoko and Hashimoto, Ryo and Watanabe, Masahiro},\ntitle = {The Impact of Social Norms on Hybrid Workers’ Well-Being: A Cross-Cultural Comparison of Japan and the United States},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641928},\ndoi = {10.1145/3613904.3641928},\nabstract = {Previous research has shown that workplace social norms influence employee well-being. However, such norms vary based on the cultures in which workplaces are embedded, suggesting that cultural differences may influence perceived norms about when and where work should occur. These differences, in turn, could impact employee well-being. Accordingly, through the lenses of cultural tightness-looseness and individualism-collectivism, this paper investigates cultural differences in perceived social norms, and the relationship between those norms and hybrid workers’ well-being. We conducted a survey of 1,000 Japanese and 1,000 American hybrid workers. Results indicated that American respondents perceived stronger norms and demonstrated a higher willingness to conform to norms compared to Japanese respondents. Additionally, strong injunctive norms were positively associated with well-being among Americans but not among Japanese. Interviews (N = 24) showed that Japanese perceived injunctive norms negatively, while Americans saw them positively. We discuss implications for future remote-collaboration technologies in hybrid-work settings.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1027},\nnumpages = {19},\nkeywords = {Cross-Cultural Comparison, Hybrid Work, Social Norms, Well-Being},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642685,\nauthor = {Khadpe, Pranav and Le, Lindy and Nowak, Kate and Iqbal, Shamsi T and Suh, Jina},\ntitle = {DISCERN: Designing Decision Support Interfaces to Investigate the Complexities of Workplace Social Decision-Making With Line Managers},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642685},\ndoi = {10.1145/3613904.3642685},\nabstract = {Line managers form the first level of management in organizations, and must make complex decisions, while maintaining relationships with those impacted by their decisions. Amidst growing interest in technology-supported decision-making at work, their needs remain understudied. Further, most existing design knowledge for supporting social decision-making comes from domains where decision-makers are more socially detached from those they decide for. We conducted iterative design research with line managers within a technology organization, investigating decision-making practices, and opportunities for technological support. Through formative research, development of a decision-representation tool—DISCERN—and user enactments, we identify their communication and analysis needs that lack adequate support. We found they preferred tools for externalizing reasoning rather than tools that replace interpersonal interactions, and they wanted tools to support a range of intuitive and calculative decision-making. We discuss how design of social decision-making supports, especially in the workplace, can more explicitly support highly interactional social decision-making.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1028},\nnumpages = {18},\nkeywords = {Deliberation, Organizational Decision-making, Technology Probe, User Enactments},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642469,\nauthor = {Hu, Donghan and Lee, Sang Won},\ntitle = {Exploring the Effectiveness of Time-lapse Screen Recording for Self-Reflection in Work Context},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642469},\ndoi = {10.1145/3613904.3642469},\nabstract = {Effective self-tracking in working contexts empowers individuals to explore and reflect on past activities. Recordings of computer activities contain rich metadata that can offer valuable insight into users’ previous tasks and endeavors. However, presenting a simple summary of time usage may not effectively engage users with data because it is not contextualized, and users may not understand what to do with the data. This work explores time-lapse videos as a visual-temporal medium to facilitate self-reflection among workers in productivity contexts. To explore this space, we conducted a four-week study (n = 15) to investigate how a computer screen’s history of states can help workers recall previous undertakings and gain comprehensive insights via self-reflection. Our results support that watching time-lapse videos can enhance self-reflection more effectively than traditional self-tracking tools by providing contextual clues about users’ past activities. The experience with both traditional tools and time-lapse videos resulted in increased productivity. Additionally, time-lapse videos assist users in cultivating a positive understanding of their work. We discuss how multimodal cues, such as time-lapse videos, can complement personal informatics tools.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1029},\nnumpages = {14},\nkeywords = {Behavior Change, Self-reflection, Self-tracking, Visual History},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642502,\nauthor = {Wong, Emily and S\\'{a}nchez Esquivel, Juan and Leiva, Germ\\'{a}n and Gr\\o{}nb\\ae{}k, Jens Emil Sloth and Velloso, Eduardo},\ntitle = {Practice-informed Patterns for Organising Large Groups in Distributed Mixed Reality Collaboration},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642502},\ndoi = {10.1145/3613904.3642502},\nabstract = {Collaborating across dissimilar, distributed spaces presents numerous challenges for computer-aided spatial communication. Mixed reality (MR) can blend selected surfaces, allowing collaborators to work in blended f-formations (facing formations), even when their workstations are physically misaligned. Since collaboration often involves more than just participant pairs, this research examines how we might scale MR experiences for large-group collaboration. To do so, this study recruited collaboration designers (CDs) to evaluate and reimagine MR for large-scale collaboration. These CDs were engaged in a four-part user study that involved a technology probe, a semi-structured interview, a speculative low-fidelity prototyping activity and a validation session. The outcomes of this paper contribute (1) a set of collaboration design principles to inspire future computer-supported collaborative work, (2) eight collaboration patterns for blended f-formations and collaboration at scale and (3) theoretical implications for f-formations and space-place relationships. As a result, this work creates a blueprint for scaling collaboration across distributed spaces.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1030},\nnumpages = {18},\nkeywords = {collaboration, f-formations, mixed reality, scale, space and place},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642330,\nauthor = {Yeh, Yen-Ting and Joshi, Nikhita and Vogel, Daniel},\ntitle = {The Effects of Update Interval and Reveal Method on Writer Comfort in Synchronized Shared-Editors},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642330},\ndoi = {10.1145/3613904.3642330},\nabstract = {Synchronized shared-editors like Google Docs allow people to write together, but there is no “privacy of writing” which can make writers feel uncomfortable. We propose methods to give writers more control over when and how their edits are shown to collaborators to increase comfort. These are in the form of different update strategies composed of an update interval and a reveal method. Results from an experiment with simulated observers show that alternative update strategies can be beneficial, each having their own pros and cons. A follow-up experiment with writer and observer pairs validates these findings and shows that observers are amenable to experiencing short delays caused by alternative update strategies. Our work shows that synchronous writing tools should support alternative update strategies that preserve both collaborator awareness and writer comfort.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1031},\nnumpages = {13},\nkeywords = {Collaborative writing, Synchronized shared-editors, Writer comfort},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642419,\nauthor = {Mu, Qianqian and Borowski, Marcel and Gr\\o{}nb\\ae{}k, Jens Emil Sloth and B\\o{}dker, Susanne and Hoggan, Eve},\ntitle = {Whispering Through Walls: Towards Inclusive Backchannel Communication in Hybrid Meetings},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642419},\ndoi = {10.1145/3613904.3642419},\nabstract = {Backchannel communication, like whispering or instant messaging, is common in meetings and holds significant value. However, it remains largely unexplored in the context of hybrid meetings with co-located and remote participants. To address this gap, we derived unique challenges of backchannel communication in hybrid meetings through an interview study. These challenges inspired a new voice-based backchannel communication system, WhisperChannel, aimed to be inclusive and low effort. WhisperChannel enables users to whisper remotely to anyone in the meeting — similar to whispering to co-located seat neighbors. In a user study with three groups, each having two sessions of hybrid meetings, we investigated the inclusiveness and effort of WhisperChannel for both co-located and remote participants. We provide insights into the benefits and limitations of using remote whispering. We show that WhisperChannel helped remote and co-located participants feel more included while requiring low effort to use, however, also introduced new challenges in backchannel communication.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1032},\nnumpages = {16},\nkeywords = {backchannel communication, hybrid meetings, virtual meetings, whispering},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642459,\nauthor = {Sharma, Nikhil and Liao, Q. Vera and Xiao, Ziang},\ntitle = {Generative Echo Chamber? Effect of LLM-Powered Search Systems on Diverse Information Seeking},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642459},\ndoi = {10.1145/3613904.3642459},\nabstract = {Large language models (LLMs) powered conversational search systems have already been used by hundreds of millions of people, and are believed to bring many benefits over conventional search. However, while decades of research and public discourse interrogated the risk of search systems in increasing selective exposure and creating echo chambers—limiting exposure to diverse opinions and leading to opinion polarization, little is known about such a risk of LLM-powered conversational search. We conduct two experiments to investigate: 1) whether and how LLM-powered conversational search increases selective exposure compared to conventional search; 2) whether and how LLMs with opinion biases that either reinforce or challenge the user’s view change the effect. Overall, we found that participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias. These results present critical implications for the development of LLMs and conversational search systems, and the policy governing these technologies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1033},\nnumpages = {17},\nkeywords = {Confirmation Bias, Conversational Search, Echo Chamber Effect, Generative AI, Information Diversity, Information Seeking, Large Language Models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641995,\nauthor = {Sarma, Abhraneel and Pu, Xiaoying and Cui, Yuan and Correll, Michael and Brown, Eli T and Kay, Matthew},\ntitle = {Odds and Insights: Decision Quality in Exploratory Data Analysis Under Uncertainty},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641995},\ndoi = {10.1145/3613904.3641995},\nabstract = {Recent studies have shown that users of visual analytics tools can have difficulty distinguishing robust findings in the data from statistical noise, but the true extent of this problem is likely dependent on both the incentive structure motivating their decisions, and the ways that uncertainty and variability are (or are not) represented in visualisations. In this work, we perform a crowd-sourced study measuring decision-making quality in visual analytics, testing both an explicit structure of incentives designed to reward cautious decision-making as well as a variety of designs for communicating uncertainty. We find that, while participants are unable to perfectly control for false discoveries as well as idealised statistical models such as the Benjamini-Hochberg, certain forms of uncertainty visualisations can improve the quality of participants’ decisions and lead to fewer false discoveries than not correcting for multiple comparisons. We conclude with a call for researchers to further explore visual analytics decision quality under different decision-making contexts, and for designers to directly present uncertainty and reliability information to users of visual analytics tools. The supplementary materials are available at: https://osf.io/xtsfz/.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1034},\nnumpages = {14},\nkeywords = {decision-making, multiple comparisons problem, uncertainty visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642515,\nauthor = {Murakami, Taichi and Fujita, Kazuyuki and Hara, Kotaro and Takashima, Kazuki and Kitamura, Yoshifumi},\ntitle = {SwapVid: Integrating Video Viewing and Document Exploration with Direct Manipulation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642515},\ndoi = {10.1145/3613904.3642515},\nabstract = {Videos accompanied by documents—document-based videos—enable presenters to share contents beyond videos and audience to use them for detailed content comprehension. However, concurrently exploring multiple channels of information could be taxing. We propose SwapVid, a novel interface for viewing and exploring document-based videos. SwapVid seamlessly integrates a video and a document into a single view and lets the content behaves as both video and a document; it adaptively switches a document-based video to act as a video or a document upon direct manipulation (e.g., scrolling the document, manipulating the video timeline). We conducted a user study with twenty participants, comparing SwapVid to a side-by-side video/document views. Results showed that our interface reduces time and physical workload when exploring slide-based documents based on video referencing. Based on the study findings, we extended SwapVid with additional functionalities and demonstrated that it further extends the practical capabilities.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1035},\nnumpages = {13},\nkeywords = {Document-based video interfaces, Lecture videos, Screen-shared documents, Video-document matching},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642638,\nauthor = {Spatharioti, Sofia Eleni and Goldstein, Daniel G and Hofman, Jake M},\ntitle = {Using Open Data to Automatically Generate Localized Analogies},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642638},\ndoi = {10.1145/3613904.3642638},\nabstract = {Numerical analogies (or “perspectives”) that translate unfamiliar measurements into comparisons with familiar reference objects (e.g., “275,000 square miles is roughly as large as Texas”) have been shown to aid readers’ recall, estimation, and error detection for numbers. However, because familiar reference objects are culture-specific, analogies do not always generalize across audiences. Crowdsourcing perspectives has proven effective but is limited by scalability issues and a lack of crowdworking markets in many regions. In this research, we develop an automated technique for generating localized perspectives. We utilize several open data sources for relevance signals and develop a surprisingly simple model capable of localizing analogies to new audiences without any retraining from human judges. We validate the model by testing it in both a new domain and with a different linguistic audience residing in another country. We release the compiled dataset of 400,000 reference objects to the research community.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1036},\nnumpages = {13},\nkeywords = {crowdsourcing, measurement, numeracy, perspectives},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642267,\nauthor = {Jun, Eunice and Misback, Edward and Heer, Jeffrey and Just, Rene},\ntitle = {rTisane: Externalizing conceptual models for data analysis prompts reconsideration of domain assumptions and facilitates statistical modeling},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642267},\ndoi = {10.1145/3613904.3642267},\nabstract = {Statistical models should accurately reflect analysts’ domain knowledge about variables and their relationships. While recent tools let analysts express these assumptions and use them to produce a resulting statistical model, it remains unclear what analysts want to express and how externalization impacts statistical model quality. This paper addresses these gaps. We first conduct an exploratory study of analysts using a domain-specific language (DSL) to express conceptual models. We observe a preference for detailing how variables relate and a desire to allow, and then later resolve, ambiguity in their conceptual models. We leverage these findings to develop rTisane, a DSL for expressing conceptual models augmented with an interactive disambiguation process. In a controlled evaluation, we find that analysts reconsidered their assumptions, self-reported externalizing their assumptions accurately, and maintained analysis intent with rTisane. Additionally, rTisane enabled some analysts to author statistical models they were unable to specify manually. For others, rTisane resulted in models that better fit the data or enabled iterative improvement.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1037},\nnumpages = {16},\nkeywords = {domain-specific language, end-user elicitation, end-user programming, linear modeling, statistical analysis},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642074,\nauthor = {Huang, Forrest and Li, Gang and Li, Tao and Li, Yang},\ntitle = {Automatic Macro Mining from Interaction Traces at Scale},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642074},\ndoi = {10.1145/3613904.3642074},\nabstract = {Macros are building block tasks of our everyday smartphone activity (e.g., \"login\", or \"booking a flight\"). Effectively extracting macros is important for understanding mobile interaction and enabling task automation. These macros are however difficult to extract at scale as they can be comprised of multiple steps yet hidden within programmatic components of mobile apps. In this paper, we introduce a novel approach based on Large Language Models (LLMs) to automatically extract semantically meaningful macros from both random and user-curated mobile interaction traces. The macros produced by our approach are automatically tagged with natural language descriptions and are fully executable. We conduct multiple studies to validate the quality of extracted macros, including user evaluation, comparative analysis against human-curated tasks, and automatic execution of these macros. These experiments and analyses demonstrate the effectiveness of our approach and the usefulness of extracted macros in various downstream applications.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1038},\nnumpages = {16},\nkeywords = {Large Language Model, Macro, Mobile UI, User Task},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642754,\nauthor = {Subramonyam, Hari and Pea, Roy and Pondoc, Christopher and Agrawala, Maneesh and Seifert, Colleen},\ntitle = {Bridging the Gulf of Envisioning: Cognitive Challenges in Prompt Based Interactions with LLMs},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642754},\ndoi = {10.1145/3613904.3642754},\nabstract = {Large language models (LLMs) exhibit dynamic capabilities and appear to comprehend complex and ambiguous natural language prompts. However, calibrating LLM interactions is challenging for interface designers and end-users alike. A central issue is our limited grasp of how human cognitive processes begin with a goal and form intentions for executing actions, a blindspot even in established interaction models such as Norman’s gulfs of execution and evaluation. To address this gap, we theorize how end-users ‘envision’ translating their goals into clear intentions and craft prompts to obtain the desired LLM response. We define a process of Envisioning by highlighting three misalignments on not knowing: (1) what the task should be, (2) how to instruct the LLM to do the task, and (3) what to expect for the LLM’s output in meeting the goal. Finally, we make recommendations to narrow the gulf of envisioning in human-LLM interactions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1039},\nnumpages = {19},\nkeywords = {cognitive psychology, large language models, prompt-based interactions},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642834,\nauthor = {He, Zeyu and Huang, Chieh-Yang and Ding, Chien-Kuang Cornelia and Rohatgi, Shaurya and Huang, Ting-Hao Kenneth},\ntitle = {If in a Crowdsourced Data Annotation Pipeline, a GPT-4},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642834},\ndoi = {10.1145/3613904.3642834},\nabstract = {Recent studies indicated GPT-4 outperforms online crowd workers in data labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk). However, these studies were criticized for deviating from standard crowdsourcing practices and emphasizing individual workers’ performances over the whole data-annotation process. This paper compared GPT-4 and an ethical and well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments from 200 scholarly articles using the CODA-19 scheme. Two worker interfaces yielded 127,080 labels, which were then used to infer the final labels through eight label-aggregation algorithms. Our evaluation showed that despite best practices, MTurk pipeline’s highest accuracy was 81.5\\%, whereas GPT-4 achieved 83.6\\%. Interestingly, when combining GPT-4’s labels with crowd labels collected via an advanced worker interface for aggregation, 2 out of the 8 algorithms achieved an even higher accuracy (87.5\\%, 87.0\\%). Further analysis suggested that, when the crowd’s and GPT-4’s labeling strengths are complementary, aggregating them could increase labeling accuracy.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1040},\nnumpages = {25},\nkeywords = {Crowdsourcing, Data Annotation, GPT-4, Large Language Model},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642055,\nauthor = {Yu, Zhengyan and Namkung, Hun and Guo, Jiang and Milner, Henry and Goldfoot, Joel and Wang, Yang and Sekar, Vyas},\ntitle = {SEAM-EZ: Simplifying Stateful Analytics through Visual Programming},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642055},\ndoi = {10.1145/3613904.3642055},\nabstract = {Across many domains (e.g., media/entertainment, mobile apps, finance, IoT, cybersecurity), there is a growing need for stateful analytics over streams of events to meet key business outcomes. Stateful analytics over event streams entails carefully modeling the sequence, timing, and contextual correlations of events to dynamic attributes. Unfortunately, existing frameworks and languages (e.g., SQL, Flink, Spark) entail significant code complexity and expert effort to express such stateful analytics because of their dynamic and stateful nature. Our overarching goal is to simplify and democratize stateful analytics. Through an iterative design and evaluation process including a foundational user study and two rounds of formative evaluations with 15 industry practitioners, we created SEAM-EZ, a no-code visual programming platform for quickly creating and validating stateful metrics. SEAM-EZ features a node-graph editor, interactive tooltips, embedded data views, and auto-suggestion features to facilitate the creation and validation of stateful analytics. We then conducted three real-world case studies of SEAM-EZ with 20 additional practitioners. Our results suggest that practitioners who previously could not or had to spend significant effort to create stateful metrics using traditional tools such as SQL or Spark can now easily and quickly create and validate such metrics using SEAM-EZ.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1041},\nnumpages = {23},\nkeywords = {data analytics, metrics, stateful computation, visual programming},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641899,\nauthor = {Reza, Mohi and Laundry, Nathan M and Musabirov, Ilya and Dushniku, Peter and Yu, Zhi Yuan “Michael” and Mittal, Kashish and Grossman, Tovi and Liut, Michael and Kuzminykh, Anastasia and Williams, Joseph Jay},\ntitle = {ABScribe: Rapid Exploration \\& Organization of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641899},\ndoi = {10.1145/3613904.3641899},\nabstract = {Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art Large Language Models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new variations without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers’ flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration and organization of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly modify variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text fields for rapid in-place comparisons using mouse-over interactions on a popup toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances user perceptions of the revision process (d = 2.41, p < 0.001) compared to a popular baseline workflow, and provides insights into how writers explore variations using LLMs.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1042},\nnumpages = {18},\nkeywords = {datasets, gaze detection, neural networks, text tagging},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642217,\nauthor = {Lin, Susan and Warner, Jeremy and Zamfirescu-Pereira, J.D. and Lee, Matthew G and Jain, Sauhard and Cai, Shanqing and Lertvittayakumjorn, Piyawat and Huang, Michael Xuelin and Zhai, Shumin and Hartmann, Bjoern and Liu, Can},\ntitle = {Rambler: Supporting Writing With Speech via LLM-Assisted Gist Manipulation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642217},\ndoi = {10.1145/3613904.3642217},\nabstract = {Dictation enables efficient text input on mobile devices. However, writing with speech can produce disfluent, wordy, and incoherent text and thus requires heavy post-processing. This paper presents Rambler, an LLM-powered graphical user interface that supports gist-level manipulation of dictated text with two main sets of functions: gist extraction and macro revision. Gist extraction generates keywords and summaries as anchors to support the review and interaction with spoken text. LLM-assisted macro revisions allow users to respeak, split, merge, and transform dictated text without specifying precise editing locations. Together they pave the way for interactive dictation and revision that help close gaps between spontaneously spoken words and well-structured writing. In a comparative study with 12 participants performing verbal composition tasks, Rambler outperformed the baseline of a speech-to-text editor + ChatGPT, as it better facilitates iterative revisions with enhanced user control over the content while supporting surprisingly diverse user strategies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1043},\nnumpages = {19},\nkeywords = {AI, LLM, STT, dictation, speech, speech-to-text, text composition, writing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642134,\nauthor = {Dhillon, Paramveer S. and Molaei, Somayeh and Li, Jiaqi and Golub, Maximilian and Zheng, Shaochun and Robert, Lionel Peter},\ntitle = {Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642134},\ndoi = {10.1145/3613904.3642134},\nabstract = {Advances in language modeling have paved the way for novel human-AI co-writing experiences. This paper explores how varying levels of scaffolding from large language models (LLMs) shape the co-writing process. Employing a within-subjects field experiment with a Latin square design, we asked participants (N=131) to respond to argumentative writing prompts under three randomly sequenced conditions: no AI assistance (control), next-sentence suggestions (low scaffolding), and next-paragraph suggestions (high scaffolding). Our findings reveal a U-shaped impact of scaffolding on writing quality and productivity (words/time). While low scaffolding did not significantly improve writing quality or productivity, high scaffolding led to significant improvements, especially benefiting non-regular writers and less tech-savvy users. No significant cognitive burden was observed while using the scaffolded writing tools, but a moderate decrease in text ownership and satisfaction was noted. Our results have broad implications for the design of AI-powered writing tools, including the need for personalized scaffolding mechanisms.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1044},\nnumpages = {18},\nkeywords = {Generative AI, Human-AI collaboration, co-writing, writing assistants},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3641895,\nauthor = {Hoque, Md Naimul and Mashiat, Tasfia and Ghai, Bhavya and Shelton, Cecilia D. and Chevalier, Fanny and Kraus, Kari and Elmqvist, Niklas},\ntitle = {The HaLLMark Effect: Supporting Provenance and Transparent Use of Large Language Models in Writing with Interactive Visualization},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3641895},\ndoi = {10.1145/3613904.3641895},\nabstract = {The use of Large Language Models (LLMs) for writing has sparked controversy both among readers and writers. On one hand, writers are concerned that LLMs will deprive them of agency and ownership, and readers are concerned about spending their time on text generated by soulless machines. On the other hand, AI-assistance can improve writing as long as writers can conform to publisher policies, and as long as readers can be assured that a text has been verified by a human. We argue that a system that captures the provenance of interaction with an LLM can help writers retain their agency, conform to policies, and communicate their use of AI to publishers and readers transparently. Thus we propose HaLLMark, a tool for visualizing the writer’s interaction with the LLM. We evaluated HaLLMark with 13 creative writers, and found that it helped them retain a sense of control and ownership of the text.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1045},\nnumpages = {15},\nkeywords = {Creative writing, LLMs, agency, co-writing, visualization.},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642693,\nauthor = {Kim, Taewan and Shin, Donghoon and Kim, Young-Ho and Hong, Hwajung},\ntitle = {DiaryMate: Understanding User Perceptions and Experience in Human-AI Collaboration for Personal Journaling},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642693},\ndoi = {10.1145/3613904.3642693},\nabstract = {With their generative capabilities, large language models (LLMs) have transformed the role of technological writing assistants from simple editors to writing collaborators. Such a transition emphasizes the need for understanding user perception and experience, such as balancing user intent and the involvement of LLMs across various writing domains in designing writing assistants. In this study, we delve into the less explored domain of personal writing, focusing on the use of LLMs in introspective activities. Specifically, we designed DiaryMate, a system that assists users in journal writing with LLM. Through a 10-day field study (N=24), we observed that participants used the diverse sentences generated by the LLM to reflect on their past experiences from multiple perspectives. However, we also observed that they are over-relying on the LLM, often prioritizing its emotional expressions over their own. Drawing from these findings, we discuss design considerations when leveraging LLMs in a personal writing practice.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1046},\nnumpages = {15},\nkeywords = {Diary, Human-AI collaborative writing, Journaling, Personal writing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642549,\nauthor = {G\\\"{o}ldi, Andreas and Wambsganss, Thiemo and Neshaei, Seyed Parsa and Rietsche, Roman},\ntitle = {Intelligent Support Engages Writers Through Relevant Cognitive Processes},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642549},\ndoi = {10.1145/3613904.3642549},\nabstract = {Student peer review writing is prevalent and important in education for fostering critical thinking and learning motivation. However, it often entails challenges such as high effort and writer’s block. Leaving students unsupported may thus diminish the efficacy of the process. Large Language Models (LLMs) offer a potential remedy, but their utility hinges on user-centered design. Guided by design-determining constructs from the Cognitive Process Theory of Writing, we developed an intelligent writing support tool to alleviate these challenges, aiding 1) ideation and 2) evaluation. A randomized experiment (n=120) confirmed users were less inclined to utilize the tool’s intelligent features when offered pre-supplied ideas or evaluations, validating our approach. Moreover, students engaged not less but more with their writing if support was available, indicating an enhanced experience. Our research illuminates design choices for enhancing LLM-based tools’ usability and user experience, specifically optimizing intelligent writing support tools to facilitate student peer review.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1047},\nnumpages = {12},\nkeywords = {Artifact or System, Creativity Support, Education/Learning, Schools/Educational Setting},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642625,\nauthor = {Li, Zhuoyan and Liang, Chen and Peng, Jing and Yin, Ming},\ntitle = {The Value, Benefits, and Concerns of Generative AI-Powered Assistance in Writing},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642625},\ndoi = {10.1145/3613904.3642625},\nabstract = {Recent advances in generative AI technologies like large language models raise both excitement and concerns about the future of human-AI co-creation in writing. To unpack people’s attitude towards and experience with generative AI-powered writing assistants, in this paper, we conduct an experiment to understand whether and how much value people attach to AI assistance, and how the incorporation of AI assistance in writing workflows changes people’s writing perceptions and performance. Our results suggest that people are willing to forgo financial payments to receive writing assistance from AI, especially if AI can provide direct content generation assistance and the writing task is highly creative. Generative AI-powered assistance is found to offer benefits in increasing people’s productivity and confidence in writing. However, direct content generation assistance offered by AI also comes with risks, including decreasing people’s sense of accountability and diversity in writing. We conclude by discussing the implications of our findings.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1048},\nnumpages = {25},\nkeywords = {AI writing assistant, Human-AI co-creation, Large language model},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642406,\nauthor = {Benharrak, Karim and Zindulka, Tim and Lehmann, Florian and Heuer, Hendrik and Buschek, Daniel},\ntitle = {Writer-Defined AI Personas for On-Demand Feedback Generation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642406},\ndoi = {10.1145/3613904.3642406},\nabstract = {Compelling writing is tailored to its audience. This is challenging, as writers may struggle to empathize with readers, get feedback in time, or gain access to the target group. We propose a concept that generates on-demand feedback, based on writer-defined AI personas of any target audience. We explore this concept with a prototype (using GPT-3.5) in two user studies (N=5 and N=11): Writers appreciated the concept and strategically used personas for getting different perspectives. The feedback was seen as helpful and inspired revisions of text and personas, although it was often verbose and unspecific. We discuss the impact of on-demand feedback, the limited representativity of contemporary AI systems, and further ideas for defining AI personas. This work contributes to the vision of supporting writers with AI by expanding the socio-technical perspective in AI tool design: To empower creators, we also need to keep in mind their relationship to an audience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1049},\nnumpages = {18},\nkeywords = {Human-AI interaction, Large language models, Personas, Text feedback, Writing assistance},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642414,\nauthor = {Shaer, Orit and Cooper, Angelora and Mokryn, Osnat and Kun, Andrew L and Ben Shoshan, Hagit},\ntitle = {AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642414},\ndoi = {10.1145/3613904.3642414},\nabstract = {The growing availability of generative AI technologies such as large language models (LLMs) has significant implications for creative work. This paper explores twofold aspects of integrating LLMs into the creative process – the divergence stage of idea generation, and the convergence stage of evaluation and selection of ideas. We devised a collaborative group-AI Brainwriting ideation framework, which incorporated an LLM as an enhancement into the group ideation process, and evaluated the idea generation process and the resulted solution space. To assess the potential of using LLMs in the idea evaluation process, we design an evaluation engine and compared it to idea ratings assigned by three expert and six novice evaluators. Our findings suggest that integrating LLM in Brainwriting could enhance both the ideation process and its outcome. We also provide evidence that LLMs can support idea evaluation. We conclude by discussing implications for HCI education and practice.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1050},\nnumpages = {17},\nkeywords = {Brainwriting, LLM, human-AI collaboration},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642105,\nauthor = {Qin, Hua Xuan and Jin, Shan and Gao, Ze and Fan, Mingming and Hui, Pan},\ntitle = {CharacterMeet: Supporting Creative Writers' Entire Story Character Construction Processes Through Conversation with LLM-Powered Chatbot Avatars},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642105},\ndoi = {10.1145/3613904.3642105},\nabstract = {Support for story character construction is as essential as characters are for stories. Building upon past research on early character construction stages, we explore how conversation with chatbot avatars embodying characters powered by more recent technologies could support the entire character construction process for creative writing. Through a user study (N=14) with creative writers, we examine thinking and usage patterns of CharacterMeet, a prototype system allowing writers to progressively manifest characters through conversation while customizing context, character appearance, voice, and background image. We discover that CharacterMeet facilitates iterative character construction. Specifically, participants, including those with more linear usual approaches, alternated between writing and personalized exploration through visualization of ideas on CharacterMeet while visuals and audio enhanced immersion. Our findings support research on iterative creative processes and the growing potential of personalizable generative AI creativity support tools. We present design implications for leveraging chatbot avatars in the creative writing process.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1051},\nnumpages = {19},\nkeywords = {Creative Writing, Creativity Support, Human-AI Collaboration, Large Language Models, Writing Assistants},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642743,\nauthor = {Weber, Florian and Wambsganss, Thiemo and Neshaei, Seyed Parsa and Soellner, Matthias},\ntitle = {LegalWriter: An Intelligent Writing Support System for Structured and Persuasive Legal Case Writing for Novice Law Students},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642743},\ndoi = {10.1145/3613904.3642743},\nabstract = {Novice students in law courses or students who encounter legal education face the challenge of acquiring specialized and highly concept-oriented knowledge. Structured and persuasive writing combined with the necessary domain knowledge is challenging for many learners. Recent advances in machine learning (ML) have shown the potential to support learners in complex writing tasks. To test the effects of ML-based support on students’ legal writing skills, we developed the intelligent writing support system LegalWriter. We evaluated the system’s effectiveness with 62 students. We showed that students who received intelligent writing support based on their errors wrote more structured and persuasive case solutions with a better quality of legal writing than the current benchmark. At the same time, our results demonstrated the positive effects on the students’ writing processes.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1052},\nnumpages = {23},\nkeywords = {Adaptive learning, Learning from errors, Learning systems, Legal education, Writing support systems},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642320,\nauthor = {Cai, Runze and Janaka, Nuwan and Chen, Yang and Wang, Lucia and Zhao, Shengdong and Liu, Can},\ntitle = {PANDALens: Towards AI-Assisted In-Context Writing on OHMD During Travels},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642320},\ndoi = {10.1145/3613904.3642320},\nabstract = {While effective for recording and sharing experiences, traditional in-context writing tools are relatively passive and unintelligent, serving more like instruments rather than companions. This reduces primary task (e.g., travel) enjoyment and hinders high-quality writing. Through formative study and iterative development, we introduce PANDALens, a Proactive AI Narrative Documentation Assistant built on an Optical See-Through Head Mounted Display that supports personalized documentation in everyday activities. PANDALens observes multimodal contextual information from user behaviors and environment to confirm interests and elicit contemplation, and employs Large Language Models to transform such multimodal information into coherent narratives with significantly reduced user effort. A real-world travel scenario comparing PANDALens with a smartphone alternative confirmed its effectiveness in improving writing quality and travel enjoyment while minimizing user effort. Accordingly, we propose design guidelines for AI-assisted in-context writing, highlighting the potential of transforming them from tools to intelligent companions.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1053},\nnumpages = {24},\nkeywords = {AI, HMD, Human-AI collaborative writing, in-context writing, large language model, multimodal information, smart glasses, travel blog},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642697,\nauthor = {Lee, Mina and Gero, Katy Ilonka and Chung, John Joon Young and Shum, Simon Buckingham and Raheja, Vipul and Shen, Hua and Venugopalan, Subhashini and Wambsganss, Thiemo and Zhou, David and Alghamdi, Emad A. and August, Tal and Bhat, Avinash and Choksi, Madiha Zahrah and Dutta, Senjuti and Guo, Jin L.C. and Hoque, Md Naimul and Kim, Yewon and Knight, Simon and Neshaei, Seyed Parsa and Shibani, Antonette and Shrivastava, Disha and Shroff, Lila and Sergeyuk, Agnia and Stark, Jessi and Sterman, Sarah and Wang, Sitong and Bosselut, Antoine and Buschek, Daniel and Chang, Joseph Chee and Chen, Sherol and Kreminski, Max and Park, Joonsuk and Pea, Roy and Rho, Eugenia Ha Rim and Shen, Zejiang and Siangliulue, Pao},\ntitle = {A Design Space for Intelligent and Interactive Writing Assistants},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642697},\ndoi = {10.1145/3613904.3642697},\nabstract = {In our era of rapid technological advancement, the research landscape for writing assistants has become increasingly fragmented across various research communities. We seek to address this challenge by proposing a design space as a structured way to examine and explore the multidimensional space of intelligent and interactive writing assistants. Through community collaboration, we explore five aspects of writing assistants: task, user, technology, interaction, and ecosystem. Within each aspect, we define dimensions and codes by systematically reviewing 115 papers, while leveraging the expertise of researchers in various disciplines. Our design space aims to offer researchers and designers a practical tool to navigate, comprehend, and compare the various possibilities of writing assistants, and aid in the design of new writing assistants.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1054},\nnumpages = {35},\nkeywords = {Artificial Intelligence, Design Space, Language Models, Writing Assistants, Writing Support Tools},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642794,\nauthor = {Choi, DaEun and Hong, Sumin and Park, Jeongeon and Chung, John Joon Young and Kim, Juho},\ntitle = {CreativeConnect: Supporting Reference Recombination for Graphic Design Ideation with Generative AI},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642794},\ndoi = {10.1145/3613904.3642794},\nabstract = {Graphic designers often get inspiration through the recombination of references. Our formative study (N=6) reveals that graphic designers focus on conceptual keywords during this process, and want support for discovering the keywords, expanding them, and exploring diverse recombination options of them, while still having room for designers’ creativity. We propose CreativeConnect, a system with generative AI pipelines that helps users discover useful elements from the reference image using keywords, recommends relevant keywords, generates diverse recombination options with user-selected keywords, and shows recombinations as sketches with text descriptions. Our user study (N=16) showed that CreativeConnect helped users discover keywords from the reference and generate multiple ideas based on them, ultimately helping users produce more design ideas with higher self-reported creativity, compared to the baseline system without generative pipelines. While CreativeConnect was shown effective in ideation, we discussed how CreativeConnect can be extended to support other types of tasks in creativity support.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1055},\nnumpages = {25},\nkeywords = {Creativity support tool, Graphic Design ideation, Machine Learning, Reference recombination},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642096,\nauthor = {Shen, Yulin and Shen, Yifei and Cheng, Jiawen and Jiang, Chutian and Fan, Mingming and Wang, Zeyu},\ntitle = {Neural Canvas: Supporting Scenic Design Prototyping by Integrating 3D Sketching and Generative AI},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642096},\ndoi = {10.1145/3613904.3642096},\nabstract = {We propose Neural Canvas, a lightweight 3D platform that integrates sketching and a collection of generative AI models to facilitate scenic design prototyping. Compared with traditional 3D tools, sketching in a 3D environment helps designers quickly express spatial ideas, but it does not facilitate the rapid prototyping of scene appearance or atmosphere. Neural Canvas integrates generative AI models into a 3D sketching interface and incorporates four types of projection operations to facilitate 2D-to-3D content creation. Our user study shows that Neural Canvas is an effective creativity support tool, enabling users to rapidly explore visual ideas and iterate 3D scenic designs. It also expedites the creative process for both novices and artists who wish to leverage generative AI technology, resulting in attractive and detailed 3D designs created more efficiently than using traditional modeling tools or individual generative AI platforms.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1056},\nnumpages = {18},\nkeywords = {3D sketching, generative AI, prototyping, scenic design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",
    "@inproceedings{10.1145/3613904.3642218,\nauthor = {Lee, Seung Won and Jo, Tae Hee and Jin, Semin and Choi, Jiin and Yun, Kyungwon and Bromberg, Sergio and Ban, Seonghoon and Hyun, Kyung Hoon},\ntitle = {The Impact of Sketch-guided vs. Prompt-guided 3D Generative AIs on the Design Exploration Process},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642218},\ndoi = {10.1145/3613904.3642218},\nabstract = {Various modalities have emerged in the field of 3D generative AI (GenAI) to enhance design outcomes. While some designers find inspiration in prompts to guide their design options, others prefer sketching to embody creative visions. Nonetheless, the impact of the different modalities of 3D GenAI on the design process remains largely unexplored. This study examines the utilization of prompt- and sketch-guided modalities within the design process by conducting linkography and workflow analyses with 12 designers. The results revealed that prompts played a pivotal role in stimulating initial ideation, whereas sketches played a crucial role in embodying design ideas. This investigation highlights the distinct contributions of these modalities at different phases of the design process, suggesting the potential for a more refined and synergistic collaboration between humans and AI. By elucidating the diverse functions of sketches and prompts, we propose prospective directions for the UX framework of the 3D GenAI.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {1057},\nnumpages = {18},\nkeywords = {3D Reconstruction, AI in Design, Design Process, Generative AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}"
]