[
    "@inproceedings{10.1145/3613905.3650845,\nauthor = {Panchanadikar, Ruchi and Freeman, Guo and Li, Lingyuan and Schulenberg, Kelsea and Hu, Yang},\ntitle = {\"A New Golden Era\" or \"Slap Comps\": How Non-Profit Driven Indie Game Developers Perceive the Emerging Role of Generative AI in Game Development},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650845},\ndoi = {10.1145/3613905.3650845},\nabstract = {While the boom in generative AI technologies continues to transform modern creative work, it also introduces new and urgent risks. We endeavor to unpack these complicated phenomena by focusing on how non-profit driven indie game developers perceive the opportunities and challenges of using generative AI for their creative practices. Based on qualitative analysis of 1,540 posts and comments from online forums dedicated to this creative community, we provide early empirical evidence of the potential for generative AI to shape the trajectory of creative technology communities such as non-profit driven indie game developers. These insights may inform future research regarding AI’s impacts on the nature of creative work and the growth of creative workforces in technology, which may also help design future AI technologies to support rather than harm these creative communities.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {1},\nnumpages = {7},\nkeywords = {Creativity, Game Development, Generative AI, Indie Game Development},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650889,\nauthor = {Kablo, Emiram and Kader, Katharina and Arias-Cabarcos, Patricia},\ntitle = {\"I'm actually going to go and change these passwords\": Analyzing the Usability of Credential Audit Interfaces in Password Managers},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650889},\ndoi = {10.1145/3613905.3650889},\nabstract = {Credential audit interfaces in password managers play a crucial role in enhancing user security by identifying weak, reused, or exposed passwords. However, existing research lacks a comprehensive analysis of the usability and motivations of adopters and non-adopters. To address this gap, we conducted 11 semi-structured interviews with users and non-users of credential audit tools, all of whom use password managers. Our study reveals security as the primary motivator for adoption. Despite a potential challenge in handling overwhelming results by the audit reports, participants showed commitment to security and suggested potential benefits of prioritization techniques for a better overview of important results. Transparency and detailed explanations for password weaknesses were identified as user needs. Our broader discussion on password manager adoption underscores the significance of security and convenience as key adoption factors.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {2},\nnumpages = {13},\nkeywords = {Credential Audit, Password Managers, Usability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651024,\nauthor = {Saaty, Morva and Andrus, Natalie and Abdelgawad, Norhan and Chandran, Jennifer and Noneman, Brett and Jackson, Justice and Alading, Kun and Hassan, Taha and Mccrickard, D. Scott and Misra, Shalini and Wernstedt, Kris},\ntitle = {\"Is Long-distance Hiking an Emotional Roller Coaster?\" Evaluating Emotions and Weather Effects on the Appalachian Trail},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651024},\ndoi = {10.1145/3613905.3651024},\nabstract = {The growing prevalence of emotion research in human-computer interaction highlights the crucial role of emotion in understanding humans’ behavioral states and experiences. Online media, especially community blogs, enable reflection of people’s emotions on a large scale, while emotion recognition offers profound insights into the underlying reasons behind the expressed emotions. Within these capabilities, this paper uses an emotion recognition model and topic modeling analysis to examine Appalachian Trail long-distance hikers’ blogs on the Trail Journals platform and identify key driving factors regarding expressed emotions. We also examined the influence of weather, as an inseparable factor of outdoor activities, on hikers’ emotions and its nuances in long-distance hiking. The findings emphasize the emotional aspect of long-distance hiking, which can facilitate mental support from the community in the future. We discuss the usefulness of blogs in gaining insight into writers conveying their emotional experiences.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {3},\nnumpages = {8},\nkeywords = {Appalachian Trail, Community Blogs, Emotion Recognition, Outdoor Computing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651111,\nauthor = {Milesi, Mikaela E and Alfredo, Riordan and Echeverria, Vanessa and Yan, Lixiang and Zhao, Linxuan and Tsai, Yi-Shan and Martinez-Maldonado, Roberto},\ntitle = {\"It's Really Enjoyable to See Me Solve the Problem like a Hero\": GenAI-enhanced Data Comics as a Learning Analytics Tool},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651111},\ndoi = {10.1145/3613905.3651111},\nabstract = {Data comics are an emergent storytelling format that can enable non-experts to consume salient insights from data. Despite some research investigating the use of comic strips in education, there is limited evidence relating to how students perceive data comics about their own data as a way to reflect on their learning experience. In this paper, we summarise nursing students’ perceptions of the advantages and limitations of data comics by presenting personalised Multimodal Learning Analytics (LA) data in data comics form. We present GenAI-enhanced data comic prototypes created using a combination of the generative artificial intelligence tool, Midjourney, and graphics illustration software. Our findings indicate that while students see the potential of data comics as an engaging and enjoyable visualisation technique, concerns remain regarding the perceived lack of professionalism associated with this format.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {4},\nnumpages = {7},\nkeywords = {data comics, information visualisation, learning analytics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650946,\nauthor = {Germanakos, Panagiotis},\ntitle = {\"It's Time!\" Toward a Human-AI Quantum Experience Design Paradigm: Reinventing the Theoretical Framework of HCI},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650946},\ndoi = {10.1145/3613905.3650946},\nabstract = {In an AI-driven digital age, it appears that traditional Human-Computer Interaction (HCI) theories and paradigms need to be reassessed, as they should be realigned and augmented to better serve a broader purpose and context. Current Human-Centered Design (HCD) approaches lack the flexibility, volatility and openness to accommodate the new requirements of a highly demanding intelligent digital reality, like randomness, uncertainty, multi-purpose dynamic interactions, adaptability, data-driven design and decision-making, etc. In this respect, this paper (a) embraces an alternative Quantum User Experience theoretical stance – by creating parallels to the fields of (Quantum) Physics, to facilitate the understanding of HCI phenomena in a blended digital-physical ecosystem; and (b) proposes a Human-AI Quantum Experience Design Paradigm that focuses on the dynamics generated by the respective micro- and macro-experiences. Accordingly, the 5 phases: Explore. Define. Engineer. Transform. Adopt., are outlined and brought into perspective of an extended HCD scope. Main aim is to present a promising theoretical standpoint to the HCI community that might be able to address core theoretical challenges and discrepancies, and complement HCD practices towards inclusive and sustainable human experiences.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {5},\nnumpages = {6},\nkeywords = {HCI, Human-centered Artificial Intelligence, Paradigm, Quantum Computing, Quantum User Experience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651036,\nauthor = {Freeman, Guo and Hu, Yang and Panchanadikar, Ruchi and Hall, Amelia L and Schulenberg, Kelsea and Li, Lingyuan},\ntitle = {\"My Audience Gets to Know Me on a More Realistic Level\": Exploring Social VR Streamers’ Unique Strategies to Engage with Their Audiences},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651036},\ndoi = {10.1145/3613905.3651036},\nabstract = {Social Virtual Reality (VR) platforms are increasingly transforming online social spaces by enhancing embodied and immersive social interactions. A growing body of HCI research has focused on various interaction dynamics within social VR spaces. However, how social VR users also endeavor to share their activities outside the social VR platform, such as on 2D live streaming platforms, is an increasingly popular yet understudied phenomenon that blends social VR and live streaming research. Based on eight interviews with experienced social VR streamers, this paper empirically investigates social VR streamers’ streaming practices and strategies to engage with their audiences as compared to non-VR streaming. We provide one of the first empirical evidence of how social VR streaming can creatively combine immersive VR activities with interactive live streaming. This informs future research on understanding and designing future platforms to better support these complicated social dynamics blending both virtual and physical worlds.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {6},\nnumpages = {7},\nkeywords = {Audience Management, Live Streaming, Online Social Interaction, Social Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650839,\nauthor = {Hou, Yuki and Tamoto, Haruki and Miyashita, Homei},\ntitle = {\"My agent understands me better\": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650839},\ndoi = {10.1145/3613905.3650839},\nabstract = {In this study, we propose a novel human-like memory architecture designed for enhancing the cognitive abilities of large language model (LLM)-based dialogue agents. Our proposed architecture enables agents to autonomously recall memories necessary for response generation, effectively addressing a limitation in the temporal cognition of LLMs. We adopt the human memory cue recall as a trigger for accurate and efficient memory recall. Moreover, we developed a mathematical model that dynamically quantifies memory consolidation, considering factors such as contextual relevance, elapsed time, and recall frequency. The agent stores memories retrieved from the user’s interaction history in a database that encapsulates each memory’s content and temporal context. Thus, this strategic storage allows agents to recall specific memories and understand their significance to the user in a temporal context, similar to how humans recognize and recall past experiences.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {7},\nnumpages = {7},\nkeywords = {Intelligent Agents, Large Language Models, Memory Retrieval Models, User Experience, User Interface},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651074,\nauthor = {Kukshinov, Eugene and Tu, Joseph and Szita, Kata and Senthil Nathan, Kaushall and Nacke, Lennart E.},\ntitle = {\"Never The Same\": Systematic Analysis of the Methodological Issues in the Presence Studies That Employ Questionnaires},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651074},\ndoi = {10.1145/3613905.3651074},\nabstract = {Presence is a psychological state that is usually measured via questionnaires. Many presence researchers assume self-report questionnaires are standardized. However, we do not know how reliable they are. This knowledge gap impacts the accuracy and validity of data collected through these questionnaires. Reliable and accurate data collection is crucial to trust findings in presence research. Inaccurate or unreliable data could lead to incorrect conclusions. This impacts theoretical understanding and practical applications. To address this, we conducted a comprehensive systematic review of 100 empirical quantitative presence studies. Our goal was to uncover the underlying issues with these self-report questionnaires. So, we explored the employment of these questionnaires and analyzed the specific reasons for measuring presence. We show patterns and inconsistencies in the current methodologies, as presence questionnaires are frequently utilized in a non-standardised manner. We will propose well-grounded improvements and constructive approaches based on our findings. These will improve the validity and effectiveness of these questionnaires and lead to more consistent and replicable results in presence research.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {8},\nnumpages = {7},\nkeywords = {Methods, Presence, Questionnaires, Reliability, Systematic Analysis},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650778,\nauthor = {LC, RAY and Zamfirescu-Pereira, J.D. and Friedman, Natalie and Fu, Kexue and Li, Yanheng and Ju, Wendy},\ntitle = {\"Sit on me please\": Investigating Perception of Furniture Robotic Movements Using Video Prototyping},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650778},\ndoi = {10.1145/3613905.3650778},\nabstract = {As robots become embedded to greater extents in human environments, mobile furniture robots could be used to create narrate expressive movement to influence user behavior in a room. To investigate how different robot movement may affect the way humans perceive the robot’s expressiveness, responsiveness, and spatial presence, we created video prototypes of chair-robots (chairbots) interacting with individuals or dyads. We used crowdsourcing to evaluate how people perceive these different movements, providing a quantitative overview of which particular movements are particularly effective in engaging perception for expressiveness and responsiveness. This work provides a still-in-progress understanding of perceptions of mobile robotic furniture actions in spatial contexts, suggesting future design strategies for real-life smart furniture interventions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {9},\nnumpages = {6},\nkeywords = {Machine Influence, Persuasive Technology, Smart Furniture},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650756,\nauthor = {Liu, Michael Xieyang and Liu, Frederick and Fiannaca, Alexander J. and Koo, Terry and Dixon, Lucas and Terry, Michael and Cai, Carrie J.},\ntitle = {\"We Need Structured Output\": Towards User-centered Constraints on Large Language Model Output},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650756},\ndoi = {10.1145/3613905.3650756},\nabstract = {Large language models can produce creative and diverse responses. However, to integrate them into current developer workflows, it is essential to constrain their outputs to follow specific formats or standards. In this work, we surveyed 51 experienced industry professionals to understand the range of scenarios and motivations driving the need for output constraints from a user-centered perspective. We identified 134 concrete use cases for constraints at two levels: low-level, which ensures the output adhere to a structured format and an appropriate length, and high-level, which requires the output to follow semantic and stylistic guidelines without hallucination. Critically, applying output constraints could not only streamline the currently repetitive process of developing, testing, and integrating LLM prompts for developers, but also enhance the user experience of LLM-powered features and applications. We conclude with a discussion on user preferences and needs towards articulating intended constraints for LLMs, alongside an initial design for a constraint prototyping tool.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {10},\nnumpages = {9},\nkeywords = {Constrained generation, Large language models, Survey},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651080,\nauthor = {Okolo, Chinasa T. and Lin, Hongjin},\ntitle = {\"You can’t build what you don’t understand\": Practitioner Perspectives on Explainable AI in the Global South},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651080},\ndoi = {10.1145/3613905.3651080},\nabstract = {AI for Social Good (AI4SG) has been advocated as a way to address social impact problems using emerging technologies, but little research has examined practitioner motivations behind building these tools and how practitioners make such tools understandable to stakeholders and end users, e.g., through leveraging techniques such as explainable AI (XAI). In this study, we interviewed 12 AI4SG practitioners to understand their experiences developing social impact technologies and their perceptions of XAI, focusing on projects in the Global South. While most of our participants were aware of XAI, many did not incorporate these techniques due to a lack of domain expertise, difficulty incorporating XAI into their existing workflows, and perceiving XAI as less valuable for end users with low levels of AI and digital literacy. Our work reflects on the shortcomings of XAI for real-world use and advocates for a reimagined agenda for human-centered explainability research.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {11},\nnumpages = {10},\nkeywords = {AI for Social Good, Artificial Intelligence, Explainable AI, Human-Centered Design, Social Impact},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651867,\nauthor = {Zhou, Hanqing and Nikolova, Anastasia and An, Pengcheng},\ntitle = { 'My lollipop dropped...'–Probing Design Opportunities for SEL Agents through Children's Peer Co-Creation of Social-Emotional Stories},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651867},\ndoi = {10.1145/3613905.3651867},\nabstract = {This Late-Breaking Work explores the significance of socio-emotional learning (SEL) and the challenges inherent in designing child-appropriate technologies, namely storytelling agents, to support SEL. We aim to probe their needs and preferences regarding agents for SEL by conducting co-design which involves children co-creating characters and social emotional stories. We conducted collaborative story-making activities with children aged four to six years old. Our findings could inform the design of both verbal and nonverbal interactions of agents, which are to be aligned with children’s understanding and interest. Based on the child-led peer co-design, our work enhances the understanding of SEL agent designs and behaviors tailored to children’s socio-emotional needs, thereby offering practical implications for the more effective SEL tools in future HCI research and practice.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {12},\nnumpages = {8},\nkeywords = {Agent, Co-design, Collaborative storytelling, Generative AI, Social emotional learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650833,\nauthor = {Sung, Ching-Ying and Cherng, Fu-Yin and Chiu, Yi-Lun and Chen, Peng-Hsi and Chen, Bing-Yu},\ntitle = {3CPEs: Concrete Computational Concepts Programming Environments for Elementary Computer Science Education},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650833},\ndoi = {10.1145/3613905.3650833},\nabstract = {This study introduces the Concrete Computational Concepts Programming Environments (3CPEs), an innovative approach designed to help elementary students grasp abstract computational concepts through concrete metaphors using tangible and everyday objects. Traditional programming environments for children focus on concrete programming language syntax and semantics yet often fall short in conveying advanced computational concepts. To address this gap, 3CPEs combine the hands-on appeal of computer-science unplugged activities with the versatility of programming environments, using familiar objects and gamified elements to visualize abstract computational and algorithmic ideas. An example is CT (Computational Thinking) Chef, a 3CPEs implementation to enhance elementary students’ understanding of programming concepts. Through design workshops with five elementary programming teachers, we preliminarily identified teaching challenges and evaluated 3CPEs’ efficacy as an educational tool. The feedback underscores CT Chef’s potential in facilitating the teaching of intricate computational concepts for young learners.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {13},\nnumpages = {7},\nkeywords = {Computational Thinking, Concrete Metaphor Design, Elementary Education, Programming Environment},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650938,\nauthor = {Schmitz, Martin and Sch\\\"{o}n, Dominik and Klagemann, Henning and Kosch, Thomas},\ntitle = {3DA: Assessing 3D-Printed Electrodes for Measuring Electrodermal Activity},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650938},\ndoi = {10.1145/3613905.3650938},\nabstract = {Electrodermal activity (EDA) reflects changes in skin conductance, which are closely tied to human psychophysiological states. For example, EDA sensors can assess stress, cognitive workload, arousal, or other measures tied to the sympathetic nervous system for interactive human-centered applications. Yet, current limitations involve the complex attachment and proper skin contact with EDA sensors. This paper explores the concept of 3D printing electrodes for EDA measurements, integrating sensors into arbitrary 3D-printed objects, alleviating the need for complex assembly and attachment. We examine the adaptation of conventional EDA circuits for 3D-printed electrodes, assessing different electrode shapes and their impact on the sensing accuracy. A user study (N=6) revealed that 3D-printed electrodes can measure EDA with similar accuracy, suggesting larger contact areas for improved precision. We derive design implications to facilitate the integration of EDA sensors into 3D-printed devices to foster diverse integration into everyday objects for prototyping physiological interfaces.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {14},\nnumpages = {7},\nkeywords = {3D Printing, Electrodermal Activity, Physiological Sensing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3652036,\nauthor = {Gupta, Naman and Das, Sanchari and Walsh, Kate and Chatterjee, Rahul},\ntitle = {A Critical Analysis of the Prevalence of Technology-Facilitated Abuse in US College Students},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3652036},\ndoi = {10.1145/3613905.3652036},\nabstract = {The ubiquitous use of technology by college students makes them vulnerable to harassment, harm, and intimidation via technological means. We evaluate the prevalence of such technology-facilitated abuse (TFA) among college students in the USA using a critical, feminist, and trauma-informed lens, which is essential to inform policymakers and advocates who support students. We surveyed 776 college students in a large R1 university located in the Midwest region of the USA to examine the prevalence of TFA faced by students marginalized by socio-economic factors, the support sought by student survivors, and the efficacy of support structures. Our findings indicate that <Formula format=\"inline\"><TexMath><?TeX $70\\%$?></TexMath><AltText>Math 1</AltText><File name=\"chiea24-663-inline1\" type=\"svg\"/></Formula> students experience TFA, but more than half of them do not seek support. Among the survivors who seek support, <Formula format=\"inline\"><TexMath><?TeX $93\\%$?></TexMath><AltText>Math 2</AltText><File name=\"chiea24-663-inline2\" type=\"svg\"/></Formula> students solely rely on informal resources like friends and family, and <Formula format=\"inline\"><TexMath><?TeX $6\\%$?></TexMath><AltText>Math 3</AltText><File name=\"chiea24-663-inline3\" type=\"svg\"/></Formula> solely seek support from formal networks such as survivor services or law enforcement. Therefore, we call on policymakers to direct attention to TFA, create tailored interventions to support marginalized students and propose campus-wide campaigns to spread awareness among college students.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {15},\nnumpages = {12},\nkeywords = {college populations, computer security and privacy, critical, critical race theory, domestic violence, gender-based violence, intimate partner violence, technology abuse, technology-facilitated abuse, transgender, trauma, trauma-informed computing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651012,\nauthor = {Faklaris, Cori and Dabbish, Laura and Hong, Jason I.},\ntitle = {A Framework for Reasoning about Social Influences on Security and Privacy Adoption},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651012},\ndoi = {10.1145/3613905.3651012},\nabstract = {Much research has found that social influences (such as social proof, storytelling, and advice-seeking) help boost security awareness. But we have lacked a systematic approach to tracing how awareness leads to action, and to identifying which social influences can be leveraged at each step. Toward this goal, we develop a framework that synthesizes our design ideation, expertise, prior work, and new interview data into a six-step adoption process. This work contributes a prototype framework that accounts for social influences by step. It adds to what is known in the literature and the SIGCHI community about the social-psychological drivers of security adoption. Future work should establish whether this process is the same regardless of culture, demographic variation, or work vs. home context, and whether it is a reliable theoretical basis and method for designing experiments and focusing efforts where they are likely to be most productive.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {16},\nnumpages = {13},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650836,\nauthor = {He, Kunlei and Levine, Julian and Cervera, Kelsyann and Ojeda-Ramirez, Santiago and Xu, Ying and Warschauer, Mark},\ntitle = {A Home Study of Parent-Child Co-Reading with a Bilingual Conversational Agent},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650836},\ndoi = {10.1145/3613905.3650836},\nabstract = {Conversational agents (CAs) are increasingly prevalent in children’s lives, serving as educational companions, particularly in shared reading activities. While effective for monolingual children’s learning, there exists a gap in meeting the unique needs of the rapidly expanding bilingual child population, who face dual challenges of school readiness and heritage language maintenance. Moreover, most current CAs, designed for one-to-one interactions with children, neglect the importance of parents’ active participation in shared reading. Our study introduces the development and home deployment of a bilingual CA, integrated within e-books, designed to foster parent-child joint engagement in shared reading, thereby promoting children’s bilingual language development. Results of the study indicated high levels of family engagement in co-reading activities over an extended period, with observable language learning gains in children. This study provides valuable design implications for designing effective and engaging CAs for bilingual families.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {17},\nnumpages = {8},\nkeywords = {Conversational agents, bilingualism, home deployment, parent-child interactions, shared reading},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650915,\nauthor = {Dublin, Tamar and Ezer, Talia Sofia and Zuckerman, Oren},\ntitle = {A Journey Inward: The Somaesthetic Experience of a Heated Walking Carpet},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650915},\ndoi = {10.1145/3613905.3650915},\nabstract = {We present a novel walking carpet with integrated heat elements, designed to promote introspection while walking. The three-meters carpet has 10 heater pads spread in a step-by-step pattern, allowing participants to explore the heat sensation using their feet. In an iterative design process, inspired by the somaesthetic appreciation design approach, we created a two-layer fabric-based carpet with heat elements and embroidered visual signifiers. The experience was evaluated in an exploratory qualitative study with 10 participants, who walked on the carpet back and forth without shoes. The thematic analysis of post-study interviews revealed four themes: exploration of the physical body, feelings evoked by the warmth sensation on the feet, contemplation evoked by the warmth sensation, and impact on attention. Our work suggests that coupling the ancient wisdom of walking as a contemplative practice with heat elements produces a novel interactive experience that promotes introspection of physical, mental, and cultural processes.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {18},\nnumpages = {7},\nkeywords = {Introspection, Mind-Body Connection, Mindful walking, Somaesthetic design, Somatic Awareness, Targeted Warmth, Well-being},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3652007,\nauthor = {Abe, Yuta and Iriyama, Taishi and Komuro, Takashi and Shimasaki, Kohei and Ishii, Idaku},\ntitle = {A Mid-air Multi-touch Interface using an Ultrafast Pan-tilt Camera},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3652007},\ndoi = {10.1145/3613905.3652007},\nabstract = {We propose an interface that enables bare hand interaction with fingers of both hands even from a distance, using an ultrafast pan-tilt camera and a wide view camera. In this interface, a user can perform multi-touch interaction with a large display using mid-air finger motions. In order to acquire accurate finger motions required for multi-touch interaction, a wide view camera is used to detect the positions of the user’s hands, and an ultrafast pan-tilt camera is utilized to obtain the images around the hands with high spatial resolution. In the experiments, we evaluated the accuracy of acquired finger motions, and showed that the proposed interface can capture both large motions of the user’s whole body and small motions of the fingers with high accuracy. We also implemented two example applications, a drawing application and a photo layout application, using the proposed interface.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {19},\nnumpages = {6},\nkeywords = {active sensing, bare hand interaction, hand gesture recognition},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651057,\nauthor = {Lyu, Yao and Zhang, He and Niu, Shuo and Cai, Jie},\ntitle = {A Preliminary Exploration of YouTubers' Use of Generative-AI in Content Creation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651057},\ndoi = {10.1145/3613905.3651057},\nabstract = {Content creators increasingly utilize generative artificial intelligence (Gen-AI) on platforms such as YouTube, TikTok, Instagram, and various blogging sites to produce imaginative images, AI-generated videos, and articles using Large Language Models (LLMs). Despite its growing popularity, there remains an underexplored area concerning the specific domains where AI-generated content is being applied, and the methodologies content creators employ with Gen-AI tools during the creation process. This study initially explores this emerging area through a qualitative analysis of 68 YouTube videos demonstrating Gen-AI usage. Our research focuses on identifying the content domains, the variety of tools used, the activities performed, and the nature of the final products generated by Gen-AI in the context of user-generated content.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {20},\nnumpages = {7},\nkeywords = {Affiliated Marketing, Artificial Intelligence, Content Creation, Content Creator, Generative AI, Professional Development, User-generated Content, YouTube},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651078,\nauthor = {Ruocco, Martina and Jay, Caroline and Parsia, Bijan},\ntitle = {A Qualitative Study of Human Theorizing about Robot Bodily Behavior},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651078},\ndoi = {10.1145/3613905.3651078},\nabstract = {This research investigates the cognitive frameworks individuals use to understand and justify robot behaviour. Participants watched a robot learning how to manipulate objects through unguided experiential learning. Across multiple rounds of try-and-fail, participants were asked to predict the robot’s performance with unfamiliar objects and speculate on its cognitive operations, including reasoning, sensing, and actuation. To assess the potential impact of performance on theorization, participants were divided into two groups: one exposed to a predominantly successful robot, and the other to a predominantly failing robot. The findings reveal intriguing patterns: participants, regardless of their condition, consistently engaged in theorizing about the robot in similar quantities and ways. Those who observed the robot failing more frequently often subsequently predicted success, perhaps due to a desire to offer the robot “another chance”, or because probabilistically they felt success was more likely following several failures.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {21},\nnumpages = {11},\nkeywords = {human-robot interaction, theory of mind, trust},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651034,\nauthor = {De, Ankolika},\ntitle = {A Situated-Infrastructuring of WhatsApp for Business in India},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651034},\ndoi = {10.1145/3613905.3651034},\nabstract = {WhatsApp has become a pivotal communication tool in India, transcending cultural boundaries and deeply integrating into the nation’s digital landscape. Meta’s introduction of WhatsApp for Business aligns seamlessly with the platform’s popularity, offering businesses a crucial tool. However, the monetization plans pose challenges, particularly for smaller businesses, in balancing revenue goals with accessibility. This study, employing discourse analysis, examines Meta’s infrastructuring of WhatsApp in India, emphasizing the dynamic interplay of technological, social, and cultural dimensions. Consequently, it highlights potential power differences caused by the deployment of WhatsApp for Business followed by its gradual but significant modifications, encouraging scholars to investigate the implications and ethics of rapid technological changes, particularly for marginalized users.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {22},\nnumpages = {7},\nkeywords = {Infrastructure, communication, decoloniality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651110,\nauthor = {Limke, Ally and Hill, Marnie and Catet\\'{e}, Veronica and Barnes, Tiffany},\ntitle = {A Survey of K-12 Teacher Needs for an Online Programming Learning System},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651110},\ndoi = {10.1145/3613905.3651110},\nabstract = {This article examines US K-12 computing teacher needs for a programming learning system. We surveyed 39 K-12 teachers about the necessity of programming learning system features. We found that teachers needed to view student code remotely, student code auto-save, differentiation of student assignments, and a tutorial library for students to learn about the programming environment. In addition to rating feature usefulness, we also asked teachers to list features or needs that the survey did not address. Through qualitative responses, we found that teachers wanted cheating prevention and detection, the ability to freeze and project code onto student screens, and student and classroom-level analytics. We also compare the needs of teachers who teach computing as the main subject in their classroom to the needs of teachers who integrate computing into another discipline. This research can inform the development of programming learning systems to better support teachers and their students.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {23},\nnumpages = {7},\nkeywords = {K-12, K-12 teachers, Online learning environments, Programming learning, learning systems},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650786,\nauthor = {Gao, Jie and Gebreegziabher, Simret Araya and Choo, Kenny Tsu Wei and Li, Toby Jia-Jun and Perrault, Simon Tangi and Malone, Thomas W},\ntitle = {A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650786},\ndoi = {10.1145/3613905.3650786},\nabstract = {With ChatGPT’s release, conversational prompting has become the most popular form of human-LLM interaction. However, its effectiveness is limited for more complex tasks involving reasoning, creativity, and iteration. Through a systematic analysis of HCI papers published since 2021, we identified four key phases in the human-LLM interaction flow—planning, facilitating, iterating, and testing—to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard Prompting, Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the “5W1H” guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and LLM abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {24},\nnumpages = {11},\nkeywords = {Human-LLM Interaction, Large Language Models, Taxonomy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650929,\nauthor = {Moruzzi, Caterina and Margarido, Solange},\ntitle = {A User-centered Framework for Human-AI Co-creativity},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650929},\ndoi = {10.1145/3613905.3650929},\nabstract = {The pervasive adoption and use of Generative AI tools have prompted a discourse on determining the optimal balance between automation and human agency in processes executed by users with the assistance of this technology. This paper presents a user-centered co-creativity framework that identifies key dimensions responsible for the modulation of agency and control between users and AI in co-creative processes. It also suggests an actionable way of implementing the framework in a customization tool, in the form of a mobile application, which can be used to tailor the interaction between users and the AI system following the dimensions proposed in the framework. The paper contributes new insights within the literature on Computational Creativity and Human-Computer Interaction interested in the investigation of modalities of co-creation between humans and AI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {25},\nnumpages = {9},\nkeywords = {Co-creativity, Customization tool, Framework, Game design, Mixed-initiative},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650730,\nauthor = {Chheang, Vuthea and Weston, Brian Thomas and Cerda, Robert William and Au, Brian and Giera, Brian and Bremer, Peer-Timo and Miao, Haichao},\ntitle = {A Virtual Environment for Collaborative Inspection in Additive Manufacturing},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650730},\ndoi = {10.1145/3613905.3650730},\nabstract = {Additive manufacturing (AM) techniques have been used to enhance the design and fabrication of complex components for various applications in the medical, aerospace, energy, and consumer products industries. A defining feature for many AM parts is the complex internal geometry enabled by the printing process. However, inspecting these internal structures requires volumetric imaging, i.e., X-ray CT, leading to the well-known challenge of visualizing complex 3D geometries using 2D desktop interfaces. Furthermore, existing tools are limited to single-user systems making it difficult to jointly discuss or share findings with a larger team, i.e., the designers, manufacturing experts, and evaluation team. In this work, we present a collaborative virtual reality (VR) for the exploration and inspection of AM parts. Geographically separated experts can virtually inspect and jointly discuss data. It also supports VR and non-VR users, who can be spectators in the VR environment. Various features for data exploration and inspection are developed and enhanced via real-time synchronization. We followed usability and interface verification guidelines using Nielsen’s heuristics approach. Furthermore, we conducted exploratory and semi-structured interviews with domain experts to collect qualitative feedback. Results reveal potential benefits, applicability, and current limitations. The proposed collaborative VR environment provides a new basis and opens new research directions for virtual inspection and team collaboration in AM settings.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {26},\nnumpages = {7},\nkeywords = {Additive Manufacturing, Collaborative VR, Digital Twins, Virtual Inspection, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650803,\nauthor = {Ahmad, Mak and Macvean, Andrew and Karger, David R and Ma, Kwan-Liu},\ntitle = {AI-Enhanced API Design: A New Paradigm in Usability and Efficiency},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650803},\ndoi = {10.1145/3613905.3650803},\nabstract = {This study uses mixed methods to evaluate API design methods, focusing on design and consumption phases. Our goal was to understand the impact of API governance approaches on productivity and usability. A controlled developer experiment (n=34) demonstrated a 10\\% increased requirement fulfillment using API Improvement Proposals (AIPs) and linter versus no protocols. Meanwhile, 73\\% of 33 surveyed API consumers preferred AIP-aligned designs for enhanced usability and comprehensibility. Complementing this, a custom large language model called the API Architect received average expert ratings of just 5/10 for specification quality, revealing gaps versus manual design. The quantitative performance metrics combined with qualitative user feedback provide evidence from multiple angles that strategically integrating industry best practices with maturing AI capabilities can meaningfully improve API design outcomes. This research offers empirical insights from developer and consumer perspectives to advance scholarly discourse and industry practice regarding optimal API design workflows.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {27},\nnumpages = {6},\nkeywords = {API Design, API Usability, Design Reviews, LLM},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650861,\nauthor = {Dunnell, Kevin and Agarwal, Gauri and Pataranutaporn, Pat and Lippman, Andrew and Maes, Pattie},\ntitle = {AI-Generated Media for Exploring Alternate Realities},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650861},\ndoi = {10.1145/3613905.3650861},\nabstract = {This research investigates the potential of AI-generated media in enabling users to create and engage with alternate versions of reality. Drawing inspiration from the speculative design approaches, we propose leveraging modern AI techniques for the procedural generation of text, audio, and video to construct interactive possible futures. As a proof of concept, we developed \"OpenOpenAI,\" a web platform that harnesses AI to depict varying renditions of a hypothetical 2024 keynote address by Sam Altman, CEO of OpenAI, based on user input. Although the platform may not influence the actual direction of the keynote address by Sam Altman and the direction of OpenAI, the system encourages participants to explore and imagine other ways that AI development could go and reminds them of alternate choices and values they could advocate for. Through a pilot user study, we seek to answer two research questions: 1) How might AI-generated media help users expand their perceived range of possible futures? and 2) How might a tool for simulating alternate realities be used to better understand the general public’s opinion on the explored topic? The findings of this study contribute to the growing body of knowledge on the responsible use of AI for exploring speculative futures and understanding public opinion on critical issues such as the development of AI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {28},\nnumpages = {8},\nkeywords = {AI, Alternate Reality, Generative AI, Human-Computer Interaction, LLM},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651091,\nauthor = {Mehta, Pratham Darrpan and Karanth, Harsha and Yang, Haoyang and Slesnick, Timothy C and Shaw, Fawwaz and Chau, Duen Horng},\ntitle = {ARCollab: Towards Multi-User Interactive Cardiovascular Surgical Planning in Mobile Augmented Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651091},\ndoi = {10.1145/3613905.3651091},\nabstract = {Surgical planning for congenital heart diseases requires a collaborative approach, traditionally involving the 3D-printing of physical heart models for inspection by surgeons and cardiologists. Recent advancements in mobile augmented reality (AR) technologies have offered a promising alternative, noted for their ease-of-use and portability. Despite this progress, there remains a gap in research exploring the use of multi-user mobile AR environments for facilitating collaborative cardiovascular surgical planning. We are developing ARCollab, an iOS AR application designed to allow multiple surgeons and cardiologists to interact with patient-specific 3D heart models in a shared environment. ARCollab allows surgeons and cardiologists to import heart models, perform gestures to manipulate the heart, and collaborate with other users without having to produce a physical heart model. We are excited by the potential for ARCollab to make long-term real-world impact, thanks to the ubiquity of iOS devices that will allow for ARCollab ’s easy distribution, deployment and adoption.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {29},\nnumpages = {6},\nkeywords = {Augmented Reality, Mobile Collaboration, Surgical Planning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650760,\nauthor = {Hoang, Trong-Vu and Nguyen, Quang-Binh and Ly, Duy-Nam and Le, Khanh-Duy and Nguyen, Tam and Tran, Minh-Triet and Le, Trung-Nghia},\ntitle = {ARtVista: Gateway To Empower Anyone Into Artist},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650760},\ndoi = {10.1145/3613905.3650760},\nabstract = {Drawing is an art that enables people to express their imagination and emotions. However, individuals usually face challenges in drawing, especially when translating conceptual ideas into visually coherent representations and bridging the gap between mental visualization and practical execution. In response, we propose ARtVista - a novel system integrating AR and generative AI technologies. ARtVista not only recommends reference images aligned with users’ abstract ideas and generates sketches for users to draw but also goes beyond, crafting vibrant paintings in various painting styles. ARtVista also offers users an alternative approach to create striking paintings by simulating the paint-by-number concept on reference images, empowering users to create visually stunning artwork devoid of the necessity for advanced drawing skills. We perform a pilot study and reveal positive feedback on its usability, emphasizing its effectiveness in visualizing user ideas and aiding the painting process to achieve stunning pictures without requiring advanced drawing skills. The source code will be available at https://github.com/htrvu/ARtVista.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {30},\nnumpages = {8},\nkeywords = {AI-generated visual art, AR Painting, Text-to-image},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650935,\nauthor = {Naikar, Vinaya Hanumant and Subramanian, Shwetha and Tigwell, Garreth W.},\ntitle = {Accessibility Feature Implementation Within Free VR Experiences},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650935},\ndoi = {10.1145/3613905.3650935},\nabstract = {Virtual reality (VR) enables many exciting, immersive experiences; however, those experiences are sometimes inaccessible. Researchers have explored methods to improve VR accessibility, but it is also important to ensure that we have a clear understanding of the prevalence of current accessibility feature implementation to identify what support is less common for different impairments (e.g., vision, motor, hearing). In our small-scale study, we focused on free VR experiences since everybody can install those apps and games without any cost. We inspected 106 free VR experiences available for the Meta Quest 2 so that we could determine if relevant accessibility features were available for the modes of interaction present, as well as the overall prevalence of accessibility features across the sample of VR experiences. We found that 36.8\\% of our sample did not include any accessibility features, and many other VR experiences could have benefited from additional accessibility features. We make recommendations that can help to ensure future VR experiences are implemented with more accessibility in mind.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {31},\nnumpages = {9},\nkeywords = {Accessible design, free software, virtual reality.},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651059,\nauthor = {Lu, Feiyu and Chen, Mengyu and Hsu, Hsiang and Deshpande, Pranav and Wang, Cheng Yao and MacIntyre, Blair},\ntitle = {Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651059},\ndoi = {10.1145/3613905.3651059},\nabstract = {Mixed Reality (MR) could assist users’ tasks by continuously integrating virtual content with their view of the physical environment. However, where and how to place these content to best support the users has been a challenging problem due to the dynamic nature of MR experiences. In contrast to prior work that investigates optimization-based methods, we are exploring how reinforcement learning (RL) could assist with continuous 3D content placement that is aware of users’ poses and their surrounding environments. Through an initial exploration and preliminary evaluation, our results demonstrate the potential of RL to position content that maximizes the reward for users on the go. We further identify future directions for research that could harness the power of RL for personalized and optimized UI and content placement in MR.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {32},\nnumpages = {7},\nkeywords = {Mixed reality, adaptive user interface, mobile scenarios, reinforcement learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650942,\nauthor = {Rebelo, Ana Rita and Ferreira, Pedro A. and N\\'{o}brega, Rui},\ntitle = {Adaptive Virtual Environments in Small Physical Spaces: Navigation Design for Customized User Experiences in Virtual Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650942},\ndoi = {10.1145/3613905.3650942},\nabstract = {In Virtual Reality (VR), navigating within small physical spaces often relies on handheld controller methods such as teleportation and joystick movements. While effective, these methods diverge from natural movement. This paper introduces the development of a VR tool designed to enhance immersion by customizing virtual environments for small physical spaces. By employing non-Euclidean and impossible spaces strategies, our solution enables overlapping virtual environments to fit the user’s physical constraints, allowing exploration of the space through walking. This tool can benefit different areas of application, such as virtual visits to museums, training simulations and emergency scenarios. As a use case in this paper, we address virtual apartment tours. This study demonstrates the potential for more user-centric and accessible VR solutions, contributing to delivering more meaningful experiences in daily and professional settings. To assess the viability and potential applications of our approach, we conducted semi-structured interviews with 10 Human-Computer Interaction (HCI) researchers. These interviews yielded insights into practical applications, identified usability challenges, and suggested strategies to enhance user experience.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {33},\nnumpages = {7},\nkeywords = {Impossible spaces, Navigation, Non-Euclidean spaces, Procedural generation, Virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650969,\nauthor = {Saha, Anik and Rahman, Naimur and Ahmed, Nova},\ntitle = {Addressing the Technology Learning Divide Using Co-Learning with Familiarization Method},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650969},\ndoi = {10.1145/3613905.3650969},\nabstract = {Empowerment of low-literate women and marginalized communities through learning technology is emphasized by researchers. Co-learning techniques using familiarization methods can effectively help low-tech literate people learn technology, which is underexplored. This pilot study was conducted in two phases. Phase-I explored technology’s learning and adaptation strategies with a qualitative study, where the researcher considered N=36 female participants in urban and rural regions of 3 major divisions of Bangladesh. Phase-II was conducted N=16 (Parent=8, Children=8) participants in Dhaka division urban and rural settings with a series of workshops to understand the impact, opportunities, and challenges of co-learning and familiarization methods in terms of learning technology. Findings show that technology co-learning with familiarization methods greatly aids low-literate women in adopting and pursuing technology rapidly. The findings are essential to provide insights to the CHI, and CSCW community regarding opinions on technology learning and adaptation scenarios of low-tech literate women in developing countries perspectives.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {34},\nnumpages = {8},\nkeywords = {Co-learning, Familiarization, Marginal Community, Tech Adoption, Technology Learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650802,\nauthor = {Maehigashi, Akihiro and Fukuchi, Yosuke and Yamada, Seiji},\ntitle = {Adjusting Amount of AI Explanation for Visual Tasks},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650802},\ndoi = {10.1145/3613905.3650802},\nabstract = {Explainable AI (XAI) has been designed to render AI comprehensible to humans by providing explanations of its processes. However, an excessive amount of explanation can lead to cognitive overload in users and the development of inappropriate trust in AI. To explore the appropriate amount of explanation, this study investigated the effects of explanation type (AI attention heatmap, AI goal, and AI reliability) on trust in XAI for a traditional visual search task in Experiment 1, and the effects of presenting adjusted explanations on task performance for an applied task in Experiment 2. As a result, displaying AI results alone increased trust and task performance for a low-complexity task, and displaying AI results with AI attention heatmaps that have high interpretability increased trust and task performance for a high-complexity task. This study demonstrated the importance of adjusting the amount of AI explanation to develop trust appropriately and improve task performance.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {35},\nnumpages = {7},\nkeywords = {AI attention heatmap, AI goal, AI reliability, Explanation, Interpretability, Trust, Visual identification, Visual search, XAI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650895,\nauthor = {Jang, Yugyeong and Hyun, Kyung Hoon},\ntitle = {Advancing 3D CAD with Workflow Graph-Driven Bayesian Command Inferences},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650895},\ndoi = {10.1145/3613905.3650895},\nabstract = {Advancements in 3D generative AI have significantly improved design capabilities, particularly for creating 3D objects and environments. However, the focus of Generative AI on mesh-based models limits opportunities for detailed modifications. Accurate and complex 3D modeling is crucial for manufacturing, which requires high precision and considerable mental effort. This complexity often leads to variability in efficiency among designers, with some employing faster and more accurate techniques and others using less efficient workflows. This variation undergoes the need to optimize modeling sequences. By inferring a user’s intended designs, tailored commands and sequences can be suggested to enhance the precision of 3D modeling. Addressing this, we propose a system that predicts user modeling steps using an inference model based on behavior, thereby promoting efficient workflow and precise command usage. User studies demonstrate that our system minimizes modeling errors, streamlines processes, and offers recommendations for effective command usage.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {36},\nnumpages = {6},\nkeywords = {3D Modeling Workflow, Bayesian Information Gain, Computational Design, Computer-Aided Design, Design Command Inference},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650868,\nauthor = {Cai, Zhenyao and Park, Seehee and Nixon, Nia and Doroudi, Shayan},\ntitle = {Advancing Knowledge Together: Integrating Large Language Model-based Conversational AI in Small Group Collaborative Learning},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650868},\ndoi = {10.1145/3613905.3650868},\nabstract = {In today’s educational landscape, students learn collaboratively, where students benefit from both peer interactions and facilitator guidance. Prior research in Human-Computer Interaction (HCI) and Computer-Supported Collaborative Learning (CSCL) has explored chatbots and AI techniques to aid such collaboration. However, these methods often depend on predefined dialogues (which limits adaptability), are not based on collaborative learning theories, and do not fully recognize the learning context. In this paper, we introduce an Large Language Model (LLM)-powered conversational AI, designed to enhance small group learning through its advanced language understanding and generation capabilities. We detail the iterative design process, final design, and implementation. Our preliminary evaluation indicates that the bot performs as designed but points to considerations in the timing of interventions and bot’s role in discussions. The evaluation also reveals that learners perceive the bot’s tone and behavior as important for engagement. We discuss design implications for chatbot integration in collaborative learning and future research directions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {37},\nnumpages = {9},\nkeywords = {AI facilitator, Collaborative Learning, Human-AI Collaboration},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650741,\nauthor = {Hutchinson, Adryana and Munyendo, Collins W. and Aviv, Adam J and Mayer, Peter},\ntitle = {An Analysis of Password Managers’ Password Checkup Tools},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650741},\ndoi = {10.1145/3613905.3650741},\nabstract = {Password managers (PMs) have been widely recommended to users to generate and store random, secure, and unique passwords across websites. Using a PM is often not enough however, especially if users store passwords that are guessable, or have been breached. To assist users in updating insecure passwords, PMs come with “checkup\" features that report the strength of users’ passwords. However, there has yet to be a systematic study of the features offered as part of these checkups, and the consistency of the checkup advice across different PMs. In this paper, we conduct a preliminary analysis of 14 PMs’ password checkup features, recording how many passwords are reported weak and compromised. We find that many PMs fail to report breached credentials. Weak passwords were also under-reported by PMs. This analysis forms the basis for a larger study on the consistencies of PM checkup tools and how users perceive and use them.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {38},\nnumpages = {7},\nkeywords = {authentication, password managers},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650854,\nauthor = {Aubin Le Qu\\'{e}r\\'{e}, Marianne and Tran, Lauren and Berger, Andrew and Nguyen, Christopher and Venkatesan, Rohini and Xu, Yuanhang and Sorokina, Anastasia and Naaman, Mor},\ntitle = {An Article a Day: Towards Personal Informatics for Healthy News Consumption},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650854},\ndoi = {10.1145/3613905.3650854},\nabstract = {Technological logs can enable us to self-reflect on our habits through data. Avid news consumers, who read many articles and take an interest in news media, may have a pronounced interest in tracking their news habits. From a lens of personal informatics, we asked twelve participants about their goals when consuming news. We visualized participant news consumption data in a dashboard to elicit self-reflections and desired actions. We find that avid news consumers are motivated by a desire to feel informed without sacrificing their mental health, dual goals which a news consumption self-tracking tool may help to achieve. However, misalignment between perceived news habits and trace data may lead to negative ruminations for users if their logged data reveals they read less serious news, less often, or less in-depth than their preference. We discuss the implications of these findings for the future development of personal informatics news consumption tools.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {39},\nnumpages = {9},\nkeywords = {elicitation study, news, personal informatics, quantified self, self-tracking, wellbeing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651096,\nauthor = {Lo, Priscilla Y.},\ntitle = {An Autoethnographic Reflection of Prompting a Custom GPT Based on Oneself},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651096},\ndoi = {10.1145/3613905.3651096},\nabstract = {What if you could have a chat with yourself? OpenAI’s introduction of custom GPTs in November 2023 provides an opportunity for non-technical users to create specialized generative artificial intelligence chatbots. Users can write prompts in plain language rather than code to instruct how the system should behave. What can one learn from using non-technical methods to develop a specific chatbot persona? To explore this, I conducted an autoethnography of my experience developing and interacting with a custom GPT based on myself. My findings include a discussion of my experiences throughout the process, and its impact on my personal introspection and understanding of prompt engineering. I summarize first-hand challenges and insights intended to inspire further discussion on the topic of generative AI and chatbots.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {40},\nnumpages = {9},\nkeywords = {autoethnography, chatbot, generative artificial intelligence, large language model, persona},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650903,\nauthor = {Cashman, Thomas Joseph and Hutton, Tim and de La Gorce, Martin and Tak\\'{a}cs, Tibor and Criminisi, Antonio and undefinedor\\dj{}evi\\'{c}, Milica and Dubaji\\'{c}, Goran and Marjanovi\\'{c}, undefinedor\\dj{}e and Oko\\v{s}anovi\\'{c}, Milena and Rankovi\\'{c}, Vuka\\v{s}in and Razumeni\\'{c}, Ivan and Ro\\v{s}ko, Bojan and \\v{S}arki\\'{c}, Teo and Skakun, Marko and Stojanovi\\'{c}, Milo\\v{s} and Veli\\v{c}kovi\\'{c}, Nikola and Jovanovi\\'{c}, Predrag and Panda, Payod and Tankelevitch, Lev and Rintel, Sean},\ntitle = {An Equal Seat at the Table: Exploring Videoconferencing with Shared Spatial Context combined with 3D Video Representations},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650903},\ndoi = {10.1145/3613905.3650903},\nabstract = {Work video meetings in the traditional grid interface have inclusion, effectiveness, and fatigue problems, due in part to the difficulty of directing or communicating attention. Virtual 3D meeting spaces have value, but representing people in them is a challenge. Avatars face resistance, and 2D video is limited to near-frontal views, constraining the spatial layout. We present a novel experimental system for virtual meeting rooms that predicts 3D video of users in real-time from a standard webcam, positions them in a shared 3D space, and renders a controllable first-person view. We report study results comparing this system to a traditional grid, and to 2D video of people in the same 3D space. While spatial layouts fared better in terms of attention and co-presence, the traditional grid was more comfortable and professional. This is likely due to unsettled 3D design, the need for manual control, and a preference for the familiar.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {41},\nnumpages = {9},\nkeywords = {inclusion, monocular depth estimation, social presence, spatiality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650763,\nauthor = {Gmeiner, Frederic and Conlin, Jamie Lynn and Tang, Eric Handa and Martelaro, Nikolas and Holstein, Kenneth},\ntitle = {An Evidence-based Workflow for Studying and Designing Learning Supports for Human-AI Co-creation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650763},\ndoi = {10.1145/3613905.3650763},\nabstract = {Generative artificial intelligence (GenAI) systems introduce new possibilities for enhancing professionals’ workflows, enabling novel forms of human–AI co-creation. However, professionals often struggle to learn to work with GenAI systems effectively. While research has begun to explore the design of interfaces that support users in learning to co-create with GenAI, we lack systematic approaches to investigate the effectiveness of these supports. In this paper, we present a systematic approach for studying how to support learning to co-create with GenAI systems, informed by methods and concepts from the learning sciences. Through an experimental case study, we demonstrate how our approach can be used to study and compare the impacts of different types of learning supports in the context of text-to-image GenAI models. Reflecting on these results, we discuss directions for future work aimed at improving interfaces for human–AI co-creation.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {42},\nnumpages = {15},\nkeywords = {Case Study, Generative AI, Human–AI Co-creation, Human–AI Interaction, Learning, Study Method, Support Interfaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650899,\nauthor = {Chan, Laveda and Showkat, Dilruba and To, Alexandra},\ntitle = {An Exploration of Learned Values Through Lived Experiences to Design for BIPOC Students’ Flourishing},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650899},\ndoi = {10.1145/3613905.3650899},\nabstract = {Prior research has primarily focused on the negative experiences faced by BIPOC students and sought to identify ways to counter harms. In contrast, our work seeks to characterize and support the positive, valuable, and meaningful experiences of BIPOC to support their holistic thriving at predominantly white institutions (PWIs). Experiences where students can share their stories and engage with racial identity development, are paramount to their flourishing. However, these stories and connections are not widely accessible. In this work, we gathered stories of meaningful experiences by conducting a qualitative semi-structured interview with 17 BIPOC students from diverse races and ethnic backgrounds, all studying across various PWIs. Our preliminary findings revealed that students derive meaningful experiences by engaging in activities that lead to the cultivation of learned values and personal growth. We discuss and situate our findings within a positive design framework to support BIPOC students’ sustained well-being and flourishing.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {43},\nnumpages = {7},\nkeywords = {BIPOC, CHI, CSCW, Predominantly White Institution (PWI), meaningful experiences},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650981,\nauthor = {Lim, Chungman and Seifi, Hasti and Park, Gunhyuk},\ntitle = {An Interactive Tool for Simulating Mid-Air Ultrasound Tactons on the Skin},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650981},\ndoi = {10.1145/3613905.3650981},\nabstract = {Mid-air ultrasound haptic technology offers a myriad of temporal and spatial parameters for contactless haptic design. Yet, predicting how these parameters interact to render an ultrasound signal is difficult before testing them on a mid-air ultrasound haptic device. Thus, haptic designers often use a trial-and-error process with different parameter combinations to obtain desired tactile patterns (i.e., Tactons) for user applications. We propose an interactive tool with five temporal and three spatiotemporal design parameters that can simulate the temporal and spectral properties of stimulation at specific skin points. As a preliminary verification, we measured vibrations induced from the ultrasound Tactons varying on one temporal and two spatiotemporal parameters. The measurements and simulation showed similar results for three different ultrasound rendering techniques, suggesting the efficacy of the simulation tool. We present key insights from the simulation and discuss future directions for enhancing the capabilities of simulations.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {44},\nnumpages = {7},\nkeywords = {Computational Simulation, Mid-Air Haptics, Ultrasound Tacton Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650866,\nauthor = {Onishi, Yuki and Onishi, Eiko and Takashima, Kazuki and Kitamura, Yoshifumi},\ntitle = {Anesth-on-the-Go: Designing Portable Game-based Anesthetic Simulator for Education},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650866},\ndoi = {10.1145/3613905.3650866},\nabstract = {Gaining extensive practice in anesthesia procedures is critical for medical students. However, the current high-fidelity simulators for clinical anesthesia practice limit such vital opportunities due to their professional-level functions, large size, and high installation costs. In this paper, we propose Anesth-on-the-Go, a portable game-based anesthesia simulator that allows medical students to repeatedly practice clinical anesthesia procedures anywhere, anytime on a conventional personal computer. The proposed simulator is designed for medical students, and it allows them to manipulate controls appropriately for various anesthesia procedures according to pre-determined surgical scenarios. Based on iterative interviews with clinical anesthesia instructors, we designed a simulation interface with four key components: 1) a monitor, 2) a decision-making board, 3) a surgical field view, and 4) a communication view. As our initial evaluation, we introduced the simulator in the training curriculum for medical students and verified its effect on their proficiency level in subsequent training with high-fidelity simulation. The results show that individual and prior training with our simulator improved each student’s overall performance (i.e., score) and stimulated discussion among teammates during the subsequent formal training. The evaluation’s findings also indicate that the simulator experience can facilitate post-training reflection.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {45},\nnumpages = {7},\nkeywords = {Anesthesiology, graphical interface, surgical simulation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650962,\nauthor = {Li, Jingya and Zuo, Tengjia and Van Der Spek, Erik and Hu, Jun},\ntitle = {Animated Scale: Adaption of the Motivational Scale for User Testing with Children},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650962},\ndoi = {10.1145/3613905.3650962},\nabstract = {Addressing the challenge of accurately assessing motivational levels in young children through traditional questionnaires, such as the Intrinsic Motivation Inventory (IMI) and the Player Experience of Need Satisfaction (PENS) questionnaire, this study introduces a novel approach by transforming bipolar questions into unipolar ones and employing animated scales using Memoji available on the iOS system. The paper details the design process, application, and evaluation of these animated scales through pilot studies involving 62 children aged 7-14, highlighting the potential of this approach in enhancing children’s comprehension and engagement with questionnaires.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {46},\nnumpages = {6},\nkeywords = {Intrinsic Motivation Scale, Player Experience of Need Satisfaction Scale, Testing with Children},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651053,\nauthor = {Sun, Zhida and Reani, Manuele and Luo, Yunzhong and Bao, Zhuolan},\ntitle = {Anthropomorphism in Chatbot Systems Between Gender and Individual Differences},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651053},\ndoi = {10.1145/3613905.3651053},\nabstract = {Online advisory chatbots are becoming increasingly popular and are often used to support user decision-making in different sectors. The design of such systems has been the object of investigation since it can alter the user’s perception and cognition. This study investigates the effect of chatbot design, gender, and individual tendencies on perceived anthropomorphism in a financial investment scenario. Our online experiment with 116 participants revealed that manipulating chatbot design can alter the user perception of anthropomorphism contingent on both gender and anthropomorphism tendencies. Male users with a strong anthropomorphism tendency exhibited a higher degree of perceived anthropomorphism compared to female users with a similar tendency. However, this effect was only observed when the chatbots themselves lacked design elements that promote anthropomorphism. Our novel finding provides theoretical and practical implications to the domain of chatbot system design and applications.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {47},\nnumpages = {7},\nkeywords = {Anthropomorphism, Chatbot Design, Gender, Online Experiment},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650846,\nauthor = {Wang, Lu and Smriti, Diva and Yuan, Hao and Huh-Yoo, Jina},\ntitle = {Artificial Intelligence Systems for Supporting Informal Caregivers of People Living with Alzheimer's Disease or Related Dementias: A Systematic Review},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650846},\ndoi = {10.1145/3613905.3650846},\nabstract = {Informal caregivers of people living with Alzheimer’s disease or related dementias (PLWD) face challenges like obtaining personalized information and monitoring PLWD’s health. Rapid advancements in technology, especially in sophisticated and controversial areas like artificial intelligence (AI), prompted our study to assess AI’s potential and challenges in supporting the needs of informal caregivers of PLWD. Caregiving activities require dynamic, often unpredictable, and sometimes emotionally draining tasks that deal with a large amount of information. We conducted a systematic review to understand what AI technology has been developed to support informal caregivers of PLWD. We collected 920 papers from ACM Digital Library, IEEE Xplore, and PubMed. Screening and eligibility evaluation resulted in 16 papers for full-text review. We present which documented needs of informal caregivers have been explored by the existing research, and the contexts of the AI solutions including interfaces, data, and algorithms, as well as their effectiveness, challenges, and limitations.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {48},\nnumpages = {11},\nkeywords = {Alzheimer’s disease, Artificial Intelligence, Dementia, Human-centered design, Informal caregivers},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651041,\nauthor = {Bird, Charlotte},\ntitle = {Artists and AI: Creative Interactions and Tensions},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651041},\ndoi = {10.1145/3613905.3651041},\nabstract = {Advancements in artificial intelligence (AI) technologies have enabled new forms of artwork production. Artists engaged with AI have developed individual practices within their field. In this paper, we present a qualitative study to explore the tensions that arise in this new practice. We conducted and analysed 10 interviews with artists who work creatively with AI. We broaden the scope of attention beyond only the mechanisms of user-AI interaction to consider the personal, inter-personal, practical and ethical tensions for participants. We uncover how participants manage aspects such as expectation, reception and criticism.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {49},\nnumpages = {6},\nkeywords = {AI Art, Artificial Intelligence, Creative AI, Cultural Studies, Qualitative Interviews},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650784,\nauthor = {Han, Jiyeon and Park, Jimin and Huh, Jinyoung and Oh, Uran and Do, Jaeyoung and Kim, Daehee},\ntitle = {AscleAI: A LLM-based Clinical Note Management System for Enhancing Clinician Productivity},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650784},\ndoi = {10.1145/3613905.3650784},\nabstract = {While clinical notes are essential to the field of healthcare, they pose several challenges for clinicians since it is difficult to write down medical information, review prior notes, and extract the desired information at the same time while examining a patient. Thus, we designed a system that can automatically generate clinical notes from dialogues between patients and clinicians and provide specific information upon clinicians’ query using a Large Language Model (LLM) both in real-time. To explore how this system can be used to support clinicians in practice, we conducted an interview with six clinicians followed by a design probe study with the current version of our system for feedback. Findings suggest that our system has the potential to enable clinicians to write and access clinical notes and examine the patients simultaneously with reduced cognitive loads and increased efficiency and accuracy.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {50},\nnumpages = {7},\nkeywords = {Large language model, clinical note, design probe, interview},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650941,\nauthor = {Pandjaitan, Adrian and Strecker, Jannis and Bektas, Kenan and Mayer, Simon},\ntitle = {AuctentionAR - Auctioning Off Visual Attention in Mixed Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650941},\ndoi = {10.1145/3613905.3650941},\nabstract = {Mixed Reality technologies are increasingly interwoven with our everyday lives. A variety of powerful Head Mounted Displays have recently entered consumer electronics markets, and more are under development, opening new dimensions for spatial computing. This development will likely not stop at the advertising industry either, as first forays into this area have already been made. We present AuctentionAR which allows users to sell off their visual attention to interested parties. It consists of a HoloLens 2, a remote server executing the auctioning logic, the YOLOv7 model for image recognition of products which may induce an advertising intent, and several bidders interested in advertising their products. As this system comes with substantial privacy implications, we discuss what needs to be considered in future implementation so as to make this system a basis for a privacy preserving MR advertising future.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {51},\nnumpages = {6},\nkeywords = {auctioning, notifications, object detection, visual attention},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651043,\nauthor = {Zhang, Xishuo and Liu, Lin and Wang, Yi and Liu, Xiao and Wang, Hailong and Arora, Chetan and Liu, Haichao and Wang, Weijia and Hoang, Thuong},\ntitle = {Auto-Generated Personas: Enhancing User-centered Design Practices among University Students},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651043},\ndoi = {10.1145/3613905.3651043},\nabstract = {Personas are commonly used in User-centered Design (UCD) activities to help designers better understand users’ needs. However, there is still a reliance on traditional approaches such as interviews and ethnography for building personas in UCD activities. To this end, we developed an auto-generating persona system to enhance practices in UCD course activities. Our persona system is developed based on the GPT-4 model, the DALL-E 2 model, and knowledge graphs. Hence, our persona system includes three main features of our persona system: automated processing of survey data, automatic generation of 2D avatars, and providing options for automatic or customized entity generation. We recruited a total of 22 participants to evaluate our persona system. Our findings confirmed that there was a significant difference in terms of efficiency, satisfaction, accuracy, and diversity. Meanwhile, participants provided both positive and negative feedback regarding our persona system. As on-going work, we discuss the current limitations of our persona system and explore future research directions to further improve its capabilities and effectiveness.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {52},\nnumpages = {7},\nkeywords = {GPT-4, Knowledge Graphs, Personas, User Studies},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650798,\nauthor = {Reif, Emily and Qian, Crystal and Wexler, James and Kahng, Minsuk},\ntitle = {Automatic Histograms: Leveraging Language Models for Text Dataset Exploration},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650798},\ndoi = {10.1145/3613905.3650798},\nabstract = {Making sense of unstructured text datasets is perennially difficult, yet increasingly relevant with Large Language Models. Data practitioners often rely on dataset summaries, especially distributions of various derived features. Some features, like toxicity or topics, are relevant to many datasets, but many interesting features are domain specific: instruments and genres for a music dataset, or diseases and symptoms for a medical dataset. Accordingly, data practitioners often run custom analyses for each dataset, which is cumbersome and difficult, or use unsupervised methods. We present AutoHistograms, a visualization tool leveraging LLMs. AutoHistograms automatically identifies relevant entity-based features, visualizes them, and allows the user to interactively query the dataset for new categories of entities. In a user study with (n=10) data practitioners, we observe that participants were able to quickly onboard to AutoHistograms, use the tool to identify actionable insights, and conceptualize a broad range of applicable use cases. Together, this tool and user study contribute to the growing field of LLM-assisted sensemaking tools.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {53},\nnumpages = {9},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650818,\nauthor = {Cohn, Michelle and Pushkarna, Mahima and Olanubi, Gbolahan O. and Moran, Joseph M. and Padgett, Daniel and Mengesha, Zion and Heldreth, Courtney},\ntitle = {Believing Anthropomorphism: Examining the Role of Anthropomorphic Cues on Trust in Large Language Models},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650818},\ndoi = {10.1145/3613905.3650818},\nabstract = {People now regularly interface with Large Language Models (LLMs) via speech and text (e.g., Bard) interfaces. However, little is known about the relationship between how users anthropomorphize an LLM system (i.e., ascribe human-like characteristics to a system) and how they trust the information the system provides. Participants (n=2,165; ranging in age from 18-90 from the United States) completed an online experiment, where they interacted with a pseudo-LLM that varied in modality (text only, speech + text) and grammatical person (“I” vs. “the system”) in its responses. Results showed that the “speech + text” condition led to higher anthropomorphism of the system overall, as well as higher ratings of accuracy of the information the system provides. Additionally, the first-person pronoun (“I”) led to higher information accuracy and reduced risk ratings, but only in one context. We discuss these findings for their implications for the design of responsible, human–generative AI experiences.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {54},\nnumpages = {15},\nkeywords = {anthropomorphism, first-person pronoun \"I\", large language models, text-to-speech (TTS), trust},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650751,\nauthor = {Farzand, Habiba and Al Baiaty Suarez, David and Goodge, Thomas and Macdonald, Shaun Alexander and Marky, Karola and Khamis, Mohamed and Cairns, Paul},\ntitle = {Beyond Aesthetics: Evaluating Response Widgets for Reliability \\& Construct Validity of Scale Questionnaires},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650751},\ndoi = {10.1145/3613905.3650751},\nabstract = {Scale questionnaires are psychometric tools that capture perspectives and experiences. Consequently, these tools need to be reliable and valid. In this paper, we investigate the impact of response widgets - the UI elements that allow users to answer scale items - on the overall scale reliability and construct validity of three varied length scale questionnaires in a user study (N=30). Our results reveal that optimum reliability was achieved using radio buttons and dropdowns in all varied-length questionnaires. Further, valid results were produced utilising the slider and dropdown. No significant differences were found in time consumption, but click count was significantly higher with dropdown. Radio buttons scored lower in format satisfaction than others, and dropdown was the least effective in ease of selection and quick completion. In light of these results, we conclude that response widgets are more than just aesthetics and should be selected as per the researcher’s aims.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {55},\nnumpages = {7},\nkeywords = {UI response styles, reliability, scale questionnaires, user experience, validity},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651093,\nauthor = {Ma, Xiao and Mishra, Swaroop and Liu, Ariel and Su, Sophie Ying and Chen, Jilin and Kulkarni, Chinmay and Cheng, Heng-Tze and Le, Quoc and Chi, Ed},\ntitle = {Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651093},\ndoi = {10.1145/3613905.3651093},\nabstract = {Large language model (LLM) powered chatbots are primarily text-based today, and impose a large interactional cognitive load, especially for exploratory or sensemaking tasks such as planning a trip or learning about a new city. Because the interaction is textual, users have little scaffolding in the way of structure, informational “scent”, or ability to specify high-level preferences or goals. We introduce ExploreLLM that allows users to structure thoughts, help explore different options, navigate through the choices and recommendations, and to more easily steer models to generate more personalized responses. We conduct a user study and show that users find it helpful to use ExploreLLM for exploratory or planning tasks, because it provides a useful schema-like structure to the task, and guides users in planning. The study also suggests that users can more easily personalize responses with high-level preferences with ExploreLLM.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {56},\nnumpages = {12},\nkeywords = {Artificial Intelligence, Chatbots, Graphical User Interfaces, Interaction, Large Language Models, Learning from Instruction., Natural Language Interfaces, Prompting, Schema, Task Decomposition},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651004,\nauthor = {Amirkhani, Sima and Alizadeh, Fatemeh and Randall, Dave and Stevens, Gunnar},\ntitle = {Beyond Dollars: Unveiling the Deeper Layers of Online Romance Scams Introducing “Body Scam”},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651004},\ndoi = {10.1145/3613905.3651004},\nabstract = {As online romance surges, so does the popularity of online romance scam. While existing research predominantly emphasizes financial scams, our study introduces “body scam” which is not for financial gain, but just for sexual abuse. We conducted interviews with 20 victims of online dating fraud with sexual scam experience in Iran, delving into their story in a context where dating is legally, socially, and normatively complex. Through inductive coding, our findings reveal a notable shift in victimization risks, particularly for women, with a growing emphasis on predatory sexual intent rather than purely financial motivation. This study not only sheds light on the evolving landscape of online dating risks but also underscores the significance of understanding the nuances of sexual intentions in the Iranian context.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {57},\nnumpages = {6},\nkeywords = {Body Scam, Iran, Online Dating Fraud, Online Romance Scam, Sextortion, Sexual Intention},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650816,\nauthor = {Kim, Taehyun and Nisar, Harris and Lindgren, Robb and Zhang, Jiahao and Tang, Xiaoyu and Lira, Matthew and Talhan, Aishwari},\ntitle = {Beyond the Screen: Gestural Perspective-Taking with a Biochemistry Simulation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650816},\ndoi = {10.1145/3613905.3650816},\nabstract = {Significant research has been conducted on how students’ gestures aid in learning scientific concepts, yet there remains a gap in understanding the impact of gesture-based interactions between students and simulations on their interpretation of visualized scientific phenomena. Addressing this, our paper presents a usability test conducted on a dynamic equilibrium visualization simulation developed for introductory college courses. Through a user study involving 40 participants, we conducted a qualitative evaluation to determine how students interpret gesture-controlled simulations. The findings confirm that students generally interpret visualized scientific concepts effectively and that interacting through gestures enhances their interpretation of the simulations. Additionally, this paper discusses the limitations of the current study and suggests directions for future research.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {58},\nnumpages = {6},\nkeywords = {Dynamic equilibrium, Gesture-controlled simulation, Science interpretation, User testing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651064,\nauthor = {Paden, Jamal R and Narechania, Arpit and Endert, Alex},\ntitle = {BiasBuzz: Combining Visual Guidance with Haptic Feedback to Increase Awareness of Analytic Behavior during Visual Data Analysis},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651064},\ndoi = {10.1145/3613905.3651064},\nabstract = {During visual data analysis, users may inadvertently focus more on certain aspects of data, affecting analysis outcome(s). Existing tools primarily rely on visual cues (e.g., highlight already visited data) to increase user awareness of such analytic behaviors. We believe this single, visual modality is a passive form of guidance that adds to users’ cognitive load already engaged in analysis. We investigate how a dual modality (visual guidance and haptic feedback) can capture users’ attention and more actively guide them in their pursuits. We interface an existing visual data analysis tool with a gaming mouse. This enhanced system tracks user interactions and communicates biases by vibrating the mouse (haptic) and simultaneously displaying contextual information in the tool (visual). A formative study with nine users revealed that this dual modality increased analytical awareness in some cases but some users found the haptic mouse vibrations to be distracting and disturbing, informing the design of future multimodal user interfaces.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {59},\nnumpages = {7},\nkeywords = {guidance, haptic, visual data analysis, visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650988,\nauthor = {Lee, Sang-Hyun and Lee, Joon Hyub and Bae, Seok-Hyung},\ntitle = {Bimanual Interactions for Surfacing Curve Networks in VR},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650988},\ndoi = {10.1145/3613905.3650988},\nabstract = {We propose an interactive system for authoring 3D curve and surface networks using bimanual interactions in virtual reality (VR) inspired by physical wire bending and film wrapping. In our system, the user can intuitively author 3D shapes by performing a rich vocabulary of interactions arising from a minimal gesture grammar based on hand poses and firmness of hand poses for constraint definition and object manipulation. Through a pilot test, we found that the user can quickly and easily learn and use our system and become immersed in 3D shape authoring.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {60},\nnumpages = {7},\nkeywords = {B\\'{e}zier curve network, VR, bimanual gesture, surface modeling},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651035,\nauthor = {Kang, Hyeonsu B and Lin, David Chuan-En and Martelaro, Nikolas and Kittur, Aniket and Chen, Yan-Ying and Hong, Matthew K.},\ntitle = {BioSpark: An End-to-End Generative System for Biological-Analogical Inspirations and Ideation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651035},\ndoi = {10.1145/3613905.3651035},\nabstract = {Nature often inspires solutions for complex engineering problems, but it is challenging for designers to discover relevant analogies and synthesize from them. Here, we present an end-to-end system, BioSpark, that generates biological-analogical mechanisms and provides an interactive interface for comprehension and ideation. From a small seed set of expert-curated mechanisms, BioSpark’s pipeline iteratively expands them by constructing and traversing organism taxonomies, aiming to overcome both data sparsity in expert curation and limited conceptual diversity in purely automated analogy generation. The interface helps designers recognize and understand relevant analogs to design problems using four interaction features. We conduct an exploratory study with design students to showcase how BioSpark facilitated analogical transfer of ideas but was limited in conveying active ingredients, the core abstraction underpinning how mechanisms work. We discuss this limitation and other implications such as generative hallucination that could facilitate shifts in human exploration of new design spaces.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {61},\nnumpages = {13},\nkeywords = {Analogies, Design Creativity, Diversity-enhanced Generation, Ideation, Large Language Models, Nature},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651032,\nauthor = {Esparraguera, Liam Franco and Selberg, Kristoffer and Lou, Brian and Sun, Jenny and Desta, Beza and Monroy-Hern\\'{a}ndez, Andr\\'{e}s and Abtahi, Parastoo},\ntitle = {Breaking the Plane: Exploring Real-Time Visualization of 3D Surfaces in Augmented Reality with Handwritten Input},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651032},\ndoi = {10.1145/3613905.3651032},\nabstract = {We introduce Breaking the Plane, an augmented reality (AR) application built for AR headsets that enables users to visualize 3D mathematical functions using handwritten input. Researchers have demonstrated overlaying 3D visualizations of mathematical concepts through AR enhances learning motivation and comprehension, and equation parsing makes the authoring of teaching materials more time-efficient for instructors. Previous works have developed AR systems that separately employ equation parsing and 3D mathematical visualizations, but work has yet to be done to combine those features by enabling real-time interactions and dynamic visualizations that help users learn in situ. We explore this by developing an AR system featuring handwritten equation parsing, graph manipulation, and a 3D function plotter. We found that our system significantly surpassed other systems in engagement, achieved comparable ease of use to a popular visualization tool, was considered the most effective in aiding problem-solving, and was highly preferred by participants for future use.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {62},\nnumpages = {9},\nkeywords = {Augmented Reality (AR), Educational Technology, Interactive Learning Tools, Mathematics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650943,\nauthor = {Theodore, Grace and Huang, Jing-Yuan and Cheng, Lung-Pan and Hung, Yi-Ping},\ntitle = {Breathm: A Calm Device with Personalized Slow Breathing Guidance},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650943},\ndoi = {10.1145/3613905.3650943},\nabstract = {We present Breathm, a haptic feedback cushion designed to lead user into a state of relaxation. To achieve this, Breathm provides slow breathing guidance by expanding and contracting, mirroring the natural abdominal movements during inhalation and exhalation. Instead of providing a fixed target breathing rate, Breathm continuously monitors user’s breathing and adjusts the target throughout the guidance process, ensuring a smooth transition from user’s natural breathing rate to the intended slower pace. Through this design, Breathm facilitates user to calm their breath and relax. A user study is conducted to evaluate the impact of user’s breath mirror period on the guidance design. Our results show that, within the same timeframe, participants achieve longer breath extension when this mirror period is incorporated.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {63},\nnumpages = {5},\nkeywords = {Calm interface, breathing guidance, meditation, respiration coaching},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650740,\nauthor = {Gamage, Bhanuka and Holloway, Leona and McDowell, Nicola and Do, Thanh-Toan and Price, Nicholas Seow Chiang and Lowery, Arthur James and Marriott, Kim},\ntitle = {Broadening Our View: Assistive Technology for Cerebral Visual Impairment},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650740},\ndoi = {10.1145/3613905.3650740},\nabstract = {Over the past decade, considerable research has been directed towards assistive technologies to support people with vision impairments using machine learning, computer vision, image enhancement, and/or augmented/virtual reality. However, this has almost totally overlooked a growing demographic: people with Cerebral Visual Impairment (CVI). Unlike Ocular Vision Impairments (OVI), CVI arises from damage to the brain’s visual processing centres. This paper introduces CVI and reveals a wide research gap in addressing the needs of this demographic. Through a scoping review, we identified 14 papers at the intersection of these technologies and CVI. Of these, only three papers described assistive technologies focused on people living with CVI, with the others focusing on diagnosis, understanding, simulation or rehabilitation. Our findings highlight the opportunity for the Human-Computer Interaction and Assistive Technologies research community to explore and address this underrepresented domain, thereby enhancing the quality of life for people with CVI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {64},\nnumpages = {9},\nkeywords = {assistive devices, augmented reality, cerebral visual impairment, computer vision, machine learning, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651026,\nauthor = {Wan, Hongyu and Zhang, Jinda and Suria, Abdulaziz Arif and Yao, Bingsheng and Wang, Dakuo and Coady, Yvonne and Prpa, Mirjana},\ntitle = {Building LLM-based AI Agents in Social Virtual Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651026},\ndoi = {10.1145/3613905.3651026},\nabstract = {In this paper, we introduce the design and evaluation of an LLM-based AI agent for human-agent interaction in Virtual Reality (VR). Our AI agent system leverages GPT-4, a Large Language Model (LLM) to simulate human behavior. Our LLM-based agent, deployed in VRChat as a Non-playable Character (NPC), exhibits the ability to respond to a player by providing context-relevant responses followed by appropriate facial expressions and body gestures. Our preliminary evaluation yielded the most optimal parameters for generating the most plausible responses. With our system, we lay the groundwork for future development of LLM-based NPCs in VR.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {65},\nnumpages = {7},\nkeywords = {GPT-4, Generative Agents, Human-Computer Interaction, Large Language Models, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651030,\nauthor = {Sun, Xiaoqing and Wang, Jingyi and Zhou, Yan and Wang, Suhan and Li, Yixuan and Ren, Xipei},\ntitle = {CO-Coffee: A Technology Probe Study to Facilitate Coffee Breaks in Open Offices},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651030},\ndoi = {10.1145/3613905.3651030},\nabstract = {Coffee breaks serve as a good opportunity for workers to break up sedentary work and socialize with their colleagues. In this paper, we present a technology probe study using CO-Coffee, a sociotechnical system designed to encourage mutual invitation between coworkers for drinking coffee and visualize the process of each social break in an open office. The one-week probe study was conducted in three workplaces with 28 office workers to gain some qualitative insights into usage patterns and user experiences with CO-Coffee. Our results revealed how this probe facilitated coffee breaks in different working contexts and how participants established rituals around CO-Coffee to engage in break times. We emphasized the importance of positive workplace norms on break-taking behaviors and discussed design implications for reshaping workplace ambiance to encourage active and equal participations in break times.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {66},\nnumpages = {7},\nkeywords = {Office vitality, coffee break, social interactions, technology probe},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650953,\nauthor = {Harmon, Sarah},\ntitle = {Can AI Help Inspire Healthy Eating? Exploring the Potential of Generated Motivational Texts and Images Related to Healthy Food Choices},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650953},\ndoi = {10.1145/3613905.3650953},\nabstract = {A variety of open-source resources are currently available to potentially help inspire healthy eating, which is an important part of supporting one’s overall well-being. We explored several ways in which one might harness web-driven and AI processes to find texts and images created by two approaches: traditional human authoring and generated by humans using support from AI models. We then recruited Mechanical Turk study participants to investigate how individuals would perceive these texts and images in the context of inspiring healthy eating.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {67},\nnumpages = {5},\nkeywords = {healthy eating inspiration, online food communities},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650952,\nauthor = {Wei, Jiafu and Chang, Chia-Ming and Yang, Xi and Igarashi, Takeo},\ntitle = {CanvasPic: An Interactive Tool for Freely Generating Facial Images Based on Spatial Layout},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650952},\ndoi = {10.1145/3613905.3650952},\nabstract = {In real-world usage, existing GAN image generation tools come up short due to their lack of intuitive interfaces and limited flexibility. To overcome these limitations, we developed CanvasPic, an innovative tool for flexible GAN image generation. Our tool introduces a novel 2D layout design that allows users to intuitively control image attributes based on real-world images. By interacting with the distances between images in the spatial layout, users are able to conveniently control the influence of each attribute on the target image and explore a wide range of generated results. Considering practical application scenarios, a user study involving 24 participants was conducted to compare our tool with existing tools in GAN image generation. The results of the study demonstrate that our tool significantly enhances the user experience, enabling more effective achievement of desired generative results.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {68},\nnumpages = {8},\nkeywords = {CanvasPic, GAN, Human-centered AI, Interactive System, Interview, User Interface},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650972,\nauthor = {Wang, Zezhong and Hao, Shan and Carpendale, Sheelagh},\ntitle = {Card-Based Approach to Engage Exploring Ethics in AI for Data Visualization},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650972},\ndoi = {10.1145/3613905.3650972},\nabstract = {We present AI-VIS EthiCards, a card-based approach to explore ethics tailored for AI for visualization. The continuous integration of artificial intelligence and data visualization has brought about increased efficiency and benefits, yet inevitably raises ethical concerns. The emerging field of AI for visualization is marked by its inherent complexity, making it crucial for researchers, designers, and practitioners to cultivate ethical literacy and contemplate moral obligations within this intricate environment. These cards aim to aid users in learning, discussing, and reflecting on the ethical dilemmas that may arise from the integration of AI technology and visualization. The AI-VIS EthiCard set contains six themes: Goals, AI-VIS Tasks, Technologies, Ethical Principles, People-In-Focus, and Challenges, proposes various modes of use, including theoretical exploration, and design development simulations, with five activities. We aim to offer users an exploratory and open approach to discussions, providing multiple perspectives to guide ethical considerations when applying AI for visualization. The full set of cards is available at https://aivisethicards.github.io/.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {69},\nnumpages = {7},\nkeywords = {Artificial intelligence, Card, Data visualization, Education, Ethics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650782,\nauthor = {Cui, Fengzhen and Wang, Yuntao and Lei, Shenshen and Shi, Yuanchun},\ntitle = {CardboardHRV: Bridging Virtual Reality and Biofeedback with a Cost-Effective Heart Rate Variability System},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650782},\ndoi = {10.1145/3613905.3650782},\nabstract = {We introduce CardboardHRV, an affordable and effective heart rate variability (HRV) biofeedback system leveraging Cardboard VR. Designed for easy access to HRV biofeedback without sacrificing therapeutic value, we adapted the Google Cardboard VR headset with an optical fiber modification. This enables the camera of the inserted phone to capture the photoplethysmography (PPG) signal from the user’s lateral forehead, enabling CardboardHRV to accurately calculate the heart rate variability as a basis for biofeedback. Furthermore, we’ve integrated an engaging biofeedback game to assist users throughout their sessions, enhancing user engagement and the overall experience. In a preliminary user evaluation, CardboardHRV demonstrated comparable therapeutic outcomes to traditional HRV biofeedback systems that require an additional electrocardiogram (ECG) device, proving itself as a more cost-effective and immersive alternative.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {70},\nnumpages = {6},\nkeywords = {Heart Rate Variability Biofeedback, Human Computer Interaction, Photoplethysmography(PPG), Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650928,\nauthor = {Du, Wantong and Zhu, Zhiying and Xu, Xinhui and Che, Haoyuan and Chen, Shi},\ntitle = {CareerSim: Gamification Design Leveraging LLMs For Career Development Reflection},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650928},\ndoi = {10.1145/3613905.3650928},\nabstract = {Graduates often struggle with challenges in their careers stemming from a lack of self-construction and decisive decision-making. The HCI community increasingly focuses on gamification as a means to foster reflection, with the emerging developments in LLM providing new opportunities for personalized gamified experiences. However, existing solutions fall short in terms of personalization and real-world data references. In response, our research introduces CareerSim, a role-playing game that leverages LLM’s generation and reasoning capabilities. By integrating these with real-world based databases and game mechanics, we delve into gamification’s role in reflective self-construction and decision-making. We aim to inspire designers to effectively incorporate LLMs in gamification design.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {71},\nnumpages = {7},\nkeywords = {Career development, Gamification design, Large Language Models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650921,\nauthor = {Ponochevnyi, Nazar and Kuzminykh, Anastasia},\ntitle = {Chart What I Say: Exploring Cross-Modality Prompt Alignment in AI-Assisted Chart Authoring},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650921},\ndoi = {10.1145/3613905.3650921},\nabstract = {Recent chart-authoring systems, such as Amazon Q in QuickSight and Copilot for Power BI, demonstrate an emergent focus on supporting natural language input to share meaningful insights from data through chart creation. Currently, chart-authoring systems tend to integrate voice input capabilities by relying on speech-to-text transcription, processing spoken and typed input similarly. However, cross-modality input comparisons in other interaction domains suggest that the structure of spoken and typed-in interactions could notably differ, reflecting variations in user expectations based on interface affordances. Thus, in this work, we compare spoken and typed instructions for chart creation. Findings suggest that while both text and voice instructions cover chart elements and element organization, voice descriptions have a variety of command formats, element characteristics, and complex linguistic features. Based on these findings, we developed guidelines for designing voice-based authoring-oriented systems and additional features that can be incorporated into existing text-based systems to support speech modality.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {72},\nnumpages = {7},\nkeywords = {data visualization, natural language corpus, natural language interface, visualization authoring, visualization specification, voice interface},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650800,\nauthor = {He, Shuqi and Yu, Lingyun},\ntitle = {Charting Beyond Sight with DataStory: Sensory Substitution and Storytelling in Visual Literacy Education for Visually Impaired Children},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650800},\ndoi = {10.1145/3613905.3650800},\nabstract = {Visualizations transform raw data into accessible insights. However, they often exclude the visually impaired community. Despite tools like alt-texts and screen readers for translating visual information, visual literacy remains a fundamental barrier to interpreting visual data. Standard visual literacy education is less accessible to visually impaired learners. Our study explores visualization education through alternative sensory channels for visually impaired students. This work has three main contributions: examining educational practices for visually impaired students at a special education school; defining design requirements focused on sensory substitution; and developing a storytelling-based assistive learning prototype. The final design, DataStory, is a tactile storybook with embedded visualizations and sonified data narratives. To evaluate its feasibility, we conducted a pilot study with proxy students and a special education teacher. The paper concludes with a discussion on limitations and future directions in assistive visualization education, promoting further research into equitable educational tools for visually impaired children.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {73},\nnumpages = {8},\nkeywords = {Assistive learning technologies, Sensory substitution, Storytelling, Visually impaired education},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651031,\nauthor = {Huang, Shaoshuai and Zhao, Xuandong and Wei, Dapeng and Song, Xinheng and Sun, Yuanbo},\ntitle = {Chatbot and Fatigued Driver: Exploring the Use of LLM-Based Voice Assistants for Driving Fatigue},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651031},\ndoi = {10.1145/3613905.3651031},\nabstract = {This study explores the application of Large Language Model (LLM)-based voice assistants, such as ChatGPT-4, in mitigating passive driving fatigue, while aiming to enhance driving performance and safety. Employing an empirical approach with a driving simulator and the LLM-based voice assistant \"Driver Mate,\" the study focuses on conversation complexity and frequency as independent variables, and utilizes electroencephalogram (EEG), driving simulator data, and scales for measures. The findings reveal the effectiveness of in-vehicle LLM-based voice assistants, highlighting that low-complexity, high-frequency conversation is optimal for driver alertness and acceptance, while low-complexity, low-frequency interactions significantly improve driving performance. This study innovatively investigates the role of LLM-based voice assistants in alleviating driving fatigue, offering practical suggestions for future in-vehicle systems.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {74},\nnumpages = {8},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650898,\nauthor = {He, Rui and Cao, Ying and Hoorn, Johan F. and Wei, Huaxin},\ntitle = {Cinemassist: An Intelligent Interactive System for Real-Time Cinematic Composition Design},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650898},\ndoi = {10.1145/3613905.3650898},\nabstract = {Designing cinematic compositions, which involves moving cameras through a scene, is essential yet challenging in filmmaking. Machinima filmmaking provides a real-time virtual environment for exploring different compositions, but it still requires significant cinematography skills and creativity. To address this, we introduce Cinemassist, a tool that helps users develop camera trajectories in a 3D animation by generating multiple composition proposals for creative exploration. Preliminary user study indicates that our system can generate useful design suggestions for experts and novices, and facilitate users’ exploration and evaluation during the cinematic composition design process.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {75},\nnumpages = {7},\nkeywords = {Creativity Support Tool, Digital Filmmaking, Intelligent Cinematography, Machine Learning, Machinima},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650830,\nauthor = {Sehrt, Jessica and Yilmaz, Ugur and Kosch, Thomas and Schwind, Valentin},\ntitle = {Closing the Loop: The Effects of Biofeedback Awareness on Physiological Stress Response Using Electrodermal Activity in Virtual Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650830},\ndoi = {10.1145/3613905.3650830},\nabstract = {This paper presents the results of a user study examining the impact of biofeedback awareness on the effectiveness of stress management, utilizing Electrodermal Activity (EDA) as the primary metric within an immersive Virtual Reality (VR). Employing a between-subjects design (N=30), we probed whether informing individuals of their capacity to manipulate the VR environment’s weather impacts their physiological stress responses. Our results indicate lower EDA levels of participants who were informed of their biofeedback control than those participants who were not informed about their biofeedback control. Interestingly, the participants who were informed about the control over the environment also manifested variations in their EDA responses. Participants who were not informed of their ability to control the weather showed decreased EDA measures until the end of the biofeedback phase. This study enhances our comprehension of the significance of awareness in biofeedback in immersive settings and its potential to augment stress management techniques.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {76},\nnumpages = {7},\nkeywords = {Awareness, Biofeedback, Electrodermal Activity, Stress, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650893,\nauthor = {Chandra, Sowmya and Thompson, Valerie and Vargas Mel\\'{e}ndez, Grecia B and Purnell, Tanjala S. and Martin-Hammond, Aqueasha},\ntitle = {Co-Designing Emotion: Supporting Positive Affect in a Culturally Sensitive Application for African-American Heart Health},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650893},\ndoi = {10.1145/3613905.3650893},\nabstract = {Heart disease is a leading cause of injury and death among African Americans in the United States. Culturally sensitive design of health technologies can be useful for motivating health behavior change, especially in underserved communities. Yet, designing culturally sensitive technologies requires careful consideration to identify appropriate approaches for tailoring applications to the needs of the target population. In the paper, we explore the relationship between user values and emotions for informing the design of a culturally sensitive heart health application for African Americans. We partner with members and local community advocates involved in an existing evidenced-based community heart health intervention to understand their values and explore implications for designing for positive emotional experiences in a future digital application. We contribute a discussion of emotional intentions important to the design of a future application and suggestions for designers of similar applications.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {77},\nnumpages = {6},\nkeywords = {co-design, community-based design, culturally sensitive design, emotion},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650790,\nauthor = {Chandran, Krishnan and Dimitriou, Paschalis and Wand, Lorenza and McGinity, Matthew},\ntitle = {Co-located Mixed Reality Classrooms: Four Case Studies from Higher Education},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650790},\ndoi = {10.1145/3613905.3650790},\nabstract = {Multi-user mixed reality (MR) holds great potential for classroom based teaching, but what are the most effective teaching experiences, formats and pedagogical techniques for this emerging technology? Through a series of case studies involving four professors and their students, we explore the pedagogical potential of headset-based co-located MR. Using a custom platform, a series of immersive lectures were developed and delivered covering a range of themes, each including a structured design process, live immersive lectures with students and post-lesson interviews and analysis. From these experiments emerged a common set of educational possibilities offered by the medium, including spatiotemporal narratives, direct and joint attention, virtual travel, collaborative interaction and spatial thinking. Key challenges facing the adoption of MR in educational settings were also identified. This work demonstrates an effective method for exploring the possibilities of MR and provides insights for educators, researchers and developers interested in integrating MR into educational settings.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {78},\nnumpages = {7},\nkeywords = {Augmented and Virtual Reality, Case Study, Higher Education, Mixed Reality Classrooms, Multi-User Mixed Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650797,\nauthor = {Park, Gun Woo (Warren) and Panda, Payod and Tankelevitch, Lev and Rintel, Sean},\ntitle = {CoExplorer: Generative AI Powered 2D and 3D Adaptive Interfaces to Support Intentionality in Video Meetings},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650797},\ndoi = {10.1145/3613905.3650797},\nabstract = {Current online meeting technologies lack holistic support for reducing the effort of planning and running meetings. We present CoExplorer2D and CoExplorerVR, generative AI (GenAI)-driven technology probes for exploring the significant transformative potential of GenAI to augment these aspects of meetings. In each system, before the meeting, these systems generate tools that allow synthesis and ranking of attendees’ key issues for discussion, and likely phases that a meeting would require to cover these issues. During the meeting, these systems use speech recognition to generate 2D or VR window layouts with appropriate applications and files for each phase, and recognize the attendees’ progress through the meeting’s phases. We argue that these probes show the potential of GenAI to contribute to reducing the effort required for planning and running meetings, providing participants with a more engaging and effective meeting experiences.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {79},\nnumpages = {10},\nkeywords = {adaptive user interface, design, effectiveness, facilitation, generative AI, goals, intent recognition, meetings, planning, speech recognition, videoconferencing, virtual reality, windowing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650881,\nauthor = {Shukla, Prakash and Naik, Suchismita and Obi, Ike and Bui, Phuong and Parsons, Paul},\ntitle = {Communication Challenges Reported by UX Designers on Social Media: An Analysis of Subreddit Discussions},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650881},\ndoi = {10.1145/3613905.3650881},\nabstract = {The rise of UX design as a profession has been marked by an influx of new practitioners and an increase in dedicated educational programs. Despite research focusing on UX methods and practices, less attention has been paid to the real-world challenges faced by design professionals. Even less attention has been given to communication challenges in design practice. This study analyzes communication challenges in UX design by analyzing posts from two subreddits: r/userexperience and r/UXDesign. Key findings highlight prevalent issues such as misalignment of expectations with stakeholders, difficulties in idea conveyance, and complexities in negotiation and collaboration within teams. The research echoes previous findings on the critical role of effective communication in design practice and suggests a gap in UX education concerning real-world communication challenges.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {80},\nnumpages = {6},\nkeywords = {Design practice, Reddit analysis, UX design, design complexity},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650975,\nauthor = {Reese, May Lynn and Smirnova, Anastasia},\ntitle = {Comparing ChatGPT and Humans on World Knowledge and Common-sense Reasoning Tasks: A case study of the Japanese Winograd Schema Challenge},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650975},\ndoi = {10.1145/3613905.3650975},\nabstract = {Large Language Models (LLMs) like ChatGPT have great potential to influence the way people search for information and make decisions. Those effects are already felt in many fields, so it’s necessary to get a better understanding of how they work and how their linguistic competencies and decision-making heuristics compare to humans. Benchmark tests on these skills have been primarily developed in English. Due to disparities in training data in different languages, LLM performance in English may not necessarily translate to other languages. It is essential to expand the domain of evaluation beyond English. In this study we evaluate ChatGPT performance on world knowledge and commonsense reasoning tasks vis-\\`{a}-vis human performance in Japanese. Our results show that ChatGPT achieves lower accuracy compared to humans. We also report differences in naturalness judgements. These results shed light on the differences between humans and LLMs and can inform future studies.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {81},\nnumpages = {9},\nkeywords = {ChatGPT (GPT-3.5), Japanese, NLU Benchmark tests, Winograd Schema Challenge},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651103,\nauthor = {Spittle, Becky and Panda, Payod and Tankelevitch, Lev and Inkpen, Kori and Tang, John and Junuzovic, Sasa and Qi, Qianqian and Sweeney, Pat and Wilson, Andrew D and Buxton, William A.S. and Sellen, Abigail and Rintel, Sean},\ntitle = {Comparing the Agency of Hybrid Meeting Remote Users in 2D and 3D Interfaces of the Hybridge System},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651103},\ndoi = {10.1145/3613905.3651103},\nabstract = {Hybridge is an experimental system for exploring the design of remote inclusion for hybrid meetings. In-room users see remote participants on individual displays positioned around a table, and remotes see video feeds from the room integrated into a digital twin of the meeting room. Remotes can choose where to appear in and view the meeting room from. We designed two digital interfaces for remote attendees, one using a 2D canvas, and the other using a 3D digital twin of the room as the medium of interaction. To decide which interface to use for future evaluation, we conducted a within-subjects comparison of 24 groups completing survival tasks. We found that 3D outperformed 2D in the participants’ perceived sense of awareness, sense of agency, and physical presence. The majority of participants also subjectively preferred 3D over 2D. We discuss design recommendations based on usage patterns and participant comments, and plans for further research.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {82},\nnumpages = {12},\nkeywords = {2D, 3D, agency, asymmetry, collaboration, design, effectiveness, hybrid, immersive, inclusion, meetings, spatiality, videoconferencing, virtual},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650878,\nauthor = {Chaudhry, Beenish Moalla},\ntitle = {Concerns and Challenges of AI Tools in the UI/UX Design Process: A Cross-Sectional Survey},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650878},\ndoi = {10.1145/3613905.3650878},\nabstract = {The study aimed to gain a comprehensive understanding of AI integration in UI/UX design process from the perspective of professionals directly engaged with these tools. Two surveys were conducted in August 2023, to explore attitudes, ethical concerns, and future views on AI-assisted design technologies through both closed and open-ended questions. Using both thematic and descriptive analyses, the results showed that designers were cautiously optimistic about AI tools. This shows how important it is to think about ethics, understand how AI works, and find a good balance between integrating AI into the design workflow. These insights are crucial for AI tool developers, design educators, and the design community to develop more effective, ethical, and user-centric AI tools.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {83},\nnumpages = {6},\nkeywords = {UX, artificial intelligence, design process, professionals, recommendations, survey, tools},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650882,\nauthor = {Cai, Jie and Patel, Aashka and Naderi, Azadeh and Wohn, Donghee Yvette},\ntitle = {Content Moderation Justice and Fairness on Social Media: Comparisons Across Different Contexts and Platforms},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650882},\ndoi = {10.1145/3613905.3650882},\nabstract = {Social media users may perceive moderation decisions by the platform differently, which can lead to frustration and dropout. This study investigates users’ perceived justice and fairness of online moderation decisions when they are exposed to various illegal versus legal scenarios, retributive versus restorative moderation strategies, and user-moderated versus commercially moderated platforms. We conduct an online experiment on 200 American social media users of Reddit and Twitter. Results show that retributive moderation delivers higher justice and fairness for commercially moderated than for user-moderated platforms in illegal violations; restorative moderation delivers higher fairness for legal violations than illegal ones. We discuss the opportunities for platform policymaking to improve moderation system design.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {84},\nnumpages = {9},\nkeywords = {Content Moderation, Justice and Fairness, Platform Governance, Policymaking, Social Media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651121,\nauthor = {Bandukda, Maryam and Wang, Yichen and Perusquia-Hernandez, Monica and Li, Franklin Mingzhe and Holloway, Catherine},\ntitle = {Context matters: Investigating information sharing in mixed-visual ability social interactions},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651121},\ndoi = {10.1145/3613905.3651121},\nabstract = {Social inclusion of disabled people has been a topic of interest in HCI research led by the rise of ubiquitous and camera-based technologies. As the research area is increasing, a comprehensive understanding of blind, partially sighted (BPS), and sighted people's needs in various social settings is needed to fully inform the design of social technologies. To address this, we conducted semi-structured individual and group interviews with 12 BPS and eight sighted participants. Our findings show that context-dependent information-sharing needs of BPS and sighted people vary across social contexts (illustrated in Figure 1). While currently depending on support from sighted companions, BPS participants expressed a strong sense of independence and agency. We discuss the tensions between BPS people's information needs, sighted people's privacy concerns, and implications for the design of social technologies to support the social inclusion of BPS people.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {85},\nnumpages = {8},\nkeywords = {Social interaction, accessibility, blind, non-verbal social cues, privacy, visual impairment},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650767,\nauthor = {Nepal, Subigya and Pillai, Arvind and Campbell, William and Massachi, Talie and Choi, Eunsol Soul and Xu, Xuhai and Kuc, Joanna and Huckins, Jeremy F and Holden, Jason and Depp, Colin and Jacobson, Nicholas and Czerwinski, Mary P and Granholm, Eric and Campbell, Andrew},\ntitle = {Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650767},\ndoi = {10.1145/3613905.3650767},\nabstract = {MindScape aims to study the benefits of integrating time series behavioral patterns (e.g., conversational engagement, sleep, location) with Large Language Models (LLMs) to create a new form of contextual AI journaling, promoting self-reflection and well-being. We argue that integrating behavioral sensing in LLMs will likely lead to a new frontier in AI. In this Late-Breaking Work paper, we discuss the MindScape contextual journal App design that uses LLMs and behavioral sensing to generate contextual and personalized journaling prompts crafted to encourage self-reflection and emotional development. We also discuss the MindScape study of college students based on a preliminary user study and our upcoming study to assess the effectiveness of contextual AI journaling in promoting better well-being on college campuses. MindScape represents a new application class that embeds behavioral intelligence in AI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {86},\nnumpages = {8},\nkeywords = {AI, Behavioral Sensing, Journaling, Large Language Models, Mental Health, Passive Sensing, Self-reflection, Smartphones, Well-being},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650812,\nauthor = {Jansen, Anniek and Leborgne, Fran\\c{c}ois and Wang, Qiurui and Zhang, Chao},\ntitle = {Contextualizing the “Why”: The Potential of Using Visual Map As a Novel XAI Method for Users with Low AI-literacy},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650812},\ndoi = {10.1145/3613905.3650812},\nabstract = {The surge of Artificial Intelligence (AI) for automatic decision-making raises concerns about transparency and interpretability of AI models. Explainable AI (XAI) addresses this by providing insights into AI predictions. Despite the availability of various methods for explaining decisions based on tabular data, there is no consensus on their effectiveness for different types of users. This paper introduces a novel XAI method, the Visual Map, and presents a human-grounded evaluation study comparing it with three common XAI methods. In an online experiment (N = 49), participants with either high or low AI-literacy evaluated all four methods in terms of explanation satisfaction, cognitive load, and overall evaluation in the same classification task environment. High AI-literacy participants were largely indifferent to the four methods, whereas low AI-literacy participants favoured the visual map, perceiving it as the least cognitively demanding. Our findings contribute to the evaluation and development of XAI methods for different types of end-users.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {87},\nnumpages = {7},\nkeywords = {AI-literacy, Artificial intelligence, Counterfactual, Decision tree, Explainable AI, Human-grounded evaluation, SHAP, Visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651891,\nauthor = {Li, Brenna and Wang, Amy and Strachan, Patricia and S\\'{e}guin, Julie Anne and Lachgar, Sami and Schroeder, Karyn C and Fleck, Mathias S and Wong, Renee and Karthikesalingam, Alan and Natarajan, Vivek and Matias, Yossi and Corrado, Greg S and Webster, Dale and Liu, Yun and Hammel, Naama and Sayres, Rory and Semturs, Christopher and Schaekermann, Mike},\ntitle = {Conversational AI in health: Design considerations from a Wizard-of-Oz dermatology case study with users, clinicians and a medical LLM},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651891},\ndoi = {10.1145/3613905.3651891},\nabstract = {Although skin concerns are common, access to specialist care is limited. Artificial intelligence (AI)-assisted tools to support medical decisions may provide patients with feedback on their concerns while also helping ensure the most urgent cases are routed to dermatologists. Although AI-based conversational agents have been explored recently, how they are perceived by patients and clinicians is not well understood. We conducted a Wizard-of-Oz study involving 18 participants with real skin concerns. Participants were randomly assigned to interact with either a clinician agent (portrayed by a dermatologist) or an LLM agent (supervised by a dermatologist) via synchronous multimodal chat. In both conditions, participants found the conversation to be helpful in understanding their medical situation and alleviate their concerns. Through qualitative coding of the conversation transcripts, we provide insight on the importance of empathy and effective information-seeking. We conclude with design considerations for future AI-based conversational agents in healthcare settings.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {88},\nnumpages = {10},\nkeywords = {Artificial Intelligence, Chatbot, Dermatology, Large Language Models, Medical, Wizard-of-Oz},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650992,\nauthor = {Canossa, Alessandro and Azadvar, Ahmad and Zhu, Jichen and Harteveld, Casper and Pirker, Johanna},\ntitle = {Country as a proxy for culture? An exploratory study of players in an Online Multiplayer Game},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650992},\ndoi = {10.1145/3613905.3650992},\nabstract = {Relatively little is known about whether and how people from different co jichen.zhu@gmail.com untries play differently. Existing literature is still debating whether countries can serve as proxies for cultures. This paper is among the first to present a large-scale exploratory study (n = 14,361 from 107 countries) trying to validate empirically whether it is acceptable to utilize countries to group players of Ubisoft's Rainbow Six Siege, a popular online multiplayer game. We developed a survey collecting various player information, including demographics, habits, appreciation, and several psychological measures. Through a data-driven approach, this paper examines whether the variance in responses within users from the same country is less than the variance identified between users from different countries. If the answer is positive, then the approach of utilizing country as a proxy for culture could be justified empirically. The results show that players from the same countries respond to the survey in a marginally more homogeneous way than players from different countries.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {89},\nnumpages = {7},\nkeywords = {country, culture, geography, online games, player experience, psychological measurements},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650762,\nauthor = {Zhou, Xiaofei and Tang, Jingwan and Lyu, Hanjia and Liu, Xinyi and Zhang, Zhenhao and Qin, Lichen and Au, Fiona and Sarkar, Advait and Bai, Zhen},\ntitle = {Creating an authoring tool for K-12 teachers to design ML-supported scientific inquiry learning},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650762},\ndoi = {10.1145/3613905.3650762},\nabstract = {Despite significant advances in machine learning (ML) applications within science, there is a notable gap in its integration into K-12 education to enhance data literacy and scientific inquiry (SI) skills. To address this gap, we enable K-12 teachers with limited technical expertise to apply ML for pattern discovery and explore how ML can empower educators in teaching SI. We design a web-based tool, ML4SI, for teachers to create ML-supported SI learning activities. This tool can also facilitate collecting data about the interaction between ML techniques and SI learning. A pilot study with three K-12 teachers provides insights to prepare the next generation for the era of big data through ML-supported SI learning.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {90},\nnumpages = {7},\nkeywords = {AI education, K-12 STEM education, Machine learning, data visualization, end-user programming, scientific inquiry},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650780,\nauthor = {Kaushik, Smirity and Sharma, Tanusree and Yu, Yaman and Ali, Amna F and Wang, Yang and Zou, Yixin},\ntitle = {Cross-Country Examination of People’s Experience with Targeted Advertising on Social Media},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650780},\ndoi = {10.1145/3613905.3650780},\nabstract = {Social media effectively connects businesses with diverse audiences. However, research related to targeted advertising and social media is rarely done beyond Western contexts. Through an online survey with 412 participants in the United States and three South Asian countries (Bangladesh, India, and Pakistan), we found significant differences in participants’ ad preferences, perceptions, and coping behaviors that correlate with individuals’ country of origin, culture, religion, and other demographic factors. For instance, Indian and Pakistani participants preferred video ads to those in the US. Participants relying on themselves (horizontal individualism) also expressed more concerns about the security and privacy issues of targeted ads. Muslim participants were more likely to hide ads as a coping strategy than other religious groups. Our findings highlight that people’s experiences with targeted advertising are rooted in their national, cultural, and religious backgrounds—an important lesson for the design of ad explanations and settings, user education, and platform governance.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {91},\nnumpages = {10},\nkeywords = {Privacy, South Asia, Targeted Advertisement},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650842,\nauthor = {Ahmed, Nimra and Liu, Xindi and Al-Hazwani, Ibrahim and Huang, Elaine M.},\ntitle = {Cultural Dimensions and Mental Health Technology: A Systematic Review of Hofstede's Dimensions in Shaping Mental Health Experiences},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650842},\ndoi = {10.1145/3613905.3650842},\nabstract = {This paper explores the influence of cultural factors on mental health help-seeking behaviors and the subsequent implications for the design of mental health technologies. Using Hofstede’s Cultural Dimensions as a framework, we conducted a comprehensive literature review to examine how cultural variations affect patient behaviors in seeking mental health support. This review categorically analyses literature corresponding to each of Hofstede’s five dimensions – Power Distance, Individualism vs. Collectivism, Masculinity vs. Femininity, Uncertainty Avoidance, and Long-Term Orientation. The findings reveal significant cultural influences on help-seeking behaviors, highlighting the need for culturally sensitive approaches in mental health technology design. This study underscores the importance of cultural awareness in the design and deployment of mental health technologies, offering insights for future research and development in this field.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {92},\nnumpages = {13},\nkeywords = {Cross-Cultural, Culture, E-Mental Health, Hofstede, Mental Health},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650746,\nauthor = {Gim, Bocheon and Kang, Seongjun and Kim, Gwangbin and Yeo, Dohyeon and Hwang, Seokhyun and Kim, Seungjun},\ntitle = {Curving the Virtual Route: Applying Redirected Steering Gains for Active Locomotion in In-Car VR},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650746},\ndoi = {10.1145/3613905.3650746},\nabstract = {This study examines the feasibility of user-applied active locomotion in In-Car Virtual Reality (VR), overcoming the passivity in mobility of previous In-Car VR experiences where the virtual movement was synchronized with the real movement of the car. We present the concept of virtual steering gains to quantify the magnitude of user-applied redirection from the real car’s path. Through a user study where participants applied various levels of steering gains in an active virtual driving task, we assessed usability factors through measures of motion sickness, spatial presence, and overall acceptance. Results indicate a range of acceptable steering gains in which active locomotion improves spatial presence without significantly increasing motion sickness. Future works will attempt to further validate a steering gain threshold in which active locomotion in In-Car VR can be applicable.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {93},\nnumpages = {7},\nkeywords = {autonomous vehicles, locomotion, motion sickness, redirection, sensory misalignment, spatial presence, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651011,\nauthor = {Dullaert, Sander and Ranasinghe, Champika and Degbelo, Auriol and Bouali, Nacir},\ntitle = {Data Physicalization with Haptic Variables: Exploring Resistance and Friction},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651011},\ndoi = {10.1145/3613905.3651011},\nabstract = {Data Physicalizations have the potential to create interactive and more engaging data experiences and to make data more accessible to a broader range of users than those reached by visualizations alone. Tapping into this potential necessitates an understanding of the strengths/weaknesses of the different encoding variables available to designers, and when these variables can be employed to convey data. This work provides a preliminary investigation of two kinesthetic variables (resistance and friction) and their performance during the answering of minima/maxima/cluster questions. We evaluated both encoding modalities with users for their efficiency and effectiveness in a lab-based study. While neither modality was found to be significantly more efficient or accurate, most users preferred reading data through resistance.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {94},\nnumpages = {8},\nkeywords = {data physicalization, embodied experiences, encoding variables, friction, haptic variables, resistance},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651099,\nauthor = {Seaborn, Katie and Itagaki, Tatsuya and Watanabe, Mizuki and Wang, Yijia and Geng, Ping and Fujii, Takao and Mandai, Yuto and Kojima, Miu and Yoshida, Suzuka},\ntitle = {Deceptive, Disruptive, No Big Deal: Japanese People React to Simulated Dark Commercial Patterns},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651099},\ndoi = {10.1145/3613905.3651099},\nabstract = {Dark patterns and deceptive designs (DPs) are user interface elements that trick people into taking actions that benefit the purveyor. Such designs are widely deployed, with special varieties found in certain nations like Japan that can be traced to global power hierarchies and the local socio-linguistic context of use. In this breaking work, we report on the first user study involving Japanese people (n=30) experiencing a mock shopping website injected with simulated DPs. We found that Alphabet Soup and Misleading Reference Pricing were the most deceptive and least noticeable. Social Proofs, Sneaking in Items, and Untranslation were the least deceptive but Untranslation prevented most from cancelling their account. Mood significantly worsened after experiencing the website. We contribute the first empirical findings on a Japanese consumer base alongside a scalable approach to evaluating user attitudes, perceptions, and behaviours towards DPs in an interactive context. We urge for more human participant research and ideally collaborations with industry to assess real designs in the wild.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {95},\nnumpages = {8},\nkeywords = {Dark Patterns, Deceptive Design, Deceptive Design Pattern, Japan, Manipulative Design, Persuasive Design, User Interface Design, User Study},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650731,\nauthor = {Kim, Yuwon and Park, Jinseok and Choi, Hojin and Loeser, Martin and Ryu, Hokyoung and Seo, Kyoungwon},\ntitle = {Decoding Behavior: Utilizing Virtual Reality Digital Marker and Machine Learning for Early Detection of Mild Cognitive Impairment},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650731},\ndoi = {10.1145/3613905.3650731},\nabstract = {The imperative for early mild cognitive impairment (MCI) detection is underscored by the limitations of traditional biomarkers, high cost and invasiveness, and they often fail to capture behavioral changes in MCI patients associated with impaired instrumental activities of daily living (IADL). This study introduces a cost-effective, non-invasive alternative using digital markers, “virtual kiosk test”, which involves performing IADL tasks such as ordering food via a kiosk in virtual reality (VR) to detect MCI at an early stage. Involving 20 healthy controls and 31 MCI patients, four key behavioral features within VR digital markers effectively differentiate groups: hand movement speed, proportion of fixation duration, time to completion, and the number of errors. A machine learning model demonstrated high effectiveness with 93.3\\% accuracy, 100\\% sensitivity, 83.3\\% specificity, 90\\% precision, and a 94.7\\% F1-score in group differentiation. Findings suggest that observing behaviors via the virtual kiosk test within 5 minutes can be an efficient approach for early MCI detection, acting as reliable VR digital markers.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {96},\nnumpages = {8},\nkeywords = {Behavior, Digital marker, Early detection, Machine learning, Mild cognitive impairment, Virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650877,\nauthor = {Yalcin, Murat and Latoschik, Marc Erich},\ntitle = {DeepFear: Game Usage within Virtual Reality to Provoke Physiological Responses of Fear},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650877},\ndoi = {10.1145/3613905.3650877},\nabstract = {The investigation and the classification of the physiological signals involved in fear perception is complicated due to the difficulties in reliably eliciting and measuring the complex construct of fear. Especially, using Virtual Reality (VR) games can well elicit the physiological responses, then it can be used developing treatments in healthcare domain. In this study, we carried out exploratory physiological data analysis and wearable sensory device feasibility for the responses of fear. We contributed 1) to use a of-the-shelf commercial game (Half Life-Alyx) to provoke fear emotion, 2) to demonstrate a performance analysis with different deep learning models like Convolutional Neural Network (CNN), Long-Short Term Memory (LSTM) and Transformer, 3) to investigate the most responsive physiological signal by comprehensive data analysis and best sensory device in terms of multi-level of fear classification. Accuracy metrics, f1-scores and confusion matrices showed that ECG and ACC are the most significant two signals for fear recognition.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {97},\nnumpages = {8},\nkeywords = {Deep Learning, Fear Classification, Game, Machine Learning, Physiological Data, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650772,\nauthor = {Nash, James David and Steer, Cameron and Dinca, Teodora and Sharma, Adwait and Favaratto Santos, Alvaro and Wildgoose, Benjamin Timothy and Ager, Alexander and Clarke, Christopher and Alexander, Jason},\ntitle = {DeformIO: Dynamic Stiffness Control on a Deformable Force-sensing Display},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650772},\ndoi = {10.1145/3613905.3650772},\nabstract = {Introducing DeformIO, a novel deformable display with co-located force input and variable stiffness output. Unlike prior work, our approach does not require pin arrays or re-configurable panels. Instead, we leveraged pneumatics and resistive sensing to enable force detection and stiffness control on a soft continuous surface. This allows users to perceive rich tactile feedback on a soft surface and replicates the benefits of fluid finger movement from traditional glass-based screens. Using a robotic arm, we conducted a series of evaluations with 3,267 trials to quantify the performance of touch and force input, as well as stiffness output. Additionally, our study confirmed users’ ability to apply multiple force inputs simultaneously and distinguish stiffness levels. We illustrate how DeformIO enhances interaction through a vision for everyday interaction and include two implemented self-contained demonstrations.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {98},\nnumpages = {8},\nkeywords = {Deformable Display, Force Input, Pneumatics, Variable Stiffness},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650926,\nauthor = {Guo, Ge (Serena) and Qiu, Zhiwen},\ntitle = {DentAR: Innovating Dental Visits with Sensory Experiences in AR for People with Autism Spectrum Disorder},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650926},\ndoi = {10.1145/3613905.3650926},\nabstract = {In dental clinics, individuals with Autism Spectrum Disorder (ASD) often face heightened anxiety and discomfort due to Sensory Processing Disorders (SPD), exacerbated by the clinical environment. This study introduces DentAR, an Augmented Reality (AR) environment designed to mitigate SPD-related anxiety in dental settings for individuals with ASD. Developed via a participatory design approach with occupational therapists, dentists, and caregivers, DentAR utilized the sensory integration strategy to modify the sensory characteristics of the dental clinic, by employing visual and auditory stimuli to enhance the dental experience for those with SPD. Preliminary expert evaluations suggest potential benefits in using AR for sensory integration, emphasizing the importance of personalization and user empowerment in the design of therapeutic interventions. This research demonstrates AR’s utility in addressing sensory processing challenges in clinical environments, to facilitate accessible therapeutic interventions for diverse sensory needs.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {99},\nnumpages = {7},\nkeywords = {ASD, Augmented Reality (AR), SPD, behavior intervention, context, dental clinic},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650742,\nauthor = {Morita, Takafumi and Sakura, Rei and Aoyama, Kanon and Imamura, Tomomi and Han, Changyo and Kakehi, Yasuaki},\ntitle = {Design and Fabrication for Dynamic Color-Changing on Curved 3D-Printed Surfaces},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650742},\ndoi = {10.1145/3613905.3650742},\nabstract = {Recent studies have presented various methods for changing the color of objects using thermochromic materials. However, many methods face challenges related to heat sources, leading to limitations in object shape, size, and the speed of color change. In this paper, we propose a novel dynamic and rapid color-changing method designed for curved 3D-printed surfaces. We explored the use of Carbo e-Therm, a carbon-based electrically-conductive heating paint, as a novel means of joule heating for color change. We developed a design and fabrication method for color change that integrates specific paint application techniques, electrode configurations, and pretreatment of base materials. This approach enables rapid color changes using thermochromic materials, across diverse 3D-printed substrates and curved surfaces. Furthermore, this work operates as a self-contained system, eliminating the need for bulky external equipment and allowing the object itself to change color. This paper details the proposed method and presents several design examples.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {100},\nnumpages = {7},\nkeywords = {Carbo e-Therm, Color-changing, Heating Paint, Personal Fabrication, Programmable Materials},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650847,\nauthor = {Lai, Zhenchi and Tsai, Wenn-Chieh and Liang, Rung-Huei},\ntitle = {Designing PhenoProbe: An Interview Tool for Inquiring into the Phenological Knowledge of Taiwanese Elders},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650847},\ndoi = {10.1145/3613905.3650847},\nabstract = {With the growing urgency of addressing climate change, HCI and interaction design fields are seeking novel yet practical solutions. Since weather patterns undergo long-term changes, elders’ phenological knowledge and everyday experiences can offer valuable insights. Nonetheless, these insights need to be adequately represented in sustainability discussions. We present the research-through-design process behind PhenoProbe, an interview tool that employs a series of cards inspired by the 24 solar terms. PhenoProbe incorporates Taiwanese seasonal soundscapes with surreal visuals generated through proverb prompts to cultivate meaningful conversations between elders and researchers. The tool is crafted to encourage elders to comfortably engage in and reflect upon their daily weather-related experiences. Our work delivers two main contributions: 1) A tool that engages elders in discussions about phenological patterns and embodied experiences, and 2) An approach that weaves familiar cultural elements into open-ended elicitation materials, fostering the involvement of older adults in the design process.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {101},\nnumpages = {6},\nkeywords = {24 Solar Terms, Cultural Probe, Elders, Elicitation Interview, Phenological Knowledge},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650819,\nauthor = {Ghosh, Samir and Wang, Yuhui and Zhou, William and Lin, Kelly and Mcveigh-Schultz, Joshua and Isbister, Katherine},\ntitle = {Designing Shared VR Tools for Spatial Scientific Sensemaking About Wildfire Evacuation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650819},\ndoi = {10.1145/3613905.3650819},\nabstract = {We posit that scientists who work with spatial data could benefit from shared VR tools that allow them to do immersive sensemaking together to generate new insights and approaches to their work. We engaged with researchers studying wildfire evacuation who currently use 2D spatial simulations to develop prototypes of shared VR interface features to allow them to explore and discuss their data. We present results of preliminary interviews, our design process and mocked-up features (callouts, shared time scrubbing, 3D marquee selection, photospheres, and metaboard), and follow-up feedback from the scientists about these features. Scientists found these features useful, in particular callouts due to ability to indicate objects of interest and examine relevant metadata and offered suggestions for extending and developing the features in future prototyping work.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {102},\nnumpages = {5},\nkeywords = {Scientific Sensemaking, Social VR, Wildfire Resilience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650925,\nauthor = {Zhang, Yaying and Shi, Rongkai and Liang, Hai-Ning},\ntitle = {Designing Stick-Based Extended Reality Controllers: A Participatory Approach},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650925},\ndoi = {10.1145/3613905.3650925},\nabstract = {This work explores the design of stick-shaped tangible user interfaces (TUI) for Extended Reality (XR). While sticks are widely used in everyday objects, their applications as a TUI in XR have not been systematically studied. We conducted a participatory design session with twelve experts in XR and HCI to investigate the affordances of stick-based objects and how to utilize them in XR. As a result, we present a taxonomy of stick-based objects’ affordances and propose three types of stick-based XR controllers and their dynamic variations. The paper discusses design considerations for selecting the appropriate stick-based form in XR TUI design.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {103},\nnumpages = {6},\nkeywords = {Device Form Factor, Extended Reality, Handheld Device, Stick Shape, Tangible User Interface, Taxonomy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650912,\nauthor = {Jones, Brennan and Xu, Yan and Li, Qisheng and Scherer, Stefan},\ntitle = {Designing a Proactive Context-Aware AI Chatbot for People's Long-Term Goals},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650912},\ndoi = {10.1145/3613905.3650912},\nabstract = {When pursuing new complex goals such as fitness or sustainability, people often seek advice from various sources. Large language models (LLMs) such as ChatGPT have recently emerged as popular sources for information seeking, action discovery, and goal planning. However, such tools require users to provide detailed prompts, are not adaptive to the user’s personal attributes or real-time contexts, and are merely reactive to the user’s prompts rather than proactively guiding the user at opportune moments. We share the design of an LLM-based chatbot app that proactively recommends actions to the user for their goals based on context factors that can be detected or inferred by the user’s smartphone (e.g., location, time, weather) and the user’s personal profile. An early pilot field study reveals that participants enjoyed the chatbot as a personal assistant that was adaptable and flexible to their needs and kept them motivated by discovering actions toward their goals.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {104},\nnumpages = {7},\nkeywords = {chatbots, context-aware computing, human-AI interaction, human-agent interaction, language models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650886,\nauthor = {Gammelg\\r{a}rd-Larsen, Anders and van Berkel, Niels and Skov, Mikael B. and Kjeldskov, Jesper},\ntitle = {Designing for Human-AI Interaction: Comparing Intermittent, Continuous, and Proactive Interactions for a Music Application},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650886},\ndoi = {10.1145/3613905.3650886},\nabstract = {Designing effective and user-centred interactions between humans and AI systems poses fundamental challenges. The behaviour of AI systems is complex and uncertain, making it difficult to envision and craft optimal user experiences. Improved frameworks are needed to guide the design of human-AI interaction. In this paper, we develop and evaluate prototypes for a music application, representing three distinct paradigms of human-AI interaction: Intermittent, Continuous, and Proactive. Through qualitative user interviews with 12 participants, we compare the user experience across these prototypes, shedding light on potential challenges and opportunities for the paradigms represented. We found that the three prototypes exhibit distinct characteristics in terms of supported goals and user control. This case study contributes to a deeper understanding of the complexities involved in designing AI systems and offers insights for the development of more user-centred AI applications.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {105},\nnumpages = {8},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650948,\nauthor = {Goyal, Nitesh and Chang, Minsuk and Terry, Michael},\ntitle = {Designing for Human-Agent Alignment: Understanding what humans want from their agents},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650948},\ndoi = {10.1145/3613905.3650948},\nabstract = {Our ability to build autonomous agents that leverage Generative AI continues to increase by the day. As builders and users of such agents it is unclear what parameters we need to align on before the agents start performing tasks on our behalf. To discover these parameters, we ran a qualitative empirical research study about designing agents that can negotiate during a fictional yet relatable task of selling a camera online. We found that for an agent to perform the task successfully, humans/users and agents need to align over 6 dimensions: 1) Knowledge Schema Alignment 2) Autonomy and Agency Alignment 3) Operational Alignment and Training 4) Reputational Heuristics Alignment 5) Ethics Alignment and 6) Human Engagement Alignment. These empirical findings expand previous work related to process and specification alignment and the need for values and safety in Human-AI interactions. Subsequently we discuss three design directions for designers who are imagining a world filled with Human-Agent collaborations.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {106},\nnumpages = {6},\nkeywords = {Agents, Generative AI, Human-AI Alignment, Human-Agent Alignment, Large Language Models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650906,\nauthor = {Shi, Yan and Gong, Lidan and Lu, Yiwen and Liu, Lijuan},\ntitle = {DiSandbox: A Low-cost Digital Sandbox Tool to Support Psychological Analysis and Therapy for Left-behind Children},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650906},\ndoi = {10.1145/3613905.3650906},\nabstract = {This paper introduces DiSandbox, a low-cost digital tool designed to support left-behind children in rural China facing psychological challenges. Comprising a range of miniature molds within a sand tray, children can freely express themselves, with an inbuilt camera capturing their creations. These images are sent to their parents’ mobile application, where an AI assistant interprets the works and assesses the children’s psychological conditions. The platform also offers professional psychological counseling, delivering personalized support. Our research shows that DiSandbox can effectively analyze children’s creations to provide initial psychological assessments, encouraging parents to take greater interest in their children’s mental health.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {107},\nnumpages = {6},\nkeywords = {Digital design, Left-behind children, Psychological assessment tools, Sandbox game, Toolkits},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650977,\nauthor = {Last, Yorick and Geels, Jorrit and Schraffenberger, Hanna},\ntitle = {Digital Dotted Lines: Design and Evaluation of a Prototype for Digitally Signing Documents Using Identity Wallets},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650977},\ndoi = {10.1145/3613905.3650977},\nabstract = {Documents are largely stored and shared digitally. Yet, digital documents are still commonly signed using (copies of) handwritten signatures, which are sensitive to fraud. Though secure, cryptography-based signature solutions exist, they are hardly used due to usability issues. This paper proposes to use digital identity wallets for securely and intuitively signing digital documents with verified personal data. Using expert feedback, we implemented this vision in an interactive prototype. The prototype was assessed in a moderated usability test (N = 15) and a subsequent unmoderated remote usability test (N = 99). While participants generally expressed satisfaction with the system, they also misunderstood how to interpret the signature information displayed by the prototype. Specifically, signed documents were also trusted when the document was signed with irrelevant personal data of the signer. We conclude that such unwarranted trust forms a threat to usable digital signatures and requires attention by the usable security community.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {108},\nnumpages = {11},\nkeywords = {Digital Identity Wallets, Prototyping, Trust},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650768,\nauthor = {Lee, Seung-Jun and Sutthiwanna, Siripon and Lee, Joon Hyub and Bae, Seok-Hyung},\ntitle = {DirActor: Creating Interaction Illustrations by Oneself through Directing and Acting Simultaneously in VR},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650768},\ndoi = {10.1145/3613905.3650768},\nabstract = {In HCI research papers, interaction illustrations are essential to vividly expressing user scenarios arising from novel interactions. However, creating these illustrations through drawing or photography can be challenging, especially when they involve human figures. In this study, we propose the DirActor system that helps researchers create interaction illustrations in VR that can be used as-is or post-processed, by becoming both the director and the actor simultaneously. We reproduced interaction illustrations from past ACM CHI Best and Honorable Mention papers using the proposed system to showcase its usefulness and versatility.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {109},\nnumpages = {6},\nkeywords = {bare hands, interaction illustration, virtual reality, voice command},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650757,\nauthor = {Cohn, Michelle and Bandodkar, Grisha and Sangani, Raj Bharat and Predeck, Kristin and Zellou, Georgia},\ntitle = {Do People Mirror Emotion Differently with a Human or TTS Voice? Comparing Listener Ratings and Word Embeddings},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650757},\ndoi = {10.1145/3613905.3650757},\nabstract = {Speakers increasingly communicate with technology using spoken language and display behaviors from human-human interaction, such as mirroring the pronunciation patterns of text-to-speech (TTS) voices. Yet, the magnitude and directionality of mirroring varies across prior studies comparing human and TTS addressees. The current study uses two approaches to assess mirroring of emotionally expressive speech produced by a human and a TTS voice. We compare AXB perceptual similarity (holistic human listener judgment, n=109 raters), and distance in latent space (wav2vec 2.0) on a dataset of single word shadowing in a cohort of 36 United States English speakers. Results show that both AXB and wav2vec 2.0 capture mirroring toward emotional prosody for both human and TTS voices. We discuss these findings in terms of theories of computer personification and emotional mirroring, as well as their contributions toward methodological advancements in phonetic entrainment.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {110},\nnumpages = {10},\nkeywords = {Embeddings, Emotional Expressiveness, Phonetic Mirroring, Text-to-speech (TTS) Voices},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650862,\nauthor = {Jung, Yongnam and Chen, Cheng and Jang, Eunchae and Sundar, S. Shyam},\ntitle = {Do We Trust ChatGPT as much as Google Search and Wikipedia?},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650862},\ndoi = {10.1145/3613905.3650862},\nabstract = {Although studies, audits, and anecdotal observations have shown that information generated by ChatGPT is not always accurate, many users tend to show unwarranted trust in this new source. Do they consider ChatGPT to be like any other online information source such as Google and Wikipedia, without realizing that generative AI technology creates content that is not necessarily based on facts? Why do they trust information from ChatGPT? Understanding how users perceive content from generative AI tools is crucial because it can help reduce unwarranted trust in inaccurate information and mitigate the spread of misinformation. A focus group and interview study (N=14) revealed that thankfully not all users trust ChatGPT-generated information as much as Google Search and Wikipedia. It also shed light on the primary psychological considerations when trusting an online information source, namely perceived gatekeeping, and perceived information completeness. In addition, technological affordances such as interactivity and crowdsourcing were also found to be important for trust formation. We discuss theoretical and practical implications for design of generative AI interfaces.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {111},\nnumpages = {9},\nkeywords = {ChatGPT, Google, Trust, Wikipedia},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651069,\nauthor = {Gali, Olga and Ercan, Sena Beste and Hepdo\\u{g}an, Deniz and Atherton, Gray and Cross, Liam and Pares, Narcis},\ntitle = {DragonIce, an Initial Full-Body Mixed Reality Experience to Facilitate Interpersonal Synchrony in Children},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651069},\ndoi = {10.1145/3613905.3651069},\nabstract = {This study introduces DragonIce, an initial full-body interactive Mixed Reality experience designed, in collaboration with children, to facilitate Interpersonal Synchrony (IPS) among groups of four children. Through an iterative usability study, involving 48 children (8-10 years old), and a mix-method approach to triangulate findings, the research evaluates the system's effectiveness in (i) user-system interaction; (ii) guiding users' movements for IPS; and (iii) overall satisfaction. This iterative design process unveiled insights into synchronization mechanics, informing the development of more sophisticated and enriched interactive experiences. The findings suggest a promising trajectory for assessing the potential impact of IPS on prosocial behavior in the future. Moreover, the study anticipates future exploration of DragonIce's benefits in creating a supportive social environment, particularly for children facing difficulties in social interaction and motor skills, such as those on the Autism Spectrum.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {112},\nnumpages = {7},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651089,\nauthor = {Rosenberg, Karl Toby and Kazi, Rubaiat Habib and Wei, Li-Yi and Xia, Haijun and Perlin, Ken},\ntitle = {DrawTalking: Towards Building Interactive Worlds by Sketching and Speaking},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651089},\ndoi = {10.1145/3613905.3651089},\nabstract = {We introduce DrawTalking, a prototype system enabling an approach that empowers users to build interactive worlds by sketching and speaking. The approach emphasizes user control and flexibility, and gives programming-like capability without requiring code. An early open-ended study shows the mechanics resonate and are applicable to many creative-exploratory use cases, with the potential to inspire and inform research in future natural interfaces for creative exploration and authoring.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {113},\nnumpages = {8},\nkeywords = {creativity, human-AI collaboration, multimodal, play, programmability, prototyping, sketching},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651045,\nauthor = {Wu, Qin and Xiao, Xiangming and Liu, Yugui and Billinghurst, Mark and Nanayakkara, Suranga},\ntitle = {Early Autism Screening in Children Using Facial Recognition},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651045},\ndoi = {10.1145/3613905.3651045},\nabstract = {Facial data of children aged 2 to 10 years was collected from three institutions in a large city; an autism rehabilitation center, a rehabilitation hospital, and an inclusive kindergarten. The dataset comprised facial data of 65 children diagnosed with autism, and 47 children with typical developmental. We employed the VGG-16 model to develop a facial feature recognition-based early screening system, which involves feature extraction and image processing of eyes, eyebrows, noses, and mouths. The data was processed and categorized using a Convolutional Neural Network (CNN) model, and the accuracy of this algorithm was validated. Cross-testing with the public database Kaggle and our dataset demonstrated an accuracy rate of up to 94\\% for the current training set. This indicates that the model trained by our system is proficient in classifying children’s facial data and maintains high precision on our database.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {114},\nnumpages = {7},\nkeywords = {Algorithm, Assistive technology, Children with autism, Facial recognition, Screening and earlier diagnosis},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651023,\nauthor = {An, Ning and Gui, Fang and Liu, Yang and Chen, Honglin and Yang, Jiaoyun},\ntitle = {EasyTell: A Caregiver-Centered Prototype for Innovating Everyday Eldercare through Bite-Sized Digital Storytelling},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651023},\ndoi = {10.1145/3613905.3651023},\nabstract = {Digital storytelling offers notable benefits for older adults, particularly those with cognitive impairment. Yet, existing digital storytelling methods are often research-oriented and disconnected from eldercare routines. To extend digital storytelling into everyday eldercare for normal aging, we designed EasyTell, a WeChat mini-program prototype through a four-stage, user-centered design process at a community eldercare center in Hefei, China. Beginning with semi-structured interviews with 13 older adults, the design process evolved through three iterative workshops collaborating with 17 caregivers. Our findings underscore the importance of scenario-based prompts in engaging older adults in storytelling and confirm the viability of a bite-sized story collection method featuring recording, real-time transcription, and manual correction. This paper contributes to the HCI field by illustrating the bite-sized digital storytelling, facilitated by scenario-based prompts, can enhance eldercare, highlighting the potential of digital tools to nurture human connection in everyday caregiving scenarios.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {115},\nnumpages = {8},\nkeywords = {Digitial storytelling, Eldercare, Story collection, User-centered design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650905,\nauthor = {Jeong, Jin-Woo and Jeong, Jae-Yeop},\ntitle = {Effect of Onset Position of Ray Casting in Virtual Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650905},\ndoi = {10.1145/3613905.3650905},\nabstract = {In virtual reality (VR), interaction techniques significantly influence user experience and performance. In this study, we explore the effect of the onset position of ray casting in VR through a target selection task. We designed the pointing task (i.e., Fitts’ law task) for data collection with various onset positions. We then analyzed data to explore how different onset positions affect the individuals’ performance for 3D interaction in terms of pointing and selection. Results showed significant effects for each participant yet no generalized impact was found across all participants. Results highlight the complexity of human-computer interaction in ray casting and suggest that a one-size-fits-all approach may not be effective. For future research and practical applications, our findings advocate a personalized approach that takes into account the specific preferences and responses of individual users. This tailored strategy could potentially improve the effectiveness and usability of interactive systems using ray cast techniques.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {116},\nnumpages = {7},\nkeywords = {Fitts’ law, pointing, ray-casting, target selection, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650958,\nauthor = {Kim, Munjeong and Kim, Joongseok and Kim, Sunjun},\ntitle = {Effects of Computer Mouse Lift-off Distance Settings in Mouse Lifting Action},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650958},\ndoi = {10.1145/3613905.3650958},\nabstract = {This study investigates the effect of Lift-off Distance (LoD) on a computer mouse, which refers to the height at which a mouse sensor stops tracking. Although a low LoD is generally preferred to avoid unintended cursor movement in mouse lifting (=clutching), especially in first-person shooter games, it may increase tracking errors. We conducted a psychophysical experiment to measure the perceptible differences between different LoD settings, and we quantitatively measured unintended cursor movement and tracking errors at four levels of LoD while users performed mouse lifting. The results quantified the amount of the two types of errors, which revealed the trade-off between them in the varying levels of LoD. Our findings offer valuable information on optimal LoD settings, which could serve as a guide for choosing a proper mouse device for enthusiastic gamers.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {117},\nnumpages = {7},\nkeywords = {Clutching, Gaming, Lift-off Distance, LoD, Mouse, Mouse lifting, Pointing, User Performance},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650939,\nauthor = {Zhou, Shuo and Segawa, Norihisa},\ntitle = {Electrical Muscle Stimulation-Based Approach for Enhancing Hand-eye Coordination Training},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650939},\ndoi = {10.1145/3613905.3650939},\nabstract = {Hand-eye coordination refers to the harmonization between visual information and hand movements. In this coordination, the information perceived by the eyes is used to guide the movements of the hands. When faced with an unfamiliar activity, it is challenging to develop coordination between the hands in a short period. This study proposes a method for hand coordination training using Electrical Muscle Stimulation (EMS). By employing EMS, it is possible to enhance attention distribution during bilateral hand movements and improve hand-eye coordination, potentially enabling individuals to train hand coordination skills in a shorter period. Ten participants performed a series of experiments with and without EMS. The results showed that the use of EMS rapidly improved participants’ hand-eye coordination skills during a customized training program.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {118},\nnumpages = {6},\nkeywords = {Attention, EMS, Electrical muscle stimulation, Hand Control, Hand-eye Coordination, Motion Perception, Sensorimotor, Training},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650998,\nauthor = {Deshpande, Manoj and Magerko, Brian},\ntitle = {Embracing Embodied Social Cognition in AI: Moving Away from Computational Theory of Mind},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650998},\ndoi = {10.1145/3613905.3650998},\nabstract = {As artificial intelligence becomes more integral to daily life, the need to design AI systems capable of understanding human interactions is increasingly important. This paper delves into the integration of social cognition in AI, tracing back to its historical foundations and examining seminal theories like Newell’s Bands of Cognition, Minsky’s Society of Mind, etc., which have emphasized the importance of social cognition since AI’s inception. We highlight the shortcomings of traditional computational theory of mind approaches, particularly in their failure to capture the embodied nature of social cognition. Advocating for including embodied socio-cognitive perspectives, we draw on theories such as Participatory Sensemaking and frameworks like Observable Creative Sensemaking. The paper further demonstrates the practical implementation of these concepts in AI through two case studies: one in co-creative dance AI and another in text-to-image generative AI systems.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {119},\nnumpages = {7},\nkeywords = {AI, Embodiment, Sensemaking, Social Cognition},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650864,\nauthor = {Sun, Jiaming and Li, Zhuying and Peng, Xiaolan},\ntitle = {EmoEcho: Designing Emotion Mimicry Mechanics for Enhancing Social Engagement in Digital Games},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650864},\ndoi = {10.1145/3613905.3650864},\nabstract = {Social interaction plays a critical role in enhancing player experiences in bodily play. Although previous research has extensively explored physical mimicry in movement-based games for social engagement, the incorporation of emotional expressions, a significant aspect of bodily interaction, remains underexplored. This study aims to broaden the understanding of bodily play by integrating emotion mimicry, examining its impact on enhancing social connections. We present “EmoEcho”, a two-player 2D side-scrolling game that embeds emotion mimicry into its core gameplay. EmoEcho allows players to dynamically alter the game environment using facial expressions, fostering a unique shared interactive experience. A pilot study compares emotion-based and keyboard inputs, revealing that emotion input significantly enhances the social dynamics of gameplay. This paper contributes to digital play by highlighting the potential of emotion mimicry in enhancing social connections, paving the way for more emotionally engaging and socially resonant gaming experiences.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {120},\nnumpages = {6},\nkeywords = {Emotion Input, Facial Expression, Games, Social Games},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650987,\nauthor = {Hautasaari, Ari and Aramaki, Minami and Chujo, Rintaro and Naemura, Takeshi},\ntitle = {EmoScribe Camera: A Virtual Camera System to Enliven Online Conferencing with Automatically Generated Emotional Text Captions},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650987},\ndoi = {10.1145/3613905.3650987},\nabstract = {Ideally, for lively discussions to occur during online meetings, the participants should turn on both their camera and microphone. In practice this is not always possible, and meeting participants may opt to use a text chat to communicate their ideas and reactions instead. However, text messages are also time-consuming and labor-intensive to type as well as omit many of the emotional cues available through visual and audio channels. To address these issues, we propose EmoScribe Camera, a virtual camera system that generates images of automatic text captions in real time and outputs them as a software-based virtual camera that simulates a physical camera. We report on the results of a user study evaluating the efficacy of EmoScribe Camera as an alternative communication channel during online conferences when participants have their camera and microphone turned off.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {121},\nnumpages = {7},\nkeywords = {automated speech recognition, caption, emotion, font, online conferencing, virtual camera},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650754,\nauthor = {Sabnis, Nihar and Nagashima, Tomohiro},\ntitle = {Empowering Learners: Chatbot-Mediated 'Learning-by-Teaching'},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650754},\ndoi = {10.1145/3613905.3650754},\nabstract = {Chatbots and online learning platforms provide synthesized information to learners. However, research shows learning is particularly effective when learners themselves teach someone. Prior work has explored an interactive instructional approach called ‘Learning-by-teaching’, but this approach traditionally relies on human counterparts, limiting it to their interest and co-located settings. To overcome these limitations, we investigated whether we can empower learners using chatbot-mediated ‘learning-by-teaching.’ We designed an agnostic, open-source chatbot replicating a virtual student, to which learners teach to learn. We conducted an experiment involving 24 students to evaluate the effectiveness of chatbot-mediated teaching compared to textbook-based problem-solving practice. Results indicate that teaching the chatbot benefits student learning than textbook-based problem-solving. This work highlights the effectiveness of chatbots, envisioning their design as virtual students to mediate ‘learning-by-teaching’.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {122},\nnumpages = {9},\nkeywords = {Learning-by-Teaching, chatbots, teachable agents, virtual students},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651122,\nauthor = {Park, Minju and Kim, Sojung and Lee, Seunghyun and Kwon, Soonwoo and Kim, Kyuseok},\ntitle = {Empowering Personalized Learning through a Conversation-based Tutoring System with Student Modeling},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651122},\ndoi = {10.1145/3613905.3651122},\nabstract = {As the recent Large Language Models(LLM’s) become increasingly competent in zero-shot and few-shot reasoning across various domains, educators are showing a growing interest in leveraging these LLM’s in conversation-based tutoring systems. However, building a conversation-based personalized tutoring system poses considerable challenges in accurately assessing the student and strategically incorporating the assessment into teaching within the conversation. In this paper, we discuss design considerations for a personalized tutoring system that involves the following two key components: (1) a student modeling with diagnostic components, and (2) a conversation-based tutor utilizing LLM with prompt engineering that incorporates student assessment outcomes and various instructional strategies. Based on these design considerations, we created a proof-of-concept tutoring system focused on personalization and tested it with 20 participants. The results substantiate that our system’s framework facilitates personalization, with particular emphasis on the elements constituting student modeling. A web demo of our system is available at http://rlearning-its.com.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {123},\nnumpages = {10},\nkeywords = {Intelligent Tutoring System, Personalized Learning, Student Modeling},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651048,\nauthor = {Milton, Matthew Charles and FakhrHosseini, Shabnam and Lee, Chaiwoo and Coughlin, Joseph},\ntitle = {Empowering Robot Designers: A Digital Tool for Early-Stage Social Robot Prototyping and Communication},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651048},\ndoi = {10.1145/3613905.3651048},\nabstract = {While social robots promise to benefit people with emotional and informational needs, persistent concerns exist around their ability to effectively manage expectations and meet user needs. In this study, a digital toolkit was developed as a possible solution to support the early-stage design of social robots and address the current absence of a structured, research-informed approach. The prototype toolkit provides an interactive and visual process to assist designers of different backgrounds and varying degrees of technical knowledge; and to guide the development team throughout iterative ideation, prototyping, and communication regarding new social robot designs. The toolkit employs established human-robot interaction principles and insights from existing research to incorporate a guiding framework in the design process. This paper presents key prototype features, an outline of the user flow, a preliminary expert evaluation of the tool’s concept and usability, and future aims.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {124},\nnumpages = {9},\nkeywords = {Creativity Support, HCI for Development, Prototyping, User Experience Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650805,\nauthor = {Dayanandan, Kailas and Lall, Brejesh},\ntitle = {Enabling Multi-modal Conversational Interface for Clinical Imaging},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650805},\ndoi = {10.1145/3613905.3650805},\nabstract = {Human-computer interaction research has to play a vital role in increasing the adoption of deep learning models in clinical settings, as their adoption is low despite models surpassing/matching the clinician’s performance on many medical imaging tasks. Conversational AI has been successful as an interface for general information; however, there is a research gap for multi-modal conversational interface design for safety-critical clinical imaging systems. Our research points to the important role of multi-modal chat in improving usability and explainability through textual and visual explanations. Our main contributions include design principles for conversational interfaces in clinical imaging systems, the importance of multi-modal responses, and an understanding of the usefulness of mimicking clinician/radiologist interactions to improve usability. We show that diagnosis descriptions and visual responses improve the multi-modal conversational interface. The multi-modal conversational interface can help improve the adoption of deep learning systems in clinical settings, improving clinicians’ efficiency and patient outcomes.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {125},\nnumpages = {13},\nkeywords = {Chest X-Ray, Conversational AI, Generative AI, Healthcare},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650983,\nauthor = {Steenstra, Ian and Murali, Prasanth and Perkins, Rebecca B. and Joseph, Natalie and Paasche-Orlow, Michael K and Bickmore, Timothy},\ntitle = {Engaging and Entertaining Adolescents in Health Education Using LLM-Generated Fantasy Narrative Games and Virtual Agents},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650983},\ndoi = {10.1145/3613905.3650983},\nabstract = {Games have been successfully used to provide engaging health interventions for adolescents. However, translating health education goals into a playable game has historically taken many person-months of effort, involving game designers, scriptwriters, and artists. This work presents an exploratory study into rapidly developing physician-validated health education games for adolescents using virtual agents and LLMs. We evaluated this approach in an intervention to promote Human Papillomavirus (HPV) vaccination among adolescents, as lack of knowledge and vaccine hesitancy contribute to suboptimal HPV vaccination rates. We conducted a between-subjects randomized study comparing a fantasy narrative game to a non-gamified pedagogical virtual agent, with both interventions conveying the same HPV information. Among our study’s 9-12-year-old adolescent participants, our findings demonstrate large pre-to-post improvements in HPV knowledge for both conditions. The gamified intervention showed higher engagement and entertainment than the pedagogical agent based on participant interviews, demonstrating that gamification enriched the educational experience for adolescents.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {126},\nnumpages = {8},\nkeywords = {Adolescents, ChatGPT, Gamification, Image Generation, Large Language Models (LLMs), Midjourney, Serious Games, Vaccination Promotion, Virtual Agents},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650994,\nauthor = {Polidoro, Fabiola and Liu, Yu and Craig, Paul},\ntitle = {Enhancing Mobile Visualisation Interactivity: Insights on a Mixed-fidelity Prototyping Approach},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650994},\ndoi = {10.1145/3613905.3650994},\nabstract = {The dynamic nature of mobile devices presents significant challenges for mobile information visualisation design. The limited screen space of mobile devices requires increased user interaction to effectively accommodate information, posing challenges for touch interaction design. In turn, gesture design must balance ergonomics, adaptability to diverse contexts, and intuitive user experiences. Early design evaluation through low-fidelity prototypes is essential for detecting usability issues in a cost-effective and timely manner. To compensate the limited support for touch gestures and animated transitions offered by existing prototyping tools, we are developing a proof-of-concept for a tool that facilitates the creation of interactive animated prototypes of mobile visualisations. This paper presents the results of an exploratory test conducted on a preliminary version of our tool. We report four reflections about the challenges emerged during testing that, we believe, can foster further discussions and research towards obtaining tools specifically tailored to mobile infovis prototyping.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {127},\nnumpages = {7},\nkeywords = {Exploratory Test, Interaction Design, Mobile Information Visualisation, Prototyping},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651068,\nauthor = {Kumar, Chandan and Saini, Bhupender Kumar and Staab, Steffen},\ntitle = {Enhancing Online Meeting Experience through Shared Gaze-Attention},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651068},\ndoi = {10.1145/3613905.3651068},\nabstract = {Eye contact represents a fundamental element of human social interactions, providing essential non-verbal signals. Traditionally, it has played a crucial role in fostering social bonds during in-person gatherings. However, in the realm of virtual and online meetings, the capacity for meaningful eye contact is often compromised by the limitations of the platforms we use. In response to this challenge, we present an application framework that leverages webcams to detect and share eye gaze attention among participants. Through the framework, we organized 13 group meetings involving a total of 43 participants. The results highlight that the inclusion of gaze attention can enrich interactive experiences and elevate engagement levels in online meetings. Additionally, our evaluation of two levels of gaze sharing schemes indicates that users predominantly favor viewing gaze attention directed toward themselves, as opposed to visualizing detailed attention, which tends to lead to distraction and information overload.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {128},\nnumpages = {6},\nkeywords = {collaboration, eye contact, gaze attention, social interaction, virtual meeting},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651104,\nauthor = {Wang, Jindi and Ivrissimtzis, Ioannis and Li, Zhaoxing and Shi, Lei},\ntitle = {Enhancing User Experience in Chinese Initial Text Conversations with Personalised AI-Powered Assistant},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651104},\ndoi = {10.1145/3613905.3651104},\nabstract = {ABSTRACT In the rapidly evolving landscape of text-based communication, the importance of the initial interaction phase remains paramount. This study investigates the potential benefits that a proposed AI chat assistant equipped with text recommendation and polishing functionalities can bring during initial textual interactions. The system allows the users to personalise the language style, choosing between humorous and respectful. They can also choose between three different levels of AI extraversion to suit their preferences. Results of user evaluations indicate the system received a “good” usability rating, affirming its effectiveness. Users reported heightened comfort levels and increased willingness to continue interactions when using the AI chat assistant. The analysis of the results offers insights into harnessing AI to amplify user engagement, especially in the critical initial stage of textual interaction.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {129},\nnumpages = {7},\nkeywords = {Chat assistant, Computer-Assisted Human Interaction, Conversational AI, Personalisation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650826,\nauthor = {Suzuki, Takashi},\ntitle = {Enhancing the Human Tactile Sensitivity through an Individually Optimized Imperceptible Vibration Stimulation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650826},\ndoi = {10.1145/3613905.3650826},\nabstract = {Stochastic Resonance (SR) is known as a method to increase tactile sensitivity by applying weak vibrations. We investigated how the tactile sensitivity can be enhanced using sinusoidal vibrations with a specific frequency instead of conventionally applied white noise. The results demonstrated that sensory sensitivity with sinusoidal vibrations between 200 and 400 Hz increased much more than that obtained with white noise vibrations. To confirm the enhanced sensitivity effects, we propose a test based on cloth discrimination. Accordingly, it was found that the average correct rate increased using vibrations generated with optimal frequency compared to white noise vibrations. Notably, some participants stated a significant increase in tactile sensitivity when using the optimal vibration for SR. As a result, we showed that by using the optimal sinusoidal wave noise for each participant, it is possible to improve the tactile sensitivity far beyond that achieved with conventional SR using white noise.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {130},\nnumpages = {6},\nkeywords = {cloth discrimination, fingertip, sinusoidal vibration, stochastic resonance, tactile, tactile sensitivity, vibration, white noise},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651088,\nauthor = {Anderson, Barrett R and Shah, Jash Hemant and Kreminski, Max},\ntitle = {Evaluating Creativity Support Tools via Homogenization Analysis},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651088},\ndoi = {10.1145/3613905.3651088},\nabstract = {The evaluation of creativity support tools (CSTs)—software systems intended to support human creativity—remains an open problem, in part because creativity is fundamentally difficult to define and in part because different CSTs target a wide variety of different creative domains. We propose a new, general-purpose evaluation criterion for CSTs: namely, the extent to which a CST homogenizes the creative output of its users. We also demonstrate one way to conduct this kind of homogenization analysis, leveraging semantic similarity between embeddings of users’ creative outputs to quantify the degree of homogenization that a CST induces.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {131},\nnumpages = {7},\nkeywords = {creativity support tools, divergent ideation, large language models, user study},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651027,\nauthor = {Yoon, Suhwoo and Kim, Soobin and Park, Gyeryeong and Lim, Hajin},\ntitle = {Evaluating How Desktop Companion Robot Behaviors Influence Work Experience and Robot Perception},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651027},\ndoi = {10.1145/3613905.3651027},\nabstract = {Desktop companion robots have attracted increasing attention, yet their application to support office workers requires further exploration. In envisioning the role of desktop companion robots as a helpful “peer” for office workers, we conducted a lab experiment (N=36) to examine how desktop companion robots’ behavior influenced office workers’ productivity, work experience, and robot perception. Participants performed a digitization task alongside robots under three conditions: (a) static robot, displaying no interactive behaviors; (b) work-along robot, mirroring the participants’ work/rest patterns; and (c) work-opposite robot, active during participants’ rest times and inactive during work. Results demonstrated that each robot condition led to distinct perceptions of the robot (e.g., as a pet, workmate, or supervisor) and peer effects (i.e., peer presence, peer support, and peer pressure), significantly influencing participants’ task experience. Our findings offer design implications for designing future desktop companion robots to better support office workers.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {132},\nnumpages = {7},\nkeywords = {Desktop companion robot, peer effect, productivity, work experience, workplace},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650896,\nauthor = {Omidvar Tehrani, Behrooz and M, Ishaani and Anubhai, Anmol},\ntitle = {Evaluating Human-AI Partnership for LLM-based Code Migration},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650896},\ndoi = {10.1145/3613905.3650896},\nabstract = {The potential of Generative AI, especially Large Language Models (LLMs), to transform software development is remarkable. In this paper, we focus on one area in software development called “code migration”. We define code migration as the process of transitioning the language version of a code repository by converting both the source code and its dependencies. Carefully designing an effective human-AI partnership is essential for boosting developer productivity and faster migrations when performing code migrations. Though human-AI partnerships have been generally explored in the literature, their application to code migrations remains largely unexamined. In this work, we leverage an LLM-based code migration tool called Amazon Q Code Transformation to conduct semi-structured interviews with 11 participants undertaking code migrations. We discuss human’s role in the human-AI partnership (human as a director and a reviewer) and define a trust framework based on various model outcomes to earn trust with LLMs. The guidelines presented in this paper offer a vital starting point for designing human-AI partnerships that effectively augment and complement human capabilities in software development with Generative AI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {133},\nnumpages = {8},\nkeywords = {Application Modernization, Code Migration, Human-AI Partnership, Human-in-the-Loop Techniques, Trust Framework},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650870,\nauthor = {Feng, Yuan-Ling and Gao, Mingyue and Li, Chih-Heng and Yao, Zhihao and Mi, Haipeng},\ntitle = {ExBreath: Explore the Expressive Breath System as Nonverbal Signs towards Semi-unintentional Expression},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650870},\ndoi = {10.1145/3613905.3650870},\nabstract = {Biosignals, functioning as additional nonverbal signs, have the potential to introduce novel social experiences. The content and presentation forms of biosignals play a crucial role in shaping the final social behavior and user experience. Initiating with the exploration of expressive breath display, we developed ExBreath—an organ-like, inflatable silicon device, which was subsequently utilized in an exploratory study. Based on the findings, breath elicits semi-unintentional expressions, where participants trust that the signals from ExBreath represent the other’s true state, while they deliberately controlled their breath to fulfill various social purposes. Moreover, design opportunities for breath-based interactive systems are revealed, including expanding harmonious communication space, reducing social pressure, and eliciting reflection on cultural norms.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {134},\nnumpages = {7},\nkeywords = {Affective Communication, Breath, Breath Sharing, Collocated Interaction, Pneumatic Display, Self-Expression, Social Wearable},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651010,\nauthor = {Seo, Kiyeal and Rhim, Jungwook and Gweon, Gahgene},\ntitle = {Examining the Impact of Video Reality-level to Support Transition from Screen Time to Screen-Free Time (TSSF)},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651010},\ndoi = {10.1145/3613905.3651010},\nabstract = {Transition from Screen time to Screen-Free time (TSSF) refers to stopping video-watching and transiting to other offline activities. Motivated from priming effect, we explore how video reality-level, which is the degree of using real elements in the video such as the backgrounds and characters, can affect children's TSSF. Children aged 3-10 were assigned to watch videos with three different reality-levels: low, mid, and high. A total of 83 parent-child dyads participated in a two-week between-subject experimental study. We analyzed four different aspects of TSSF: transition rate, problematic behavior, screen time dependence, and rule compliance. Our experiment showed that compared to children who watched low-reality videos, children who watched high-reality videos exhibited more successful TSSF in three aspects: transition rate, problematic behavior and screen time dependence.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {135},\nnumpages = {6},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650815,\nauthor = {Aseniero, Bon Adriel and Lee, Michael and Wang, Yi and Zhou, Qian and Shahmansouri, Nastaran and Goldstein, Rhys},\ntitle = {Experiential Views: Towards Human Experience Evaluation of Designed Spaces using Vision-Language Models},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650815},\ndoi = {10.1145/3613905.3650815},\nabstract = {Experiential Views is a proof-of-concept in which we explore a method of helping architects and designers predict how building occupants might experience their designed spaces using AI technology based on Vision-Language Models. Our prototype evaluates a space using a pre-trained model that we fine-tuned with photos and renders of a building. These images were evaluated and labeled based on a preliminary set of three human-centric dimensions that characterize the Social, Tranquil, and Inspirational qualities of a scene. We developed a floor plan visualization and a WebGL-based 3D-viewer that demonstrate how architectural design software could be enhanced to evaluate areas of a built environment based on psychological or emotional criteria. We see this as an early step towards helping designers anticipate emotional responses to their designs to create better experiences for occupants.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {136},\nnumpages = {7},\nkeywords = {architectural design, human-centric building design, vision-language models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650930,\nauthor = {Elahi, Md Fazle and Li, Tianyi and Tian, Renran},\ntitle = {Exploring Collective Theory of Mind on Pedestrian Behavioral Intentions},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650930},\ndoi = {10.1145/3613905.3650930},\nabstract = {While crowdsourcing is commonly used for objective labeling, eliciting subjective annotations, like estimating mental states or perception of other’s intention, remains challenging. This study investigates crowdsourcing’s potential to predict pedestrian behavioral intentions. We recruited 120 participants to predict pedestrian intentions at different prediction horizons in 24 diverse videos. Our findings revealed that the status-quo bias significantly impacts intention estimation. Specifically, when asked what status the pedestrian will be, predictions inclined towards current state’s continuation over transition, with an overall accuracy of 53\\% at one-second prediction length on a balanced dataset. Rephrasing the annotation question mitigates this bias and improved the estimation accuracy to 79\\% for one-second ahead predictions, though accuracy drops with longer horizons and is affected by pedestrian actions and contextual information. Overall, this study provides insights into the factors affecting collective estimation of pedestrian intentions and aims to improve crowdsourcing cognitive labels for training better AV-pedestrian interaction algorithms.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {137},\nnumpages = {8},\nkeywords = {Behavior Prediction, Crowdsourcing, Human Cognitive State},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650859,\nauthor = {Park, Miran and Park, Kyuri and Cho, Hyewon and Choi, Hwan and Lim, Hajin},\ntitle = {Exploring Design Approaches for Reducing Viewers' Discomfort with Distressing Short-form Videos},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650859},\ndoi = {10.1145/3613905.3650859},\nabstract = {The growing popularity of short-form video platforms and their reliance on algorithmic recommendations highlights the risk of viewers unintentionally encountering distressing content. Therefore, we investigated viewers’ experiences with distressing content and developed design approaches to alleviate their discomfort. Through in-depth interviews, we discovered that participants perceived and reacted differently to “socially inappropriate content,” which violated societal norms, and to “personally discomforting content,” which triggered negative reactions on a personal level. Further, participants expressed frustration with the lack of transparency in content reporting processes, the challenges in tailoring recommendation algorithms to avoid distressing content, and the limitations of post-exposure feedback mechanisms. To address these challenges, we conceptualized three design approaches focused on enhancing reporting process transparency, providing users with granular control over content recommendations, and allowing for preemptive adjustments to their content feeds. Our findings and proposed design approaches may provide valuable directions for improving viewer well-being on short-form video platforms.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {138},\nnumpages = {8},\nkeywords = {algorithmic experience, algorithmic recommendation, content curation algorithm, content filtering, digital well-being, discomfort, distress, short-form videos, video platform},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650911,\nauthor = {Ji, Ruihua and Chang, Zhuang and Wang, Shuxia and Billinghurst, Mark},\ntitle = {Exploring Effective Real-Time Ergonomic Guidance Methods for Immersive Virtual Reality Workspace},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650911},\ndoi = {10.1145/3613905.3650911},\nabstract = {Studying and working in Virtual Reality (VR) provides an immersive experience and enables free body movement for input. However, poor working posture in VR can result in discomfort and even lead to musculoskeletal problems over time. In this work, we explore how to use Mediapipe Blazepose to provide effective real-time ergonomic posture guidance methods in a virtual office environment using (1) auditory, (2) visual, and (3) combined auditory-visual cues. We do this in a within-subject design user study comparing these three conditions and a baseline condition where no guidance is provided. Our results indicate that all three guidance conditions significantly improved users’ ergonomic posture maintenance compared with the baseline condition. However, we found that the combined auditory-visual guidance is the most effective and preferred method. We also discuss multi-modal interaction methods, privacy concerns, and directions for future research.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {139},\nnumpages = {6},\nkeywords = {Ergonomic guidance, Immersive Workspace, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650777,\nauthor = {Yao, Haonan and Zhao, Lixiang and Liang, Hai-Ning and Liu, Yu and Li, Yue and Yu, Lingyun},\ntitle = {Exploring Embodied Asymmetric Two-Handed Interactions for Immersive Data Exploration},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650777},\ndoi = {10.1145/3613905.3650777},\nabstract = {Embodied interaction plays a crucial role in facilitating effective data exploration within immersive environments, enhancing user experience, understanding, and exploring complex data presented in the virtual space. While embodied two-handed interaction has demonstrated considerable potential, there remains a gap in understanding how varying levels of embodiment impact asymmetric two-hand interactions for immersive data exploration. In this study, we systematically investigate this aspect by combining three settings (direct, indirect, and fixed) on the visualization control hand and two settings (direct, indirect) on the action hand. This combination results in six conditions that span varying levels of embodiment. We compared these conditions under two fundamental visualization tasks, focusing on curve brushing and object manipulation. Our discussion revolves around the use of techniques related to the specific requirements of the tasks, the characteristics of each condition, and users’ experience and expertise in the VR environment. Building upon these discussions, we offer suggestions for designing embodied two-handed interactions for immersive data exploration.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {140},\nnumpages = {10},\nkeywords = {Embodied interaction, immersive data exploration, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651086,\nauthor = {Dong, Haoyu and Tran, Tram Thi Minh and Verstegen, Rutger and Cazacu, Silvia and Gao, Ruolin and Hoggenm\\\"{u}ller, Marius and Dey, Debargha and Franssen, Mervyn and Sasalovici, Markus and Bazilinskyy, Pavlo and Martens, Marieke},\ntitle = {Exploring Holistic HMI Design for Automated Vehicles: Insights from a Participatory Workshop to Bridge In-Vehicle and External Communication},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651086},\ndoi = {10.1145/3613905.3651086},\nabstract = {Human–Machine Interfaces (HMIs) for automated vehicles (AVs) are typically divided into two categories: internal HMIs for interactions within the vehicle, and external HMIs for communication with other road users. In this work, we examine the prospects of bridging these two seemingly distinct domains. Through a participatory workshop with automotive user interface researchers and practitioners, we facilitated a critical exploration of holistic HMI design by having workshop participants collaboratively develop interaction scenarios involving AVs, in-vehicle users, and external road users. The discussion offers insights into the escalation of interface elements as an HMI design strategy, the direct interactions between different users, and an expanded understanding of holistic HMI design. This work reflects a collaborative effort to understand the practical aspects of this holistic design approach, offering new perspectives and encouraging further investigation into this underexplored aspect of automotive user interfaces.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {141},\nnumpages = {9},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650937,\nauthor = {Xiao, Ruiwei and Hou, Xinying and Stamper, John},\ntitle = {Exploring How Multiple Levels of GPT-Generated Programming Hints Support or Disappoint Novices},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650937},\ndoi = {10.1145/3613905.3650937},\nabstract = {Recent studies have integrated large language models (LLMs) into diverse educational contexts, including providing adaptive programming hints, a type of feedback focuses on helping students move forward during problem-solving. However, most existing LLM-based hint systems are limited to one single hint type. To investigate whether and how different levels of hints can support students’ problem-solving and learning, we conducted a think-aloud study with 12 novices using the LLM Hint Factory, a system providing four levels of hints from general natural language guidance to concrete code assistance, varying in format and granularity. We discovered that high-level natural language hints alone can be helpless or even misleading, especially when addressing next-step or syntax-related help requests. Adding lower-level hints, like code examples with in-line comments, can better support students. The findings open up future work on customizing help responses from content, format, and granularity levels to accurately identify and meet students’ learning needs.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {142},\nnumpages = {10},\nkeywords = {GPT, Help-seeking, Introductory Programming, Large Language Model, Programming Hint},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650933,\nauthor = {Mo, Wen and Singh, Aneesha and Holloway, Catherine},\ntitle = {Exploring Information Needs for Tracking to Support Using Wheelchairs in Everyday Life},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650933},\ndoi = {10.1145/3613905.3650933},\nabstract = {Past research on wheelchair user tracking technologies has emphasized physical activity support, upper body pain alleviation, and accessibility mapping. However, little is known about what information users consider important in tracking to support their daily use of wheelchairs. To address this gap, we took a holistic view through an online survey (53 responses) and discovered the overall need to track beyond accessibility and physical activities, including a keen interest in monitoring ‘wheelchair health’, social causes, and concerns regarding data accountability for policy change. Our study contributes by delineating the unmet information needs in wheelchair tracking and advocating for more research interests to develop and design tracking tools in Human-Computer Interactions (HCI) that enrich the everyday experiences of wheelchair users.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {143},\nnumpages = {7},\nkeywords = {Accessibility, Inclusive Design, Mobility Impairment, Personal Informatics, Survey, Tracking Technologies, Wheelchairs},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650990,\nauthor = {Yang, Yuewen and Viranda, Thalia and Van Meter, Anna R. and Choudhury, Tanzeem and Adler, Daniel A.},\ntitle = {Exploring Opportunities to Augment Psychotherapy with Language Models},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650990},\ndoi = {10.1145/3613905.3650990},\nabstract = {Natural language data, like patient narratives, are crucial in psychotherapy, yet psychotherapists face challenges using these qualitative data to tailor treatment to patient needs. Innovations in natural language processing, including breakthroughs in language models (LMs), show opportunities like summarizing conversational data into quantitative information. In this study, we investigated how LM-based tools can augment patient measurements and treatment delivery in psychotherapy. Through formative interviews and design provocation sessions with a total of six psychotherapists, we identified three opportunities: 1) to quantify and summarize extensive qualitative data for easier retrieval and monitoring of treatment progress; 2) to give clinicians a structured tool to support their patients’ learning; and 3) to facilitate treatment personalization. Our findings suggest that LM-based tools can potentially facilitate data-driven clinical practice, reduce cognitive and administrative burdens, and improve treatment quality. Additionally, our research paves the way for developing LM-based tools that can be integrated into psychotherapy.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {144},\nnumpages = {8},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650761,\nauthor = {Zheng, Nianzhao and Li, Jialong and Li, Nianyu and Zhang, Mingyue and Cai, Jinyu and Tei, Kenji},\ntitle = {Exploring Optimal eHMI Display Location for Various Vehicle Types: A VR User Study},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650761},\ndoi = {10.1145/3613905.3650761},\nabstract = {External human-machine interfaces (eHMI) are anticipated to enhance pedestrian interactions with automated vehicles (AVs) and increase social acceptance. While previous studies have focused on enhancing eHMI design by exploring various modalities and color preferences, identifying the optimal eHMI location on vehicles remains a significant challenge. Existing research has begun to explore different display locations for eHMI, but a thorough examination of the factors related to vehicle type, such as vehicle size, that influence pedestrian preferences is still lacking. Therefore, we conducted a Virtual Reality user study to assess pedestrian reactions to various AVs equipped with eHMI in different positions. Our findings reveal that larger vehicles are perceived as less safe by pedestrians, yet their increased visibility from a distance, due to longer light bands, impacts pedestrians’ crossing decisions. Additionally, the location and height of the eHMI relative to eye level are crucial, with inappropriate positioning leading to diminished effectiveness.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {145},\nnumpages = {7},\nkeywords = {display location, eHMI, vehicle type, vehicle-pedestrian interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650857,\nauthor = {Shindo, Masato and Aoki, Ryosuke},\ntitle = {Exploring Potential of Electromyography-based Avatar Operation Using Residual Muscles of ALS Individuals: Case Study on Avatar DJ Performance},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650857},\ndoi = {10.1145/3613905.3650857},\nabstract = {Assistive medical device, such as gastrostomy and ventilation, plays a crucial role in extending the lifespan of individuals with amyotrophic lateral sclerosis (ALS). Additionally, augmentative and alternative communication (AAC) systems are essential for maintaining their ability to communicate. Designing an effective AAC system requires balancing the user’s expressive capability with the usability of the user interface. In this study, we focus on not gaze input and electroencephalograph (EEG) signal input commonly used in AAC systems but electromyography (EMG) signal input as a novel modality to augment their embodiment into a virtual space. Since EMG signals can be measured even with slight movements of residual muscles, EMG-based AAC systems provide a potential avenue for communication. We successfully demonstrated the feasibility of our EMG-based avatar communication system by enabling Masatane Muto, a DJ musician and creator living with ALS, to perform live remotely from Japan to the Ars Electronica Festival 2023 in Linz, Austria using the system. Through this case study and a pilot study, we aimed to explore the potential of EMG-based AAC systems by discussing three perspectives: augmented embodiment, prolonged use, and extensibility into real environments in diverse contexts.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {146},\nnumpages = {7},\nkeywords = {amyotrophic lateral sclerosis (ALS), avatar operation, electromyography},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651095,\nauthor = {Li, Lingyuan and Freeman, Guo and Duan, Wen},\ntitle = {Exploring Redesigning Digital P2P Payments to Facilitate Social Connections: A Participatory Design Approach},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651095},\ndoi = {10.1145/3613905.3651095},\nabstract = {Existing work has highlighted the increasingly important role of digital peer-to-peer (P2P) payments (i.e., money transactions between individuals via online or mobile services) in our social lives. Indeed, such transactions continue to shape how we deal with money when interacting with each other. Still, little research has explored how these insights are understood by actual digital P2P payments users and can be translated into practical designs to improve current digital P2P payment systems. As our first endeavor to address this gap, the study adopted a Participatory Design approach to investigate how actual users envision and redesign future digital P2P payments to better facilitate their social connections with known contacts. Our study contributes to the current HCI discourse on the increasing interplay of financial technology and social connections by (1) uncovering nuanced user perceptions of digital P2P payments in everyday social interactions and (2) offering creative and actionable future directions to redesign digital P2P payments to further facilitate social connections based on user-generated design envisions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {147},\nnumpages = {8},\nkeywords = {digital P2P payments, interpersonal relationships, participatory design, research through design, social connections},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650924,\nauthor = {Tong, Wai and Xia, Meng and Qu, Huamin},\ntitle = {Exploring Stage Lighting Education in Metaverse},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650924},\ndoi = {10.1145/3613905.3650924},\nabstract = {This paper investigates stage lighting education in the metaverse from a practical perspective. We conducted participatory design with practitioners and stakeholders from a local university to develop a VR-based stage lighting system for the Technical Theater Arts course. Over six months, we derived a list of design requirements (e.g., Level of realism serves the purpose of learning) and developed a prototype VR system for stage lighting education. Our contributions include the establishment of design requirements for stage lighting education in the metaverse, the development of a prototype system, and insights from integrating VR in course development. This research paves the way for further exploration and refinement of VR applications in educational settings.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {148},\nnumpages = {6},\nkeywords = {Stage lighting education, participatory design, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650963,\nauthor = {Chen, Shih-Chu and Chiu, Ko-Jen and Hsu, Pei-Fang},\ntitle = {Exploring Subjective and Objective Measures of System Effectiveness},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650963},\ndoi = {10.1145/3613905.3650963},\nabstract = {The effectiveness of information systems (IS) is essential for evaluating user performance between their behavior and computer. However, the effectiveness evaluation poses challenges due to the multidimensional nature of measures, especially in evaluating business intelligence systems (BI). This study aims to investigate measures from both subjective and objective of BI effectiveness and classify BI effectiveness into five dimensions based on the IS maturity model, including Operate, Consolidate, Integrate, Optimize, and Innovate. The study compares results obtained through self-reported with computer-recorded measures to gain a comprehensive understanding of system utilization. This research received 1480 questionnaires, accompanied by the collection of 1-year log data reflecting the actual system usage of each respondent. The findings reveal a partial alignment between self-reported and actual usage frequency depending on divergent effectiveness levels. These insights contribute to the complex dynamics underlying user behavior and provide practical implications for organizations seeking to enhance effective BI utilization.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {149},\nnumpages = {8},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650764,\nauthor = {Englhardt, Zachary and Li, Richard and Nissanka, Dilini and Zhang, Zhihan and Narayanswamy, Girish and Breda, Joseph and Liu, Xin and Patel, Shwetak and Iyer, Vikram},\ntitle = {Exploring and Characterizing Large Language Models for Embedded System Development and Debugging},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650764},\ndoi = {10.1145/3613905.3650764},\nabstract = {Large language models (LLMs) have shown remarkable abilities to generate code. However, their ability to develop software for physical computing and embedded systems, which requires cross-domain hardware and software knowledge, has not been thoroughly studied. We observe through our experiments and a 15-user pilot study that even when LLMs fail to produce working code, they can generate helpful reasoning about embedded design tasks, as well as specific debugging suggestions for both novice and expert developers. These results highlight the potential to develop AI assistants to dramatically lower the barrier to entry for working with hardware. To evaluate the capabilities and limitations of LLMs, we develop an automated testbench to quantify LLM performance on embedded programming tasks and perform 450 trials. We leverage these findings to analyze how programmers interact with these tools including their productivity and sense of fulfillment and outline a human-AI collaborative workflow for developing and debugging embedded systems.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {150},\nnumpages = {9},\nkeywords = {Embedded Systems Development, GPT, Large Language Models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650851,\nauthor = {Guo, Qingyu and Yuan, Kangyu and He, Changyang and Peng, Zhenhui and Ma, Xiaojuan},\ntitle = {Exploring the Evolvement of Artwork Descriptions in Online Creative Community under the Surge of Generative AI: A Case Study of DeviantArt},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650851},\ndoi = {10.1145/3613905.3650851},\nabstract = {The rise of AI-generated content (AIGC) is transforming online creative communities (OCCs) and posing challenges to their regulation. Artwork description may reveal creators’ practice and motivation in creating and sharing artwork. Understanding the influence of AIGC on creators’ descriptions of shared artwork could be helpful for community regulation. In this work, we collect 235K posts from DeviantArt, a large creative community that allows uploading AIGC. We confirm the prevalence of AIGC in the community. Through an open coding on 800 randomly sampled posts, we identify five themes in artwork descriptions. We quantitatively examine how these themes are affected by the prevalence of AIGC via statistical analysis. Results indicate a shift towards commercial opportunities and a reduced focus on copyright since the prevalence of AIGC. Descriptions for AI-generated artworks are more likely to direct members to other creations than those for human-created artworks. Finally, we discuss insights for OCCs.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {151},\nnumpages = {7},\nkeywords = {AI-generated content, Online creative community, generative AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650892,\nauthor = {Guo, Alicia and Pataranutaporn, Pat and Maes, Pattie},\ntitle = {Exploring the Impact of AI Value Alignment in Collaborative Ideation: Effects on Perception, Ownership, and Output},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650892},\ndoi = {10.1145/3613905.3650892},\nabstract = {AI-based virtual assistants are increasingly used to support daily ideation tasks. The values or bias present in these assistants can influence output in hidden ways and affect how people perceive the ideas produced with these AI assistants, leading to implications for the design of AI-based tools. We explored the effects of AI assistants with different values on the ideation process and user perception of idea quality, ownership, assistant competence, and values present in the output. Our study tasked 180 participants with brainstorming practical solutions to a set of problems with AI assistants of different values. Results show no significant difference in self-evaluation of idea quality and perception of the assistant based on value alignment; however, ideas generated reflected the AI’s values and feeling of ownership is affected. This highlights an intricate interplay between AI values and human ideation, suggesting careful design considerations for future AI-supported brainstorming tools.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {152},\nnumpages = {11},\nkeywords = {AI Assistants, Brainstorming, Human-AI Interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651102,\nauthor = {Song, Hyungwoo and Kim, Kyusik and Kim, Kitae and Ryu, Jeongwoo and Suh, Bongwon},\ntitle = {Exploring the Impact of User-Participated Customization in Experiencing Chatbot Failure},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651102},\ndoi = {10.1145/3613905.3651102},\nabstract = {Traditionally, personalized AI systems have offered customized experiences by inferring user preferences from their system usage data, which can be referred to as data-driven personalization. With the advent of GPTs, opportunities for users to directly engage in the customization process have increased, which we term user-participated customization. In this study, we explored the impact of user-participated customization on user reactions to chatbot failures. We involved twenty-two participants in total, with fourteen for user-participated customization and eight for data-driven personalization. Both groups experienced frustration with chatbot failures, yet their responses differed. Those who customized their chatbots predominantly displayed retrying intentions, in contrast to those using pre-personalized chatbots, who primarily showed giving-up intentions. Moreover, we noted different customizations were preferred for task-oriented and social-oriented chatbots. This study suggests that user-participated customization has the potential to foster the Ikea effect. This effect is expected to mitigate negative experiences with chatbots.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {153},\nnumpages = {7},\nkeywords = {chatbot failure, customization, personalization, travel agent},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650813,\nauthor = {Iop, Alessandro and Viberg, Olga and Francis, Kristi and Norstr\\\"{o}m, Vilhelm and Persson, David Mattias and Wallin, Linus and Romero, Mario and Matviienko, Andrii},\ntitle = {Exploring the Influence of Object Shapes and Colors on Depth Perception in Virtual Reality for Minimally Invasive Neurosurgical Training},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650813},\ndoi = {10.1145/3613905.3650813},\nabstract = {Minimally invasive neurosurgery (MIS) involves inserting a medical instrument, e.g., a catheter, through a small incision to target an area inside the patient’s body. Training surgeons to perform MIS is challenging since the surgical site is not directly visible from their perspective. In this paper, we conducted two pilot studies focused on object shapes and colors to collect preliminary results on their influence on depth perception for MIS in Virtual Reality. In the first study (N = 8), participants inserted a virtual catheter into objects of different shapes. In the second study (N = 5), they observed the insertion of a virtual catheter into objects of different colors and backgrounds under different lighting conditions. We found that participants’ precision decreased with distance and was lower with the skull shape than with a cube. Moreover, depth perception was higher with blue backgrounds under better lighting conditions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {154},\nnumpages = {7},\nkeywords = {depth perception, minimally invasive neurosurgery, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651108,\nauthor = {Park, BoGyeom and Kim, Yuwon and Park, Jinseok and Choi, Hojin and Kim, Seong-Eun and Ryu, Hokyoung and Seo, Kyoungwon},\ntitle = {Exploring the Multimodal Integration of VR and MRI Biomarkers for Enhanced Early Detection of Mild Cognitive Impairment},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651108},\ndoi = {10.1145/3613905.3651108},\nabstract = {Early detection of mild cognitive impairment (MCI) is crucial to impede dementia progression. Virtual reality (VR) biomarkers are adept at detecting impairments in instrumental activities of daily living (IADL), whereas magnetic resonance imaging (MRI) biomarkers excel in measuring observable structural changes in the brain. However, the efficacy of integrating VR and MRI biomarkers to improve early MCI detection remains unclear. This study aims to evaluate and compare the effectiveness of VR and MRI biomarkers and investigates the potential of their combined use for more accurate early MCI detection. Through support vector machine analysis, distinct characteristics were observed. For identifying MCI, VR biomarkers demonstrated high specificity (90.0\\%), whereas MRI showed high sensitivity (90.9\\%). The combination of both biomarkers yielded superior results in accuracy (94.4\\%), sensitivity (100.0\\%), and specificity (90.9\\%). Drawing from these results, we suggest a sequential diagnostic approach, employing VR for initial screening and MRI for subsequent confirmation of MCI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {155},\nnumpages = {8},\nkeywords = {Biomarker, Magnetic resonance imaging, Mild cognitive impairment, Virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651996,\nauthor = {Li, Ziming and Babar, Pinaki Prasanna and Barry, Mike and Peiris, Roshan L},\ntitle = {Exploring the Use of Large Language Model-Driven Chatbots in Virtual Reality to Train Autistic Individuals in Job Communication Skills},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651996},\ndoi = {10.1145/3613905.3651996},\nabstract = {Autistic individuals commonly encounter challenges in communicating with others which can lead to difficulties in obtaining and maintaining jobs. Thus, job training programs have emphasized training the communication skills of autistic individuals to improve their employability. Hence, we developed a virtual reality application that features avatars as chatbots powered by Large Language Models (LLMs), such as GPT-3.5 Turbo, and employs speech-based interactions with users. The use of LLM-driven chatbots allows job coaches to create training scenarios for trainees using text prompts. We conducted a preliminary study with three autistic trainees and two job coaches to gather early-stage feedback on the application’s usability and user experience. In the study, the trainee participants were asked to interact with the application in two scenarios involving customer interactions. Our findings indicate that our application shows promise for training job communication. Furthermore, we discuss its user experience aspects from the trainees’ and job coaches’ perspectives.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {156},\nnumpages = {7},\nkeywords = {Accessibility, Autism Spectrum Disorder, Communication Skill Training, Generative AI, Job-related Soft Skill, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650984,\nauthor = {Yamakawa, Naoto and Maruta, Tatsuya and Kinoshita, Yuichiro and Go, Kentaro},\ntitle = {Extending Fitts' Law for Tapping on Surfaces Angled in the Depth Direction},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650984},\ndoi = {10.1145/3613905.3650984},\nabstract = {Recently, various shape-changing devices have been reported in the human-computer interaction field. However, the applicability of Fitts’ law to these devices has not been efficiently discussed. Thus, in this study, we discuss the realization of a model that can predict the tapping time of displays angled in the depth direction. We first measured the tapping time of angled touch screens for 18 participants. Based on the results, we expanded and validated the Fitts’ law model by considering the operator’s line of sight. The results of regression analysis between the tapping time and the expanded model indicated that R2 = 0.97 and R2 = 0.95 for the displays angled to the front and back, respectively. We also considered the obstruction due to display height in the convex configuration and tapping difficulty in the concave configuration. The regression analysis for the modified model demonstrated R2 = 0.93 for both front-angled and back-angled displays.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {157},\nnumpages = {7},\nkeywords = {Angled surface, Finger input, Fitts’ law, Shape-changing display, Touch screen},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650856,\nauthor = {Nasiri, Nahid and Isbister, Katherine and Schweitzer, Julie B and Borden, Jared and Cottrell, Peter S and Shapiro, Daniel Goodman},\ntitle = {Extracting the Affective Content of Fidgeting in Adults with ADHD via Machine Learning and a Hand-held Soft Tangible Device},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650856},\ndoi = {10.1145/3613905.3650856},\nabstract = {This paper examines the task of extracting affect from the fidgeting behavior of adults with Attention Deficit/Hyperactivity Disorder (ADHD) engaged in emotional self-regulation experiments. We describe a fidgeting appliance developed to capture a tactile signal from spontaneous hand fidgeting, clarify its affordances, and present machine learning methods for processing this data. We show that hand-fidgeting carries a strong affective signal by recognizing the affective state produced by emotion-inducing videos and predicting self-assessed measures of anxiety and anxiety management collected after a standardized stress test. We conclude by discussing the potential impact of practical methods for data collection and analysis of fidgeting behavior on ADHD detection and management.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {158},\nnumpages = {8},\nkeywords = {ADHD, Affective content, emotional self-regulation, fidgeting behaviors, machine learning, tactile signals, touch},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650876,\nauthor = {Lankes, Michael and Ghosh, Samir and Lesser, Charles Bishop and Isbister, Katherine},\ntitle = {Eye Ball: Gazing as a Dilemma in a Competitive Virtual Reality Game},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650876},\ndoi = {10.1145/3613905.3650876},\nabstract = {In the evolving landscape of gaming technology, our research explores the integration of eye-tracking in VR games, moving beyond traditional applications. This paper presents \"Eye Ball,\" a competitive VR game that introduces a novel gaze-based mechanic, creating strategic dilemmas in gameplay. Our approach utilizes the qualities of gaze behavior to enhance social interaction in games, offering a more immersive gaming experience. In \"Eye Ball,\" players can generate a shield by closing their eyes, providing protection at the strategic cost of visual awareness. This mechanic challenges traditional game design, balancing the power of protection with the vulnerability of reduced situational awareness. A comparative study was conducted to examine the effects of this gaze-based dilemma. Our findings demonstrate the potential of gaze mechanics in enhancing the depth of social VR games. Our research contributes to the broader field of HCI, highlighting the potential of gaze interaction in creating immersive and socially complex gaming experiences.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {159},\nnumpages = {7},\nkeywords = {Gaze-based interaction, VR games, social games},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650865,\nauthor = {Zhu, Yifei and Peak, Joseph and Kelsey-Adkins, Eryn Rachael and Smith, C. Estelle},\ntitle = {FETCH: Fostering and Enhancing Teamwork, Communication, and Healthy Community Among Animal Shelter Volunteers through Mobile Technology},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650865},\ndoi = {10.1145/3613905.3650865},\nabstract = {Although prior literature has explored technologies for generally supporting volunteer work, no studies have accounted for the specific and contextually-situated technology needs of animal shelters and the volunteers who keep them running. In this paper, we present a case study in which we: 1) conduct formative work (need-finding surveys, interviews) with a local animal shelter; 2) use an iterative human-centered design process to build a mobile app called FETCH catered to this community’s priorities; and 3) conduct user testing sessions to assess FETCH. We found that during shifts at the shelter, volunteers face challenges with communication and information management. We designed FETCH to help dog walkers with information management between shifts and community development. Users found FETCH practical, effective and accessible; Moreover, the results of this case study can help inform future projects that aim to develop technology for animal shelters and rescues which perform vital services for society and animal wellbeing.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {160},\nnumpages = {9},\nkeywords = {Volunteering, animal rescue, animal shelter, human-centered-design, mobile application, volunteers},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650910,\nauthor = {Sch\\\"{o}mbs, Sarah and Pan, Jiahe and Zhang, Yan and Goncalves, Jorge and Johal, Wafa},\ntitle = {FaceVis: Exploring a Robot's Face for Affective Visualisation Design},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650910},\ndoi = {10.1145/3613905.3650910},\nabstract = {In this paper we examine the use of a robot’s face as an interface for affective visualisation design, a concept that we name FaceVis. We conducted a design workshop with 9 experts to explore metaphorical ideas on how to leverage a robot’s physicality, appearance and agency to convey data and communicate emotion. We present insights on potential challenges, benefits and pitfalls when using a robot’s face to visualise data. Our results show that this approach has the potential to enhance user engagement, support self-reflection and elicit empathic concern. We contribute three design considerations and provide future research directions to investigate a robot’s face as an interface for visualisation design.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {161},\nnumpages = {10},\nkeywords = {affective visualisation design, data visualisation, faces, social robot},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651044,\nauthor = {Bu, Fanjun and Bremers, Alexandra W.D. and Colley, Mark and Ju, Wendy},\ntitle = {Field Notes on Deploying Research Robots in Public Spaces},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651044},\ndoi = {10.1145/3613905.3651044},\nabstract = {Human-robot interaction requires to be studied in the wild. In the summers of 2022 and 2023, we deployed two trash barrel service robots through the wizard-of-oz protocol in public spaces to study human-robot interactions in urban settings. We deployed the robots at two different public plazas in downtown Manhattan and Brooklyn for a collective of 20 hours of field time. To date, relatively few long-term human-robot interaction studies have been conducted in shared public spaces. To support researchers aiming to fill this gap, we would like to share some of our insights and learned lessons that would benefit both researchers and practitioners on how to deploy robots in public spaces. We share best practices and lessons learned with the HRI research community to encourage more in-the-wild research of robots in public spaces and call for the community to share their lessons learned to a GitHub repository.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {162},\nnumpages = {6},\nkeywords = {HRI, behavioral elicitation, field experiment, social robots, urban spaces, wizard-of-oz},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650840,\nauthor = {Hung, Ching-Wen and Liang, Chung-Han and Chen, Bing-Yu},\ntitle = {FingerPuppet: Finger-Walking Performance-based Puppetry for Human Avatar},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650840},\ndoi = {10.1145/3613905.3650840},\nabstract = {Performance-based digital puppetry has gained widespread popularity in various fields, including gaming, storytelling, animation editing, etc. Human hands are well-suited for manipulating digital avatars with their skill and ability to perform multiple movements. In this work, we adopted the finger-walking technique, a natural and intuitive method of performance, as an interface for controlling human avatars. We first conducted a preliminary study to explore the range of finger-walking movements preferred by casual users and identified several general types of finger-walking performances. Based on the study results, we selected five common example animations from a database suitable for finger-walking performance. To manipulate human avatars, we developed a method that maps finger-walking motions to leg motions using rotation mapping and matches similar leg motions in the example animations to generate expressive full-body motions. We also implemented a prototype interactive storytelling application to demonstrate the effectiveness of our system in developing responsive and reliable human avatar motions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {163},\nnumpages = {6},\nkeywords = {finger-walking, motion retargeting, performance-based input},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650869,\nauthor = {Ozdemir, Mehmet and Doubrovski, Zjenja},\ntitle = {Foam2Form: 4D Printing with Programmable Foaming},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650869},\ndoi = {10.1145/3613905.3650869},\nabstract = {For heat-triggered shape-changing 3D prints, active and restrictive segments need to be 3D printed next to each other to obtain the desired morphing of an object. Current single-material methods rely on locally controlling the orientation of the printing lines to adjust the amount and direction of shrinkage. This approach, however, limits design freedom as it restricts the shape and fabrication of the objects. Moreover, it results in undesirable deformations in more complex and larger designs. Addressing these challenges, we introduce Foam2Form, a method that forms active and restrictive segments by programming the shape-memory properties of foaming PLA during the printing process. We propose to use the material in a non-foamed state for active segments and in a foamed state for restrictive and passive segments, which results in more stable 4D designs free from unwanted deformations. We present the first results of this low-cost 4D printing method and demonstrate its capabilities with various application examples.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {164},\nnumpages = {8},\nkeywords = {4D printing, foaming PLA, shape memory, shape-changing interfaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650917,\nauthor = {Ishii, Ayaka},\ntitle = {Food Printing with Electrolysis Bubbles for Texture Control},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650917},\ndoi = {10.1145/3613905.3650917},\nabstract = {Research has been conducted regarding 3D food printing that additively manufactures edible objects. Some studies control the texture of printed foods by creating an intricate internal structure, which is limited by the resolution that can be printed with soft paste-like foods. This study proposes a method for controlling the texture of 3D-printed gummy candy by selectively generating fine foam structures inside the gummy using electrolysis. We establish a printing technique for generating bubbles in the gummy dough by determining the optimal material properties, developing an energization control system for electrolysis and implementing a design tool for the printing paths. Our results show that the ratio between the number of foamed layers and the total number of layers could control the ease with which breaking occurs while maintaining the original gummy’s elasticity. We discuss the possibilities of exploring human-food interactions through our approach while demonstrating a gummy that contains foam structures in distinctive patterns.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {165},\nnumpages = {7},\nkeywords = {Bubble, Electrolysis, Foam, Food printing, Food texture, Gummy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651028,\nauthor = {Rao, Junkai and Zhou, Feng and Dai, Ju and Li, Chi and Hu, Yong},\ntitle = {FormationCreator: Designing A VR Dance Formation System for Intangible Cultural Heritage Dance},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651028},\ndoi = {10.1145/3613905.3651028},\nabstract = {Intangible Cultural Heritage (ICH) dance refers to traditional dance forms and practices that are recognized as part of a community’s ICH, which is a living heritage. However, due to the decrease in the number of dance forms and inheritors, the urgency to protect this particular dance form has become imminent. The formations in dance play a crucial role in conveying the meaning of the dance. In this paper, we research 10 ICH dances in Yunnan Province, summarize their characteristics, and propose a dance formation choreography system by voice, FormationCreator. The dance formations are expressed parametrically through voice in our FormationCreator, allowing for a more flexible definition and reproduction of these formations. Our research seeks to offer initial insights and fresh design concepts for the field of virtual choreography.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {166},\nnumpages = {7},\nkeywords = {Dance Formation, Dance Rehearsal, Information Visualization, Intangible Cultural Heritage Dance, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650773,\nauthor = {Mal, David and Wolf, Erik and D\\\"{o}llinger, Nina and Botsch, Mario and Wienrich, Carolin and Latoschik, Marc Erich},\ntitle = {From 2D-Screens to VR: Exploring the Effect of Immersion on the Plausibility of Virtual Humans},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650773},\ndoi = {10.1145/3613905.3650773},\nabstract = {Virtual humans significantly contribute to users’ plausible XR experiences. However, it may be not only the congruent rendering of the virtual human but also the degree of immersion having an impact on virtual humans’ plausibility. In a low-immersive desktop-based and a high-immersive VR condition, participants rated realistic and abstract animated virtual humans regarding plausibility, affective appraisal, and social judgments. First, our results confirmed the factor structure of a preliminary virtual human plausibility questionnaire in VR. Further, the appearance and behavior of realistic virtual humans were overall perceived as more plausible compared to abstract virtual humans, an effect that increased with high immersion. Moreover, only for high immersion, realistic virtual humans were rated as more trustworthy and sympathetic than abstract virtual humans. Interestingly, we observed a potential uncanny valley effect for low but not for high immersion. We discuss the impact of a natural perception of anthropomorphic and realistic cues in VR and highlight the potential of immersive technology to elicit distinct effects in virtual humans.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {167},\nnumpages = {8},\nkeywords = {Agent, Anthropomorphism, Avatar, Immersion, Presence, Realism},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651050,\nauthor = {Dev, Harshal and Gupta, Raj and Kumar, Dhruv},\ntitle = {From Cash to Cashless: UPI's Impact on Spending Behavior among Indian Users},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651050},\ndoi = {10.1145/3613905.3651050},\nabstract = {The emergence of digital payment systems has transformed how individuals conduct financial transactions, offering convenience, security, and efficiency. One groundbreaking innovation making waves in the Indian financial landscape is the Unified Payments Interface (UPI). Existing work has explored how digital payments benefit a country’s economy and GDP. However, our study explores how the introduction of UPI has influenced spending behavior among Indian users on an \"individual\" level. We gathered 235 valid survey responses encompassing diverse demographics and interviewed 20 survey respondents. Approximately 75\\% of the survey respondents reported increased spending due to UPI, with only 7\\% indicating reduced spending. Significantly, 91.5\\% of the respondents reported satisfaction with their UPI usage. Also, 95.2\\% of the survey respondents found making payments via UPI convenient. Our research also provides suggestions for UPI applications and various stakeholders to enhance digital payment systems, enabling users to make informed decisions and fostering responsible financial management.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {168},\nnumpages = {10},\nkeywords = {Cashless Transactions, Digital Payments, Unified Payments Interface (UPI), User Study},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650779,\nauthor = {Fang, Ning and Pluyter, Jon and Bakker, Saskia and Jacobs, Igor and Luyer, Misha and Nederend, Joost and Raijmakers, Jeroen and Chen, Lin-Lin and Funk, Mathias},\ntitle = {From Experience to Experience: Key Insights for Improved Interaction with AI in Radiology},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650779},\ndoi = {10.1145/3613905.3650779},\nabstract = {Artificial Intelligence (AI) decision-making tools for radiology demonstrated potential capacity to improve radiology work in several tasks such as tumor detection. However, relatively low acceptance in clinical practice demonstrates the challenge of incorporating end-users’ lived experience and their opinions to improve the interaction between clinicians and AI solutions. Therefore, we conducted semi-structured interviews with radiologists and technicians who had lived experience with current or the prior generations of radiology AI tools (e.g., Computer Aided Decision tools). Three key themes were elicited. Firstly, the role of AI, addresses how radiology professionals interact with radiology AI; the second theme, adoption in practice, discusses the requirements for easy usage and smooth transition; the third theme, building appropriate trust, explores influencing factors of clinicians’ trust towards radiology AI. Our findings call attention to the adoption of actionable recommendations on the interaction design and the importance of individual tailored functionalities in radiology AI systems.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {169},\nnumpages = {7},\nkeywords = {Adoption, Healthcare AI, Human-AI collaboration, Radiology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650765,\nauthor = {Ali, Naima Samreen and Ahn, Teulebit and Agha, Zainab and Park, Jinkyung Katie and Wisniewski, Pamela J.},\ntitle = {From Ideas to Impact: Cracking the Code for Effectively Engaging Teens in Long-Term Online Safety Research},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650765},\ndoi = {10.1145/3613905.3650765},\nabstract = {We solicited feedback from seven teens (aged 15-17) after they served for approximately two years on a Youth Advisory Board (YAB) for online safety. During this program, we engaged with them in various activities, including reviewing our lab’s research protocols and co-designing online safety solutions, while teaching them User Experience (UX) design skills. We found out that while teens valued the opportunity to enhance their design and research skills to strengthen their career profiles, they faced challenges regarding long-term commitment issues due to busier schedules, building long-lasting peer-to-peer bonds, and limited interactivity in research tasks. Based on these insights, teens suggested several improvements, which included stricter participation rules with increased accountability, more collaboration opportunities, and leveraging visual elements to increase interactivity with content. Our study emphasizes the importance of teens’ experiences and perspectives in developing future programs for their effective involvement in online safety research.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {170},\nnumpages = {7},\nkeywords = {Adolescent Online Safety, Asynchronous Research Community, Social Media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650900,\nauthor = {Yazici, Aybars and Meija-Domenzain, Paola and Frej, Jibril and K\\\"{a}ser, Tanja},\ntitle = {GELEX: Generative AI-Hybrid System for Example-Based Learning},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650900},\ndoi = {10.1145/3613905.3650900},\nabstract = {Traditional example-based learning methods are often limited by static, expert-created content. Hence, they face challenges in scalability, engagement, and effectiveness, as some learners might struggle to relate to the examples or find them relevant. To address these challenges, we introduce GELEX (GEnerative-AI Learning through EXamples), a hybrid Artificial Intelligence (AI) system enhancing example-based learning by using large language models (LLMs). Our hybrid system incorporates mechanisms to control and evaluate the AI output, acknowledging and addressing the potential factual inaccuracies of LLMs. We instantiate our system in the cooking domain. Our approach utilizes association rule mining on a large database of recipes to identify key patterns. When learners submit a recipe for feedback, a LLM enriches it by integrating these patterns. Then, learners are prompted to actively process the example by highlighting the changes and critically assessing the modifications. This strategy transforms traditional example-based learning into a dynamic, scalable, interactive educational tool.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {171},\nnumpages = {10},\nkeywords = {Example-based Learning, Generative AI, Procedural Writing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650817,\nauthor = {Saito, Daichi and Shibata, Masashi and Wang, Yunzhuo and Igarashi, Takeo and Higuchi, Keita},\ntitle = {GHOJA: Human-in-the-Loop Joint Pose Optimization based on Geometric Constraint and Human Common-sense},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650817},\ndoi = {10.1145/3613905.3650817},\nabstract = {Assigning a motion to movable parts in 3D scanned objects is important for various applications, such as virtual reality and robotics simulation. We call this problem joint-assignment that requires determining the joint type, position, and orientation. Although a machine can calculate the joint pose that meets the geometric constraint by utilizing a 3D model, the calculation result is not uniquely determined. These results can be narrowed down uniquely based on human common-sense, such as recognizing object affordance. In this study, we propose a human-in-the-loop joint-assignment system, GHOJA (Geometric- and Human-based Optimization for Joint-Assignment). The system considers both geometric constraints and common-sense. The results of a user experiment demonstrate that GHOJA is significantly capable of satisfying the geometric constraint compared to the manual assignment by a human. In addition, this system significantly meets common-sense compared to the automatic optimization based on the geometric constraint.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {172},\nnumpages = {6},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651073,\nauthor = {Kim, YoungIn and Hwang, Seokhyun and Oh, Jeongseok and Kim, Seungjun},\ntitle = {GaitWay: Gait Data-Based VR Locomotion Prediction System Robust to Visual Distraction},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651073},\ndoi = {10.1145/3613905.3651073},\nabstract = {In VR environments, user’s sense of presence is enhanced through natural locomotion. Redirected Walking (RDW) technology can provide a wider walking area by manipulating the trajectory of the user. Considering that the user’s future position enables a broader application of RDW, research has utilized gaze data combined with past positions to reduce prediction errors. However, in VR content that are replete with creatures and decorations, gaze dispersion may deteriorate the data quality. Thus, we propose an alternative system that utilizes gait data, GaitWay, which correlates directly to user locomotion. This study involved 11 participants navigating a visually distracting three-tiered VR environment while performing designated tasks. We employed a long short-term memory network for GaitWay to forecast positions two seconds ahead and evaluated the prediction accuracy. The findings demonstrated that incorporating gaze data significantly increased errors in highly-distracted settings, whereas GaitWay consistently reduced errors, regardless of the environmental complexity.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {173},\nnumpages = {8},\nkeywords = {Artificial Intelligence, Locomotion, Prediction, Redirected Walking, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650887,\nauthor = {Sweetser, Penny and Ozdowska, Anne},\ntitle = {GameFlow Affordances: Towards a Tool for Designing Gameful Experiences},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650887},\ndoi = {10.1145/3613905.3650887},\nabstract = {Following publication in 2005, the original GameFlow model has seen thousands of citations and hundreds of applications to designing and evaluating games and gameful experiences. In the intervening years, the GameFlow model has been put to the test and validated by applying it to different game experiences to further understand those experiences and to expose any weaknesses of the model. This paper builds upon the history of applications, validation, and interrogation of the GameFlow model to redevelop and propose a new version of GameFlow, called GameFlow Affordances. GameFlow Affordances shifts GameFlow from a prescriptive list of low-level criteria, to a modernised, high-level model for affording enjoyable experiences. We review the previous commentary and recommendations on the GameFlow model, propose a new model in response to previous work, and outline next steps in developing a design tool for using GameFlow Affordances in the design of games and gameful experiences.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {174},\nnumpages = {7},\nkeywords = {GameFlow, design, enjoyment, evaluation, heuristics, player experience, video games},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650736,\nauthor = {Allgaier, Mareen and Huettl, Florentine and Hanke, Laura Isabel and Huber, Tobias and Preim, Bernhard and Saalfeld, Sylvia and Hansen, Christian},\ntitle = {Gamification Concepts for a VR-based Visuospatial Training for Intraoperative Liver Ultrasound},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650736},\ndoi = {10.1145/3613905.3650736},\nabstract = {Gamification is widely used due to its positive influence on learning by adding emotions and steering behavior. In medical VR training applications, the use of gamification is rare, and when it is implemented, it often lacks thoughtful design decisions and empirical evaluation. Using a VR-based training for intraoperative ultrasound for liver surgery, we analyzed game elements regarding their suitability and examined two in more detail: difficulty levels and a kit, where the user has to assemble a virtual liver using US. In a broad audience study, levels achieved significantly better results regarding enjoyment. Qualitative feedback from medical students directly comparing the elements revealed that they prefer the kit as well as levels for training. Our studies indicate that levels and the more interactive kit improve the learning experience, which could also be taken as a basis for similar VR-based medical training applications.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {175},\nnumpages = {8},\nkeywords = {Game Elements, Gamification, Medical Training, Ultrasound, Virtual Reality, Visuospatial Skills},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651066,\nauthor = {Grau, Jan and Mayer, Simon and Strecker, Jannis and Garcia, Kimberly and Bektas, Kenan},\ntitle = {Gaze-based Opportunistic Privacy-preserving Human-Agent Collaboration},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651066},\ndoi = {10.1145/3613905.3651066},\nabstract = {This paper introduces a novel system to enhance the spatiotemporal alignment of human abilities in agent-based workflows. This optimization is realized through the application of Linked Data and Semantic Web technologies and the system makes use of gaze data and contextual information. The showcased prototype demonstrates the feasibility of implementing such a system, where we specifically emphasize the system’s ability to constrain the dissemination of privacy-relevant information.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {176},\nnumpages = {6},\nkeywords = {Human-Agent-Collaboration, Koreografeye, Privacy-Preserving, Solid},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650771,\nauthor = {Overdevest, Nathalie and Patibanda, Rakesh and Saini, Aryan and Van Den Hoven, Elise and Mueller, Florian ‘Floyd’},\ntitle = {GazeAway: Designing for Gaze Aversion Experiences},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650771},\ndoi = {10.1145/3613905.3650771},\nabstract = {Gaze aversion is embedded in our behaviour: we look at a blank area to support remembering and creative thinking, and as a social cue that we are thinking. We hypothesise that a person's gaze aversion experience can be mediated through technology, in turn supporting embodied cognition. In this design exploration we present six ideas for interactive technologies that mediate the gaze aversion experience. One of these ideas we developed into “GazeAway”: a prototype that swings a screen into the wearer's field of vision when they perform gaze aversion. Six participants experienced the prototype and based on their interviews, we found that GazeAway changed their gaze aversion experience threefold: increased awareness of gaze aversion behaviour, novel cross-modal perception of gaze aversion behaviour, and changing gaze aversion behaviour to suit social interaction. We hope that ultimately, our design exploration offers a starting point for the design of gaze aversion experiences.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {177},\nnumpages = {6},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650827,\nauthor = {Wienrich, Carolin and Latoschik, Marc Erich and Obremski, David},\ntitle = {Gender Differences and Social Design in Human-AI Collaboration: Insights from Virtual Cobot Interactions Under Varying Task Loads},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650827},\ndoi = {10.1145/3613905.3650827},\nabstract = {This work explores the effects of users’ gender and social design features of AI under different task load conditions on human-like attributions, social impact, work performance and perceived workload, user experience, and various other measures in Human-AI Interaction (HAII). Users had to execute sorting and dispatch tasks in collaboration with a virtual cobot. The degree of social gestalt of the cobot was varied by the ability to make small talk (i.e., talkative vs. non-talkative cobot), and the task load was increased by adding a secondary task (i.e., high vs. low task load condition). Overall, the talkative cobot led to a more positive perception of the cobot and increased social qualities like sense of meaning and team membership compared to the non-talkative cobot. The following gender effect was particularly interesting. The talkative cobot had a buffering effect for women and a distraction conflict effect for men in high task load conditions. When interacting with the talkative robot, women find the high task condition less stressful. In contrast thereto, the talkative cobot was distracting for men in the high task load condition. Our results highlight that social design choices and interindividual differences influence a successful collaboration between humans and AI. The work also shows the added value of systematic XR-simulations for the investigation and design of human-centered HAIIs (eXtended AI approach).},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {178},\nnumpages = {8},\nkeywords = {Artificial Intelligence, Collaborative Robots, Gender, Human Agent Interaction, Human-AI Collaboration, Interindividual Differences},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650860,\nauthor = {Schuller, Andreas and Janssen, Doris and Blumenr\\\"{o}ther, Julian and Probst, Theresa Maria and Schmidt, Michael and Kumar, Chandan},\ntitle = {Generating personas using LLMs and assessing their viability},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650860},\ndoi = {10.1145/3613905.3650860},\nabstract = {User personas, reflecting human characteristics, play a crucial role in human-centered design, contributing significantly to ideation and product design processes. However, expressing a diverse range of product-related human characterizations poses a challenging and time-consuming task for UX experts. This paper explores the utilization of Large Language Models (LLMs) to streamline the generation of personas, thereby enhancing the efficiency of UX researchers and providing inspiration for stakeholder discussions. Towards this objective, we devised strategic prompts and guidelines involving stakeholders and potential product features, resulting in the creation of candidate user personas. These personas were then compared with those crafted by human experts in a remote study involving 11 participants assessing 16 personas each. The analysis revealed that LLM-generated personas were indistinguishable from human-written personas, demonstrating similar quality and acceptance.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {179},\nnumpages = {7},\nkeywords = {automated persona generation, generative AI, innovation processes, personas, user centered design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650829,\nauthor = {J\\\"{u}rgens, Till Maria and Hassenzahl, Marc and Christoforakos, Lara and Laschke, Matthias},\ntitle = {Giggling in the Shower: Humor Increases the Acceptance of Technology-mediated Behavioral Interventions.},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650829},\ndoi = {10.1145/3613905.3650829},\nabstract = {One way to achieve sustainability is to adopt resource-saving practices. This often requires giving up cherished routines, which can be challenging. Behavior Change Technologies or automation can facilitate this, but will inevitably introduce friction that can be perceived as unpleasant or annoying. To alleviate this, we suggest humor as a promising design element. In an explorative study (N=349), we gathered participants’ responses (e.g., experience of humor, evaluation, and affect) when confronted with verbal descriptions of different interventions that interrupt the shower routine by stopping water in a humorous way. We found that the participants’ preferred style of humor was related to their experience of humor, with a focus on self-related rather than interpersonal humor. More importantly, the experience of humor was positively associated with the evaluation of the intervention and the resulting affective experience. This suggests that experience of humor can make a potentially neutral or slightly negative situation positive.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {180},\nnumpages = {7},\nkeywords = {behavior change technology, humor, sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650965,\nauthor = {Bekta\\c{s}, Kenan and Pandjaitan, Adrian and Strecker, Jannis and Mayer, Simon},\ntitle = {GlassBoARd: A Gaze-Enabled AR Interface for Collaborative Work},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650965},\ndoi = {10.1145/3613905.3650965},\nabstract = {Recent research on remote collaboration focuses on improving the sense of co-presence and mutual understanding among the collaborators, whereas there is limited research on using non-verbal cues such as gaze or head direction alongside their main communication channel. Our system – GlassBoARd – permits collaborators to see each other’s gaze behavior and even make eye contact while communicating verbally and in writing. GlassBoARd features a transparent shared Augmented Reality interface that is situated in-between two users, allowing face-to-face collaboration. From the perspective of each user, the remote collaborator is represented as an avatar that is located behind the GlassBoARd and whose eye movements are contingent on the remote collaborator’s instant eye movements. In three iterations, we improved the design of GlassBoARd and tested it with two use cases. Our preliminary evaluations showed that GlassBoARd facilitates an environment for conducting future user experiments to study the effect of sharing eye gaze on the communication bandwidth.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {181},\nnumpages = {8},\nkeywords = {CSCW, augmented reality, eye tracking, gaze, non-verbal cues, presence, remote collaboration},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650907,\nauthor = {Zhao, Yuchen and Banerjee, Tulika and Liu, Na and Kim, Jennifer G},\ntitle = {Grow With Me: Exploring the Integration of Augmented Reality and Health Tracking Technologies to Promote Physical Activity},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650907},\ndoi = {10.1145/3613905.3650907},\nabstract = {Maintaining a healthy lifestyle requires ongoing and consistent effort. To address this challenge, a variety of technologies, such as wearable devices, have been investigated to encourage individuals to adopt healthy habits. We introduce Grow with Me that combines an Augmented Reality (AR) virtual pet with fitness tracking. In our application, the user’s goal achievement level is linked to the health and mood of the user’s virtual dog. As the user makes progress, their virtual dog learns new tricks and provides new interaction possibilities. To evaluate this solution, we conducted a mixed-methods within-subjects study with 21 participants who used both the 2D and AR versions of our application. Our results uncovered that the AR pet elicited stronger emotional responses, and participants expressed a greater willingness to use AR as a motivational tool. In this paper, we will present our study and its findings, as well as the potential of AR as a powerful tool for motivating health-related actions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {182},\nnumpages = {9},\nkeywords = {Augmented Reality, Emotional Design, Engagement, Personal Health Informatics, Persuasive Technology, Tracking Technology, Virtual Character},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651087,\nauthor = {Lu, Qiuyu and Yang, Lydia and Maheshwari, Aditi and Ni, Hengrong and Yu, Tianyu and Gu, Jianzhe and Wadhwani, Advait and Xu, Haiqing and Danielescu, Andreea and Yao, Lining},\ntitle = {Guttation Sensor: Wearable Microfluidic Chip for Plant Condition Monitoring and Diagnosis},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651087},\ndoi = {10.1145/3613905.3651087},\nabstract = {Plant life plays a critical role in the ecosystem. However, it is difficult for humans to perceive plants’ reactions because the biopotential and biochemical responses are invisible to humans. Guttation droplets contain various chemicals which can reflect plant physiology and environmental conditions in real-time. Traditionally, these droplets are collected manually and analyzed in the lab with expensive instruments. Here, we introduce the Guttation Sensor, the first on-site and low-cost monitoring technology for guttation droplets. This innovative device employs a paper-based wearable microfluidic chip capable of collecting and conducting colorimetric detection of six chemicals. We discuss this technology’s design and implementation, conduct evaluations on tomato plants, and envision how such a technology could enhance the human-plant relationship.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {183},\nnumpages = {9},\nkeywords = {guttation, low-cost, plant, sensor},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650733,\nauthor = {Golev, Oleg Aleksandrovich and Huang, Michelle and Nop, Chanketya and Vongthongsri, Kritin and Monroy-Hern\\'{a}ndez, Andr\\'{e}s and Abtahi, Parastoo},\ntitle = {Hapster: Using Apple Watch Haptics to Enable Live Low-Friction Student Feedback in the Physical Classroom},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650733},\ndoi = {10.1145/3613905.3650733},\nabstract = {The benefits of student response systems (SRSs) for in-person lectures are well-researched. However, all current SRSs only rely on a visual interface to relay information to the instructor. We describe the design and evaluation of Hapster, a prototype system that uses an Apple Watch to deliver live, aggregated student feedback to the instructor via both visual and vibro-tactile modalities. We evaluated this system with 6 instructors and 155 students at a U.S. university. Participants reported that the system was effective at delivering live student feedback and facilitating better engagement from both the instructor and the students. However, instructors also noted several challenges with differentiating and perceiving the haptic sequences while lecturing. We conclude by discussing the tradeoff between system flexibility and abuse potential while identifying opportunities for further research regarding accessibility, content moderation, and additional interaction modalities. Our results suggest that haptics can be used as an effective live feedback mechanism for instructors in the physical classroom.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {184},\nnumpages = {7},\nkeywords = {Lecture feedback, multimodal interfaces, student response systems, visuohaptic},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650989,\nauthor = {Conwill, Louisa and Anthony, Sam English and Scheirer, Walter J.},\ntitle = {Has the Virtualization of the Face Changed Facial Perception? A Study of the Impact of Photo Editing and Augmented Reality on Facial Perception},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650989},\ndoi = {10.1145/3613905.3650989},\nabstract = {Augmented reality and other photo editing filters are popular methods used to modify faces online. Considering the important role of facial perception in communication, how do we perceive this increasing number of modified faces? In this paper we present the results of six surveys that measure familiarity with different styles of facial filters, perceived strangeness of faces edited with different filters, and ability to discern whether images are filtered. Our results demonstrate that faces modified with more traditional face filters are perceived similarly to unmodified faces, and faces filtered with augmented reality filters are perceived differently from unmodified faces. We discuss possible explanations for these results, including a societal adjustment to traditional photo editing techniques or the inherent differences in the different types of filters. We conclude with a discussion of how to build online spaces more responsibly based on our results.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {185},\nnumpages = {11},\nkeywords = {Augmented Reality, Face Filter, Facial Perception, Social Media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650879,\nauthor = {Yoon, Harin and Oh, Changhoon and Jun, Soojin},\ntitle = {How Can I Trust AI? : Extending a UXer-AI Collaboration Process in the Early Stages},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650879},\ndoi = {10.1145/3613905.3650879},\nabstract = {This paper explores the integration of generative AI into UX design by focusing on developing a collaborative process that facilitates work between UX practitioners (UXers) and AI. It identifies critical barriers in UXer–AI collaboration, investigates the need for modifications in the collaboration process, and proposes a new process involving verification and decision-making stages. Through a workshop with experienced UXers, this study examined their perspectives and challenges in working with AI. The findings emphasize the need to enhance the trustworthiness of generative AI’s outputs and discuss various verification methods. This research contributes to bridging the gap between AI technology and practical UX design, paving the way for more effective and trustworthy collaboration.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {186},\nnumpages = {7},\nkeywords = {Generative AI, Human-AI Collaboration Process, UX Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650748,\nauthor = {Zhang, Min and Arce-Plata, Carlos and Price, Blaine A. and Pike, Graham and Walkington, Zoe and Elphick, Camilla and Frumkin, Lara and Philpot, Richard and Keil, Tina F. and Levine, Mark and Stuart, Avelie and Nuseibeh, Bashar and Bandara, Arosha K},\ntitle = {How Do People Use a Public Gratitude Platform in the Wild?},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650748},\ndoi = {10.1145/3613905.3650748},\nabstract = {A plethora of psychological research suggests that gratitude interventions can improve individual well-being and prosocial behavior. However, most of the existing studies focus on gratitude interventions of individuals within controlled lab-based experiments. Our work aims to explore how people practice public gratitude in their everyday lives. This paper presents the design of our public gratitude platform that allows users to freely post gratitude as a tree leaf. Based on usage data spanning 3 years since its launch, our exploratory data analysis provides empirical insights into how people engage in gratitude practices ‘in the wild’. Six types of gratitude, together with their characteristics are identified: personal, work-related groups, study-related groups, gratitude to public service, local community, and event-triggered gratitude. This paper highlights the organic growth of our gratitude platform and its innovative use. Furthermore, the limitations of our work are discussed, shedding light on potential opportunities for future research.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {187},\nnumpages = {7},\nkeywords = {Gratitude, social gratitude, technology-facilitated gratitude intervention},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650823,\nauthor = {Huang, Shih-Hong and Lin, Ya-Fang and He, Zeyu and Huang, Chieh-Yang and Huang, Ting-Hao Kenneth},\ntitle = {How Does Conversation Length Impact User’s Satisfaction? A Case Study of Length-Controlled Conversations with LLM-Powered Chatbots},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650823},\ndoi = {10.1145/3613905.3650823},\nabstract = {Users can discuss a wide range of topics with large language models (LLMs), but they do not always prefer solving problems or getting information through lengthy conversations. This raises an intriguing HCI question: How does instructing LLMs to engage in longer or shorter conversations affect conversation quality? In this paper, we developed two Slack chatbots using GPT-4 with the ability to vary conversation lengths and conducted a user study. Participants asked the chatbots both highly and less conversable questions, engaging in dialogues with 0, 3, 5, and 7 conversational turns. We found that the conversation quality does not differ drastically across different conditions, while participants had mixed reactions. Our study demonstrates LLMs’ ability to change conversation length and the potential benefits for users resulting from such changes, but we caution that changes in text form may not necessarily imply changes in quality or content.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {188},\nnumpages = {13},\nkeywords = {Chatbots, Conversation Length, Conversational Agents, Large Language Models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650841,\nauthor = {Brachman, Michelle and El-Ashry, Amina and Dugan, Casey and Geyer, Werner},\ntitle = {How Knowledge Workers Use and Want to Use LLMs in an Enterprise Context},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650841},\ndoi = {10.1145/3613905.3650841},\nabstract = {Large Language Models (LLMs) have introduced a paradigm shift in interaction with AI technology, enabling knowledge workers to complete tasks by specifying their desired outcome in natural language. LLMs have the potential to increase productivity and reduce tedious tasks in an unprecedented way. A systematic study of LLM adoption for work can provide insight into how LLMs can best support these workers. To explore knowledge workers’ current and desired usage of LLMs, we ran a survey (n=216). Workers described tasks they already used LLMs for, like generating code or improving text, but imagined a future with LLMs integrated into their workflows and data. We discuss implications for adoption and design of generative AI technologies for knowledge work.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {189},\nnumpages = {8},\nkeywords = {adoption, knowledge workers, large language models, survey},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651067,\nauthor = {Lahaye, Marcel and Rahm, Ricarda and Dymek, Andreas and Wagner, Adrian and Ernstberger, Judith and Borchers, Jan},\ntitle = {How’s Your Sewing? Investigating Metrics to Automatically Assess Sewing Expertise},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651067},\ndoi = {10.1145/3613905.3651067},\nabstract = {Makers must regularly assess their expertise when planning projects or selecting tutorials. However, personal bias makes this assessment prone to error, potentially leading to frustration, loss of materials, and discouragement. Additionally, hobbyists have limited feedback possibilities to refine their skills, unlike, for example, apprentice artisans who receive continuous instructor feedback. To address these issues, automated expertise assessment systems could help makers assess their skills and progress. However, such systems require assessment metrics, which have been studied little in the maker context so far. We derived such metrics for sewing from semi-structured interviews with ten sewing-related instructors about their evaluation process. Additionally, we showed them a sewn object and asked them to assess the creator’s expertise. From our findings, we derive criteria to use in future automated sewing expertise assessment systems. For one criterion, seam allowance, we present a functional demonstrator that automatically assesses related measurements.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {190},\nnumpages = {7},\nkeywords = {Expertise Assessment, Sewing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650732,\nauthor = {Yan, Lixiang and Echeverria, Vanessa and Fernandez-Nieto, Gloria Milena and Jin, Yueqiao and Swiecki, Zachari and Zhao, Linxuan and Ga\\v{s}evi\\'{c}, Dragan and Martinez-Maldonado, Roberto},\ntitle = {Human-AI Collaboration in Thematic Analysis using ChatGPT: A User Study and Design Recommendations},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650732},\ndoi = {10.1145/3613905.3650732},\nabstract = {Generative artificial intelligence (GenAI) offers promising potential for advancing human-AI collaboration in qualitative research. However, existing works focused on conventional machine-learning and pattern-based AI systems, and little is known about how researchers interact with GenAI in qualitative research. This work delves into researchers’ perceptions of their collaboration with GenAI, specifically ChatGPT. Through a user study involving ten qualitative researchers, we found ChatGPT to be a valuable collaborator for thematic analysis, enhancing coding efficiency, aiding initial data exploration, offering granular quantitative insights, and assisting comprehension for non-native speakers and non-experts. Yet, concerns about its trustworthiness and accuracy, reliability and consistency, limited contextual understanding, and broader acceptance within the research community persist. We contribute five actionable design recommendations to foster effective human-AI collaboration. These include incorporating transparent explanatory mechanisms, enhancing interface and integration capabilities, prioritising contextual understanding and customisation, embedding human-AI feedback loops and iterative functionality, and strengthening trust through validation mechanisms.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {191},\nnumpages = {7},\nkeywords = {ChatGPT, Generative Artificial Intelligence, Human-AI Collaboration, Qualitative Research, Thematic Analysis},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651052,\nauthor = {Li, Mengyang and Chen, Chuang and Tang, Xin and Zhu, Kuangqi and Yang, Yue and Luo, Shijian and Yao, Cheng and Ying, Fangtian and Tao, Ye and Wang, Guanyun},\ntitle = {HydroSkin: Rapid Prototyping On-Skin Interfaces via Low-Cost Hydrographic Printing},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651052},\ndoi = {10.1145/3613905.3651052},\nabstract = {We introduce HydroSkin, a low-cost, time-efficient, and accessible fabrication technique for on-skin interfaces. By mixing carbon ink with sodium alginate, we developed a low-cost conductive material suitable for hydrographic printing technology. Compared to existing fabrication methods, our approach streamlines the fabrication process and utilizes cost-effective ink without requiring specialized skills. We showcased multiple application scenarios to demonstrate how HydroSkin can enable users to customize the design and quickly fabricate their on-skin interfaces, including a finger game controller, an ear toucher, an arm control slider, and a fatigue monitoring mask.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {192},\nnumpages = {6},\nkeywords = {Electronic Skin, Fabrication, Hydrographic Printing, Low-cost, On-skin Interface},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651085,\nauthor = {Zhou, Xiaoyan and Batmaz, Anil Ufuk and Williams, Adam Sinclair and Schreiber, Dylan and Ortega, Francisco Raul},\ntitle = {I Did Not Notice: A Comparison of Immersive Analytics with Augmented and Virtual Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651085},\ndoi = {10.1145/3613905.3651085},\nabstract = {Immersive environments enable users to engage in embodied interaction, enhancing the sensemaking processes involved in completing tasks such as immersive analytics. Previous comparative studies on immersive analytics using augmented and virtual realities have revealed that users employ different strategies for data interpretation and text-based analytics depending on the environment. Our study seeks to investigate how augmented and virtual reality influences sensemaking processes in quantitative immersive analytics. Our results, derived from a diverse group of participants, indicate that users demonstrate comparable performance in both environments. However, it was observed that users exhibit a higher tolerance for cognitive load in VR and travel further in AR. Based on our findings, we recommend providing users with the option to switch between AR and VR, thereby enabling them to select an environment that aligns with their preferences and task requirements.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {193},\nnumpages = {7},\nkeywords = {AR/VR Comparison, Immersive Analytics, Sensemaking, User Interaction, User Navigation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651071,\nauthor = {Wang, Yiyuan and Hoggenm\\\"{u}ller, Marius and Zhang, Guixiang and Tran, Tram Thi Minh and Tomitsch, Martin},\ntitle = {Immersive In-Situ Prototyping: Influence of Real-World Context on Evaluating Future Pedestrian Interfaces in Virtual Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651071},\ndoi = {10.1145/3613905.3651071},\nabstract = {Pedestrian interfaces support people’s interaction with autonomous agents in traffic scenarios. Early studies relied on computer-generated (CG) environments to evaluate pedestrian interfaces in virtual reality (VR). More recently, real-world 360-degree videos have been used as an alternative to CG environments as they support immersive and realistic experiences. This paper reports on the combined use of 360-degree videos and dynamic CG interfaces as a new approach for evaluating pedestrian interfaces, referred to as immersive in-situ prototyping. We analyse participant feedback from two case studies that used this approach for evaluating pedestrian interfaces from a drone and from an autonomous vehicle. Results show that participants considered the immersive in-situ prototypes realistic, natural, and familiar and found them to facilitate connections to real-life experiences. We describe the process for developing immersive in-situ prototypes and offer technical considerations for future studies.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {194},\nnumpages = {8},\nkeywords = {360-Degree Videos, Human-Machine Interfaces, Pedestrians, Prototyping, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651033,\nauthor = {Sumedh, Veena and Ly, Peter and Ren, Yuan and Sylla, Cristina and Plata, Abigail and Arif, Ahmed Sabbir},\ntitle = {Impact of Static and Animated eBook Illustrations on Children's Engagement, Enjoyment, and Information Recall},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651033},\ndoi = {10.1145/3613905.3651033},\nabstract = {In an exploratory study, we investigated the effects of different types of illustrations on eBook reading engagement among children aged 11 to 12 years. We explored three distinct illustration styles: static (depicting significant events within the story), basic animated (animating story events), and focused animated (emphasizing specific elements or illustrating continuous change over time). Our findings suggest that animated illustrations, especially focused animated illustrations, enhance children’s engagement, increase their confidence, and improve recall. Subjective feedback highlighted a strong preference for and greater enjoyment of stories featuring animated illustrations. The majority of children believed that animated illustrations would substantially improve their eBook reading experience.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {195},\nnumpages = {11},\nkeywords = {Children, classroom, digital book, e-book, e-learning, eReader, education, learning, middle school, recall, tablet},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651003,\nauthor = {\\v{S}kola, Filip and Liarokapis, Fotis},\ntitle = {Increasing Meditation Efficiency with Virtual Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651003},\ndoi = {10.1145/3613905.3651003},\nabstract = {Meditation, as a mental training for augmenting consciousness (by improving perception, cognition, and life satisfaction with mindful awareness), has been known for thousands of years and has numerous positive associations confirmed by contemporary research. The main issue with meditation seems to be the time it requires for enduring reconfiguration of the brain that leads to lasting benefits, such as the capacity to manage sustained mindfulness (often thousands of hours in meditation sessions). With mechanisms behind meditation increasingly more elucidated, it is time to research optimizations for the meditation procedure. The current theory behind meditation explains it as a process of optimization in the brain’s predictive coding system. This article builds on this theory and describes the utilization of virtual reality for the development of sensory stimulation that seeks to a) facilitate meditation-related insights into the nature of perception, and b) increase the speed of changes in the brain’s predictive models.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {196},\nnumpages = {9},\nkeywords = {Avatar Embodiment, Meditation, Multisensory Integration, Predictive Coding, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650940,\nauthor = {Liu, Lanjing and Gao, Lan and Yao, Yaxing},\ntitle = {Integrating Family Privacy Education and Informal Learning Spaces: Characteristics, Challenges and Design Opportunities},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650940},\ndoi = {10.1145/3613905.3650940},\nabstract = {Children face increasing privacy risks and the need to navigate complex choices, while privacy education is not sufficient due to limited education scope and family involvement. We advocate for informal learning spaces (ILS) as a pioneering channel for family-based privacy education, given their established role in holistic technology and digital literacy education, which specifically targets family groups. In this paper, we conducted an interview study with five families to understand revealing current approaches to privacy education and engagement with ILS for family-based learning. Our findings highlight ILS’s trans-formative potential in family privacy education, considering existing practices and challenges. We discuss the reason for family-based privacy education in ILS and identify potential design opportunities. Additionally, we outline our future work, which involves expanding participant involvement and conducting co-design activities with family groups to create design prototypes.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {197},\nnumpages = {9},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650922,\nauthor = {Yim, Youchan and Tanaka, Fumihide},\ntitle = {Integration of a Shape Memory Alloy with a Soft Pneumatic Actuator to Improve the Haptic Interaction Performance of a Soft Social Robot},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650922},\ndoi = {10.1145/3613905.3650922},\nabstract = {This paper discusses the integration of a soft pneumatic actuator (SPA) with a shape memory alloy (SMA) to improve haptic technologies in social robotics. The use of traditional rigid actuators in robots can lead to unnatural interactions, while SPAs offer flexibility but may lack precision. SMAs are known for their shape memory effect and precision, but they also have limitations, such as time-consuming heating-cooling cycles. Our research proposes a hybrid actuator that combines the adaptability of SPAs with the precision of SMAs to enhance haptic interaction with social robots. The hybrid actuator is designed to overcome the individual limitations of SPAs and SMAs and achieve a synergistic effect. As a first attempt, the hybrid actuator is intended for use in Reliebo, an inflatable social robot, to provide more natural and responsive interactions. This approach has the potential to improve the emotional and haptic aspects of human-computer interaction and social robotics.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {198},\nnumpages = {8},\nkeywords = {Soft Pneumatic Actuator, haptic device, social touch, soft robotics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650758,\nauthor = {Gonzalez, Eric J and Chatterjee, Ishan and Gonzalez-Franco, Mar and Cola\\c{c}o, Andrea and Ahuja, Karan},\ntitle = {Intent-driven input device arbitration for XR},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650758},\ndoi = {10.1145/3613905.3650758},\nabstract = {Interactions with Extended Reality Head-Mounted Displays (XR HMDs) require precise, intuitive, and efficient input methods. Current approaches either rely on power-intensive sensors, such as cameras for hand tracking, or specialized hardware such as controllers. Previous work has explored the use of familiar, available devices such as smartphones and smartwatches as more a more practical input alternative. However, this approach risks interaction overload – how can one determine whether the user’s gestures on the watch or phone are directed toward control of the XR device or the mobile device itself? To this end, we propose a novel method for cross-device input arbitration based on the relative orientation between the HMD and target device as measured by on-device IMUs. In a validation study with 6 users, we demonstrate 93.7\\% accuracy in estimating the intended device of interaction. Our method offers a practical, energy-efficient way to leverage users’ existing devices for input and enable seamless cross-device experiences in XR.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {199},\nnumpages = {5},\nkeywords = {Arbitration, Cross-device, Input, Mixed Reality, Multi-device},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650769,\nauthor = {Yang, Xiaoying and Sayono, Jacob and Xu, Jess and Zhang, Yang},\ntitle = {Interaction-Power Stations: Turning Environments into Ubiquitous Power Stations for Charging Wearables},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650769},\ndoi = {10.1145/3613905.3650769},\nabstract = {Despite the promise of wearable devices, people can be discouraged from using them due to the necessity for frequent charging and the subsequent interruption of usage. On another front, an inexhaustible yet unexploited power source can be found in the environment in the form of people’s physical interaction with ambient objects, generating a substantial amount of kinetic energy. We proposed Interaction-Power Stations, a new energy harvesting approach for wearables, leveraging interaction energy from people and simultaneously charging their wearables through capacitive couplings of the human body. We designed circuits and mechanical mechanisms retrofitted to various objects to convert kinetic energy into electrical signals that travel through the user body at capacitive frequencies, and deliver energy to multiple on-body receivers. We validated our design by preliminary tests and evaluated the system through a short user study which indicates promise for future work.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {200},\nnumpages = {8},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651098,\nauthor = {Li, Xiaolong and Yao, Cheng and Zhou, Yujie and Feng, Shuyue and Li, Zhengke and Cheng, Yiming and Huang, Shichao and Dong, Haoye and Xue, Mengru and Wang, Guanyun},\ntitle = {Intercircuit: Electroplating with Cavities for Fast Fabrication of Complex and High-Performance 3D Circuits},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651098},\ndoi = {10.1145/3613905.3651098},\nabstract = {In Human-Computer Interaction, multi-material 3D printing is increasingly recognized for its capacity to produce conductive interactive objects. While various fabrication techniques have emerged in this domain, achieving the integration of highly conductive 3D structures within printed objects continues to pose a significant technical hurdle. Our study introduces Intercircuit, a novel integrated technique developed explicitly for fabricating highly conductive interactive objects. This method employs multi-material printing to form unified components along with their embedded structures of lower conductivity, enhancing conductivity through targeted plating. Distinct from traditional multi-material printing and surface conductivity augmentation methods, the Intercircuit method facilitates the creation of complex 3D circuitry while ensuring superior conductivity. Furthermore, this study introduces a supportive design tool aimed at aiding users in crafting conductive frameworks. Moreover, the practical application of this method is further elucidated through a series of case studies.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {201},\nnumpages = {6},\nkeywords = {3D printed electronics;Selective Electroplating, conductive filament;},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650991,\nauthor = {Vahedian Movahed, Saniya and Dimino, James and Farrell, Andrew and Irankhah, Elyas and Ghosh, Srija and Jain, Garima and Mahipal, Vaishali and Rayavaram, Pranathi and Sanusi, Ismaila Temitayo and Salas, Erika and Wolkowicz, Kelilah and Narain, Sashank and Martin, Fred},\ntitle = {Introducing Children to AI and ML with Five Software Exhibits},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650991},\ndoi = {10.1145/3613905.3650991},\nabstract = {Artificial intelligence (AI) and machine learning (ML) have a deepening impact in our world. For empowered citizenship and career readiness, elementary and middle school students need to understand these technologies. To provide engaging introductory experiences, we created five original interactive software exhibits introducing children to hands-on activities in AI and ML. The exhibits were tested by 125 elementary and middle school students (aged 7 to 14). Four themes emerged: Students recognized that AI and ML systems can process data from cameras (perception); they saw that these systems responded to their training input (trust); they appreciated the practical import of AI/ML systems (affective and cognitive attitudes); and students were introduced to models and modes (specialization). This paper presents our goals in creating the exhibits and results from our initial testing with children. Our work contributes to the literature on formal and informal activities for introducing AI and ML to young learners.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {202},\nnumpages = {6},\nkeywords = {Artificial Intelligence, K–8 Students, Machine Learning, Models, Software Tools},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651042,\nauthor = {Guo, Jiajing and Mohanty, Vikram and Piazentin Ono, Jorge H and Hao, Hongtao and Gou, Liang and Ren, Liu},\ntitle = {Investigating Interaction Modes and User Agency in Human-LLM Collaboration for Domain-Specific Data Analysis},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651042},\ndoi = {10.1145/3613905.3651042},\nabstract = {Despite demonstrating robust capabilities in performing tasks related to general-domain data-operation tasks, Large Language Models (LLMs) may exhibit shortcomings when applied to domain-specific tasks. We consider the design of domain-specific AI-powered data analysis tools from two dimensions: interaction and user agency. We implemented two design probes that fall on the two ends of the two dimensions: an open-ended high agency (OHA) prototype and a structured low agency (SLA) prototype. We conducted an interview study with nine data scientists to investigate (1) how users perceived the LLM outputs for data analysis assistance, and (2) how the two design probes, OHA and SLA, affected user behavior, performance, and perceptions. Our study revealed insights regarding participants’ interactions with LLMs, how they perceived the results, and their desire for explainability concerning LLM outputs, along with a noted need for collaboration with other users, and how they envisioned the utility of LLMs in their workflow.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {203},\nnumpages = {9},\nkeywords = {Large language model, data analysis, domain knowledge, human-AI collaboration, user agency},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651056,\nauthor = {Wang, Jiaqi and Brewster, Stephen Anthony and Hirskyj-Douglas, Ilyena},\ntitle = {Investigating Lemurs Sensory Modality in Technologies: How do Lemurs Engage with Single and Multimodal Stimuli in Zoos?},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651056},\ndoi = {10.1145/3613905.3651056},\nabstract = {Current computer-based enrichment in zoos is often limited to providing a single-sensory experience, disregarding that animals perceive the world through multiple senses. To address this, we developed and deployed a multi-sensory device for six red ruffed lemurs in a zoo, incorporating visual, auditory, and olfactory stimuli in varying combinations to determine whether multi-sensory or single-sensory engage lemurs more in using the device. The device was deployed in the lemur’s enclosure over 63 days, where when a lemur approached the device it would trigger a stimuli combination and record their engagement with the device. Framing our findings with zookeeper interviews, our initial results suggest that lemurs used the device more when it was presenting multi-modal stimuli, rather than a single stimulus. Future research will look further at individual, lemur numbers and specific sense types factors on lemur engagements with multi-sensory systems to investigate how technology can better meet their needs.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {204},\nnumpages = {7},\nkeywords = {animal-computer interaction, multi-sensory, red ruffed lemurs, single-sensory, zoo technology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650971,\nauthor = {Grasso, Antonietta Maria and Willamowski, Jutta Katharina and Park, Jisun},\ntitle = {Investigating Limits and Effectiveness of Privacy Conversations: the Case of Service Robots in an Office Environment},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650971},\ndoi = {10.1145/3613905.3650971},\nabstract = {Social robots, capable of moving and acting autonomously in spaces inhabited by humans, are becoming more and more a reality. From a privacy-sensitive design perspective, and compared to digital technology, robots present additional challenges, as they are equipped with many sensors to acquire data to move around autonomously and interact with their users. This may raise privacy concerns and privacy conversations have been proposed as a means to address such concerns. In this paper we present a study of the effectiveness of such conversations, the topics addressed and the types of questions asked. Our findings illustrate the potential of such conversations with an embodied, or, with a virtual agent, according to the user convenience and type of inquiry. They also illustrate a variety of conversation styles. These positive results are counterbalanced by current technical limitations of conversational agents. They also show that in some cases conversations with humans are preferred.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {205},\nnumpages = {9},\nkeywords = {human-robot interaction, privacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651076,\nauthor = {Khan, Emaan Bilal and Atique, Emaan and Javed, Mobin},\ntitle = {Investigating Phishing Threats in the Email Browsing Experience of Visually Impaired Individuals},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651076},\ndoi = {10.1145/3613905.3651076},\nabstract = {Phishing, a prevalent cyber threat, persists despite email platform security measures. Visual cues are vital for users to identify phishing, but visually impaired individuals (PVIs) face challenges due to reliance on screen readers. We conduct a qualitative task-based study (n=11) in a Pakistani context, analyzing the interaction of PVIs with phishing emails that target their unique vulnerabilities. Our thematic analysis reveals PVI navigation patterns, highlights challenges in detecting phishing, and provides recommendations for stakeholders.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {206},\nnumpages = {11},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651009,\nauthor = {Reiter, Thomas and Sakel, Sophia and Scharbert, Julian and Ter Horst, Julian and Back, Mitja and Van Zalk, Maarten and B\\\"{u}hner, Markus and Schoedel, Ramona},\ntitle = {Investigating Phubbing in Everyday Life: Challenges \\& Lessons for Future Research},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651009},\ndoi = {10.1145/3613905.3651009},\nabstract = {The ubiquitous presence of smartphones has made them an integral part of our social lives. A well-known example of this phenomenon is phubbing, where smartphone use distracts people from their daily interpersonal interactions. While previous research has mostly relied on often biased global self-reports, our work introduces a novel approach to assessing phubbing in real life. To this end, we conducted an empirical study that integrated experience sampling and mobile sensing methods to obtain a more objective measure of phubbing behavior. Based on the evaluation of our concept, we contribute insights on reliable phubbing assessment in real life and the design of phubbing-aware technologies based on it. By highlighting the challenges associated with existing methods, we aim to stimulate discussion in the field of HCI and encourage the development of socially friendly technologies that benefit real-life interpersonal interactions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {207},\nnumpages = {8},\nkeywords = {Experience Sampling, Mobile Sensing, Phubbing, Smartphone Usage, Technoference},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651039,\nauthor = {Dixon, Emma and Xiao, Xiang and Michaels, Rain Breaw and Zhong, Yu and Narayanan, Ajit and Buehler, Erin},\ntitle = {Investigating the Potential of User Interface Shortcuts and Control Panes to Support Mobile Phone Use by People with Mild Dementia – a Diary Study},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651039},\ndoi = {10.1145/3613905.3651039},\nabstract = {Given the necessity of mobile phones to participate in everyday life and past research indicating the inaccessibility of mobile phones for people with dementia, there is a need to design more accessible mobile interactions to ensure their full inclusion in society. We conducted a two-week diary study with three people with mild dementia to investigate their experiences with and perceptions of user interface (UI) shortcuts and control panes. Findings from this study demonstrate the potential of UI shortcuts and control panes to address specific mobile phone accessibility challenges people with mild dementia experience. This work provides a technical contribution in the form of control panes for mobile phones, as well as design considerations to improve the viability of mobile phone shortcuts and controls panes as future assistive technology for individuals with diverse cognitive abilities.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {208},\nnumpages = {7},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651105,\nauthor = {Yoo, Yejoon and Segal, Jonathan Isaac and Hayes, Aleshia and Won, Andrea Stevenson},\ntitle = {Just Look at Them! Encouraging Self-Reflection on Teacher Gaze Behavior through Data Visualizations in Virtual Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651105},\ndoi = {10.1145/3613905.3651105},\nabstract = {Virtual classroom simulations are an exciting avenue to give teachers a way to reflect on their teaching behavior, in particular, nonverbal behavior. In two within-participants studies, we explore how visualizing participants’ gaze, using four different data visualizations, affected participants’ behaviors and self-reflection in an immersive virtual reality classroom simulation. We compared a Control condition with no data visualization, an updating Bar Graph over each \"student\" agent head, and two Fade-In/Fade-Out conditions where the opacity of “students” changed based on whether they were in the field of view of the participant. We found that participants preferred the Bar Graph visualizations, and this condition changed participants’ behavior the most compared to the Control condition. We discuss design implications for virtual classroom simulations as a self-reflection tool for teachers.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {209},\nnumpages = {9},\nkeywords = {3D Data Visualization, Gaze Data, Immersive Visualization, User Interface, Virtual Reality Simulation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651001,\nauthor = {Wang, Yijia and Seaborn, Katie},\ntitle = {Kawaii Computing: Scoping Out the Japanese Notion of Cute in User Experiences with Interactive Systems},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651001},\ndoi = {10.1145/3613905.3651001},\nabstract = {Kawaii computing is a new term for a steadily growing body of work on the Japanese notion of “cute” in human-computer interaction (HCI) research and practice. Kawaii is distinguished from general notions of cute by its experiential and culturally-sensitive nature. While it can be designed into the appearance and behaviour of interactive agents, interfaces, and systems, kawaii also refers to certain affective and cultural dimensions experienced by culturally Japanese users, i.e., kawaii user experiences (UX) and mental models of kawaii elicited by the socio-cultural context of Japan. In this scoping review, we map out the ways in which kawaii has been explored within HCI research and related fields as a factor of design and experience. We illuminate theoretical and methodological gaps and opportunities for future work on kawaii computing.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {210},\nnumpages = {9},\nkeywords = {Kawaii computing, computer agents, human-computer interaction, kawaii, literature review, robots, scoping review, virtual characters},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650792,\nauthor = {Higasa, Taichi and Tanaka, Keitaro and Feng, Qi and Morishima, Shigeo},\ntitle = {Keep Eyes on the Sentence: An Interactive Sentence Simplification System for English Learners Based on Eye Tracking and Large Language Models},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650792},\ndoi = {10.1145/3613905.3650792},\nabstract = {Language learners should read challenging texts regularly. However, using dictionaries or search engines to look up difficult expressions can be time-consuming and distracting. To address this, we have developed a system combining eye tracking with Large Language Models (LLMs) to simplify sentences automatically, allowing learners to focus on the content. The system incorporates user-tailored models that estimate users’ comprehension of sentences using gaze data and sentence information. The system also features user-triggered simplification, resulting from iterative design improvements. We conducted a user study with 17 English learners where they read English text using either our system or a baseline involving online dictionaries and search engines. Our system significantly improved both reading speed and comprehension, especially for complex sentences. The gaze-based simplification improved concentration on the content, allowing for an interruption-free reading experience. It could assist in daily reading practice, particularly for extensive reading focused on large volumes of text at a rapid pace.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {211},\nnumpages = {7},\nkeywords = {Eye tracking, human-computer interaction, machine learning, sentence simplification},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650904,\nauthor = {Roig-Maim\\'{o}, Maria Francesca and Mas-Sans\\'{o}, Ramon and Mart\\'{\\i}nez-Bueso, Pau and Salinas-Bueno, Iosune},\ntitle = {Kill Two Stacks with One Stone: \"Fitts\" Yourself while Doing Rehabilitation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650904},\ndoi = {10.1145/3613905.3650904},\nabstract = {In this work, we describe our experience testing the potential dual role of rehabilitation exergames: as a therapeutic tool and as a means of studying Fitts’ Law. We initially created a game to encourage head movements for cervical rehabilitation. However, we soon recognized its potential to expand beyond its original purpose due to its resemblance to the ISO one-directional test based on Fitts’ paradigm. Our investigation delves into the impact of two apparatuses (exergame and one-directional test) on throughput, movement time, and error rate; analyzing also the effect of the sex factor. We present a novel integration of rehabilitation and Fitts’ law evaluation to provoke the HCI community to look beyond the mere pursuit of technological goals and to exploit every opportunity to deepen into the study of human behavior: from an exergame as a bridge to rehabilitation, to a HCI tool.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {212},\nnumpages = {7},\nkeywords = {Fitts’ Law, ISO 9241-411, Throughput, cervical rehabilitation, exergame, head-tracker interface},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650970,\nauthor = {Morag Yaar, Noa and Sadka, Ofir and Shatil, Itay and Aharonson, Maayan and Efrima, Bar and Barda, Tal and Hayat, Mira and Zuckerman, Oren and Erel, Hadas},\ntitle = {Kitchef: A TUI for Parent-Child Cooking Together},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650970},\ndoi = {10.1145/3613905.3650970},\nabstract = {The benefit of cooking with children goes beyond the act of preparing food. Studies highlight how cooking together can help children develop openness to diverse food, a sense of independence, and can enrich the parent-child relationship. However, challenges such as parents concerns from the messy process and children’s lack of engagement, often hinder the motivation to cook together. We present the design and preliminary evaluation of a Tangible User Interface (TUI) for encouraging parents and children to cook together. The TUI is designed as a cooking recipe smart box and a pair of wristbands. A preliminary study with five families revealed that the TUI was successful in allowing for more control and visibility while maintaining flexibility in the cooking process. It also enhanced teamwork, high engagement, and physical closeness. Our results suggest that the TUI can assist parents and children in cooking together.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {213},\nnumpages = {7},\nkeywords = {Cooking together, Parent-child interaction, TUI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650873,\nauthor = {Mo, Wen and Dechant, Martin Johannes and Marquardt, Nicolai and Ayobi, Amid and Singh, Aneesha and Holloway, Catherine},\ntitle = {Knowledge Work on Airplanes: Challenges, Workarounds, and Design Implications},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650873},\ndoi = {10.1145/3613905.3650873},\nabstract = {Digital technologies provide significant potential to transform people's lived experiences of working in confined spaces. However, our understanding of the challenges and workarounds of digital knowledge work on long-haul flights is not well documented. To address this research gap, we present the findings of a semi-structured interview study with 21 participants investigating the nuances of working on airplanes (WoA). We identify contradictory attitudes towards WoA and challenges that go beyond spatial limitations, such as well-being issues, feelings of surveillance, and logistic hurdles across the entire journey. Based on this understanding, we discuss design implications, from portable and functional digital tools to discreet interaction techniques for improving WoA experiences and the potential to extend them to other confined workspaces.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {214},\nnumpages = {7},\nkeywords = {Additional Keywords and Phrases},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650944,\nauthor = {De Souza, Jessica and Chamberlain, Kristina and Wang, Edward Jay},\ntitle = {LCBuddy: Towards a Smartphone-based Self-Assessment Tool for Postpartum Patients With Breast Pain},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650944},\ndoi = {10.1145/3613905.3650944},\nabstract = {Telehealth has significantly enhanced various medical sectors, notably in postpartum care, by increasing breastfeeding rates and reducing postpartum depression. However, issues in patient management and triage, particularly in high-demand telehealth lactation consultations, lead to LC burnout, unnecessary costs and diagnostic delays. To address these challenges, this study introduces a pain assessment tool for telehealth lactation consultations, enhancing patient-provider communication through momentary assessment and AI mediation. It features a user interface with image quality assessment and condition classification algorithms, aiding remote diagnosis and triage. The tool guides patients and uses a severity-based feedback system, helping LCs prioritize cases. This method boosts tele-lactation service efficiency, streamlines LC workload, and serves as a model for specialized healthcare triaging. Future work includes refining the feedback system and evaluating the tool in clinical settings to standardize symptom severity grading and estimate its impact, potentially improving expert patient triaging in remote healthcare settings.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {215},\nnumpages = {7},\nkeywords = {Remote healthcare, breast pain assessment, breastfeeding support, digital health systems, telehealth},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650755,\nauthor = {Kahng, Minsuk and Tenney, Ian and Pushkarna, Mahima and Liu, Michael Xieyang and Wexler, James and Reif, Emily and Kallarackal, Krystal and Chang, Minsuk and Terry, Michael and Dixon, Lucas},\ntitle = {LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650755},\ndoi = {10.1145/3613905.3650755},\nabstract = {Automatic side-by-side evaluation has emerged as a promising approach to evaluating the quality of responses from large language models (LLMs). However, analyzing the results from this evaluation approach raises scalability and interpretability challenges. In this paper, we present LLM Comparator, a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation. The tool supports interactive workflows for users to understand when and why a model performs better or worse than a baseline model, and how the responses from two models are qualitatively different. We iteratively designed and developed the tool by closely working with researchers and engineers at Google. This paper details the user challenges we identified, the design and development of the tool, and an observational study with participants who regularly evaluate their models.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {216},\nnumpages = {7},\nkeywords = {Visual analytics, generative AI, large language models, machine learning evaluation, side-by-side evaluation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650828,\nauthor = {Kolla, Mahi and Salunkhe, Siddharth and Chandrasekharan, Eshwar and Saha, Koustuv},\ntitle = {LLM-Mod: Can Large Language Models Assist Content Moderation?},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650828},\ndoi = {10.1145/3613905.3650828},\nabstract = {Content moderation is critical for maintaining healthy online spaces. However, it remains a predominantly manual task. Moderators are often exhausted by low moderator-to-posts ratio. Researchers have been exploring computational tools to assist human moderators. The natural language understanding capabilities of large language models (LLMs) open up possibilities to use LLMs for online moderation. This work explores the feasibility of using LLMs to identify rule violations on Reddit. We examine how an LLM-based moderator (LLM-Mod) reasons about 744 posts across 9 subreddits that violate different types of rules. We find that while LLM-Mod has a good true-negative rate (92.3\\%), it has a bad true-positive rate (43.1\\%), performing poorly when flagging rule-violating posts. LLM-Mod is likely to flag keyword-matching-based rule violations, but cannot reason about posts with higher complexity. We discuss the considerations for integrating LLMs into content moderation workflows and designing platforms that support both AI-driven and human-in-the-loop moderation.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {217},\nnumpages = {8},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651029,\nauthor = {Wang, Chao and Hasler, Stephan and Tanneberg, Daniel and Ocker, Felix and Joublin, Frank and Ceravola, Antonello and Deigmoeller, Joerg and Gienger, Michael},\ntitle = {LaMI: Large Language Models for Multi-Modal Human-Robot Interaction},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651029},\ndoi = {10.1145/3613905.3651029},\nabstract = {This paper presents an innovative large language model (LLM)-based robotic system for enhancing multi-modal human-robot interaction (HRI). Traditional HRI systems relied on complex designs for intent estimation, reasoning, and behavior generation, which were resource-intensive. In contrast, our system empowers researchers and practitioners to regulate robot behavior through three key aspects: providing high-level linguistic guidance, creating \"atomic actions\" and expressions the robot can use, and offering a set of examples. Implemented on a physical robot, it demonstrates proficiency in adapting to multi-modal inputs and determining the appropriate manner of action to assist humans with its arms, following researchers’ defined guidelines. Simultaneously, it coordinates the robot’s lid, neck, and ear movements with speech output to produce dynamic, multi-modal expressions. This showcases the system’s potential to revolutionize HRI by shifting from conventional, manual state-and-flow design methods to an intuitive, guidance-based, and example-driven approach. Supplementary material can be found at https://hri-eu.github.io/Lami/},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {218},\nnumpages = {10},\nkeywords = {Assisting robot, Human-robot interaction, Large language model},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651112,\nauthor = {Zhao, Yijun and Pan, Jiangyu and Dong, Yan and Dong, Tianshu and Wang, Guanyun and Ying, Fangtian and Shen, Qihang and Cao, Jiacheng},\ntitle = {Language Urban Odyssey: A Serious Game for Enhancing Second Language Acquisition through Large Language Models},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651112},\ndoi = {10.1145/3613905.3651112},\nabstract = {Traditional second language acquisition (SLA) often lacks deep immersion in authentic environments, presenting high learning and resource challenges. To overcome this, we introduced \"Language Urban Odyssey\" (LUO), a serious game designed to offer an affordable language practice environment. LUO combines Large Language Models (LLMs) with game-based learning, creating an immersive and interactive experience. Players interact with AI-driven characters in a fictional city, leveraging ChatGPT 3.5’s capabilities for simulating real language use and cultural diversity. The game aims to reduce language learning barriers, ignite interest, and provide practical scenarios. Test results show LUO significantly boosts interest and proficiency in language learning. Players praise its engaging narrative, interactive dialogues, and adaptive experience. However, while LUO is beneficial, it’s crucial to recognize gamified learning’s limits; genuine language fluency still requires real-life communication practice and validation.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {219},\nnumpages = {7},\nkeywords = {Educational Games, Human-AI Interaction, Large Language Models, Second Language Acquisition},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650995,\nauthor = {Krishnappa Babu, Pradeep Raj and Di Martino, J. Matias and Carpenter, Kimberly L.H. and Compton, Scott and Davis, Naomi and Eichner, Brian and Espinosa, Steven and Franz, Lauren and Perochon, Sam and Dawson, Geraldine and Sapiro, Guillermo},\ntitle = {Large-scale Validation of a Scalable and Portable Behavioral Digital Screening Tool for Autism at Home},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650995},\ndoi = {10.1145/3613905.3650995},\nabstract = {Autism, characterized by challenges in socialization and communication, benefits from early detection for prompt and timely intervention. Traditional autism screening questionnaires often exhibit reduced accuracy in primary care settings and significantly underperform underprivileged populations. We present findings on the effectiveness of an autism screening digital application (app) that can be administered at primary care clinics and also by caregivers at home. A large-scale validation was conducted with 1052 toddlers aged 16–40 months. Among them, 223 were subsequently diagnosed with autism. The age-appropriate interactive app utilized strategically designed stimuli, presented on the screen of the iPhone or iPad, to evoke behaviors related to social attention, facial expressions, head movements, blinking rate, and motor responses, which can be detected with the device's sensors and automatically quantified through computer vision (CV) and machine learning. The algorithm, combining various digital biomarkers, demonstrated strong accuracy: Area under the receiver operating characteristic curve (AUC) = 0.93, sensitivity = 86.0\\%, specificity = 91.0\\%, and precision = 71\\%, for distinguishing autistic versus non-autistic toddlers, marking a strong foundation as a digital phenotyping tool in the autism research, notably without any costly equipment like eye tracking devices and at home administered by caregivers.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {220},\nnumpages = {7},\nkeywords = {Autism, Computer Vision, Machine learning, Screening tool},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650894,\nauthor = {Lin, Yujun and Jo, Jeyeon},\ntitle = {LeatherBoard: Sustainable On-body Rapid Prototyping with Leather Scraps and Machine Embroidery},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650894},\ndoi = {10.1145/3613905.3650894},\nabstract = {The increasing demand for wearable electronics calls for more effective prototyping methods that deliver reliability while maintaining user comfort and versatility. Traditional methods often rely on rigid materials, leading to skin discomfort and challenges in rapid prototyping. Meanwhile, the fashion and electronics industries contribute to environmental issues by generating significant waste, including tons of leather scraps every year. This paper explores the potential of these leather scraps as a resource for creating flexible breadboards. We introduce LeatherBoard, a sustainable approach that uses machine embroidery with conductive threads for efficient and standardized prototyping. Our contributions include the development of LeatherBoard and its evaluation in terms of electrical reliability and durability. We also demonstrate its potential through three prototyped applications, showcasing LeatherBoard's strength as a prototyping platform.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {221},\nnumpages = {7},\nkeywords = {Leather, On-body interface, Rapid prototyping, Sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650949,\nauthor = {Oelen, Allard and Auer, S\\\"{o}ren},\ntitle = {Leveraging Large Language Models for Realizing Truly Intelligent User Interfaces},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650949},\ndoi = {10.1145/3613905.3650949},\nabstract = {The number of published scholarly articles is growing at a significant rate, making scholarly knowledge organization increasingly important. Various approaches have been proposed to organize scholarly information, including describing scholarly knowledge semantically leveraging knowledge graphs. Transforming unstructured knowledge, presented within articles, to structured and semantically represented knowledge generally requires human intelligence and labor since natural language processing methods alone typically do not render sufficient precision and recall for many applications. With the recent developments of Large Language Models (LLMs), it becomes increasingly possible to provide truly intelligent user interfaces guiding humans in the transformation process. We present an approach to integrate non-intrusive LLMs guidance into existing user interfaces. More specifically, we integrate LLM-supported user interface components into an existing scholarly knowledge infrastructure. Additionally, we provide our experiences with LLM integration, detailing best practices and obstacles. Finally, we evaluate the approach using a small-scale user evaluation with domain experts.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {222},\nnumpages = {8},\nkeywords = {Intelligent User Interface, LLM Interface, Scholarly Knowledge Graphs},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651115,\nauthor = {Shih, Jasmine Y and Mohanty, Vishal and Katsis, Yannis and Subramonyam, Hari},\ntitle = {Leveraging Large Language Models to Enhance Domain Expert Inclusion in Data Science Workflows},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651115},\ndoi = {10.1145/3613905.3651115},\nabstract = {Domain experts can play a crucial role in guiding data scientists to optimize machine learning models while ensuring contextual relevance for downstream use. However, in current workflows, such collaboration is challenging due to differing expertise, abstract documentation practices, and lack of access and visibility into low-level implementation artifacts. To address these challenges and enable domain expert participation, we introduce CellSync, a collaboration framework comprising (1) a Jupyter Notebook extension that continuously tracks changes to dataframes and model metrics and (2) a Large Language Model powered visualization dashboard that makes those changes interpretable to domain experts. Through CellSync’s cell-level dataset visualization with code summaries, domain experts can interactively examine how individual data and modeling operations impact different data segments. The chat features enable data-centric conversations and targeted feedback to data scientists. Our preliminary evaluation shows that CellSync provides transparency and promotes critical discussions about the intents and implications of data operations.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {223},\nnumpages = {11},\nkeywords = {Collaborative ML, computational notebooks, data subset visualization, prompt engineering, tabular datasets},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650821,\nauthor = {Kumar, Chandan and Wemmer, Eileen and Rahangdale, Apeksha Manoj and Rosenthal-von der P\\\"{u}tten, Astrid Marieke},\ntitle = {Leveraging Nonverbal Signals for Hands-Free Input in Digital Surveys},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650821},\ndoi = {10.1145/3613905.3650821},\nabstract = {Collecting end-user feedback is crucial for assessing the acceptance and viability of any product or service. In the digital realm, Likert scale surveys are the most common method for gathering feedback, with end users explicitly selecting their responses using a mouse or touch input. Conversely, in social environments, nonverbal interactions are often used as an intuitive means of expressing agreement or disagreement. In this work, we explore the utilization of nonverbal interactions such as facial expressions and head gestures as a novel approach to elicit user responses in digital surveys. This presents a valuable hands-free alternative for obtaining public feedback in a non-intrusive manner, particularly enabling individuals with motor impairments to participate in surveys. We outline our framework and the evaluation study with 16 participants to assess the acceptance and feasibility of the proposed hands-free method for responding to Likert scale survey questions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {224},\nnumpages = {6},\nkeywords = {digital survey, facial expressions, hands-free interaction, head movements, likert scale input, multimodal interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650993,\nauthor = {Venkatasubramanian, Krishna and Ranalli, Tina-Marie},\ntitle = {Leveraging technology for post-abuse peer support for people with intellectual and developmental disabilities},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650993},\ndoi = {10.1145/3613905.3650993},\nabstract = {Our aim in this paper is to leverage technology to facilitate peer support for abuse survivors with intellectual and developmental disabilities (I/DD). In this regard, we talked to staff from a trauma services agency in the US that has experience providing post-abuse peer-support work. We found that: originally peer support sessions were exclusively in-person sessions that enabled the establishment of a productive connection and building trust between the survivor and the peer leader. However, since COVID-19, peer support sessions has also been provided online via teleconferencing and structured presentations. These online sessions have the advantage that they make it easier for survivors to participate in peer support. However, they often end up diminishing the peer-survivor connection that in-person sessions enabled. Based on these findings, we suggest design of technologies that can enable online peer support to establish a productive connection between survivors and peers as in-person sessions do.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {225},\nnumpages = {7},\nkeywords = {design, developmental disability, intellectual disability, peer-support, trauma},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650897,\nauthor = {Aiordachioae, Adrian and Vatavu, Radu-Daniel},\ntitle = {Lifelogging in Mixed Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650897},\ndoi = {10.1145/3613905.3650897},\nabstract = {Lifelogging is a growing social phenomenon where personal devices capture records of one’s life for future recalling and reminiscing. At the same time, mixed reality worlds are increasingly available due to advances in head-mounted display technology. However, no exploration has been conducted on lifelogging in such emerging worlds, where the lifelog should indiscriminately capture the experience of both the physical and virtual. In this paper, we introduce the concept of “mixed reality lifelogging” and specify it with the frameworks of the Metaverse Roadmap and broadcasting informational feeds as alternate realities. Second, we present a technical prototype for HoloLens. Third, we report findings from a preliminary experiment involving a lifelog recorded under varying densities of virtual objects anchored in the physical world. We conclude with future work opportunities for mixed reality lifelogging.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {226},\nnumpages = {8},\nkeywords = {HoloLens, Lifelogging, augmented reality, head-mounted displays, mixed reality, smartglasses},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650932,\nauthor = {Xu, Rachel and Le, Nhu and Park, Rebekah and Murray, Laura Cecilia},\ntitle = {Like-minded, like-bodied: How users (18-26) trust online eating and health information},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650932},\ndoi = {10.1145/3613905.3650932},\nabstract = {This paper investigates the relationship between social media and eating practices amongst 42 internet users aged 18-26. We conducted in-person ethnographic research in the US and India to observe how young people navigated eating and health information online. Participants portrayed themselves online using a vocabulary we have labeled “the good life\": performing holistic health through diet, exercise, and idealized lifestyles. We observed participants engaging in behaviors of disordered eating while actively eschewing them. When online, participants valued personal testimonies, and readily tested tips from content creators who they believed shared similar beliefs and bodies to them. In doing so, they did not engage in probabilistic thinking and opened themselves to potential harm. We found that social media feeds did not unidirectionally influence participants—they often reflected participants’ internalized views of health, in an intertwined, non-linear journey. Reducing the online spread of disordered eating practices requires addressing it within young people's social context.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {227},\nnumpages = {7},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651107,\nauthor = {Tiemann, Jan and McGinity, Matthew and Sbalzarini, Ivo F. and G\\\"{u}nther, Ulrik},\ntitle = {Live and Interactive 3D Photomanipulation under the Microscope using Virtual Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651107},\ndoi = {10.1145/3613905.3651107},\nabstract = {State-of-the-art microscopes, as used in cell biology, are not only capable of capturing 3D images, but also permit manipulation of (sub-)cellular structures using techniques such as optical traps, optogenetics or laser ablation. However, such microscopes are still controlled using 2D interfaces, prohibiting actual 3-dimensional manipulation. We present microscenery, a virtual reality (VR) microscope control software, designed to facilitate 3D laser ablation experiments. We combine microscopy automation with VR rendering and intuitive controller-based input to empower biologists with the precision of laser-based techniques while providing the full 3D spatial context of their sample. We describe the design goals and architecture of the software and illustrate the potential of the system by conducting a brief expert review study for 3D ablation experiments. Our results suggest VR is not only an effective interface for microscopic manipulations, but can enable novel experiments which are either impossible with traditional 2D interfaces, or prohibitively time-consuming.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {228},\nnumpages = {7},\nkeywords = {human-computer interaction, microscope control, virtual reality, visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650982,\nauthor = {Liebers, Carina and Pf\\\"{u}tzenreuter, Niklas and Prochazka, Marvin and Megarajan, Pranav and Furuno, Eike and L\\\"{o}ber, Jan and Stratmann, Tim C. and Auda, Jonas and Degraen, Donald and Gruenefeld, Uwe and Schneegass, Stefan},\ntitle = {Look Over Here! Comparing Interaction Methods for User-Assisted Remote Scene Reconstruction},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650982},\ndoi = {10.1145/3613905.3650982},\nabstract = {Detailed digital representations of physical scenes are key in many cases, such as historical site preservation or hazardous area inspection. To automate the capturing process, robots or drones mounted with sensors can algorithmically record the environment from different viewpoints. However, environmental complexities often lead to incomplete captures. We believe humans can support scene capture as their contextual understanding enables easy identification of missing areas and recording errors. Therefore, they need to perceive the recordings and suggest new sensor poses. In this work, we compare two human-centric approaches in Virtual Reality for scene reconstruction through the teleoperation of a remote robot arm, i.e., directly providing sensor poses (direct method) or specifying missing areas in the scans (indirect method). Our results show that directly providing sensor poses leads to higher efficiency and user experience. In future work, we aim to compare the quality of human assistance to automatic approaches.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {229},\nnumpages = {8},\nkeywords = {RGBD sampling, human-robot interaction, manual sampling, teleoperation, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651061,\nauthor = {Wang, Po-Yao (Cosmos) and Lee, Nathaniel Yung Xiang and Rajesh, Rohit and Loose, Antony Smith and Semertzidis, Nathan and Mueller, Florian ‘Floyd’},\ntitle = {LuciEntry: A Modular Lucid Dream Induction Prototype},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651061},\ndoi = {10.1145/3613905.3651061},\nabstract = {Lucid dreaming is a state in which individuals become aware that they are dreaming, allowing dreamers to actively engage in controlling their dream content and transcending the limitations of the physical world. Lucid dreaming offers various mental and physical health benefits. However, current research combining multiple lucid dreaming induction techniques is often conducted with the researcher’s intervention, lacking autonomy by relying on researchers to monitor the sleep stages and activating the devices manually. Recent studies also advocate for a modular system that can integrate multiple lucid dreaming induction techniques. We present LuciEntry, a prototype that includes a mobile app guiding the user through pre-sleep cognitive training and a system that assesses the user’s sleep stage and triggers the external stimuli automatically to induce lucid dreams. We hope that this modular autonomous system will improve the research process, aiding in further research into lucid dreams.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {230},\nnumpages = {6},\nkeywords = {Lucid dreaming, autonomous, induction, interactive devices, modular, prototype, system},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651060,\nauthor = {Maunsbach, Martin and Frier, William and Hornb\\ae{}k, Kasper},\ntitle = {MAMMOTH: Mid-Air Mesh-based Modulation Optimization Toolkit for Haptics},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651060},\ndoi = {10.1145/3613905.3651060},\nabstract = {Mid-air ultrasound can recreate the missing touch from contactless interactions, such as bare hand gestures in extended reality. But designing ultrasound haptics either relies on inadequate static sensations or experts who can create dynamic sensations. We introduce MAMMOTH, an open source toolkit for Unity that automatically generates dynamic ultrasound sensations for interactions with 3D objects. The haptic feedback is achieved by extending and generalizing a path-routing algorithm for intersections between meshes. We first describe how the toolkit works and then demonstrate how it builds on previous techniques. Finally, we present how to use the toolkit to implement three distinct use cases.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {231},\nnumpages = {7},\nkeywords = {Haptic Feedback, Human-Computer Interaction, Mid-Air Haptics, Shortest Path, Toolkit, Ultrasound},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650838,\nauthor = {Hong, Yoo Jin and Park, Hye Soo and Joung, Eunki and Hong, Jihyeong},\ntitle = {MOJI: Enhancing Emoji Search System with Query Expansions and Emoji Recommendations},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650838},\ndoi = {10.1145/3613905.3650838},\nabstract = {The text-based emoji search, despite its widespread use and extensive variety of emojis, has received limited attention in terms of understanding user challenges and identifying ways to support users. In our formative study, we found the bottlenecks in text-based emoji searches, focusing on challenges in finding appropriate search keywords and user modification strategies for unsatisfying searches. Building on these findings, we introduce MOJI, an emoji entry system supporting 1) query expansion with content-relevant multi-dimensional keywords reflecting users’ modification strategies and 2) emoji recommendations that belong to each search query. The comparison study demonstrated that our system reduced the time required to finalize search keywords compared to traditional text-based methods. Additionally, users achieved higher satisfaction in final emoji selections through easy attempts and modifications on search queries, without increasing the overall selection time. We also present a comparison of emoji suggestion algorithms (GPT and iOS) to support query expansion.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {232},\nnumpages = {8},\nkeywords = {Computer-mediated Communication, Information Retrieval, Interaction Design, Usability, User Experience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651116,\nauthor = {Kao, Hsien-Te and Erickson, Isabel and Chu, Minh Duc Hoang and He, Zihao and Lerman, Kristina and Volkova, Svitlana},\ntitle = {Machine Learning Insights Into Eating Disorder Twitter Communities},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651116},\ndoi = {10.1145/3613905.3651116},\nabstract = {Eating disorders have serious impacts on young population physical, psychological, and social functioning. Health agencies are calling for new psycho-therapeutic intervention considerations that take into account online communities, which is not possible if therapists know little about eating disorders discussions online. In this paper, we leverage machine learning analytics to understand what the eating disorder communities are talking about over time and how they are talking about it. By analyzing local and global community discussions, we discovered complex group dynamics underpinning collective identities offering emotional support but also potentially perpetuating harmful behaviors. Our analysis of four local subcommunities and four global theme evolutions revealed prevalent subjects, perspectives, motivations, and linguistic patterns. We found tight-knit communities grounded in shared membership, goals, cultures, and practices. Community voices highlighting recovery journeys were limited. Our computational assessment of invisible online spaces aims to inform personalized interventions accounting for community forces in youth mental health.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {233},\nnumpages = {8},\nkeywords = {Discussion Analysis, Eating Disorders, Social Media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651120,\nauthor = {Hanafi, Maeda F and Reiss, Frederick and Katsis, Yannis and Moore, Robert and Wood, David and Falakmasir, Mohammad H and Liu, Changchang},\ntitle = {Machine-Assisted Error Discovery in Conversational AI Systems},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651120},\ndoi = {10.1145/3613905.3651120},\nabstract = {Troubles in speaking, hearing, and understanding occur routinely in any kind of conversational setting. The natural flow of conversation includes methods for “repairing” such troubles by repeating or paraphrasing all or parts of prior turns. In the case of conversational AI systems, these troubles occur due to failure of different components of the system such as the speech recognition, natural language understanding, and natural language generation. Such errors may occur infrequently, but still often enough to have a significant impact on key performance indicators (KPIs). Identifying the root cause of these errors is a complex task that requires a team to meticulously examine and interpret the interaction between the voice agent and customers. In this work, we present an interactive system, DTTool, that surfaces system-generated annotations that hint at anomalous events that lead to candidate errors that impact KPIs and demonstrate how the team could discover unknown errors using DTTool.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {234},\nnumpages = {10},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650901,\nauthor = {Fan, Yitao and Zhang, Yifu and Feng, Boyu and Pan, Deying and Zhang, Yuyang and Li, Jiaji and Liao, Xinyi and Zhao, Xiaoliang and Tao, Ye and Wang, Qi and Sun, Lingyun and Wang, Guanyun},\ntitle = {MagPixel: Modular Toolkit for Designing Interactive Magnetic Shape Displays},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650901},\ndoi = {10.1145/3613905.3650901},\nabstract = {This paper introduces MagPixel, a modular toolkit for democratizing the design and fabrication of interactive shape displays using magnetic soft materials in the realm of Human-Computer Interaction (HCI). Known for their instant responsiveness and flexibility, magnetic soft materials have been used in material science and soft robotics but remain under-explored in HCI. MagPixel addresses this gap by offering a user-friendly, cost-effective solution for creating morphing tangible interfaces actuated by external magnetic fields. Existing methods of fabricating magnetic soft interfaces are complex, requiring multiple specialized tools and skills for casting, magnetization, and assembly. By simplifying fabrication into straightforward, standardized procedures, MagPixel eliminates the need for proprietary materials and extensive resources. The toolkit provides DIY enthusiasts in the maker community with the means to effortlessly create and modify interactive magnetic soft displays, MagPixel paves the way for broader experimentation and innovation in interactive design and HCI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {235},\nnumpages = {6},\nkeywords = {Shape-changing interfaces, fabrication, magnetic materials, tangible interfaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650809,\nauthor = {Qiu, Ling and Blair, Johnna and Abdullah, Saeed},\ntitle = {Managing Finances for Persons Living with Dementia: Current Practices and Challenges for Care Partners},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650809},\ndoi = {10.1145/3613905.3650809},\nabstract = {Dementia is a global public health concern. There are more than 55 million persons living with dementia (PLwDs) globally with 10 million new cases annually. Dementia is characterized by declining cognitive and functional abilities over time. This can lead to serious challenges in financial decision-making and stability for PLwDs. Care partners are often required to undertake financial management responsibilities on behalf of PLwDs. In this preliminary work, we explore the needs and challenges for care partners in managing the finances of PLwDs. We interviewed (N=7) care partners for individuals with mild to severe dementia. We describe our initial findings on the emotional challenges for care partners, the transition of their financial responsibilities over time, and the need for preemptive planning and different collaborative financial management support throughout the course of the illness. These findings can inform accessible and supportive financial technology design for PLwDs and their care partners.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {236},\nnumpages = {7},\nkeywords = {Aging, Dementia, Financial technologies},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650745,\nauthor = {Nowak, Oliver and Becker, Lennart and Pettirsch, Sebastian Valentin and Borchers, Jan},\ntitle = {Mappings in the Home: Selecting Home Appliances in 3D Space},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650745},\ndoi = {10.1145/3613905.3650745},\nabstract = {Unlike voice assistants, remotes, and smartphones, UIs embedded into furniture and other surfaces offer silent, discreet, and unobtrusive control of smart home appliances. However, as the number of appliances grows, fitting individual controls for each onto the surfaces in our environment becomes impractical, making it necessary to select appliances before controlling them. These appliances are placed in 3D at various heights around the room, while traditional controls are laid out in 2D, complicating control-to-target mapping. We compared six UIs using mappings with spatial analogies that are either absolute or relative to the user’s position and perspective. Participants used each to select 20 targets in a simplified living room, once while looking and once eyes-free. We investigated performance and participants’ ratings for, inter alia, ease of use, mapping comprehensibility, and mental demand. Map-based controllers were most promising, but participants also ranked perspective projection with touch input highly.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {237},\nnumpages = {7},\nkeywords = {haptic, natural mappings, smart home, target selection},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650872,\nauthor = {Zulfikar, Wazeer Deen and Pierce, Cayden and Maes, Pattie},\ntitle = {MeMic: Towards Social Acceptability of User-Only Speech Recording Wearables},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650872},\ndoi = {10.1145/3613905.3650872},\nabstract = {Wearables that record audio continuously have various applications such as health monitoring and cognitive augmentation. However, they raise serious privacy concerns amongst bystanders and conversation partners, reducing their social acceptability. To address this, we designed the MeMic, a wearable that records audio only when the user speaks, driven by a hardware-based voice activity detector. A visible light on the wearable indicates when it is actively recording to enhance trust for others. We validate its performance with participants (N=12) wearing the MeMic and performing tasks. Further, an online study (N=168) compared the social acceptability of the MeMic’s self-recording paradigm versus continuous recording. We find significantly less social fears alongside reduced privacy concerns in the self-recording paradigm, thereby improving social acceptability. We also explore different conceptual form factors (glasses, pendant necklace, and behind-the-neck) for the MeMic and find that the pendant necklace is the most preferred. This work contributes towards enhancing the social comfort of wearables that continuously capture users’ speech.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {238},\nnumpages = {9},\nkeywords = {audio recording, privacy, social acceptability, wearables},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650807,\nauthor = {Vanderdonckt, Jean and Vatavu, Radu-Daniel and Manon, Julie and Saint-Guillain, Michael and Lefevre, Philippe and Marquez, Jessica J.},\ntitle = {Might as Well Be on Mars: Insights on the Extraterrestrial Applicability of Interaction Design Frameworks from Earth},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650807},\ndoi = {10.1145/3613905.3650807},\nabstract = {As humanity expands its reach into the Cosmos, the imagination-sparkling prospect of colonizing other planets, such as Mars, becomes increasingly tangible. However, establishing livable environments on Mars necessitates robust and efficient computer systems, and thus design knowledge for highly usable interactions that match users’ abilities under the unique challenges posed by other planets’ environments. In this work, we connect to current interaction design frameworks, such as Ability-based Design, Reality-based Interaction, and Sensorimotor Realities, to assess their suitability beyond Earth. Furthermore, we present insights from the user experience of interactive systems on Mars through observations collected during a mission at the Mars Desert Research Station. We use our findings to propose future research on interaction frameworks with extraterrestrial and interplanetary applicability.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {239},\nnumpages = {8},\nkeywords = {Extraterestrial, Mars mission, design frameworks, interaction design, user interface design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650874,\nauthor = {Lazaro, May Jorella and Kim, Sungho},\ntitle = {Mind the Mix: Exploring the Cognitive Underpinnings of Multimodal Interaction in Augmented Reality Systems},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650874},\ndoi = {10.1145/3613905.3650874},\nabstract = {Exploring the intricate dynamics of Multimodal Interaction (MMI) in Augmented Reality (AR), this study presents a novel conceptual framework, crafted from a review of cognitive theories. Our research delves into how input modalities, output modalities, and their combinations uniquely influence user experiences in AR environments. Recognizing a gap in the existing MMI literature, especially within the AR context, we propose a conceptual framework to understand these complex relationships. Our framework pinpoints three critical factors: the choice of input modality, the verbal processing code of outputs, and the synergistic effects of input-output combinations. These elements are hypothesized to significantly impact user interaction and performance in AR systems. This work-in-progress not only contributes to the theoretical discourse in HCI but also sets the stage for future empirical investigations, aiming to enhance user-centered design in the evolving field of AR technology.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {240},\nnumpages = {7},\nkeywords = {Augmented Reality, Cognitive Theories, Input and Output Modalities, Multimodal Interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650914,\nauthor = {Zou, Ruotong and Yin, Shuyu and Song, Tianqi and Qin, Peinuan and Lee, Yi-Chieh},\ntitle = {Mitigating Ageism through Virtual Reality: Intergenerational Collaborative Escape Room Design},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650914},\ndoi = {10.1145/3613905.3650914},\nabstract = {As virtual reality (VR) becomes more popular for intergenerational collaboration, there is a significant gap in research regarding understanding the potential to reduce ageism. Our study aims to address this gap by analyzing ageism levels before and after collaborative VR escape room experiences. We recruited 28 participants to collaborate with an older player in a challenging VR escape room game. To ensure consistent and reliable performance data of older players, our experimenters simulated older participants following specific guidelines. After completing the game, we found a significant reduction in ageism among younger participants. Furthermore, we introduce a new game mechanism that encourages intergenerational collaboration. Our research highlights the potential of collaborative VR games as a practical tool to mitigate ageism. It provides valuable insights for designing immersive VR experiences that foster enhanced intergenerational collaboration.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {241},\nnumpages = {7},\nkeywords = {ageism mitigation, social VR, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650775,\nauthor = {Uttarapong, Jirassaya and Wang, Yihan and Wohn, Donghee Yvette},\ntitle = {ModeratorHub: A Knowledge Sharing and Relationship Building Platform for Moderators},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650775},\ndoi = {10.1145/3613905.3650775},\nabstract = {This design project arose with the purpose to intervene within the current landscape of content moderation. Our team’s primary focus is community moderators, specifically volunteer moderators for online community spaces. Community moderators play a key role in up-keeping the guidelines and culture of online community spaces, as well as managing and protecting community members against harmful content online. Yet, community moderators notably lack the official resources and training that their commercial moderator counterparts have. To address this, we present ModeratorHub, a knowledge sharing platform that focuses on community moderation. In our current design stage, we focused 2 features: (1) moderation case documentation and (2) moderation case sharing. These are our team’s initial building blocks of a larger intervention aimed to support moderators and promote social support and collaboration among end users of online community ecosystems.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {242},\nnumpages = {6},\nkeywords = {Content moderation, collaboration, knowledge sharing, online community, volunteer moderators},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650806,\nauthor = {Huang, Lingtao and Xia, Chengshuo},\ntitle = {ModifyAug: Data Augmentation for Virtual IMU Signal based on 3D Motion Modification Used for Real Activity Recognition},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650806},\ndoi = {10.1145/3613905.3650806},\nabstract = {In wearable human activity recognition (HAR), the generation and utilization of virtual IMU data has recently gained attention. The use of virtual data can improve the robustness, effective features, and customized motion types chosen in the HAR system. However, few studies have focused on augmenting virtual IMU data to reduce the dependence on real IMU data during the machine learning model training phase. This work proposes modifying the reconstructed 3D motion by its joint data to generate larger 3D motion based on a given 3D motion sequence, thus augmenting the virtual IMU dataset. The method simulates the intra-difference of the same motion type in the real world. It aims to use fewer 3D motion inputs to generate the larger size of the virtual IMU dataset to recognize the real activity. The experiment demonstrated the feasibility of the proposed method and provided insight into the 3D motion modification-based augmentation method.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {243},\nnumpages = {7},\nkeywords = {3D Avatar, Human Activity Recognition, Intra-Difference, Virtual IMU Signal},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650858,\nauthor = {Chen, Yang and Yen, Ching Chiuan},\ntitle = {More Than Just Limits: How Technology Can Support Parents in Regulating Children's Eating Behaviors at Family Mealtimes},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650858},\ndoi = {10.1145/3613905.3650858},\nabstract = {To alleviate concerns regarding technology use at family mealtimes, this study explored how specific technological probes (including augmented utensils, screen-based designs, and smart objects) can aid parents in regulating children’s eating behavior. Our two-phase investigation included remote observations of 25 families with children aged 3-7 to unobtrusively understand parent-child interactions. This was complemented by retrospective semi-structured interviews to delve into food parenting practices and technology-related preferences and concerns. Our study uncovered a mismatch between parents’ perceptions and the actual eating behaviors of children, emphasizing the significance of technology in both regulating eating habits and offering objective insights to parents. While screen-based technologies were identified as the most feasible and desirable option for managing children’s eating, we proposed enhancing their functionality by integrating parental involvement and control mechanisms along with mitigating adoption barriers (e.g., technology dependence and distraction) to better support parental intervention during family meals.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {244},\nnumpages = {8},\nkeywords = {children, eating intervention, family mealtime, technology-mediated intervention},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650853,\nauthor = {Attig, Christiane and Wollstadt, Patricia and Schrills, Tim and Franke, Thomas and Wiebel-Herboth, Christiane B.},\ntitle = {More than Task Performance: Developing New Criteria for Successful Human-AI Teaming Using the Cooperative Card Game Hanabi},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650853},\ndoi = {10.1145/3613905.3650853},\nabstract = {As we shift to designing AI agents as teammates rather than tools, the social aspects of human-AI interaction become more pronounced. Consequently, to develop agents that are able to navigate the social dynamics that accompany cooperative teamwork, evaluation criteria that refer only to objective task performance will not be sufficient. We propose perceived cooperativity and teaming perception as subjective metrics for investigating successful human-AI teaming. Corresponding questionnaire scales were developed and tested in a pilot study employing the collaborative card game Hanabi, which has been identified as a unique setting for investigating human-AI teaming. Preliminary descriptive results suggest that rule-based and reinforcement learning-based agents differ in terms of perceived cooperativity and teaming perception. Future work will extend the results in a large user study to psychometrically evaluate the scales and test a conceptual framework that includes further aspects related to social dynamics in human-AI teaming.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {245},\nnumpages = {11},\nkeywords = {Human-AI teaming, collaboration, cooperation, human-autonomy teaming, social perception},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650734,\nauthor = {Gianotti, Mattia and Marini, Maria Chiara and Beccaluva, Eleonora Aida and Marulli, Matilde Maria and De Meis, Italo and Tomaiuoli, Donatella and Garzotto, Franca},\ntitle = {Multisensory Training Intervention for Hearing Impaired Children: Preliminary Results of a Pilot Study},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650734},\ndoi = {10.1145/3613905.3650734},\nabstract = {This paper examines the influence of the Interactive Multisensory Environment (iMSE) on the training of deaf children in comparison to traditional methods. Over a 7-week duration, two groups of deaf children were evaluated and trained, one utilizing the iMSE (Experimental Group) and the other employing a traditional PC-based method (Control Group). The training encompassed four different thematic categories, each with nine associated sounds. The iMSE offered an immersive and dynamic learning experience, while the PC-based method presented stimuli through a desktop computer. Results indicate that the iMSE yielded positive effects on the training outcomes of deaf children, as evidenced by improved performance and engagement. This research sheds light on the potential benefits of innovative multisensory technology in educational settings for children with hearing impairments, offering insights for future educational interventions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {246},\nnumpages = {7},\nkeywords = {Deaf children, Interactive Multisensory Environment, Interactive Smart Spaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651101,\nauthor = {Bauer, Valentin and Padovano, Tommaso and Gianotti, Mattia and Caslini, Giacomo and Garzotto, Franca},\ntitle = {MusicTraces: A collaborative music and paint activity for autistic people},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651101},\ndoi = {10.1145/3613905.3651101},\nabstract = {Painting and music therapy approaches can help to foster social interaction for autistic people. However, the tools sometimes lack of flexibility and fail to keep people’s attention. Unknowns also remain about the effect of combining these approaches. Though, very few studies have investigated how Multisensory Environments (MSEs) could help to address these issues. This paper presents the design of a full-body music and painting activity called “MusicTraces” which aims to foster collaboration between people with moderate to severe learning disabilities and complex needs, and in particular autism, within an MSE. The co-design process with caregivers and people with neurodevelopmental conditions is detailed, including a workshop, the initial design, remote iterations, and a design critique.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {247},\nnumpages = {7},\nkeywords = {Art Therapy, Autism, Multisensory Environments, Music, Neurodevelopmental Conditions, Paint},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650810,\nauthor = {Chakraborti, Mahasweta and Bonagiri, Sailendra Akash and Virg\\\"{u}ez-Ruiz, Santiago and Frey, Seth},\ntitle = {NLP4Gov: A Comprehensive Library for Computational Policy Analysis},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650810},\ndoi = {10.1145/3613905.3650810},\nabstract = {Formal rules and policies are fundamental in formally specifying a social system: its operation, boundaries, processes, and even ontology. Recent scholarship has highlighted the role of formal policy in collective knowledge creation, game communities, the production of digital public goods, and national social media governance. Researchers have shown interest in how online communities convene tenable self-governance mechanisms to regulate member activities and distribute rights and privileges by designating responsibilities, roles, and hierarchies. We present NLP4Gov, an interactive kit to train and aid scholars and practitioners alike in computational policy analysis. The library explores and integrates methods and capabilities from computational linguistics and NLP to generate semantic and symbolic representations of community policies from text records. Versatile, documented, and accessible, NLP4Gov provides granular and comparative views into institutional structures and interactions, along with other information extraction capabilities for downstream analysis.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {248},\nnumpages = {8},\nkeywords = {Collective Action, OSS Governance, Online Communities, Open Source Software, Peer Production, Policy Analysis},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650863,\nauthor = {Shahria, Md Tanzil and Banik, Nayan and Sunny, Md Samiul Haque and Rahman, Mohammad H},\ntitle = {Navigating Daily Life: Insights from Powered Wheelchair Users on Assistive Technologies and Caregiver Support},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650863},\ndoi = {10.1145/3613905.3650863},\nabstract = {The increasing number of powered wheelchair users, driven by an aging global population and advancements in medical care, highlights a critical need to understand their unique experiences and challenges, particularly regarding assistive technologies and caregiver support. Our study, drawing on qualitative data from 11 participants with diverse disabilities, investigates the complex interplay of technology, independence, and socio-economic factors in their lives. It reveals a significant gap between the potential and actual use of assistive technologies, underscoring the importance of innovation in areas where users heavily rely on caregivers. The implication of this study includes emphasizing the necessity for inclusive, adaptable, and economically accessible assistive technologies and highlighting the vital role of human support in augmenting the quality of life for individuals with mobility impairments.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {249},\nnumpages = {7},\nkeywords = {Assistive Device, Caregiver Burden, Disability Support, Powered Wheelchair User Experience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651046,\nauthor = {Lin, Hung-Wen and Lin, Chun-Huang and Huang, Chi-Hung and Chen, Hsin-Ai and Chiu, Hsiang-Chih and Yuan, Chien Wen (Tina) and Bi, Nanyi and Chao, Su-Yi and Jheng, Zih-Yun and Lin, Chun and Huang, Ming-Chyi and You, Chuang-Wen},\ntitle = {Navigating the Design Implications for Integrating Virtual-Reality Cues Clinically in Drug Psychotherapy},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651046},\ndoi = {10.1145/3613905.3651046},\nabstract = {This study investigated the utilization of virtual scenarios to evoke drug cravings and associated physical responses, revealing that self-reported craving scores did not persist beyond the virtual reality (VR) sessions for the 19 participants. Qualitative findings emphasized the importance of considering contextual factors resembling participants’ drug use experiences in managing induced cravings. The study underscores the need for careful scenario design based on the patient’s recovery stage to effectively manage the intensity of cravings. These insights contribute to the development of VR scenarios for clinical psychotherapy, offering valuable implications for future research and therapeutic applications.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {250},\nnumpages = {6},\nkeywords = {Drug addiction, psychotherapy., virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650913,\nauthor = {Berney, Manon and Ondrus, Jan and Holzer, Adrian},\ntitle = {Navigating the Shadows of Cyber Vigilantism: A Preliminary Analysis of Social Dynamics and Activities of Scambaiting},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650913},\ndoi = {10.1145/3613905.3650913},\nabstract = {From 2017 to 2022, the Internet Crime Complaint Center documented an increase of 266\\% complains for online scams. The complexity and constant evolution of these scams pose a challenge for the existing legal system, which appears ill-equipped to effectively combat them. A new movement emerged known as “scambaiting”, individuals who autonomously take on the responsibility of advocating for others and fighting against scammers. Leveraging online platforms, these individuals champion causes and educate the public, preventing further victimization. Scambaiting techniques range from entertaining activities, like wasting scammers’ time, to illegal ones, such as hacking. This form of cyber-vigilantism represents a novel and under-explored research, especially in term of human-infrastructure. Through the analysis of ten transcripts of discussions involving a sample of scambaiters, we aim to explore the social dynamics and activities of scambaiting; giving insights on collaboration, actors, and challenges within the scambaiting community. Additionally, we present suggestions for future research.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {251},\nnumpages = {6},\nkeywords = {cyber vigilantism, digilantism, online activism, scambaiting, scamming, social dynamics, vigilantism, virtual communities},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650918,\nauthor = {Li, Ge and Seaborn, Katie},\ntitle = {No Joke: An Embodied Conversational Agent Greeting Older Adults with Humour or a Smile Unrelated to Initial Acceptance},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650918},\ndoi = {10.1145/3613905.3650918},\nabstract = {Embodied conversation agents (ECAs) are increasingly being developed for older adults as assistants or companions. Older adults may not be familiar with ECAs, influencing uptake and acceptability. First impressions can correlate strongly with subsequent judgments, even of computer agents, and could influence acceptance. Using the circumplex model of affect, we developed three versions of an ECA—laughing, smiling, and neutral in expression—to evaluate how positive first impressions affect acceptance. Results from 249 older adults indicated no statistically significant effects except for general attitudes towards technology and intelligent agents. This questions the potential of laughter, jokes, puns, and smiles as a method of initial engagement for older adults.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {252},\nnumpages = {7},\nkeywords = {Acceptability, Embodied conversational agents, First impressions, Older adults, humor},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650923,\nauthor = {Ferguson, Sharon A and Van De Zande, Georgia and Olechowski, Alison},\ntitle = {No Risk, No Reward: Towards An Automated Measure of Psychological Safety from Online Communication},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650923},\ndoi = {10.1145/3613905.3650923},\nabstract = {The data created from virtual communication platforms presents the opportunity to explore automated measures for monitoring team performance. In this work, we explore one important characteristic of successful teams — Psychological Safety — or the belief that a team is safe for interpersonal risk-taking. To move towards an automated measure of this phenomenon, we derive virtual communication characteristics and message keywords related to elements of Psychological Safety from the literature. Using a mixed methods approach, we investigate whether these characteristics are present in the Slack messages from two design teams— one high in Psychological Safety, and one low. We find that some usage characteristics, such as replies, reactions, and user mentions, might be promising metrics to indicate higher levels of Psychological Safety, while simple keyword searches may not be nuanced enough. We present the first step towards the automated detection of this important, yet complex, team characteristic.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {253},\nnumpages = {7},\nkeywords = {Enterprise Communication Platforms, Psychological Safety, Teams},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650832,\nauthor = {Gao, Jiasi and Chen, An and Yi, Zhennan and Yan, Xueye and Liu, Jiayi and Zhou, Guyue and Gong, Jiangtao},\ntitle = {Not a Playroom, but a Passage: Exploring the Design Space of a Technology-mediated Calm-down Corner},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650832},\ndoi = {10.1145/3613905.3650832},\nabstract = {Preschool educators play an important role in supporting children’s emotion regulation capacities in ways that promote their socio-emotional and behavioral health. However, tracking children’s emotional changes and giving timely intervention to every child in the class simultaneously is challenging for preschool educators. In this paper, we use the \"calm-down corner,\" which is widely used in classrooms with spaces where children can regain emotional control, as a probe to explore the design space of technology-supported preschool educators dealing with children’s emotional problems through one semi-structured interview with ten preschool educators and one co-design session with another 12 preschool educators. We summarized current challenges and the design considerations from preschool educators’ perspective, which could offer valuable insights for technology-enabled systems to facilitate the emotion regulation process for educators in the future.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {254},\nnumpages = {8},\nkeywords = {educators, emotion regulation, preschooler, qualitative research},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650909,\nauthor = {Kang, Sol and Odom, William},\ntitle = {On the Design of Quologue: Uncovering Opportunities and Challenges with Generative AI as a Resource for Creating a Self-Morphing E-book Metadata Archive},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650909},\ndoi = {10.1145/3613905.3650909},\nabstract = {People can highlight text on e-books via many devices and applications, generating metadata that precisely captures and stores passages of interest in their personal digital archives. Despite the rising popularity of e-books, little research in HCI has inquired into the potential of these digital traces as a design resource for creativity and self-reflection. In this paper, we present Quologue, a web application that allows users to reconnect with their e-book highlights through ongoing dialogue and interactions. We aim to reimagine e-book highlights as design materials for creative self-expression instead of static records of past interactions. Through a design-led approach that combines research through design and autobiographical design, we reflect on three design events revealed through our process that illustrate alternative ways of interacting with one's e-book metadata through first-hand manipulation shaped by generative AI. We conclude with implications for future HCI studies and practice.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {255},\nnumpages = {16},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651037,\nauthor = {Joshi, Nikhita and Li, Richard and Li, Jiannan and Pavanatto, Leonardo and Pahud, Michel and Sharma, Jatin and Lee, Bongshin and Romat, Hugo and Buxton, William and Marquardt, Nicolai and Hinckley, Ken and Henry Riche, Nathalie},\ntitle = {Opportunistic Nudges for Task Migration Between Personal Devices},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651037},\ndoi = {10.1145/3613905.3651037},\nabstract = {Using multiple devices to exploit their strengths and mechanics for a task is referred to as “migration.” However, re-establishing context upon moving from one device to another can be cumbersome. We propose opportunistic nudges as a way to more seamlessly share content between personal devices. Opportunistic nudges appear in the bezel when a device migration occurs and can be interacted with to quickly share files and applications. However, if the user ignores them, they automatically disappear after some time. We explore the design space of opportunistic nudges through rapid prototyping and develop a preliminary design space consisting of four stages. Focusing on six design parameters of the Visualization stage, we gather feedback on the concept through an exploratory user study. Results show that opportunistic nudges can be an effective way to reduce the transaction costs of sharing content between devices.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {256},\nnumpages = {8},\nkeywords = {cross-device computing, distributed user interfaces, multi-device},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651040,\nauthor = {Zhou, Xiaofei and Xiong, Pei and Xiao, Qinqin and Bai, Zhen},\ntitle = {OptiDot: An Optical Interface for Children to Explore Dot Product and AI Recommendation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651040},\ndoi = {10.1145/3613905.3651040},\nabstract = {Understanding the inner workings of Artificial Intelligence (AI) recommendation systems may benefit children in becoming more sensible consumers of the ever-growing information in their daily lives. It may further enable deeper reflections on related ethical issues such as the filter bubble. With limited prior knowledge in math and computing, children often find AI concepts overly abstract. Inspired by optical computation, we propose a novel tangible interface, OptiDot. Through exploratory manipulation with light beams, OptiDot supports children in learning the dot product—a building block for numerous AI algorithms—and AI recommendations through embodied learning experiences. Findings of a preliminary user study with ten middle school students indicate the effectiveness of the key embodied metaphors. We also discuss the design implications and challenges of developing optical-inspired learning tools for children.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {257},\nnumpages = {7},\nkeywords = {AI literacy, Tangible interface, embodied metaphor, optics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651002,\nauthor = {Yu, Xinyan and Tran, Tram Thi Minh and Wang, Yiyuan and Mah, Kristina and Cao, Yidan and Johansen, Stine S and Johal, Wafa and Lupetti, Maria Luce and Rose, Megan and Rittenbruch, Markus and Zsolczay, Rodney G and Hoggenm\\\"{u}ller, Marius},\ntitle = {Out of Place Robot in the Wild: Envisioning Urban Robot Contextual Adaptability Challenges Through a Design Probe},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651002},\ndoi = {10.1145/3613905.3651002},\nabstract = {The increasing deployment of robots in urban spaces calls for design strategies to ensure their adaptation and to mitigate potential disruptions to complex urban contexts. Our research aims to initiate the discussion of contextual adaptability issues of urban robots by exploring everyday scenarios where their presence would appear out of place. We created a design probe for people to carry in their daily lives, facilitating them to envision the robot’s presence and capture scenarios where a robot seems to be disruptive. We collected data by distributing the probes among the research team and conducting a city walk activity using the probe at a workshop. This paper presents factors arising from the collected scenarios, encompassing temporal, spatial, cultural, and social dynamics, as well as various stakeholders that robots need to adapt to. These findings provide a blueprint and potential research directions for future research into robot contextual adaptability in urban environments.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {258},\nnumpages = {7},\nkeywords = {Design Probe, Scenarios, Urban Robot},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650961,\nauthor = {Schaap, Gabi and Van de Sande, Yana and Schraffenberger, Hanna},\ntitle = {Outperformed by AI: Interacting with Superhuman AI Changes the Way We Perceive Ourselves},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650961},\ndoi = {10.1145/3613905.3650961},\nabstract = {With AI increasingly exceeding human performance in previously quintessential human domains, the question posed in this study is how interacting with ‘superhuman’ AI will affect how we define 1) ourselves as human beings, and 2) our relationship towards the technology. To address this question, we conducted two playful quasi-experimental studies (Ntotal = 349). In both experiments, participants interacted with an AI system, and were, or were not, outperformed by it. Results consistently show that when AI outperformed the participants, they felt inferior, experienced less agency (control over the situation), and had a less positive attitude towards the AI system. This suggests that interaction with intelligent machines may well redefine our perceptions of what it means to be human in the long run and determine how we act towards this technology.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {259},\nnumpages = {7},\nkeywords = {AI Attitudes, Human-AI Interaction, Self-perception, Self-worth, Sense of Agency, Superhuman AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650753,\nauthor = {Kim, Chan Mi and Van Rompay, Thomas and Ludden, Geke},\ntitle = {Outside In: Creating Digital Nature Tailored To The Needs of Intensive Care Unit Patients},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650753},\ndoi = {10.1145/3613905.3650753},\nabstract = {Intensive care unit (ICU) environments play a crucial role in supporting patients’ recovery, and nature experiences, particularly their visual elements, are commonly used in ICUs to stimulate relaxation. Fueled by digital technology, applications of virtual nature have emerged to bring nature to environments without direct access, including windowless ICU rooms. Despite its healing potential, there is a lack of consensus and strategy in designing virtual nature catering to the diverse patients’ needs. This study investigates how to create virtual nature for intended effects promoting relaxation. Informed by a framework explaining the working mechanism underlying relaxation in nature, we introduce Digital Nature, a visual stimulation featuring a 24-hour streaming video of constantly changing virtual scenes. We describe how we incorporated an evidence-based approach into the design process of Digital Nature. A pilot study is planned to validate the effectiveness of Digital Nature, and ideas for further design implications are discussed.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {260},\nnumpages = {7},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650735,\nauthor = {Johns, MJ and Ezenwa, Emmanuel Chinedum and Lee, Seunghyun and Maiorana, Thomas and Wood, Ciel and Levano, Josh D and Tesfay, Rita Aksum and Takami, Michael and Dodd, Cameron A and Li, Madison and Manning, Hanne and Pak, Regis and Chen, Lily and Saini, Ria K and Escarce, M\\'{a}rio and Hendawy, Mennatullah and Seif El-Nasr, Magy and Melcer, Edward F. and Isbister, Katherine},\ntitle = {Participatory Design of a Serious Game to Improve Wildfire Preparedness with Community Residents and Experts},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650735},\ndoi = {10.1145/3613905.3650735},\nabstract = {Serious games can be a valuable addition to the resources available for increasing community resilience in the face of disaster. The purpose of this project is to work closely with two communities at risk of wildfire to design and develop a serious game that can be used by experts and leaders within those communities as part of their holistic approach to increasing resilience and improving readiness of their community members. We follow a community-centric approach, utilizing Participatory Design and Research through Design to engage directly with community members and local experts. This paper documents our findings at two workshops involving the early game designs and contributes to the broader exploration of the use of Participatory Design with serious games in support of community initiatives around disaster preparedness.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {261},\nnumpages = {8},\nkeywords = {Climate Change, Community Trust Building, Serious Game Design, Wildfire Resilience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650936,\nauthor = {Anthraper, Nisha and Javiya, Prachee and Iluru, Sai and Chen, Lujie Karen and Kleinsmith, Andrea},\ntitle = {PeerConnect: Co-Designing a Peer-Mentoring Support System with Computing Transfer Students},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650936},\ndoi = {10.1145/3613905.3650936},\nabstract = {In the US, nearly half of the STEM undergraduates begin their academic careers at community colleges. Transferring to four-year institutions can be challenging. Evidence suggests that mentoring can help by increasing a sense of belonging and retention. We engaged mentors and mentees from a pilot mentoring program for new transfer students in computing majors at a minority-serving institution in the Northeastern US in a co-design workshop to understand their needs and requirements for a peer-mentoring system, PeerConnect. PeerConnect aims to foster transfer students’ academic and social engagement, increase self-efficacy and belonging, and develop students’ self-regulated learning skills. Preliminary results show that students want features that push the system beyond merely measuring engagement to actively promoting it. This study contributes to HCI and CSCW work in designing support systems for mentoring and peer support programs in educational settings and to the emerging literature on student-centered learning analytics systems.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {262},\nnumpages = {7},\nkeywords = {co-design, learning analytics, peer mentoring, sense of belonging},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651008,\nauthor = {Liu, Jiawen and Yao, Yuanyuan and An, Pengcheng and Wang, Qi},\ntitle = {PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651008},\ndoi = {10.1145/3613905.3651008},\nabstract = {In children’s collaborative learning, effective peer conversations can significantly enhance the quality of children’s collaborative interactions. The integration of Large Language Model (LLM) agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants. We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem. The peer conversation transcripts were analyzed using thematic analysis. We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded. As participants, they foster children’s creative thinking but may not consistently provide timely feedback. These findings highlight potential design improvements and considerations for peer agents in both roles.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {263},\nnumpages = {6},\nkeywords = {Collaborat learning, Conversational agent, Large Language Model, Peer conversation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650789,\nauthor = {Kong, Dehui and Wang, Hongyue and Zhou, Sijie and Cui, Hong and Feng, Zhiquan},\ntitle = {PenLab: Towards Understanding of Active Collaboration for Solid Geometry Teaching},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650789},\ndoi = {10.1145/3613905.3650789},\nabstract = {With the continuous advancement of technologies such as VR and sensors, pen-based interaction has transcended the limitations of 2D interfaces. Although research on aspects of Human-Pen Interaction, such as pen grip, gesture operations, and tactile support, has been extensive, a thorough exploration of active collaborative interaction with the pen remains relatively limited. Active collaboration in Human-Pen Interaction refers to the system understanding the participants’ interaction intentions and actively providing feedback and guidance for collaboration. Facing the dilemma of inaccurate selection in pen interactions for teachers in solid geometry teaching, we have designed an interactive system for solid geometry teaching with active collaboration capabilities, consisting of a depth camera, a smart pen embedded with multiple sensors, and a virtual geometry teaching platform. By inviting participants to experience the system and collecting quantitative data on user experience and attitudes, the results indicate that the system can assist in geometry teaching with more precise and flexible interaction methods.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {264},\nnumpages = {7},\nkeywords = {active collaborative interaction, human-pen interaction, smart pen, virtual solid geometry teaching},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651055,\nauthor = {Yaar, Gal and Oberlender, Agam and Heimann Saadon, Nevo and Zuckerman, Oren and Erel, Hadas},\ntitle = {Performing a Task Alongside a Robot: Exploring the Impact of Social Comparison},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651055},\ndoi = {10.1145/3613905.3651055},\nabstract = {As robots become common in our environment, they are predicted to perform tasks alongside humans. Social psychology studies indicate that performing tasks next to others leads to social comparison. The tendency to overestimate robots’ capabilities is predicted to lead to an upward comparison that can result in negative outcomes. We evaluated whether performing a task alongside a robot would impact participants’ sense of control and their overall performance. Participants performed a search task either before or alongside a robotic dog that performed search training. Our findings indicated that performing the task alongside the robot led to a negative impact on sense of control, search efficiency, and performance accuracy. We conclude that robot designers should carefully consider the impact of robots who perform tasks alongside humans, even when there is no collaboration and when there is independence between the performance of the human and that of the robot.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {265},\nnumpages = {7},\nkeywords = {Human-Robot Interaction, Overestimation of Robots’ Capabilities, Robot, Search Task, Sense of Control, Social Comparison},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650902,\nauthor = {Smolansky, Adele and Baker, Catherine M. and Azenkot, Shiri and Milne, Lauren R.},\ntitle = {Perspectives of Assistive Technology Industry Professionals on AT for Children},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650902},\ndoi = {10.1145/3613905.3650902},\nabstract = {Assistive technology (AT) can help children with disabilities improve their learning outcomes, social skills, and autonomy, but past work has shown that AT is often abandoned or underutilized in classrooms and at home. However, little research focuses on the perspectives of the AT industry professionals who are supporting children, even though they often provide direct and indirect support to their customers. As such, we conducted semi-structured interviews with 21 AT industry professionals to investigate how AT companies are involved in the successful integration of AT into a child's life. We uncovered three strong needs: (1) improving communication channels between AT industry and support networks, (2) demonstrating the efficacy of AT and (3) exploring long term support of AT.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {266},\nnumpages = {7},\nkeywords = {accessibility, assistive technology, children with disabilities},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650966,\nauthor = {Subbaraman, Blair and Peek, Nadya},\ntitle = {Playing the Print: MIDI-Based Fabrication Interfaces to Explore and Document Material Behavior},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650966},\ndoi = {10.1145/3613905.3650966},\nabstract = {Digital fabrication software supports common activities like designing models and setting parameters. However, the increasing diversity of fabrication materials and contexts means that determining the right settings is a constant challenge. Manipulating machine parameters and observing material results is necessary for successful outcomes. In this work, we present tools to iteratively develop computer-controlled fabrication workflows. These tools generate toolpaths using Javascript code, continuously manipulate parameters during machine execution, and document the resulting material behavior. First, we present software to interactively tune 3D prints. We use a MIDI controller to modulate fabrication parameters during execution. We demonstrate our approach through a set of 3D prints created with our software. Second, we introduce software which synchronizes video of a fabrication process with the machine instructions being executed. Doing so archives the effect of manipulating machine parameters. We argue that infrastructure which encourages exploration and documentation of both code and materials are crucial to support broader uptake of fabrication technologies in creative contexts.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {267},\nnumpages = {8},\nkeywords = {3D printing, Creativity, Digital Art, Documentation, Fabrication, MIDI control, p5.fab, p5.js},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650776,\nauthor = {Luchak, Iryna and Asha, Ashratuz Zavin and Sharlin, Ehud},\ntitle = {Podscape: Exploring the Comfort Level with Pods in Pedestrian Spaces through Immersive Simulation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650776},\ndoi = {10.1145/3613905.3650776},\nabstract = {As autonomous vehicles (AVs) advance in navigating complex settings, investigating their coexistence with people and how they are perceived becomes crucial. Pods, also known as low-speed autonomous transport systems (L-SATS), are emerging in pedestrian spaces, such as museums and malls, impacting people’s comfort in these environments. This research explores how incidentally copresent persons (InCoPs) perceive pods in pedestrian spaces, focusing on their comfort levels as a preliminary factor. We conducted a virtual reality study with 10 participants, examining the importance of various variables, including pod quantity, pod group formation, passenger presence, and InCoP position. Informed by our results, we discussed insights on factors improving InCoPs’ comfort, highlighting the significance of having an enhanced sense of control, the space and freedom to move around, passengers’ awareness, and the social behaviors of other copresent pedestrians.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {268},\nnumpages = {7},\nkeywords = {Pods, copresence, pedestrians, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650825,\nauthor = {Degachi, Chadha and Mehrotra, Siddharth and Yurrita, Mireia and Niforatos, Evangelos and Tielman, Myrthe Lotte},\ntitle = {Practising Appropriate Trust in Human-Centred AI Design},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650825},\ndoi = {10.1145/3613905.3650825},\nabstract = {Appropriate trust, trust which aligns with system trustworthiness, in Artificial Intelligence (AI) systems has become an important area of research. However, there remains debate in the community about how to design for appropriate trust. This debate is a result of the complex nature of trust in AI, which can be difficult to understand and evaluate, as well as the lack of holistic approaches to trust. In this paper, we aim to clarify some of this debate by operationalising appropriate trust within the context of the Human-Centred AI Design (HCD) process. To do so, we organised three workshops with 13 participants total from design and development backgrounds. We carried out design activities to stimulate discussion on appropriate trust in the HCD process. This paper aims to help researchers and practitioners understand appropriate trust in AI through a design lens by illustrating how it interacts with the HCD process.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {269},\nnumpages = {8},\nkeywords = {AI design, appropriate trust, human-centered design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651090,\nauthor = {Kim, Sunbum and Kim, YoungIn and Lee, Geehyuk},\ntitle = {Pressure-Based Menu Selection on a Spherical Tangible Device},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651090},\ndoi = {10.1145/3613905.3651090},\nabstract = {Spherical tangible devices are increasingly utilized in augmented and virtual reality applications due to their universal shape and ease of rotation. However, their form factor presents challenges in integrating input channels, such as buttons, leading to difficulties in menu selection and mode switching—functions essential for complex tasks. In this paper, we explored the utilization of air pressure to augment the input capabilities of spherical devices. We implemented linear menu selection using a pressure-sensitive spherical tangible device and evaluated its performance through three menu selection techniques: Dwell, Quick Release, and Stroke. Our results indicated that participants achieved an average success rate of 97.6\\% and an average selection time of 1,449 ms across all selection techniques for the 12 menu items. Dwell showed higher accuracy, while Quick Release and Stroke showed faster selection times. Lastly, we demonstrated the application of performing various tasks such as object manipulation, locomotion, and text input in a virtual reality environment using a spherical tangible device through pressure-based menu selection.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {270},\nnumpages = {6},\nkeywords = {Force input, Menu selection, Pressure input, Spherical device, Tangible interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650959,\nauthor = {Qian, Yijun and Schwartz, Anna M and Zhang, Yichi and Jung, Ara and Wilds, Gabrielle and Seitz, Uri and Kim, Miso and Kramer, Arthur F and Chukoskie, Leanne},\ntitle = {Promoting Cognitive Health in Older Adults through an Exercise Game Centered around Foreign Language Learning},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650959},\ndoi = {10.1145/3613905.3650959},\nabstract = {Aging is a primary contributor to cognitive decline. Existing research demonstrates that Aerobic Exercise (AE) induces significant functional neuroplasticity changes, enhancing cognitive function. Foreign language learning (FLL) also positively impacts cognitive health and may offer resistance to age-related cognitive decline. Addressing the gap in studying the combined effects of AE and FLL on cognitive benefits, we designed an intervention using a screen-based virtual biking tour to enhance cognitive health in older adults. Our project investigates whether the simultaneous integration of AE and FLL yields superior cognitive and language learning outcomes compared to FLL learning alone. In a preliminary analysis with a small sample, we observed significantly improved Spanish language learning outcomes in both combined physical and language learning groups. The significant cognitive function improvements are also observed in the FLL group after a short-term language learning.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {271},\nnumpages = {7},\nkeywords = {Cognitive Health, Exercise Game, Learning, Older Adults},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650774,\nauthor = {Isaza-Giraldo, Andr\\'{e}s and Bala, Paulo and Campos, Pedro F. and Pereira, Lucas},\ntitle = {Prompt-Gaming: A Pilot Study on LLM-Evaluating Agent in a Meaningful Energy Game},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650774},\ndoi = {10.1145/3613905.3650774},\nabstract = {Building on previous work on incorporating large language models (LLM) in gaming, we investigate the possibility of implementing LLM as evaluating agents of open-ended challenges in serious games and its potential to facilitate a meaningful experience for the player. We contribute with a sustainability game prototype in a single natural language prompt about energy communities and we tested it with 13 participants inside ChatGPT-3.5. Two participants were already aware of energy communities before the game, and eight of the remaining 11 gained valuable knowledge about the specific topic. Comparing ChatGPT-3.5 evaluations of players’ interaction with an expert’s assessment, ChatGPT-3.5 correctly evaluated 81\\% of player’s answers. Our results are encouraging and show the potential of using LLMs as mediating agents in educational games, while also allowing easy prototyping of games through natural language prompts.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {272},\nnumpages = {12},\nkeywords = {Energy Communities, Game-based Learning, Large Language Models (LLMs), Natural Language Processing (NLP), Serious Games, Sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650890,\nauthor = {Sattar, Suliman Kalim and Curtis, Humphrey and Neate, Timothy},\ntitle = {Public Assistive Displays: Employing Public Interactive Displays to Improve Public Transport Access 4All},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650890},\ndoi = {10.1145/3613905.3650890},\nabstract = {Access to public transport services is vital for ensuring equitable participation in society. Yet, barriers to public transport can be experienced due to the challenge of communicating complex – often personal – needs to co-located passengers and staff in dynamic public transport environments. In this project, we explore the potential of leveraging public interactive displays (PIDs) to improve access to public transport systems. To start, we investigate the eminent access needs of passengers on public transport journeys and establish the importance of communication with co-located passengers as a means for ensuring more positive journey outcomes. Consequently, we build and explore the potential of a PID technology probe called the 4All Display. Our 4All Display prototype proactively recognises the contextual needs of its users and employs this data to supportively inform passengers about the needs of their fellow co-located passengers in a discreet and accessible way.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {273},\nnumpages = {8},\nkeywords = {Accessible Transport, Public Interactive Displays},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651006,\nauthor = {Bekavac, Luka Jure Lars and Mayer, Simon and Strecker, Jannis},\ntitle = {QR-Code Integrity by Design},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651006},\ndoi = {10.1145/3613905.3651006},\nabstract = {As QR codes become ubiquitous in various applications and places, their susceptibility to tampering, known as quishing, poses a significant threat to user security. In this paper we introduce SafeQR codes that address this challenge by introducing innovative design strategies to enhance QR code security. Leveraging visual elements and secure design principles, the project aims to make tampering more noticeable, thereby empowering users to recognize and avoid potential phishing threats. Further, we highlight the limitations of current user-education methods in combating quishing and propose different attacker models tailored to address quishing attacks. In addition, we introduce a multi-faceted defense strategy that merges design innovation with user vigilance. Through a user study, we demonstrate the efficacy of ’Integrity by Design’ QR codes. These innovatively designed QR codes significantly raise user suspicion in case of tampering and effectively reduce the likelihood of successful quishing attacks.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {274},\nnumpages = {9},\nkeywords = {QR code based phishing, QR codes, phishing susceptibility, privacy, quishing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650885,\nauthor = {Neupart, Mack and Ashbrook, Daniel and Barkhuus, Louise},\ntitle = {Queering the Space: Location-Based Stories for Transforming Space into Place},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650885},\ndoi = {10.1145/3613905.3650885},\nabstract = {Being part of a minority group can be isolating and lonely; in some places, members of LGBTQ+ groups build up communities and neighborhoods, but in other areas, LGBTQ+ places are more invisible. In this paper we present a system that facilitates a digital layer of stories and experiences by other LGBTQ+ individuals. We present our novel prototype and our user study of the application, where users discuss how this digital service can help building place based community among LGBTQ+. This study contributes by providing a better understanding of how the physical space of the city can be transformed into meaningful places through a digital layer of personal stories to encourage connections within the LGBTQ+ population.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {275},\nnumpages = {6},\nkeywords = {Interactive Experiences, LGBTQ+, Location-Based Services, Mobile Applications, Place},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650781,\nauthor = {Guo, Yijie and Wang, Ruhan and Feng, Yuan-Ling and Lu, Yao and Wang, Xuezhu and Tanaka, Fumihide and Mi, Haipeng},\ntitle = {ROMEO: Towards the Design of Robot with Haptic Mediation for Remote Conflict},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650781},\ndoi = {10.1145/3613905.3650781},\nabstract = {The utilization of robotic haptic stimuli in mediating remote conflicts has shown promising potential. Understanding how users figuratively interpret the metaphor of these stimuli in actual conflict scenarios is vital for the informed design of future mediation robots. The primary objective of this research is to delve into these interpretations and their implications. In this paper, we introduce a mediation robot capable of varying its temperature and softness to embody two roles: Avatar, representing one party in a conflict to express emotions and states, and Companion, serving as an emotional companion for the user. Through a study with 30 participants, we aimed to uncover and analyze the figurative explanations provided by users in response to the robot’s haptic stimuli during conflict situations. Findings indicate that temperature changes were associated with emotional states like anger (hot) and fear (cold), while softness levels were linked to concepts of rigidity (hard) and flexibility (soft). This exploration allows us to discuss the potential role of robotic haptic mediation in remote conflict, thereby contributing valuable insights for the design of future mediation robots.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {276},\nnumpages = {7},\nkeywords = {Haptic Interaction, Remote Conflict, Robot Mediation, Social Robot},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651022,\nauthor = {Feng, Shuyue and Ta, Tung D. and Huang, Shichao and Li, Xiaolong and Koyama, Kazuki and Wang, Guanyun and Yao, Cheng and Kawahara, Yoshihiro and Narumi, Koya},\ntitle = {Rapid Fabrication of Haptic and Electric Input Interfaces using 4D Printed Origami},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651022},\ndoi = {10.1145/3613905.3651022},\nabstract = {Origami is widely used in tangible input interfaces for diverse shapes and haptic feedback. However, the design and fabrication process of origami is labor-intensive, and building circuits on it is often unstable as they can easily be broken during iterative folding processes. We propose a fabrication method that helps users quickly design and fabricate haptic origami interfaces with electric sensing. By extending the previous 4D printing method [16], we developed a design software that can generate both a pattern of self-foldable shape and a pattern of circuitry. In the software, two kirigami hinges and five shape modules are available for users. Kirigami hinges protect the circuits from damage and achieve deformation sensing during the iterative folding process. Five shape modules have different haptic capabilities for different use cases. We summarized the fabrication process and designed three applications to validate our proposed method.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {277},\nnumpages = {7},\nkeywords = {4D Printing, Origami, Rapid Prototyping, Tangible Interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651077,\nauthor = {Wang, Zhiyuan and Reddy, Varun and Ingersoll, Karen and Flickinger, Tabor and Barnes, Laura E.},\ntitle = {Rapport Matters: Enhancing HIV mHealth Communication through Linguistic Analysis and Large Language Models},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651077},\ndoi = {10.1145/3613905.3651077},\nabstract = {In HIV care, a strong rapport between patient and provider is essential for strengthening trust, enhancing therapy adherence, and ultimately leading to improved health outcomes. As the adoption of digital interactions in HIV care via mobile health (mHealth) tools is emerging, maintaining rapport in these asynchronous text-based communications becomes a critical yet challenging task. In this paper, we analyze 1,740 messages from an mHealth platform, categorized by experienced clinicians as either ‘rapport-building’ or ‘information-only.’ We utilize linguistic analysis to uncover key attributes of rapport-building communication. This led to a set of machine learning (ML) models and Large Language Models (LLMs) capable of classifying these communication styles. Further, we propose the application of LLMs not only to identify but also to actively rewrite ‘information only’ messages into versions that enhance rapport building without compromising information integrity. Our research demonstrates potential advancements in HIV mHealth communication by integrating linguistic analysis with language models, leading to more effective patient-provider interactions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {278},\nnumpages = {8},\nkeywords = {Patient-clinician communication, large language models, mobile health},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650916,\nauthor = {Koch, Daniel and Medlar, Alan and Glowacka, Dorota},\ntitle = {Read the Fine Print: Assessing the User Experience of Reading in High Resolution Virtual Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650916},\ndoi = {10.1145/3613905.3650916},\nabstract = {Despite the increasing visual fidelity of commercially available virtual reality (VR) headsets, they remain ill-suited to scenarios that require reading large quantities of text. Industrial VR headsets, however, have high resolution displays for immersive simulations, such as medical training, that could be repurposed to evaluate the potential future of reading in VR. In this article, we present a within-subject study on reading preferences and user experience comparing the high resolution Varjo VR2 Pro with the lower resolution Meta Quest 2. Our results show that choice of headset can have a significant impact on reading preferences, with an effect size comparable to interindividual differences. These results demonstrate that existing VR text guidelines are inappropriate at higher resolutions. Overall, however, users preferred the reading experience of the Meta Quest 2, citing the distracting border between high and lower resolution displays and the weight of the Varjo headset as impediments to reading.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {279},\nnumpages = {7},\nkeywords = {Virtual reality, reading, user experience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650999,\nauthor = {Chien, Jennifer and Mckee, Kevin and Kay, Jackie and Isaac, William},\ntitle = {Recourse for Reclamation: Chatting with Generative Language Models},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650999},\ndoi = {10.1145/3613905.3650999},\nabstract = {Researchers and developers increasingly rely on toxicity scoring to automate generative language model outputs, in settings such as customer service, information retrieval, and content generation. However, toxicity scoring may render pertinent information inaccessible, rigidify or “value-lock” cultural norms, and prevent language reclamation processes, particularly for marginalized people. In this work, we extend the concept of algorithmic recourse to generative language models: we provide users a novel mechanism to achieve their desired prediction by dynamically setting thresholds for toxicity filtering. Users thereby exercise increased agency relative to interactions with the baseline system. A pilot study (n = 30) supports the potential of our proposed recourse mechanism, indicating improvements in usability compared to fixed-threshold toxicity-filtering of model outputs. Future work should explore the intersection of toxicity scoring, model controllability, user agency, and language reclamation processes—particularly with regard to the bias that many communities encounter when interacting with generative language models.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {280},\nnumpages = {14},\nkeywords = {Algorithmic recourse, Generative language models, Language reclamation, Toxicity scoring},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650964,\nauthor = {Gomez-Beldarrain, Garoa and Verma, Himanshu and Kim, Euiyoung and Bozzon, Alessandro},\ntitle = {Revealing the Challenges to Automation Adoption in Organizations: Examining Practitioner Perspectives From an International Airport},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650964},\ndoi = {10.1145/3613905.3650964},\nabstract = {Sustained adoption of automation is a problem for organizations, despite the promised benefits of automation and the propensity for organizations to expect it to transform their workplaces. To address this problem, previous work in HCI has mostly considered the perspectives and experiences of users interacting with automation technologies and has not considered the broader organizational context consisting of different stakeholders with varying needs and expectations around automation. Taking Amsterdam Airport Schiphol as a case study, we examine the challenges faced by practitioners responsible for integrating automation projects in its airside ecosystem, in an interview study conducted with 8 participants. Our findings reveal three challenges to the adoption of automation within the organization - the lack of consensus among different stakeholders, the need to adapt the technology to the specific context, and the undefined procedures required to maintain automation after implementation, that should be further addressed by the HCI community.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {281},\nnumpages = {7},\nkeywords = {Automation Adoption, Autonomous Systems, Aviation, Interview Study, Organization, Practitioners, Responsible Automation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651025,\nauthor = {Hu, Jenny and El-Rashid, Fatimah and Bertelsen, Sille Eva},\ntitle = {SHADE: Empowering Consumer Choice for Sustainable Fashion with AI and Digital Tooling},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651025},\ndoi = {10.1145/3613905.3651025},\nabstract = {We present SHADE, a real-world application of using digital tooling to nudge consumer behavior toward sustainable, conscious fashion. In this paper, we outline how digital tools and platforms play an important role in the shifting paradigm of the fashion industry towards conscious fashion, which herein, refers to sustainably-made apparel and accessories produced with minimal environmental and social impact. This includes, but is not limited to, ethical sourcing and responsible resource management. Specifically, we discuss how SHADE leverages extensive data, publicly available information, and AI to guide consumers towards conscious fashion choices by empowering them to make well-informed decisions at the point of online shopping. This contribution explores the fundamental principles, research conducted, and potential impact of using SHADE within the broader context of how digital tools and platforms influence the fashion industry as a whole.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {282},\nnumpages = {5},\nkeywords = {Artificial Intelligence (AI), Digital, E-commerce, End-user, Fashion, Sustainability, Web Extension},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651100,\nauthor = {Ma, Zilin and Mei, Yiyang and Gajos, Krzysztof Z. and Arawjo, Ian},\ntitle = {Schr\\\"{o}dinger's Update: User Perceptions of Uncertainties in Proprietary Large Language Model Updates},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651100},\ndoi = {10.1145/3613905.3651100},\nabstract = {Developers of proprietary large language models (LLMs), like OpenAI and Anthropic, often roll out updates, some announced and others silent, impacting transparency for users. We interviewed 21 LLM users, including end-users, developers, and academics, mainly using ChatGPT and GPT4. Many reported feelings of loss of control and uncertainty, and diminished trust due to silent updates and deprecations. Participants overall felt that proprietary LLMs were similar to traditional software with regards to updates, only differing in the impossibility in full transparency and explainability. Consequently, participants felt LLM companies departed from established software update norms without justification, in some cases appealing to the novelty of AI in order to excuse bad practices. As a result, users are inclined to shift towards fixed, open-source models, reserving proprietary LLMs for prototyping. We suggest strategies that LLM providers can adopt to better uphold transparency and trust during model updates and deprecations.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {283},\nnumpages = {9},\nkeywords = {Large Language Models, Proprietary Models, Software Updates},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650738,\nauthor = {Hsu, Ting-Yao and Huang, Chieh-Yang and Huang, Shih-Hong and Rossi, Ryan and Kim, Sungchul and Yu, Tong and Giles, C Lee and Huang, Ting-Hao Kenneth},\ntitle = {SciCapenter: Supporting Caption Composition for Scientific Figures with Machine-Generated Captions and Ratings},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650738},\ndoi = {10.1145/3613905.3650738},\nabstract = {Crafting effective captions for figures is important. Readers heavily depend on these captions to grasp the figure’s message. However, despite a well-developed set of AI technologies for figures and captions, these have rarely been tested for usefulness in aiding caption writing. This paper introduces SciCapenter, an interactive system that puts together cutting-edge AI technologies for scientific figure captions to aid caption composition. SciCapenter generates a variety of captions for each figure in a scholarly article, providing scores and a comprehensive checklist to assess caption quality across multiple critical aspects, such as helpfulness, OCR mention, key takeaways, and visual properties reference. Users can directly edit captions in SciCapenter, resubmit for revised evaluations, and iteratively refine them. A user study with Ph.D. students indicates that SciCapenter significantly lowers the cognitive load of caption writing. Participants’ feedback further offers valuable design insights for future systems aiming to enhance caption writing.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {284},\nnumpages = {9},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650974,\nauthor = {Samaradivakara, Yasith and Ushan, Thavindu and Pathirage, Asela and Sasikumar, Prasanth and Karunanayaka, Kasun and Keppitiyagama, Chamath and Nanayakkara, Suranga},\ntitle = {SeEar: Tailoring Real-time AR Caption Interfaces for Deaf and Hard-of-Hearing (DHH) Students in Specialized Educational Settings},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650974},\ndoi = {10.1145/3613905.3650974},\nabstract = {Deaf and hard-of-hearing (DHH) students often encounter challenges in their specialized educational settings, particularly in language literacy development, due to limited exposure to written and spoken language. We, therefore, collaborated with 8 DHH students and 2 Teachers of the Deaf (ToDs) from a School for Deaf in Sri Lanka to explore how to effectively utilize a real-time augmented reality (AR) caption interface in their classroom settings, aiming to enhance their learning experience. We adopted a User-Centered Design (UCD) process to develop and refine the prototype addressing the design challenges affecting the overall learning experience. User study with 12 DHH participants revealed a strong preference (91.66\\%) for our prototype and highlighted its potential to enhance the learning experiences. Our findings contribute to further exploration of the potential of tailored AR caption interfaces in addressing the educational challenges faced by DHH students and enhancing their learning experience in specialized educational settings.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {285},\nnumpages = {8},\nkeywords = {Augmented Reality, Co-Deign, Deaf, Information Interfaces and Presentation, Real-time Captioning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651081,\nauthor = {Hancock, Jamie and Hui, Ruoyun and Singh, Jatinder and Mazumder, Anjali},\ntitle = {Seeing Human Rights at Sea: How to Align Tech Development with the Needs of Maritime Human Rights Investigators and Affected Communities},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651081},\ndoi = {10.1145/3613905.3651081},\nabstract = {Significant effort has been devoted to developing tech tools that leverage data to assist human rights investigations at sea. It is unclear, however, how users incorporate data and data-related tools into their investigations, nor what is needed for tech developments to contribute more value. We present the preliminary findings of a study which used workshops and interviews to examine the practices, experiences, and views of experts who perform maritime human rights investigations. Following a thematic analysis, we find that while there is scope for developers and designers to become involved, technologists’ enthusiasm for cutting-edge solutions appears detached from the context and experience of affected communities. The result is a saturated but opaque market for under-used tech solutions, while communities’ low-tech needs remain unmet. We suggest developers can best contribute to the fight against human rights abuses at sea by adopting deeper forms of engagement that promote communities’ agency.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {286},\nnumpages = {13},\nkeywords = {OSINT, data management, exploitation, geolocation, human rights, human rights technology, labour, maritime, migration, remote sensing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651020,\nauthor = {Lyons, Kent},\ntitle = {Shape Cast: Automating 3D Design for Plaster Molds in Ceramic Slip Casting},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651020},\ndoi = {10.1145/3613905.3651020},\nabstract = {Shape Cast is our novel software tool designed to simplify the creation of plaster molds for ceramic slip casting by automating the 3D modeling process. Instead of needing to learn 3D computer-aided design (CAD) to produce molds, Shape Cast allows artists to input a single 2D profile of the desired pot. Shape Cast uses that to generate ready-to-print 3D models for plaster, accommodating factors such as clay shrinkage and mold structural requirements. We detail the mold generation process and associated software capabilities; and, we provide case studies demonstrating the capabilities of Shape Cast.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {287},\nnumpages = {7},\nkeywords = {3D design, 3D printing, Slip casting, ceramics, pottery},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650795,\nauthor = {Strecker, Jannis and Wu, Jing and Bekta\\c{s}, Kenan and Vaslin, Conrad and Mayer, Simon},\ntitle = {ShoppingCoach: Using Diminished Reality to Prevent Unhealthy Food Choices in an Offline Supermarket Scenario},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650795},\ndoi = {10.1145/3613905.3650795},\nabstract = {Non-communicable diseases, such as obesity and diabetes, have a significant global impact on health outcomes. While governments worldwide focus on promoting healthy eating, individuals still struggle to follow dietary recommendations. Augmented Reality (AR) might be a useful tool to emphasize specific food products at the point of purchase. However, AR may also add visual clutter to an already complex supermarket environment. Instead, reducing the visual prevalence of unhealthy food products through Diminished Reality (DR) could be a viable alternative: We present ShoppingCoach, a DR prototype that identifies supermarket food products and visually diminishes them dependent on the deviation of the target product’s composition from dietary recommendations. In a study with 12 participants, we found that ShoppingCoach increased compliance with dietary recommendations from 75\\% to 100\\% and reduced decision time by 41\\%. These results demonstrate the promising potential of DR in promoting healthier food choices and thus enhancing public health.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {288},\nnumpages = {8},\nkeywords = {diminished reality, extended reality, food choices, health informatics, nutrition and health},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650855,\nauthor = {Chi, Vivienne Bihe and Mehrotra, Shashank and Misu, Teruhisa and Akash, Kumar},\ntitle = {Should I Help a Delivery Robot? Cultivating Prosocial Norms through Observations},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650855},\ndoi = {10.1145/3613905.3650855},\nabstract = {We propose leveraging prosocial observations to cultivate new social norms to encourage prosocial behaviors toward delivery robots. With an online experiment, we quantitatively assess updates in norm beliefs regarding human-robot prosocial behaviors through observational learning. Results demonstrate the initially perceived normativity of helping robots is influenced by familiarity with delivery robots and perceptions of robots’ social intelligence. Observing human-robot prosocial interactions notably shifts peoples’ normative beliefs about prosocial actions; thereby changing their perceived obligations to offer help to delivery robots. Additionally, we found that observing robots offering help to humans, rather than receiving help, more significantly increased participants’ feelings of obligation to help robots. Our findings provide insights into prosocial design for future mobility systems. Improved familiarity with robot capabilities and portraying them as desirable social partners can help foster wider acceptance. Furthermore, robots need to be designed to exhibit higher levels of interactivity and reciprocal capabilities for prosocial behavior.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {289},\nnumpages = {7},\nkeywords = {delivery robots, observational learning, social norms, user study},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651075,\nauthor = {Devries, Paige S and Tran, Nina and Delk, Keith and Miga, Melanie and Taulbee, Richard Carlisle and Pidathala, Pranav and Glasser, Abraham and Kushalnagar, Raja and Vogler, Christian},\ntitle = {Sign Language-Based versus Touch-Based Input for Deaf Users with Interactive Personal Assistants in Simulated Kitchen Environments},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651075},\ndoi = {10.1145/3613905.3651075},\nabstract = {In this study, we assess the usability of interactive personal assistants (IPAs), such as Amazon Alexa, in a simulated kitchen smart home environment, with deaf and hard of hearing users. Participants engage in activities in a way that causes their hands to get dirty. With these dirty hands, they are tasked with two different input methods for IPAs: American Sign Language (ASL) in a Wizard-of-Oz design, and smart home apps with a touchscreen. Usability ratings show that participants significantly preferred ASL over touch-based apps with dirty hands, although not to a larger extent than in comparable previous work with clean hands. Participants also expressed significant enthusiasm for ASL-based IPA interaction in Netpromoter scores and in questions about their overall preferences. Preliminary observations further suggest that having dirty hands may affect the way people sign, which may pose challenges for building IPAs that natively support sign language input.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {290},\nnumpages = {9},\nkeywords = {Accessibility, Deaf and Hard of hearing, Intelligent Personal Assistants, Kitchen Environments, Usability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650843,\nauthor = {Polenz, Laureen and Joeres, Fabian and Hansen, Christian and Heinrich, Florian},\ntitle = {Simulating projective Augmented Reality Visualizations in Virtual Reality: Is VR a feasible Environment for medical AR Evaluations?},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650843},\ndoi = {10.1145/3613905.3650843},\nabstract = {Augmented Reality (AR) has demonstrated potential in medical applications, such as enhancing surgical navigation. However, evaluating medical AR visualizations entails high costs and effort to provide suitable hardware solutions. This is particularly crucial in projective AR, as these systems require several error-prone calibration and registration steps. This work investigates the suitability of Virtual Reality (VR) as a cost-effective and controlled study environment for evaluating projective AR visualizations. A virtual twin of a real laboratory environment was created, and a user study comparing two needle navigation visualizations was conducted. The study simulated identical experiments in both AR and VR to assess if similar results would emerge. Our findings indicate that both AR and VR experiments exhibited comparable effects in terms of performance and workload of both needle insertion visualizations. This study serves as a preliminary step in demonstrating the feasibility of using VR as an evaluation environment for projective AR visualizations.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {291},\nnumpages = {8},\nkeywords = {Medical Augmented Reality, Surgical Navigation, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650814,\nauthor = {Elsayed, Neven and Marriott, Kim and Smith, Ross and Thomas, Bruce H},\ntitle = {Situated Analytics Process and Mantra},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650814},\ndoi = {10.1145/3613905.3650814},\nabstract = {In the current era of digitisation, a substantial amount of data significantly influences our daily experiences and decision-making processes. This data is systematically acquired from routine social interactions, online engagements, and the numerous sensors seamlessly integrated into our surroundings. Navigating this data-rich environment poses notable challenges, particularly when immersed in mixed reality environments. Situated Analytics (SA) was introduced to offer analytical reasoning within the physical space, utilizing visual analytics reasoning techniques combined with augmented reality to enhance the physical environment with real-time contextual information. The distinctive feature of SA lies in its contextual and situational, two-way, interactive visualisation, supported by a system and knowledge analytics to optimize the decision-making process. This paper discusses in-depth the components, processes, space, and mantra of SA, listing its characteristics and addressing the challenges associated with it.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {292},\nnumpages = {9},\nkeywords = {Augmented reality, Information Visualisation, Interaction, Interactive visualisation, Situated analytics, Visual analytics, Visualisation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650920,\nauthor = {Li, Zhi and Yao, Yuan},\ntitle = {Sleep No Mosquitoes: Compensating Distance-Aware Compression in a VR Audio Game},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650920},\ndoi = {10.1145/3613905.3650920},\nabstract = {Can you catch mosquitoes at night? Sleep No Mosquitoes is a VR game with audio as the main interactive element. The intention is to explore how to compensate for the problem of distance perception compression in the VR environment through interesting mechanisms. The game is divided into entertainment mode and experimental mode, inviting 21 players and 10 experimental participants to test the factors that affect distance perception in VR. According to the analysis combining quantitative and qualitative methods, we found that three methods - dynamic head movement, dynamic audio, and sound effects of body movement - can improve players’ efficiency in catching mosquitoes, and effectively compensate for the distortion of distance perception in VR.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {293},\nnumpages = {6},\nkeywords = {Distance Compression, Game Design, Virtual Reality, Virtual audio game},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650883,\nauthor = {Sechayk, Yotam and Shamir, Ariel and Igarashi, Takeo},\ntitle = {SmartLearn: Visual-Temporal Accessibility for Slide-based e-learning Videos},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650883},\ndoi = {10.1145/3613905.3650883},\nabstract = {In the realm of e-learning, video-based content is increasingly prevalent but brings with it unique accessibility challenges. Our research, beginning with a formative study involving 53 participants, has pinpointed the primary accessibility barriers in video-based e-learning: mismatches in user pace, complex visual arrangements leading to unclear focus, and difficulties in navigating content. To tackle these barriers, we introduced SmartLearn (SL), an innovative tool designed to enhance the accessibility of video content. SL utilizes advanced video analysis techniques to address issues of focus, navigation, and pacing, enabling users to interact with video segments more effectively through a web interface. A subsequent evaluation demonstrated that SL significantly enhances user engagement, ease of access, and learnability over existing approaches. We conclude by presenting design guidelines derived from our study, aiming to promote future efforts in research and development towards a more inclusive digital education landscape.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {294},\nnumpages = {11},\nkeywords = {Accessibility, E-learning, Online learning, Temporal Accessibility, Universal Design, Video Accessibility, Visual Accessibility},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651109,\nauthor = {Segal, Jonathan Isaac and Rodriguez, Samuel and Raghavan, Akshaya and Baez, Heysil and Jung, Crescentia and Collins, Jazmin and Azenkot, Shiri and Stevenson Won, Andrea},\ntitle = {SocialCueSwitch: Towards Customizable Accessibility by Representing Social Cues in Multiple Senses},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651109},\ndoi = {10.1145/3613905.3651109},\nabstract = {In virtual environments, many social cues (e.g. gestures, eye contact, and proximity) are currently conveyed visually or auditorily. Indicating social cues in other modalities, such as haptic cues to complement visual or audio signals, will help to increase VR’s accessibility and take advantage of the platform’s inherent flexibility. However, accessibility implementations in social VR are often siloed by single sensory modalities. To broaden the accessibility of social virtual reality beyond replacing one sensory modality with another, we identified a subset of social cues and built tools to enhance them allowing users to switch between modalities to choose how these cues are represented. Because consumer VR uses primarily visual and auditory stimuli, we started with social cues that were not accessible for blind and low vision (BLV) and d/Deaf and hard of hearing (DHH) people, and expanded how they could be represented to accommodate a number of needs. We describe how these tools were designed around the principle of social cue switching, and a standard distribution method to amplify reach.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {295},\nnumpages = {7},\nkeywords = {accessibility, code sharing, collaborative development, sensory substitution},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650945,\nauthor = {Oldfield, Alison and Broomfield, Katherine and Philamore, Hemma and Sewell, Richard and Powell, Emma},\ntitle = {Softly Non-Spoken: Exploring the Potential of Soft Robotics to Support Non-Verbal Communication},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650945},\ndoi = {10.1145/3613905.3650945},\nabstract = {Existing Augmentative and Alternative Communication (AAC) technologies aim to support people with communication disability participate more fully in life and society, but these often prioritize linguistic and spoken modes of expression. Embracing a broader view of communication that includes non-verbal and embodied expressions, this interdisciplinary project explored the potential of soft robotics for developing AAC technologies that create opportunities for more nuanced forms of communication. To do this, the Softly Non-Spoken project facilitated interdisciplinary workshops and developed soft robotic objects which were then used as probes to explore how such technologies might support expressive, affective non-verbal communication. Results suggest that soft robotics materials and functions offer potential in this design space, including compelling possibilities for object-enabled human interaction that could offer more individual control around communication. The findings also propose a conceptualization of communication that encompasses embodied expression and could cultivate ambiguity and intrigue in future AAC design.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {296},\nnumpages = {7},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650927,\nauthor = {Su, Xia and Koh, Eunyee and Xiao, Chang},\ntitle = {SonifyAR: Context-Aware Sound Effect Generation in Augmented Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650927},\ndoi = {10.1145/3613905.3650927},\nabstract = {Sound plays crucial roles in enhancing user experience and immersiveness in Augmented Reality (AR). However, current AR authoring platforms lack support for creating sound effects that harmonize with both the virtual and the real-world contexts. In this work, we present SonifyAR, a novel system for generating context-aware sound effects in AR experiences. SonifyAR implements a Programming by Demonstration (PbD) AR authoring pipeline. We utilize computer vision models and a large language model (LLM) to generate text descriptions that incorporate context information of user, virtual object and real world environment. This context information is then used to acquire sound effects with recommendation, generation, and retrieval methods. The acquired sound effects can be tested and assigned to AR events. Our user interface also provides the flexibility to allow users to iteratively explore and fine-tune the sound effects. We conducted a preliminary user study to demonstrate the effectiveness and usability of our system.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {297},\nnumpages = {7},\nkeywords = {Augmented Reality, Authoring Tool, Mixed Reality, Sound},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650822,\nauthor = {Li, Xiaohan and Wang, Qixin and Wang, Zishan and Jin, Zeyu and Jia, Jia},\ntitle = {SoulSkipper: A Voice-Controlled Emotional Adaptive Game to Complement Therapy for Social Anxiety Disorder},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650822},\ndoi = {10.1145/3613905.3650822},\nabstract = {In this paper, we introduce SoulSkipper, a voice-controlled emotional adaptive game based on the use of Cognitive Behavioral Therapy (CBT) to complement therapy for social anxiety disorder (SAD). The game allows players to interact with non-player characters (NPCs) through voice input, which helps players identify negative thoughts and practice speech performance through 1) specific social scenarios, 2) emotional adaptation, and 3) real-time voice feedback. Through our initial evaluation, we find that the combination of game elements and voice interactions helps players improve their awareness of therapeutic skills associated with SAD and face social interactions in real-life scenarios more positively.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {298},\nnumpages = {7},\nkeywords = {CBT, emotional adaptation, social anxiety disorder, voice-controlled game},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650743,\nauthor = {Kwon, Soonho and Yoo, Dong Whi and Kang, Younah},\ntitle = {Spiritual AI: Exploring the Possibilities of a Human-AI Interaction Beyond Productive Goals},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650743},\ndoi = {10.1145/3613905.3650743},\nabstract = {The Human-Computer Interaction community has long endeavored to discuss technologies that go beyond productivity goals. We extend this perspective to the realm of Human-AI interaction to explore how AI could consider diverse user values, particularly in users’ prayer experiences where minimal productivity goals exist. Through diary study, our research identified user goals and behaviors that contribute to satisfying prayer experiences. Then, we conceptualized four distinct AI systems designed to celebrate the identified goals and behaviors of the users. We presented these conceptualized systems in the format of a design workbook and engaged users in evaluating them. Based on our findings, we discuss the potential of novel roles that AI could play in human lives, such as provoking deep reflections or creating indirect communities.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {299},\nnumpages = {8},\nkeywords = {Human-AI Interaction, design workbook, diary study, technospirituality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651888,\nauthor = {Wischerath, Darja and Godwin, Emily and Bocheva, Desislava and Brown, Olivia and Roscoe, Jonathan Francis and Davidson, Brittany I},\ntitle = {Spreading the Word: Exploring a Network of Mobilizing Messages in a Telegram Conspiracy Group},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651888},\ndoi = {10.1145/3613905.3651888},\nabstract = {Telegram's design prioritizes user security and minimal content moderation, making it appealing for communities banned from mainstream platforms, such as conspiracy influencers or far-right movements. We examine the bi-directional behavior of users in a conspiratorial Telegram group chat during the COVID-19 pandemic from 2020-2023. We find that the network structure of this community evolved throughout the pandemic, where the network grew both in the number of active users, as well as in the number of interactions. This increased interconnectivity coincided with surges in planning discussions for associated offline protests.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {300},\nnumpages = {8},\nkeywords = {Telegram, collective action, conspiracy theories, social network analysis},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651886,\nauthor = {Stamm, Julie C and Richardson, Daniel C and Ward, Jamie A},\ntitle = {Squeeze and Slide: Real-time continuous self-reports with physiological arousal to evaluate emotional engagement in short films of contemporary dance},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651886},\ndoi = {10.1145/3613905.3651886},\nabstract = {Engagement is a broad and multifaceted research subject. Self-report engagement data of time-based experiences such as life performance or films is mostly collected through post-hoc questionnaires. The present study compares two devices that allow for real-time continuous self-report while watching 2 short films featuring contemporary dance. The first device is a squeeze ball with a pressure sensor inside and the second device a mechanical linear slider. Users are prompted to indicate their emotional engagement throughout each film using a device. Electrodermal activity (EDA) was also recorded as an indicator of arousal. Across a study involving 31 participants, the squeeze ball and slider reveal comparable overall correlations to EDA data. However there are indications of user-preference for the squeeze ball in the context of rating emotional involvement.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {301},\nnumpages = {7},\nkeywords = {EDA, dance, embodiment, engagement, film, neuroaesthetics, real-time measures},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651058,\nauthor = {Wolf, Sara and Friedrich, Paula and Hurtienne, J\\\"{o}rn},\ntitle = {Still Not a Lot of Research? Re-Examining HCI Research on Religion and Spirituality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651058},\ndoi = {10.1145/3613905.3651058},\nabstract = {A decade after Buie and Blythe’s review \"Spirituality: There’s an App for That! (But Not a Lot of Research)\", this sequel assesses the evolving landscape of Human-Computer Interaction (HCI) research on religion and spirituality. While the enduring importance of religion and spirituality for humanity and its influence on technology use remains, the last decade has seen transformative shifts catalysed by technological advances and the global impact of the COVID-19 pandemic. This paper explores whether and how HCI research on religion and spirituality has also changed. Providing a snapshot of the current research, we document and reflect on changes in the lines of research with a shift towards community, an increased consideration of religion and spirituality in related areas such as health, education, and society, and the broadening of challenges for HCI research on religion and spirituality.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {302},\nnumpages = {15},\nkeywords = {faith, religion, scoping review, spirituality, techno-spirituality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651118,\nauthor = {Fan, Min and Cui, Xinyue and Hao, Jing and Ye, Renxuan and Ma, Wanqing and Tong, Xin and Li, Meng},\ntitle = {StoryPrompt: Exploring the Design Space of an AI-Empowered Creative Storytelling System for Elementary Children},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651118},\ndoi = {10.1145/3613905.3651118},\nabstract = {Creative storytelling is a crucial learning activity for children. We conducted formative research with three teachers and 18 children and derived five design implications. Based on the findings, we developed StoryPrompt, an interactive system that enables elementary children to co-create stories and comics with generative AI, aiming to enhance their literacy and creativity. The preliminary evaluation, involving eight children and three HCI experts, demonstrated good system usability and children's satisfactory learning experience. We discuss the advantages and considerations of our design features in using generative AI to empower creative storytelling for elementary children.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {303},\nnumpages = {8},\nkeywords = {Children, Creativity, Design, Generative AI, Literacy, Storytelling},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650848,\nauthor = {Wang, Zijie J. and Munechika, David and Lee, Seongmin and Chau, Duen Horng},\ntitle = {SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650848},\ndoi = {10.1145/3613905.3650848},\nabstract = {Computational notebooks, such as Jupyter Notebook, have become data scientists’ de facto programming environments. Many visualization researchers and practitioners have developed interactive visualization tools that support notebooks, yet little is known about the appropriate design of these tools. To address this critical research gap, we investigate the design strategies in this space by analyzing 163 notebook visualization tools. Our analysis encompasses 64 systems from academic papers and 105 systems sourced from a pool of 55k notebooks containing interactive visualizations that we obtain via scraping 8.6 million notebooks on GitHub. Through this study, we identify key design implications and trade-offs, such as leveraging multimodal data in notebooks as well as balancing the degree of visualization-notebook integration. Furthermore, we provide empirical evidence that tools compatible with more notebook platforms have a greater impact. Finally, we develop SuperNOVA, an open-source interactive browser to help researchers explore existing notebook visualization tools. SuperNOVA is publicly accessible at: https://poloclub.github.io/supernova/.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {304},\nnumpages = {17},\nkeywords = {Computational Notebook, Cross-Platform Visualization, Data Science, Design, Interactive Visualization, Systematic Review},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650849,\nauthor = {Armin, Atieh and Trybala, Joseph J and Young, Jordyn and Razi, Afsaneh},\ntitle = {Support in Short Form: Investigating TikTok Comments on Videos with #Harassment},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650849},\ndoi = {10.1145/3613905.3650849},\nabstract = {Exploring the dynamics of public discourse on social media reveals critical insights into how topics like harassment are perceived, discussed, and handled within online communities. To understand these dynamics within multimodal short-form video-based communities, we conducted topic modeling on 145,515 comments of videos tagged with #harassment on TikTok. We identified nine topics, including community responses to harassment and threats, law enforcement responses to harassment, and discussions around self-defense strategies. Our findings revealed the diverse nature of online discussions about harassment, containing empathy, polarization, frustration, and humor. These various topics underscore the significant role of TikTok as a platform for shaping public opinions on critical social issues and amplifying the voices of victims. This paper contributes to understanding how public discourse on harassment unfolds in TikTok to inform future research and policy-making to ensure safer online communities. Content Warning: This paper includes sensitive topics such as harassment, reader discretion is advised.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {305},\nnumpages = {8},\nkeywords = {Cyberbully, Harassment, Interpersonal Harm, Online Safety, Public Discourse, Social Media, TikTok, Topic Modeling},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650968,\nauthor = {Lee, Duri and Nam, Kyungmin and Lee, Uichin},\ntitle = {Supporting Interpersonal Emotion Regulation of Call Center Workers via Customer Voice Modulation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650968},\ndoi = {10.1145/3613905.3650968},\nabstract = {Call center workers suffer from the aggressive voices of customers. In this study, we explore the possibility of proactive voice modulation or style transfer, in which a customer’s voice can be modified in real time to mitigate emotional contagion. As a preliminary study, we conducted an interview with call center workers and performed a scenario-based user study to evaluate the effects of voice modulation on perceived stress and emotion. We transformed the customer’s voice by modulating its pitch and found its potential value for designing a user interface for proactive voice modulation. We provide new insights into interface design for proactively supporting call center workers during emotionally stressful conversations.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {306},\nnumpages = {8},\nkeywords = {customer voice modulation with emotion detection, emotion contagion, interpersonal emotion regulation, user interface design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650787,\nauthor = {Choe, Kiroong and Park, Seokhyeon and Jung, Seokweon and Kim, Hyeok and Yang, Ji Won and Hong, Hwajung and Seo, Jinwook},\ntitle = {Supporting Novice Researchers to Write Literature Review using Language Models},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650787},\ndoi = {10.1145/3613905.3650787},\nabstract = {A literature review requires more than summarization. While language model-based services and systems increasingly assist in analyzing accurate content in papers, their role in supporting novice researchers to develop independent perspectives on literature remains underexplored. We propose the design and evaluation of a system that supports the writing of argumentative narratives from literature. Based on the barriers faced by novice researchers before, during, and after writing, identified through semi-structured interviews, we propose a prototype of a language-model-assisted academic writing system that scaffolds the literature review writing process. A series of workshop studies revealed that novice researchers found the support valuable as they could initiate writing, co-create satisfying contents, and develop agency and confidence through a long-term dynamic partnership with the AI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {307},\nnumpages = {9},\nkeywords = {human-AI collaboration, literature review, novice researcher},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650973,\nauthor = {Cowit, Noah and Yu, Junnan},\ntitle = {Supporting Physically Active CS-Ed for Children: Exploring the Design of Physical Play Friendly Coding Blocks},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650973},\ndoi = {10.1145/3613905.3650973},\nabstract = {In this work, we share our design exploration of coding blocks to incorporate physical play into programming kits for children's computing education. First, we cover the tradition of experiential learning in computing education, with descriptions of where physical play fits into that practice. Next, we describe children's programming workshops to explore how physical play could be incorporated into coding kits. From these workshops, we recommend a set of coding blocks for physical play divided into four categories: (1) motion sensing, (2) sound sensing, (3) proximity sensing, and (4) gameplay information. These coding blocks for the first time systematically present physical play friendly programming commands, supplementing previous works on developing coding tools to combine physical play and coding for children. Finally, we describe our implementation of these coding blocks on the micro:bit—a low-cost widely distributed computer science educational kit—and describe the results of a functionality test with nine graduate design students.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {308},\nnumpages = {7},\nkeywords = {Children, Coding blocks, Computational learning, Physical play},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650908,\nauthor = {Ogbonnaya-Ogburu, Ihudiya Finda and Israni, Aarti},\ntitle = {Supporting the Digital Aspects of Reentry for Formerly Incarcerated Individuals},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650908},\ndoi = {10.1145/3613905.3650908},\nabstract = {For formerly incarcerated individuals, reintegrating into society and learning to use digital tools for everyday tasks is essential. While reentry nonprofit programs provide social support, there is limited research on the specific strategies they use to help formerly incarcerated individuals overcome digital challenges associated with adjusting to life after prison. To address this gap, we conducted semi-structured interviews with eight nonprofit employees to understand how they support returning citizens in the digital aspects of reentry. Our research revealed that practicing self-reflection, being present, improvised teaching, and leveraging offline networks are important strategies used by these organizations. Our findings offer fresh perspectives on how these organizations aid formerly incarcerated individuals in their digital reentry process.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {309},\nnumpages = {5},\nkeywords = {digital literacy, digital reentry, formerly incarcerated, reentry},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650793,\nauthor = {Fan, Danyang and Kim, Gene S-H and Tomassetti, Olivia and Patel, Shloke Nirav and O'Modhrain, Sile and Lee, Victor R and Follmer, Sean},\ntitle = {Tangible Stats: An Embodied and Multimodal Platform for Teaching Data and Statistics to Blind and Low Vision Students},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650793},\ndoi = {10.1145/3613905.3650793},\nabstract = {Interactive data learning tools provide explorable ways for students to build intuitions about data, data representations, and statistical parameters. However, these tools rely on visual consumption and are not accessible to blind and low vision (BLV) students. In this work, we investigate opportunities to leverage active exploration, enriched with multimodal feedback and embodied interaction, to foster an understanding of the relationships among individual data values, data representations, and statistical measures. We explore these opportunities in the form of an accessible learning platform that allows students to hear and feel how statistical measures are changing in real time as they construct and manipulate physicalized data representations. We introduced the platform to four teachers of students with visual impairments (TVIs) through a two-hour-long focus group. TVIs embraced the platform’s exploratory nature and universality and recommended the consideration of additional auditory and texture-based interactions to enhance engagement.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {310},\nnumpages = {9},\nkeywords = {Accessibility, Data, Education, Embodied, Interactive Systems, Math, Statistics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651013,\nauthor = {Abe, Yuki and Tsujiguchi, Hikaru and Sakamoto, Daisuke and Ono, Tetsuo},\ntitle = {Temaneki: Map-Based Collaboration Tool for Consensus-Building in Student-Run Festival Management Teams},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651013},\ndoi = {10.1145/3613905.3651013},\nabstract = {Consensus-building is essential for successful collaborations in student festival management teams. However, instructing and building a consensus among over a hundred volunteers is time-consuming for team managers; it requires individual chats and meetings or the creation of intuitive guidance documents. To address this challenge, we developed Temaneki, a map-based authoring tool that provides asynchronous, visual, and interactive instruction within these teams. Temaneki enables managers to create step-by-step instructions with a series of annotated maps of festival venues, and subsequently provide each volunteer with these step-by-step visual instructions with asynchronous access. We analyzed the deployment results of Temaneki in a 182-member student festival management team and found that step-by-step map annotations helped managers instruct volunteers clearly and efficiently. Moreover, managers decorated the maps to make instructions more enjoyable for volunteers. These findings can guide the future directions of map and visual collaboration tools for efficient and enjoyable consensus-building in teams.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {311},\nnumpages = {8},\nkeywords = {Deployment, Festival management, Map-based interaction, User-centered design, Visual collaboration tools},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651049,\nauthor = {Sun, Qirui and Luo, Qiaoyang and Ni, Yunyi and Mi, Haipeng},\ntitle = {Text2AC: A Framework for Game-Ready 2D Agent Character(AC) Generation from Natural Language},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651049},\ndoi = {10.1145/3613905.3651049},\nabstract = {The visual appearance and personality of game characters are crucial to the player experience. Assembling characters in 2D games is a common practice with a significant demand for creativity, yet the creation process remains challenging, confined to professional graphic designers and animators. Traditional automated character creation in games usually involves manual parameter adjustment or the use of photos as references, coupled with the selection of pre-set backgrounds or professions within the game, leading to limitations in terms of freedom, usability, and consistency. However, with advancements in natural language processing, large language models (LLMs), and generative AI, we have the opportunity to reimagine the process of user-interactive character creation. This paper introduces a novel text-to-character methodology, integrating LLM technology with generative image AI, eliminating the need for reference photos or manual parameter editing while providing extensive options for character creation. Through natural language processing and automated skeletal animation, the generated 2D characters are capable of being driven across various web platforms and game engines. This exploration aims to establish a framework for driven agents that harmonizes the visual layer with linguistic personality, thereby providing a more enriched user experience in games and other applications as agents.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {312},\nnumpages = {7},\nkeywords = {2D Character Animation, Game Character Generation, Generative AI, Personalized Agents},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650919,\nauthor = {Rajan, Briana and Carradini, Stephen and Lauer, Claire},\ntitle = {The Arizona Water Chatbot: Helping Residents Navigate a Water Uncertain Future One Response at a Time},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650919},\ndoi = {10.1145/3613905.3650919},\nabstract = {The Southwestern US is a water-scarce region experiencing a megadrought more exceptional than any in the past 1200 years. It is also among the most rapidly growing, urbanizing, and diversifying areas in the country. To help people engage with the information they need to assist their communities in making decisions about water and drought preparedness, our interdisciplinary research team developed the Arizona Water Chatbot, an OpenAI-powered chatbot that uses retrieval-augmented generation to deliver information about Arizona's water situation to Arizona residents. The chatbot uses a distinctive architecture that implements guardrails to mitigate malicious content generation, ensuring the delivery of context-sensitive, relevant, accurate, and user-friendly responses. In this paper we discuss how a custom architecture provides fine-grained control of answers and increased ability to run multiple security checks that make a custom bot preferable to an OpenAI-hosted GPT for some requirements and situations. We also discuss how Waterbot is trained to incorporate Indigenous perspectives about water from the 22 American Indian tribal communities in Arizona to provide a more accurate and holistic view of the important historical, spiritual, and ecological role that water plays in the lives of Arizonans. Finally, we deliver insights on what other teams need to consider when building similar bots for public use.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {313},\nnumpages = {10},\nkeywords = {Artificial intelligence, Chatbot, Indigenous tribes, Water},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651000,\nauthor = {Alipour, Sharareh and Elahimanesh, Sina and Ghayouri Sales, Aliakbar and Mohammadi, Iman and Morassafar, Parimehr and Neshaei, Seyed Parsa},\ntitle = {The Art of Gift-Giving with Limited Preference Data: How Fashion Recommender Systems Can Help},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651000},\ndoi = {10.1145/3613905.3651000},\nabstract = {Gift shopping can be challenging due to the limited prior knowledge of the recipient’s preferences, leading to after-purchase regret. The effectiveness of Fashion Recommender Systems (FRS) in the context of gift purchases with limited preference data remains under-explored. We considered a gift-buying scenario and conducted an experiment with 192 pairs of participants to compare FRS versus humans in recommending fashion gifts to buyers. We find both FRS and humans score >50\\% correctness in recommending the right gift, even without direct interaction with gift-givers or recipients. Although the buyers know the gift receivers directly, they lead to less accuracy. Additionally, we identify gender-based differences in the recommendations. We also embed our scenario into a smartphone application. Our findings investigate the potential of FRS in cold-start scenarios with limited data and unavailable human assistance while highlighting the risks of using FRS for in-store gift purchases.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {314},\nnumpages = {7},\nkeywords = {Fashion Recommender Systems, In-store Gift Purchase, User Evaluation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650835,\nauthor = {Terenti, Mihail and Rupin, Matthieu and Reynal, Baptiste and Grisoni, Laurent and Vatavu, Radu-Daniel},\ntitle = {The Eclectic User Experience of Combined On-Screen and On-Wrist Vibrotactile Feedback in Touchscreen Input},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650835},\ndoi = {10.1145/3613905.3650835},\nabstract = {On-wrist vibrotactile feedback, such as provided through smartwatches, has been shown to have a positive impact on users’ touch input performance with mobile devices, but the user experience of combined on-screen and on-wrist vibrations has been scarcely examined. In our experiment involving twenty-two participants, an eclectic three-faceted UX emerged: (i) both on-screen and combined on-screen and on-wrist vibrations resulted in high ratings with no significant differences in UMUX scores or user preferences; (ii) negatively-connoted UX descriptors, e.g., difficulty or complexity, generally revealed less favorable UX for combined on-screen and on-wrist vibrations; (iii) positively-connoted descriptors, e.g., enjoyment or efficiency, revealed that vibrations on the wrist were not detrimental to touchscreen input UX. We use our findings to propose future work opportunities in cross-device smartphone and smartwatch interactions, which we discuss through the lenses offered by two identified dichotomies of vibrotactile feedback technical implementation: on-fingertip vs. on-wrist and single-point vs. multi-point vibrations.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {315},\nnumpages = {7},\nkeywords = {User experience, smartwatches, touch input, touchscreens, vibrotactile feedback, wearables},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650749,\nauthor = {Krop, Philipp and Koch, Martin Jakobus and Carolus, Astrid and Latoschik, Marc Erich and Wienrich, Carolin},\ntitle = {The Effects of Expertise, Humanness, and Congruence on Perceived Trust, Warmth, Competence and Intention to Use Embodied AI},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650749},\ndoi = {10.1145/3613905.3650749},\nabstract = {Even though people imagine different embodiments when asked which AI they would like to work with, most studies investigate trust in AI systems without specific physical appearances. This study aims to close this gap by combining influencing factors of trust to analyze their impact on the perceived trustworthiness, warmth, and competence of an embodied AI. We recruited 68 participants who observed three co-working scenes with an embodied AI, presented as expert/novice (expertise), human/AI (humanness), or congruent/slightly incongruent to the environment (congruence). Our results show that the expertise condition had the largest impact on trust, acceptance, and perceived warmth and competence. When controlled for perceived competence, the humanness of the AI and the congruence of its embodiment to the environment also influence acceptance. The results show that besides expertise and the perceived competence of the AI, other design variables are relevant for successful human-AI interaction, especially when the AI is embodied.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {316},\nnumpages = {9},\nkeywords = {Congruence, Framing, Intelligent Agents, Technology Acceptance, Trust, Visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650875,\nauthor = {Bosch, Esther and Welsch, Robin and Ayach, Tamim and Katins, Christopher and Kosch, Thomas},\ntitle = {The Illusion of Performance: The Effect of Phantom Display Refresh Rates on User Expectations and Reaction Times},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650875},\ndoi = {10.1145/3613905.3650875},\nabstract = {User expectations impact the evaluation of new interactive systems. Increased expectations may enhance the perceived effectiveness of interfaces in user studies, similar to a placebo effect observed in medical studies. To showcase the placebo effect, we conducted a user study with 18 participants who performed a target selection reaction time test with two different display refresh rates. Participants saw a stated screen refresh rate before every condition, which corresponded to the true refresh rate only in half of the conditions and was lower or higher in the other half. Results revealed successful priming, as participants believed in superior or inferior performance based on the narrative despite using the opposite refresh rate. Post-experiment questionnaires confirmed participants still held onto the initial narrative. Interestingly, the objective performance remained unchanged between both refresh rates. We discuss how study narratives influence subjective measures and suggest strategies to mitigate placebo effects in user-centered study designs.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {317},\nnumpages = {6},\nkeywords = {Human-AI Interfaces, Placebo, Placebo Effect, Refresh Rates, User Expectations, User Studies},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650844,\nauthor = {Walker, Johanna and Koutsiana, Elisavet and Nwachukwu, Michelle and Mero\\~{n}o Pe\\~{n}uela, Albert and Simperl, Elena},\ntitle = {The Promise and Challenge of Large Language Models for Knowledge Engineering: Insights from a Hackathon},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650844},\ndoi = {10.1145/3613905.3650844},\nabstract = {Knowledge engineering (KE) is the process of building, maintaining and using knowledge-based systems. This recently takes the form of knowledge graphs (KGs). The advent of new technologies like Large Language Models (LLMs) has the potential to improve automation in KE work due to the richness of their training data and their performance at solving natural language processing tasks. We conducted a multiple-methods study exploring user opinions and needs regarding the use of LLMs in KE. We used ethnographic techniques to observe KE workers using LLMs to solve KE tasks during a hackathon, followed by interviews with some of the participants. This interim study found that despite LLMs’ promising capabilities for efficient knowledge acquisition and requirements elicitation, their effective deployment requires an extended set of capabilities and training, particularly in prompting and understanding data. LLMs can be useful for simple quality assessment tasks, but in complex scenarios, the output is hard to control and evaluation may require novel approaches. With this study, we aim to evidence the interaction of KE stakeholders with LLMs, identify areas of potential, and understand the barriers to their effective use. We find copilot approaches may be valuable in developing processes where the human or a team of humans is assisted by generative AI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {318},\nnumpages = {9},\nkeywords = {Interviews, Knowledge Engineering, Knowledge Graph, Large Language Models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3652006,\nauthor = {Liao, Chunlin and Zhang, Yiheng and Chen, Rongrong},\ntitle = {The Sense of Agency Improves Visual Search and Holds Up Against External Salient Distractions in Virtual Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3652006},\ndoi = {10.1145/3613905.3652006},\nabstract = {Previous research has demonstrated that individuals are able to process information more quickly when they feel in control of the stimuli they are interacting with. However, it was unclear how this effect interacts with other visually salient information. To investigate this issue, we invited 39 college students to control hand-shaped object in a virtual environment while the remaining three objects displayed pre-recorded rotational movements. Following this task, we asked participants to identify a hand-shaped object with thumb pointing upward, which was the designated search target. Irrespective of the validity of the visually salient attentional cue, participants responded 11\\% faster when the search target corresponded with the previously controlled target. We concluded that the facilitation effect of a sense of agency in visual searching is robust to external distractors. Embracing the resilience of this facilitation effect arising from a sense of agency in visual search tasks can offer valuable insights for enhancing the design of diverse systems and environments to optimize human performance and experience.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {319},\nnumpages = {7},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650947,\nauthor = {Mahdavi Goloujeh, Atefeh and Sullivan, Anne and Magerko, Brian},\ntitle = {The Social Construction of Generative AI Prompts},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650947},\ndoi = {10.1145/3613905.3650947},\nabstract = {As text-to-image AI tools grow in capability and widespread use, research has focused on studying individualistic user prompt crafting strategies. Recognizing that technologies are socially constructed, this paper examines prompt engineering through a social lens. We propose reframing prompt engineering as a socio-cultural practice shaped by collective knowledge building. Through qualitative analysis of 19 semi-structured interviews with members of the MidJourney community, a text-to-image generative AI tool, we identify four socio-engagement themes: proprietary/solitary, derivative, collaborative, and provocative prompting. These themes reveal a space of social engagement modes based on personal values and motivations from individual exploration to influencing the prompt community and highlight a fine line between being inspired by others’ prompts and maintaining creative ownership. We argue that understanding distinct social engagement preferences can inform the design of AI tools to facilitate transparent prompt reuse mechanisms, integrate collaborative features, or preserve ethical concerns about prompt sharing.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {320},\nnumpages = {7},\nkeywords = {Communities of Practice, Generative AI, Prompt Engineering, Text-to-Image},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650997,\nauthor = {Chang, Weichen Joe and Seaborn, Katie and Adams, Andrew A.},\ntitle = {Theorizing Deception: A Scoping Review of Theory in Research on Dark Patterns and Deceptive Design},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650997},\ndoi = {10.1145/3613905.3650997},\nabstract = {The issue of dark patterns and deceptive designs (DPs) in everyday interfaces and interactions continues to grow. DPs are manipulative and malicious elements within user interfaces that deceive users into making unintended choices. In parallel, research on DPs has significantly increased over the past two decades. As the field has matured, epistemological gaps have also become a salient and pressing concern. In this scoping review, we assessed the academic work so far—51 papers between 2014 to 2023—to identify the state of theory in DP research. We identified the key theories employed, examined how these theories have been referenced, and call for enhancing the incorporation of theory into DP research. We also propose broad theoretical foundations to establish a comprehensive and solid base for contextualizing and informing future DP research from a variety of theoretical scopes and lenses.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {321},\nnumpages = {7},\nkeywords = {Dark Patterns, Deceptive Design, Deceptive Design Pattern, Manipulative Design, Persuasive Design, Scoping Review, Theory, User Interface Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650985,\nauthor = {Wenhart, Christiane and Hassenzahl, Marc},\ntitle = {There is an “I” in “We”: Relatedness Technologies Viewed Through the Lens of the Need for Autonomy},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650985},\ndoi = {10.1145/3613905.3650985},\nabstract = {Technologies that emotionally connect over distance have a long tradition in Human-Computer Interaction (HCI). Naturally, the design of such technologies focuses on fulfilling people's need for relatedness, i.e., the feeling of being in regular, intimate contact with people who care about you. What seems to be neglected is the possibly conflicting need for autonomy, i.e., the feeling of being the cause of one's own actions instead of feeling externally controlled. In this essay, we first discuss the potential tension between relatedness and autonomy. We then present five previously published relatedness technologies as examples of how neglecting autonomy might lead to unexpected side effects or the absence of positive effects. We suggest that designing for relatedness should be understood as designing for the ongoing negotiation of relatedness (the “we”) and autonomy (the “I”). This negotiation should not be left to people involved alone but should be integral to designing relatedness technologies.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {322},\nnumpages = {7},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650747,\nauthor = {Yu, Tianyu and Fan, Yige and Zhang, Zhixiang and Hu, Qingyu and Xu, Weiye and Mi, Haipeng and Mueller, Stefanie},\ntitle = {Thermaterial: Program Ambient Heat Transfer Behaviors on Objects through Fluidic Composites},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650747},\ndoi = {10.1145/3613905.3650747},\nabstract = {Thermal interfaces in HCI have been widely explored. Previous work either used electronics that continuously consume energy to actively control the temperatures, or relied on materials that passively create only one predefined temperature behavior, which both leave the gap of exploring objects that allow self-contained programmable temperature behaviors, utilizing materials’ dynamic thermal properties. To address this gap, we introduce Thermaterial, a method using fluidic composites with controllable thermal properties to enable temperature behaviors without continuously consuming energy, instead utilizing programmed ambient heat transfer behaviors. We first presented the fluidic composition and fabrication method of Thermaterial and evaluated the tunable thermal insulance, revealing changes of up to 3.0 times. We then explored potential applications of objects with programmable ambient heat transfer behaviors, utilizing proof-of-concept prototypes built on Thermaterial. These include reconfigurable passive thermal markers, tunable thermal wearables, containers with customized thermal-insulated layouts, and dynamic thermal-insulated enclosures.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {323},\nnumpages = {8},\nkeywords = {Programmable material, digital fabrication, thermal interface},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651072,\nauthor = {Kamihori, Mai and Ito, Kodai and Itoh, Yuichi},\ntitle = {ThermoTumbler: A Tumbler-type Device that Changes Flavor Perception by Controlling Temperature to the Lower Lip},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651072},\ndoi = {10.1145/3613905.3651072},\nabstract = {We propose ThermoTumbler that realizes affects the user’s flavor perception, including \"in-mouth aroma,\" \"aftertaste,\" \"intensity of taste,\" \"throat feeling,\" \"deliciousness,\" \"comfortable,\" \"sweetness,\" \"saltiness,\" \"sourness,\" \"bitterness,\" and \"umami,\" by presenting temperature around the lower lip without altering the beverage. The ThermoTumbler utilizes Peltier devices and heat pipes to precisely regulate the temperature at the drinking spout within the range of 0 to 65 ° C, ensuring that it presents a specific temperature to the user’s lower lip. We investigated the changes in flavor perception resulting from temperature to the lower lip using the ThermoTumbler. The evaluation experiment’s results showed that participants’ perceptions of the beverage’s \"temperature,\" \"throat feeling,\" \"deliciousness,\" and \"comfortable\" varied according to the temperature cues presented at the drinking spout. In addition, it was shown that temperature stimulation on the lower lip can accentuate a beverage’s five basic tastes.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {324},\nnumpages = {7},\nkeywords = {Flavor perception, Human Food Interaction, Pseudo-physiological reaction, Thermal sensation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650955,\nauthor = {Walker, Francesco and Favetta, Matteo and Hasker, Linde and Walker, Richard},\ntitle = {They Prefer Humans! Experimental Measurement of Student Trust in ChatGPT},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650955},\ndoi = {10.1145/3613905.3650955},\nabstract = {We investigated student trust in ChatGPT. A multiple choice questionnaire was administered to 171 students. For each question, they chose between one answer from ChatGPT and one from a human expert. Half the answers from ChatGPT and half the answers from the human expert were correct, the other half incorrect. One group saw answers labeled by source. A second group saw unlabeled answers. Participants selected more correct than incorrect answers and showed no preference for incorrect AI answers over correct human answers. We infer that they did not overtrust ChatGPT. However, while the unlabeled group preferred correct AI answers to incorrect human answers, the labeled group did not. We infer that this group undertrusted the technology, probably because of pro-human bias. While we should not underestimate the dangers of overtrust, undertrust may also be a significant issue, depriving students of valuable opportunities.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {325},\nnumpages = {7},\nkeywords = {ChatGPT, Trust, Trust in automation, Undertrust},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650967,\nauthor = {Nouraei, Farnaz and Siu, Alexa and Rossi, Ryan and Lipka, Nedim},\ntitle = {Thinking Outside the Box: Non-Designer Perspectives and Recommendations for Template-Based Graphic Design Tools},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650967},\ndoi = {10.1145/3613905.3650967},\nabstract = {Many digital graphic design tools provide design templates to help non-designers quickly get started. Users often have assets (e.g. images and text) that they want to incorporate when starting their design. Existing interaction schemes may be insufficient to help users adapt a template to work with their assets and hinder their sense of creativity and productivity. To investigate the effects of templates on non-designer workflows, we conducted a think-aloud design study (N=10) where participants designed posters using one or more templates, followed by semi-structured interviews. Our findings show that while templates help users converge to a final design quickly and partially alleviate the need for tedious decision-making, they may also lead to fixation and frustration when interactions do not support divergent thinking and flexible editing. We present design recommendations for creativity support tools (CSTs) that make use of templates, to address non-designers’ creative and practical needs.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {326},\nnumpages = {9},\nkeywords = {Creativity Support Tools, Non-Designer Experience, Qualitative Methods, Template-Based Graphic Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650867,\nauthor = {Yen, Ryan and Sultanum, Nicole and Zhao, Jian},\ntitle = {To Search or To Gen? Exploring the Synergy between Generative AI and Web Search in Programming},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650867},\ndoi = {10.1145/3613905.3650867},\nabstract = {The convergence of generative AI and web search is reshaping problem-solving for programmers. However, the lack of understanding regarding their interplay in the information-seeking process often leads programmers to perceive them as alternatives rather than complementary tools. To analyze this interaction and explore their synergy, we conducted an interview study with eight experienced programmers. Drawing from the results and literature, we have identified three major challenges and proposed three decision-making stages, each with its own relevant factors. Additionally, we present a comprehensive process model that captures programmers’ interaction patterns. This model encompasses decision-making stages, the information-foraging loop, and cognitive activities during system interaction, offering a holistic framework to comprehend and optimize the use of these convergent tools in programming.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {327},\nnumpages = {8},\nkeywords = {LLM, code generation, cognitive science, generative AI, information foraging, sensemaking, web search},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651005,\nauthor = {V S, Rajashekhar and S, Aravind and R, Aniruthvishwak and V, Narendharan and Prabhakar, Gowdham},\ntitle = {To Show or Hide - Investigating the Effect of Cursor Visibility in Gaze-controlled Interfaces},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651005},\ndoi = {10.1145/3613905.3651005},\nabstract = {In a gaze-controlled interface, the cursor is mapped to gaze points, making the cursor visible and tracked throughout the screen. We propose a novel system where we disable the visibility of the cursor movement during the pointing task and enable selection using a separate mechanical switch. As the gaze-tracker already does the pointing task, we were not required to visualize the cursor, which might distract the user from pointing at the intended target due to offsets by the eye-gaze tracker. In this study, we compared the performance of the without-cursor tracking interface and the with-cursor tracking interface. We found that the accuracy measures, average selection time, and system usability were significantly better for the without-cursor tracking interface than the with-cursor tracking interface. We are planning to extend the study to elderly users and people with motor impairments, tics, and tremors for the performance of the without-cursor tracking interface.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {328},\nnumpages = {6},\nkeywords = {Cursor tracking, Eye tracker, Mechanical Switch, Without cursor tracking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650811,\nauthor = {Dillahunt, Tawanna R and Shedden, Kerby and Filipof, Mila Ekaterina and Lee, Soyoung and Naseem, Mustafa and Toyama, Kentaro and Hui, Julie},\ntitle = {Toward a Measure of Collective Digital Capacity: An Exploratory Analysis},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650811},\ndoi = {10.1145/3613905.3650811},\nabstract = {Digital training initiatives must shift toward critical cultural and social practices that encourage full participation in community affairs. However, no measure exists to account for digital capacity at the community level. Thus, we present this late-breaking work to begin designing and validating a measure of community digital capacity and report the results of an exploratory factor analysis. The analysis is based on 553 respondents across the United States to estimate an initial three-factor structure of (1) social digital capacity, (2) individual digital capacity, and (3) infrastructure. Such questions address limitations with existing theories that do not show digital inequities in the context of underlying systemic and structural challenges posed by one’s social position. Our preliminary results suggest a potential measure for researchers and practitioners to understand whether people can access shared digital resources and activities with acceptable scientific guarantees, including favorable Akaike and Bayesian information criteria.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {329},\nnumpages = {8},\nkeywords = {Community digital capacity, Digital divide, Digital literacy, Exploratory Factor Analysis, Measure, Public Housing, Social},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651092,\nauthor = {Zhao, Yijun and Zhou, Yang and Jin, Duola and Dong, Tianshu and Wang, Guanyun and Ying, Fangtian and Shen, Qihang and Cao, Jiacheng},\ntitle = {Towards Equitable CPR: An Interactive System for Female CPR Training},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651092},\ndoi = {10.1145/3613905.3651092},\nabstract = {Bystanders are less likely to perform prompt and effective CPR on women due to psychological barriers and lack of training on female physiology. Current CPR courses, often centered on male manikins, fail to address this gap. Broadening training to include female-specific scenarios could shorten response time and improve accuracy in emergency situations. In this paper, we introduce Equi CPR, an interactive system for female CPR training. This system includes a lightweight simulator for physical feedback and a MR application for guidance and visual feedback. We also conducted a preliminary user study of Equi CPR. The result shows that the system is effective and offers a positive user experience.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {330},\nnumpages = {5},\nkeywords = {CPR, Gender differences, Haptic feedback, Interactive training, Mixed reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650950,\nauthor = {Li, Qisheng and Wu, Shaomei},\ntitle = {Towards Fair and Inclusive Speech Recognition for Stuttering: Community-led Chinese Stuttered Speech Dataset Creation and Benchmarking},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650950},\ndoi = {10.1145/3613905.3650950},\nabstract = {Despite the widespread adoption of Automatic Speech Recognition (ASR) models in voice-operated products and conversational AI agents, current ASR models perform poorly for people who stutter. One primary cause of the performance disparity is the lack of representative stuttered speech data during the development of ASR models. This work introduces the first stuttered speech dataset in Mandarin Chinese, created by a grassroots community of Chinese-speaking people who stutter to facilitate the development of inclusive and fair speech AI. Collected from 72 speakers with a wide range of stuttering characteristics, this dataset contains speech samples of both spontaneous conversations and voice command dictations from each speaker. Our analysis of the dataset shows the diversity and variability of stuttered utterances captured, highlighting its unique value in authentically representing the stuttering community in AI data. Leveraging this dataset, we benchmark popular ASR models to understand their potential biases against disfluent speech.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {331},\nnumpages = {9},\nkeywords = {AI FATE, accessibility, benchmark, datasets, speech technology, stuttered speech, stuttering},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650976,\nauthor = {Kang, Jenna Jiayi and Layton, Emily and Martin, David and Starner, Thad},\ntitle = {Towards Improving Real-Time Head-Worn Display Caption Mediated Conversations with Speaker Feedback for Hearing Conversation Partners},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650976},\ndoi = {10.1145/3613905.3650976},\nabstract = {Many products attempt to provide captioning for Deaf and Hard-of-Hearing individuals through smart glasses using automatic speech recognition. Yet there still remain challenges due to system delays and dropouts, heavy accents, and general mistranscriptions. Due to the imperfections of automatic speech recognition, there remains conversational difficulties for Deaf and Hard-of-Hearing individuals when conversing with hearing individuals. For instance, hearing conversation partners may often not realize that their Deaf or Hard-of-Hearing conversation partner is missing parts of the conversation. This study examines whether providing visual feedback of captioned conversation to hearing conversation partners can enhance conversational accuracy and dynamics. Through a task-based experiment involving 20 hearing participants we measure the impact on visual feedback of captioning on error rates, self-corrections, and subjective workloads. Our findings indicate that when given visual feedback, the average number of errors made by participants was 1.15 less (p = 0.00258) indicating a notable reduction in errors. When visual feedback is provided, the average number of self-corrections increased by 3.15 (p < 0.001), suggesting a smoother and more streamlined conversation These results show that the inclusion of visual feedback in conversation with a Deaf or Hard-of-Hearing individual can lead to improved conversational efficiency.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {332},\nnumpages = {11},\nkeywords = {Deaf, Hard-of-Hearing, accessibility, captioning, head-worn display},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650888,\nauthor = {Campreguer Fran\\c{c}a, Nathalia and Maurer, Bernhard and Altarriba Bertran, Ferran},\ntitle = {Towards Lenses for Reviewing Playfulness in HCI},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650888},\ndoi = {10.1145/3613905.3650888},\nabstract = {From playful design methods to artifacts that facilitate experiences of play, playfulness has been widely explored in HCI. However, this heterogeneous field of research lacks common ground – from a methodological, theoretical, as well as design perspective – which in turn creates problems that hinder related research outcomes. Towards common ground on playfulness, we propose these five lenses for reviewing, synthesizing, and critically engaging with literature: area of implementation, targeted audience, type of contribution, methodology, and motivation. To spark a discussion around the lenses, we probe them on a pilot review of CHI Play literature and discuss their relevance to future literature reviews that work towards making sense of the state of playfulness in HCI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {333},\nnumpages = {8},\nkeywords = {lenses, literature review, playfulness},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650744,\nauthor = {Chen, Wei Lee Yuan},\ntitle = {Towards More Secure Interactions: Understanding User Experience and Behaviour in the NFT Domain},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650744},\ndoi = {10.1145/3613905.3650744},\nabstract = {This study investigates the human errors that enable hackers to exploit and carry out social engineering attacks on the non-fungible token (NFT) ecosystem. The aim is to improve the design of decentralized applications that use NFTs to help non-technical users follow security best practices and address remaining user-side vulnerabilities. The study methods included a survey examining participants’ expertise regarding NFTs and cybersecurity, a remote security usability study investigating the pain points and common security best practices and a follow-up interview to examine participants’ experience with a crypto wallet configuration. The results show how human cognitive bias affects users’ decision to be cautious, users’ difficulty with security methods, and improvements to lessen users’ cognitive load. As NFTs expand beyond the cryptocurrency circle, multiple scams and thefts arise due to late adopters not knowing the security best practices. Therefore, increasing the public’s NFT security awareness is key to mitigating potential threats.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {334},\nnumpages = {10},\nkeywords = {Non-fungible tokens (NFTs), Social engineering},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650996,\nauthor = {Cicek, Muratcan and Manduchi, Roberto},\ntitle = {Towards Personalized Head-Tracking Pointing},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650996},\ndoi = {10.1145/3613905.3650996},\nabstract = {Head-based pointing is an effective interface for those with limited hand control, though it may involve a learning curve due to discrepancies between desired pointer movement and actual system response to head motion. We theorize that individuals have unique head movement patterns for similar tasks, necessitating tailored mappings from head to pointer motion. This was explored by analyzing video data of participants tracking a moving target on-screen using head movements. The study found that using a select set of facial landmarks outperforms other methods, like focusing on the nose-tip or rotation angles, in aligning head and pointer movements. Despite this, there is still notable bias inherent in the simple affine mapping model used. Significant variations were observed in participants’ head movements when responding to similar target paths, with diagonal movements showing a higher error rate. These insights could be crucial in creating new, personalized head-tracking interfaces, enhancing ease and efficiency.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {335},\nnumpages = {7},\nkeywords = {Head-based Interaction, Personalization, Pointing, User Study},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650739,\nauthor = {Li, Shanghao and Lane, Taylor and Hernandez, Alicia and Kabra, Vinayak and Singh, Karthik and Sit, Stefany and Soni, Nikita},\ntitle = {Towards Understanding Group Collaboration Patterns Around Mobile Augmented-Reality Interfaces for Geospatial Science Data Visualizations},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650739},\ndoi = {10.1145/3613905.3650739},\nabstract = {Research has shown benefits of providing collaborative learning opportunities in undergraduate Earth Science classrooms as it enables student groups to visualize, discuss, and make meaning of the complex Earth System Science. Augmented Reality (AR) applications have been increasingly used to present geodata visualizations in their innate three-dimensional format to support Earth Science learning. However, little is known about how student groups naturally collaborate around AR applications to interpret and make sense of geodata visualizations. To bridge this gap, we studied how groups of pre-service Earth Science teachers collaboratively explored a mobile AR application showcasing Earth's ocean system. In our findings of groups’ natural collaboration behaviors, we found that most groups tended to work closely where both group members talked about content and interacted with the AR app together. Our findings will inform the design of future mobile-AR geospatial data visualization interfaces to effectively support collaborative learning in classrooms.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {336},\nnumpages = {7},\nkeywords = {Augmented reality, Collaboration behaviors, Collaborative learning, Education technology, Geo-Science data visualization, Pre-service teachers, Touchscreen gestures},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651019,\nauthor = {Qadir, Sarvech and Niser, Andy and Caddle, Xavier V and Alsoubai, Ashwaq and Park, Jinkyung Katie and Wisniewski, Pamela J.},\ntitle = {Towards a Safer Digital Future: Exploring Stakeholder Perspectives on Creating a Sustainable Youth Online Safety Community},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651019},\ndoi = {10.1145/3613905.3651019},\nabstract = {In this study, we synthesize insights from secondary stakeholders (i.e., IT professionals, teachers, and entrepreneurs) in youth online safety regarding the use of online community platforms to raise awareness, and their effectiveness in sponsoring community engagement for developing youth online safety solutions. We created an online platform comprised of a youth online risk detection dashboard and stakeholder engagement features. We conducted semi-structured interviews with secondary stakeholders (n=10) in youth online safety to gain insights related to the use of 1) an online risk detection tool for youth, and 2) an online community platform. We present findings on the youth-focused risk detection dashboard, its educational integration, and effective consortium-building practices. Findings indicate that stakeholders emphasized the importance of privacy in managing youth’s social media data, within online youth communities. They highlighted the potential of such communities in educational settings to boost digital literacy, advocating for enhanced transparency and data protection.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {337},\nnumpages = {10},\nkeywords = {Youth online community, social media, stakeholder perspectives},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651063,\nauthor = {Dow, Travis and Pratishtha, Pratishtha and Alabood, Lorans and Jaswal, Vikram K. and Krishnamurthy, Diwakar},\ntitle = {Towards an Augmented Reality Agent to Support Communication for Nonspeaking Autistic People},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651063},\ndoi = {10.1145/3613905.3651063},\nabstract = {Many nonspeaking autistic people have learned to communicate using a letterboard. With this technique, a trained Communication and Regulation Partner (CRP) holds a plastic letterboard and reads aloud the letters an individual is pointing toward. Training to communicate in this manner is typically a lengthy process and the cost and availability of a professional CRP can be a limiting factor to an individual’s ability to progress. To address this problem, we build a system that allows an individual to wear an Augmented Reality (AR) headset and engage in spelling exercises on a holographic letterboard supported by a fully embodied, virtual CRP. Specifically, we video recorded 20-minute spelling sessions collected from 13 participants and coded these videos to understand the nature of interactions between a user and their CRP. Later, we use the coded data to derive a Markov decision process behaviour model that allows the virtual CRP (ViC) to mimic a real CRP. Our preliminary results show that ViC can achieve an accuracy of up to <Formula format=\"inline\"><TexMath><?TeX $68\\%$?></TexMath><AltText>Math 1</AltText><File name=\"chiea24-586-inline1\" type=\"svg\"/></Formula> even with the limited data we were able to collect and the simplified modeling approach we used. Thus, our work opens doors for nonspeakers to practice various aspects of spelling-based communication in the absence of a human CRP, thereby fostering increased independence and privacy.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {338},\nnumpages = {8},\nkeywords = {Accessibility, Augment Reality, Nonspeaking Autistic People, Virtual Assistant},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651021,\nauthor = {Katsini, Christina and Epiphaniou, Gregory and Maple, Carsten},\ntitle = {Towards an Evaluation Framework for Extended Reality Authentication Schemes},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651021},\ndoi = {10.1145/3613905.3651021},\nabstract = {Over the years, one of the biggest challenges for researchers has been to propose authentication schemes that will be both usable and secure and will ensure user’s privacy. The advent of Extended Reality (XR), as a new emerging technology, introduced more challenges and opportunities for authentication. Researchers have proposed several methods for user authentication, however a major challenge lies in comparing between the various methods. This comparison is crucial for XR stakeholders to understand the strengths and weaknesses of each approach, aiding informed decision-making. In addressing this challenge, this late-breaking work reports the results of a small-scale user study with experts in the field, moving towards a framework for evaluating user authentication schemes in XR.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {339},\nnumpages = {6},\nkeywords = {Extended Reality (XR), authentication, evaluation framework, usable security},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650980,\nauthor = {Min, Soyeong and Lee, Minha and Lee, Sangsu},\ntitle = {Towards the Safety of Film Viewers from Sensitive Content: Advancing Traditional Content Warnings on Film Streaming Services},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650980},\ndoi = {10.1145/3613905.3650980},\nabstract = {Traditional content warnings on film streaming services are limited to warnings in the form of text or pictograms that only offer broad categorizations at the start for a few seconds. This method does not provide details on the timing and intensity of sensitive scenes. To explore the potential for improving content warnings, we investigated users’ perceptions of the current system and their expectations for a new content warning system. This was achieved through participatory design workshops involving 11 participants. We found users’ expectations in three aspects: 1) develop a more nuanced understanding of their personal sensitivities beyond content sensitivities, 2) enable a trigger-centric film exploration process, and 3) allow for predictions regarding the timing of scenes and mitigating the intensity of sensitive content. Our study initiates a preliminary exploration of advanced content warnings, incorporating users’ specific expectations and creative ideas, with the goal of fostering safer viewing experiences.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {340},\nnumpages = {6},\nkeywords = {Content Warning, Film Viewing, Interface Design, Participatory Workshop, Streaming Services, Triggering Content, User Experience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651051,\nauthor = {Moore, Nicole and Amith, Muhammad and Neumann, Ana and Hamilton, Jane and Tang, Lu and Savas, Lara and Tao, Cui},\ntitle = {Translating motivational interviewing for the HPV vaccine into a computable ontology model for automated AI conversational interaction},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651051},\ndoi = {10.1145/3613905.3651051},\nabstract = {Human papillomavirus (HPV) vaccinations are lower than expected. To protect the onset of head and neck cancers, innovative strategies to improve the rates are needed. Artificial intelligence may offer some solutions, specifically conversational agents to perform counseling methods. We present our efforts in developing a dialogue model for automating motivational interviewing (MI) to encourage HPV vaccination. We developed a formalized dialogue model for MI using an existing ontology-based framework to manifest a computable representation using OWL2. New utterance classifications were identified along with the ontology that encodes the dialogue model. Our work is available on GitHub under the GPL v.3. We discuss how an ontology-based model of MI can help standardize/formalize MI counseling for HPV vaccine uptake. Our future steps will involve assessing MI fidelity of the ontology model, operationalization, and testing the dialogue model in a simulation with live participants.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {341},\nnumpages = {12},\nkeywords = {cancer, chat bots, conversational agents, dialogue systems, human papillomavirus, ontology, oral health, patient-provider communication},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650750,\nauthor = {El Ali, Abdallah and Venkatraj, Karthikeya Puttur and Morosoli, Sophie and Naudts, Laurens and Helberger, Natali and Cesar, Pablo},\ntitle = {Transparent AI Disclosure Obligations: Who, What, When, Where, Why, How},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650750},\ndoi = {10.1145/3613905.3650750},\nabstract = {Advances in Generative Artificial Intelligence (AI) are resulting in AI-generated media output that is (nearly) indistinguishable from human-created content. This can drastically impact users and the media sector, especially given global risks of misinformation. While the currently discussed European AI Act aims at addressing these risks through Article 52’s AI transparency obligations, its interpretation and implications remain unclear. In this early work, we adopt a participatory AI approach to derive key questions based on Article 52’s disclosure obligations. We ran two workshops with researchers, designers, and engineers across disciplines (N=16), where participants deconstructed Article 52’s relevant clauses using the 5W1H framework. We contribute a set of 149 questions clustered into five themes and 18 sub-themes. We believe these can not only help inform future legal developments and interpretations of Article 52, but also provide a starting point for Human-Computer Interaction research to (re-)examine disclosure transparency from a human-centered AI lens.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {342},\nnumpages = {11},\nkeywords = {Article 52, EU AI Act, disclosures, generative artificial intelligence, law, obligations, research questions, transparency},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650801,\nauthor = {Shusas, Erica and Forte, Andrea},\ntitle = {Trust and Transparency: An Exploratory Study on Emerging Adults' Interpretations of Credibility Indicators on Social Media Platforms},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650801},\ndoi = {10.1145/3613905.3650801},\nabstract = {The misinformation crisis across social media has disrupted critical access to information in health, politics, and public safety. Content labels have become a feature that social media platforms use to signal credibility of social media posts. Young adults receive a proportionally high percentage of their news through social media platforms, yet prior work has shown that credibility indicators are not effective signals for young audiences. This late-breaking work presents initial findings from an exploratory study into how emerging adults (ages 18-25) assess different credibility indicators currently used on social media platforms. Our findings indicate that participants have a wide variety of interpretations of the purpose and source of context labels, are supportive of automated approaches to content labeling, and trust social media platforms to oversee the application of content labels. This paper contributes these findings to the growing scholarship on content labeling and discusses their implications for designers and policymakers.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {343},\nnumpages = {7},\nkeywords = {credibility assessment, credibility indicators, emerging adults, misinformation, young people},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650837,\nauthor = {Sun, Xin and Liu, Yunjie and De Wit, Jan and Bosch, Jos A. and Li, Zhuying},\ntitle = {Trust by Interface: How Different User Interfaces Shape Human Trust in Health Information from Large Language Models},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650837},\ndoi = {10.1145/3613905.3650837},\nabstract = {The integration of Large Language Models (LLMs) with Conversational User Interfaces (CUIs) has significantly transformed health information seeking, offering interactive access to health resources. Despite the importance of trust in adopting health advice, the impact of user interfaces on trust perception in LLM-provided information remains unclear. Our mixed-methods study investigated how different CUIs (text-based, speech-based, and embodied) influence trust when using an identical LLM source. Key findings include (a) higher trust levels in information delivered via text-based interface compared to others; (b) a significant correlation between trust in the interface and the information provided; (c) participant’s prior experience, processing approach for information with different modalities and presentation styles, and usability level were key determinants of trust in health-related information. Our study sheds light on trust perceptions in health information from LLMs and its dissemination, underscoring the importance of user interface in trustworthy and effective health information seeking with LLM-powered CUIs.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {344},\nnumpages = {7},\nkeywords = {Conversational user interface, Healthcare, Human trust perception, Large language model},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650831,\nauthor = {Sultana, Sharifa and Akter, Rokeya and Sultana, Zinnat and Reza, Salim and Ahmed, Syed Ishtiaque and Rzeszotarski, Jeffrey M},\ntitle = {Understanding Environmental Sustainability and Information Practices in Global South Fish Farming},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650831},\ndoi = {10.1145/3613905.3650831},\nabstract = {This project investigates environmental sustainability and information-sharing practices in the Global South, particularly among low-resource fish farming communities. Our ongoing ethnographic study with rural Bangladeshi fish farmers found that these communities develop an understanding of environmental sustainability through formal and informal training and via close-knit groups on social platforms. When communicating with other fish farmers on social platforms, they often confront misleading information about climate and environment and struggle to validate it. Therefore, the fish farmers form support groups to combat it collectively. We believe the findings of this work will inform HCI about how to grow mass awareness of environmental sustainability among farming professionals in low-resource areas using ICTs.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {345},\nnumpages = {7},\nkeywords = {datasets, gaze detection, neural networks, text tagging},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650960,\nauthor = {Luebs, Wenhao Y. and Tigwell, Garreth W. and Shinohara, Kristen},\ntitle = {Understanding Expert Crafting Practices of Blind and Low Vision Creatives},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650960},\ndoi = {10.1145/3613905.3650960},\nabstract = {Recent research has investigated how to make digital tools accessible to blind creators, focusing on visual manipulation using gestures and other input modalities. But while prior work centers on the inaccessibility of content creation tools, less is known about how blind and low vision creators engage in analog creative activities. We explored how expert blind and low vision creatives used a variety of craft-specific and common household materials in creative activities. We interviewed vision impaired crafters about materials and tools they use, their creative practices, and their collaborations. We found that vision impaired crafters employed strategies to accessibly organize materials and that they solicited feedback from mixed-ability collaborators. These insights contribute knowledge that can inform how to structure mixed-ability design collaborations and technologies to be useful for blind and low vision participants.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {346},\nnumpages = {8},\nkeywords = {Blind and Low Vision Creatives, Crafting, Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651097,\nauthor = {Im, Jane and Toyama, Kentaro},\ntitle = {Understanding How to Design a Social Computing System That Helps PhD Students Collectively Navigate Mistreatment or Abuse in Advising Relationships},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651097},\ndoi = {10.1145/3613905.3651097},\nabstract = {People in power causing harm to those with less power is a long-standing problem across organizations. Academia is no exception. When advisors mistreat or abuse PhD students, how could a digital platform help affected PhD students connect with each other for collectively exploring solutions? To understand if there is a need for such a system, and how to design it, we conducted interviews with 10 PhD students. Our findings showed participants were overall positive about the high-level concept of a system for connecting PhD students to address problematic advising. Participants emphasized various social and technical features needed for comfortably using such a system. Simultaneously, participants had different preferences on how they would use it, based on their risk levels. We conclude by reflecting on the importance of centering users’ consent in nuanced ways when actually building the system.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {347},\nnumpages = {7},\nkeywords = {Mentoring, PhD advising, abuse of power, academia, academic bullying, consent, mistreatment, power dynamics, rankism.},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650791,\nauthor = {Ma, Donghyeok and Lee, Joon Hyub and Bae, Seok-Hyung},\ntitle = {Understanding Visual, Integrated, and Flexible Workspace for Comprehensive Literature Reviews with SketchingRelatedWork},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650791},\ndoi = {10.1145/3613905.3650791},\nabstract = {Writing an academic paper requires significant time and effort to find, read, and organize many related papers, which are complex knowledge tasks. We present a novel interactive system that allows users to perform these tasks quickly and easily on the 2D canvas with pen and multitouch inputs, turning users’ sketches and handwriting into node-link diagrams of papers and citations that users can iteratively expand in situ toward constructing a coherent narrative when writing Related Work sections. Through a pilot study involving researchers experienced in publishing academic papers, we show that our system can serve as a visual, integrated, and flexible workspace for conducting comprehensive literature reviews.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {348},\nnumpages = {7},\nkeywords = {Related work, inking, node-link diagram},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650978,\nauthor = {Jo, Jeongwon and Xie, Jingyi and Carroll, John M.},\ntitle = {Understanding and Balancing Trade-offs of Visibility in Support Requests},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650978},\ndoi = {10.1145/3613905.3650978},\nabstract = {This study delves into support-seeking behaviors in local support exchange networks, where a lack of support requests has been commonly found in prior studies. Focusing on three widely adopted approaches to making support requests in these networks, our research employed a scenario-based approach, presenting participants with three situations requiring instrumental support. Participants were asked to choose and rationalize their preferred method of request for each scenario. The study reveals that preferences varied due to factors like urgency, privacy, social image, and control over the situation. These findings highlight significant trade-offs between different approaches to asking for help. We suggest design implications aimed at facilitating more effective and psychologically comfortable support-seeking behaviors on local support exchange platforms.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {349},\nnumpages = {8},\nkeywords = {Support exchange, help-seeking behaviors, local support exchange, scenario-based design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651007,\nauthor = {Qian, Crystal and Reif, Emily and Kahng, Minsuk},\ntitle = {Understanding the Dataset Practitioners Behind Large Language Models},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651007},\ndoi = {10.1145/3613905.3651007},\nabstract = {As large language models (LLMs) become more advanced and impactful, it is increasingly important to scrutinize the data that they rely upon and produce. What is it to be a dataset practitioner doing this work? We approach this in two parts: first, we define the role of “dataset practitioners” by performing a retrospective analysis on the responsibilities of teams contributing to LLM development at a technology company, Google. Then, we conduct semi-structured interviews with a cross-section of these practitioners (N=10). We find that although data quality is a top priority, there is little consensus around what data quality is and how to evaluate it. Consequently, practitioners either rely on their own intuition or write custom code to evaluate their data. We discuss potential reasons for this phenomenon and opportunities for alignment.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {350},\nnumpages = {7},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650759,\nauthor = {Cho, Soo Hyung and Jon, Seongick and Jin, Youngkyu and Jung, Jongwoo and Oh, Changhoon},\ntitle = {Understanding the Dynamics in Creating Domain-Specific AI Design Guidelines: A Case Study of a Leading Digital Finance Company in South Korea},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650759},\ndoi = {10.1145/3613905.3650759},\nabstract = {Artificial intelligence (AI) significantly impacts the user experience (UX) across a wide range of products and services. However, many practitioners find it challenging to design the UX for AI-enhanced products. Although there are human-AI interaction design guidelines available, they are often too generic to fully address domain-specific challenges in fields such as finance and healthcare. Additionally, they sometimes overlook the constraints imposed by different working environments. In this research, we conducted a case study at one of the leading financial corporations in South Korea to develop their own AI product interaction guidelines. We interviewed 13 practitioners including bank UX practitioners, design agency UX designers, and academic UX researchers who were involved in this initiative, to gather insights into their experiences with crafting these guidelines. Our findings revealed a mixed reception towards existing AI design resources among practitioners, underscoring the necessity to incorporate elements specific to banking services, such as the company’s customer experience (CX) principles, overarching agent personas, multifaceted channels, and diverse interaction modalities. We also observed the importance of understanding the dynamics among various stakeholders within a company. We discuss the implications and design considerations for creating domain-specific resources specialized in AI interactions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {351},\nnumpages = {7},\nkeywords = {AI, CX, Guidelines, Human-AI Interaction, Interaction Design, UX},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651018,\nauthor = {Lin, Bei-Hong and Chung, Yu-Jung and Cheng, Hao-Yuan and Yen, Yu-Ting and Li, Ching-Chuan and Cherng, Fu-Yin},\ntitle = {Understanding the Effects of Short-Form Videos on Sustained Attention},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651018},\ndoi = {10.1145/3613905.3651018},\nabstract = {Understanding the rapidly expanding influences of short-form videos on cognition is critical. Previous studies have investigated the relationship between short-form video and several cognitive functions. However, few empirical studies have examined the effects of short-form video consumption on sustained attention. This study aimed to address these impacts through an online survey and a long-term field experiment. The online survey utilized a psychological test and found that an increased amount of short-form video consumption relates to poor sustained attention. The long-term field experiment clarified the directionality of the effect through long-term observation and sustained attention tests conducted at fixed intervals. We found that changing the duration of short-form video consumption does not significantly influence performance on most sustained attention tests. Our research constitutes a preliminary exploration that revealed the effects of short-form videos on sustained attention.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {352},\nnumpages = {8},\nkeywords = {Long-term Field Experiment, Short-form Video, Sustained Attention, Video Consumption},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650752,\nauthor = {Dufresne, Florian and Dubosc, Charlotte and Gorisse, Geoffrey and Christmann, Olivier},\ntitle = {Understanding the Impact of Coherence between Virtual Representations and Possible Interactions on Embodiment in VR: an Affordance Perspective},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650752},\ndoi = {10.1145/3613905.3650752},\nabstract = {The way users interact with Virtual Reality (VR) environments plays a crucial role in shaping their experience of embodying an avatar. The perception of avatars significantly influences users’ behaviour based on stereotypes, a phenomenon known as the Proteus effect. Moreover, understanding how virtual representations suggest possibilities for action (affordances) has attracted considerable attention in the human-computer interaction community. The aesthetic features of avatars may thus signify false affordances, conflicting with users’ expectations and impacting perceived plausibility, presence and embodiment. To explore this topic, this research focuses on the perception of virtual hands, comparing ghost-like and gloved representations by leveraging their affordances. This paper presents a preliminary assessment of avatar-related affordances, and the protocol of the follow-up VR experiment. Our future contribution lies in a documented study on the coherence between user representation, suggested affordances and actual agency, aiming to inform designers and researchers on how it could enhance VR qualia.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {353},\nnumpages = {7},\nkeywords = {affordance, avatar, embodiment, plausibility, presence, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650783,\nauthor = {Hou, Yihan and Cui, Hao and Chen, Rongrong and Zeng, Wei},\ntitle = {Understanding the Impact of Referent Design on Scale Perception in Immersive Data Visualization},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650783},\ndoi = {10.1145/3613905.3650783},\nabstract = {Referents are often used to enhance scale perception in immersive visualizations. Common referent designs include the considerations of referent layout (side-by-side vs. in-situ) and referent size (small vs. medium vs. large). This paper introduces a controlled user study to assess how different referent designs affect the efficiency and accuracy of scale perception across different data scales, on the performance of the size-matching task in the virtual environment. Our results reveal that in-situ layouts significantly enhance accuracy and confidence across various data scales, particularly with large referents. Linear regression analyses further confirm that in-situ layouts exhibit greater resilience to changes in data scale. For tasks requiring efficiency, medium-sized referents emerge as the preferred choice. Based on these findings, we offer design guidelines for selecting referent layouts and sizes in immersive visualizations.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {354},\nnumpages = {7},\nkeywords = {Immersive Visualization, Referent Design, Scale Perception},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650785,\nauthor = {Han, Jiyeon and Yang, Eunseo and Oh, Uran},\ntitle = {Understanding the Use of AI-Based Audio Generation Models by End-Users},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650785},\ndoi = {10.1145/3613905.3650785},\nabstract = {With the growing popularity of video platforms, the demand for copyright-free audio sources for adding background music to videos is also expected to increase. While text-to-audio-generation models can be useful for this purpose, little is known about how people perceive and use these models. To understand how generation models for audio are used and to identify their strengths and weaknesses compared to typical audio search engines, we conducted a user study with 16 participants, where they were asked to choose matching background music after watching muted videos. Findings show that participants appreciate the search engine for recommending search keywords and displaying multiple results although the outcome does not fully reflect the intent. In contrast, the generation model posed challenges in choosing proper prompts but excelled in finding the desired music. Based on these results, we suggest design considerations to improve the usability of the audio generation model for end-users.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {355},\nnumpages = {7},\nkeywords = {empirical study, search engine, text-to-audio, user experience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650979,\nauthor = {Jeung, Jun Li and Huang, Janet Yi-Ching},\ntitle = {Unlocking Memories with AI: Exploring the Role of AI-Generated Cues in Personal Reminiscing},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650979},\ndoi = {10.1145/3613905.3650979},\nabstract = {While technology-mediated reminiscing has been studied for decades, generating relevant cues to trigger personal reminiscing remains challenging. The potential of AI in generating relevant content across various domains has been recently recognized, yet its use in facilitating reminiscing is still less explored. This work aims to explore the use of AI in supporting the recall of personal memories associated with significant objects at home. We designed Treasurefinder, a device powered by a large language model (LLM) that generates open-ended questions based on stories stored in NFC-tagged physical objects or cards. We conducted an exploratory study with 12 participants, grouped in pairs, to observe reminiscing behaviors when using Treasurefinder. The results showed the AI-generated questions 1) supported individuals to recall the past, 2) provided new insights about the other person, and 3) encouraged reflection. Notably, the device facilitated active memory retrieval related to cherished objects that are often overlooked.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {356},\nnumpages = {6},\nkeywords = {AI for Personal Reminiscing, AI-Generated Memory Cues, LLMs},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651083,\nauthor = {Shibani, Antonette and Knight, Simon and Kitto, Kirsty and Karunanayake, Ajanie and Buckingham Shum, Simon},\ntitle = {Untangling Critical Interaction with AI in Students’ Written Assessment},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651083},\ndoi = {10.1145/3613905.3651083},\nabstract = {Artificial Intelligence (AI) has become a ubiquitous part of society, but a key challenge exists in ensuring that humans are equipped with the required critical thinking and AI literacy skills to interact with machines effectively by understanding their capabilities and limitations. These skills are particularly important for learners to develop in the age of generative AI where AI tools can demonstrate complex knowledge and ability previously thought to be uniquely human. To activate effective human-AI partnerships in writing, this paper provides a first step toward conceptualizing the notion of critical learner interaction with AI. Using both theoretical models and empirical data, our preliminary findings suggest a general lack of Deep interaction with AI during the writing process. We believe that the outcomes can lead to better task and tool design in the future for learners to develop deep, critical thinking when interacting with AI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {357},\nnumpages = {6},\nkeywords = {Artificial Intelligence, Assessment, CIAW, ChatGPT, Computing, Critical Interaction, Data Science, Education, GenAI, Writing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650794,\nauthor = {Kwon, Huisung and Choi, Yunjae Josephine and Lee, Sunok and Lee, Sangsu},\ntitle = {Unveiling the Inherent Needs: GPT Builder as Participatory Design Tool for Exploring Needs and Expectation of AI with Middle-Aged Users},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650794},\ndoi = {10.1145/3613905.3650794},\nabstract = {A generative session that directly involves users in the design process is an effective way to design user-centered experiences by uncovering intrinsic needs. However, engaging users who lack coding knowledge in AI system design poses significant challenges. Recognizing this, the recently revealed GPT-creating tool, which allows users to customize ChatGPT through simple dialog interactions, is a promising solution. We aimed to identify the possibility of using this tool to uncover intrinsic users’ needs and expectations towards AI. We conducted individual participatory design workshops with generative sessions focusing on middle-aged individuals. This approach helped us to delve into the latent needs and expectations of conversational AI among this demographic. We discovered a wide range of unexpressed needs and expectations for AI among them. Our research highlights the potential and value of using the GPT-creating tool as a design method, particularly for revealing the users’ unexpressed needs and expectations.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {358},\nnumpages = {6},\nkeywords = {Artificial Intelligence (AI), GPT, Generative Design, Inherent needs, LLM-based CAs, Participatory Design (PD)},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651119,\nauthor = {Pietschmann, Leon and Pickhardt, Fritz},\ntitle = {Use of Virtual Reality for Emergency Service Training},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651119},\ndoi = {10.1145/3613905.3651119},\nabstract = {This paper investigates the use of Virtual Reality (VR) in an emergency service and civil protection context. We present an exploratory user study with 30 participants sourced from the German Federal Agency for Technical Relief (THW) who were tasked with completing VR training on the assembly of a rescue platform. We evaluated the time to completion, mistakes made, cognitive load, usability, motivation, and perceived helpfulness of visual guidance elements implemented in the VR environment. The results indicate a positive effect of VR training. In addition to sourcing from a pool of emergency service workers as participants for our study, we report on the participants’ feedback concerning other potential application areas for VR training in an emergency service context, which further sets our contribution apart from previous work. The paper contributes to the field of human-computer interaction by providing insights into the complex relationship between VR training and emergency service personnel.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {359},\nnumpages = {7},\nkeywords = {AR/VR/XR/Immersive, Civil Protection, Emergency Services, User Study, Visual Guidance},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650799,\nauthor = {Elahimanesh, Sina and Mohammadi, Iman and Mosayebi, Mohammad and Zahedi Movahed, Sara and Hasani, Hosein and Rohban, Mohammad Hossein},\ntitle = {User Voices, Platform Choices: Social Media Policy Puzzle with Decentralization Salt},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650799},\ndoi = {10.1145/3613905.3650799},\nabstract = {In the current digital era, social media platforms wield crucial influence, with the potential for biased content moderation. Considering this risk, we propose a decentralized social media policy-making in this work. The noticeable difference between people’s preferences and X’s established policies in a preliminary study motivates us to design a similar website to collect more comprehensive data in a diverse community. Consequently, N=110 individuals from diverse backgrounds participated in our primary experiment to decide about content moderation on social media. For this purpose, 546 tweets in 3 categories are investigated, 3032 records are captured, and the effect of personal favor on content moderation is analyzed. Furthermore, we propose a novel AI-based method to learn the recommended policy of participants and achieve an accuracy of 79\\%. Also, by considering the suggested policy of 5 Large Language Models, it is illustrated that they cannot be the decision-makers on democratic social media platforms.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {360},\nnumpages = {10},\nkeywords = {Content Censorship, Decentralization, Decentralized Policy, Social Media, Social Network},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650852,\nauthor = {Zhang, Mingyue and Li, Jialong and Li, Nianyu and Kang, Eunsuk and Tei, Kenji},\ntitle = {User-Driven Adaptation: Tailoring Autonomous Driving Systems with Dynamic Preferences},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650852},\ndoi = {10.1145/3613905.3650852},\nabstract = {In the realm of autonomous vehicles, dynamic user preferences are critical yet challenging to accommodate. Existing methods often misrepresent these preferences, either by overlooking their dynamism or overburdening users as humans often find it challenging to express their objectives mathematically. The previously introduced framework, which interprets dynamic preferences as inherent uncertainty and includes a “human-on-the-loop” mechanism enabling users to give feedback when dissatisfied with system behaviors, addresses this gap. In this study, we further enhance the approach with a user study of 20 participants, focusing on aligning system behavior with user expectations through feedback-driven adaptation. The findings affirm the approach’s ability to effectively merge algorithm-driven adjustments with user complaints, leading to improved participants’ subjective satisfaction in autonomous systems.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {361},\nnumpages = {8},\nkeywords = {Autonomous Driving, Human on the Loop, Preference Adaptation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650850,\nauthor = {Haddad, Alexandre and Bailly, Gilles and Choussy, Olivier and Taouachi, Rabah and Avellino, Ignacio},\ntitle = {Using Live Tags for Summarizing Surgical Videos Collaboratively},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650850},\ndoi = {10.1145/3613905.3650850},\nabstract = {Surgical videos serve various purposes, including training and dissemination of science. However, their production remains limited given the time needed and the limited support from commercial video editing tools. This work explores additive summarization, a novel approach to surgical video editing based on concatenating tagged segments, rather than subtracting unimportant sequences. We conducted three workshops where pairs of surgeons (expert–novice) summarized surgical videos using a custom-built prototype. The results shows that tags support additive summarization, ease the creation of multiple summaries, and support collaborative work. Moreover, we observe that the expertise of residents impacts the division of work. These results show the potential of the additive approach, and open research avenues for future work studying its effectiveness.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {362},\nnumpages = {7},\nkeywords = {collaborative video editing, endoscopic video, surgery, video summarization, video tags},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650986,\nauthor = {Powell, Larry and Hammond, Tracy},\ntitle = {Utilizing Video and Sensor Technology for Analyzing Non-Swimmer Children in Swimming Education Programs through Observational Methods},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650986},\ndoi = {10.1145/3613905.3650986},\nabstract = {Accidental drowning stands as the third leading cause of global mortality, according to the World Health Organization. While suggestions like increased supervision and physical barriers aim to reduce drowning risk, their effectiveness is hindered by the widespread accessibility of water bodies. In contrast, swimming education has demonstrated a remarkable potential to decrease the risk of drowning by up to 90\\%. However, research on technology-supported swimming education is limited, mainly focusing on drowning detection and support for competitive swimmers. This paper addresses this gap by presenting outcomes from a user study, particularly focusing on a vulnerable demographic: children without basic swimming skills in survival-oriented beginner courses. Data from 45 participants aged 4 to 13 were collected through wearable sensors and cameras. The study’s results offer valuable insights into integrating technology into swimming education for non-swimming children, contributing to the advancement of drowning prevention strategies.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {363},\nnumpages = {8},\nkeywords = {Swimming Education, User Studies, Wearable Technology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651084,\nauthor = {Kim, Jinwook and Kim, Taesu and Lee, Jeongmi},\ntitle = {VR-SSVEPeripheral: Designing Virtual Reality Friendly SSVEP Stimuli using Peripheral Vision Area for Immersive and Comfortable Experience},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651084},\ndoi = {10.1145/3613905.3651084},\nabstract = {Recent VR HMDs embed various bio-sensors (e.g., EEG, eye-tracker) to expand the interaction space. Steady-state visual evoked potential (SSVEP) is one of the most utilized methods in BCI, and recent studies are attempting to design novel VR interactions with it. However, most of them suffer from usability issues, as SSVEP uses flickering stimuli to detect target brain signals that could cause eye fatigue. Also, conventional SSVEP stimuli are not tailored to VR, taking the same form as in a 2D environment. Thus, we propose VR-friendly SSVEP stimuli that utilize the peripheral, instead of the central, vision area in HMD. We conducted an offline experiment to verify our design (n=20). The results indicated that VR-SSVEPeripheral was more comfortable than the conventional one (Central) and functional for augmenting synchronized brain signals for SSVEP detection. This study provides a foundation for designing a VR-suitable SSVEP system and guidelines for utilizing it.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {364},\nnumpages = {7},\nkeywords = {Brain-Computer Interface, Electroencephalography (EEG), Immersion, Steady State Visually Evoked Potentials (SSVEP), Usability, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650824,\nauthor = {Kitson, Alexandra and Ahn, Sun Joo (Grace) and Gonzalez, Eric J and Panda, Payod and Isbister, Katherine and Gonzalez-Franco, Mar},\ntitle = {Virtual Games, Real Interactions: A Look at Cross-reality Asymmetrical Co-located Social Games},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650824},\ndoi = {10.1145/3613905.3650824},\nabstract = {Multiuser, multi-device environments in extended realities (XR) enable synchronous social interactions. With the freedom and flexibility to choose the most suitable device, we allow for inclusive environments where even spectators can be involved. However, existing research has mostly been conducted in controlled laboratory settings, which limits the applicability of the findings to naturalistic scenarios. We conducted a mixed methods study with social XR experts to explore situated and asymmetrical modalities in the context of XR gaming for enabling social interactions in naturalistic social settings, focusing on two games. We considered variations in available devices, spatial constraints, and users’ motivations and expertise. Our research suggests that asymmetrical interfaces may reduce barriers to entry for XR, support social connection, and promote cross-platform communication and collaboration. Together, our findings provoke critical discussions for future work on the effective deployment of asymmetrical interfaces in naturalistic scenarios and address potential technical, spatial, and social challenges.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {365},\nnumpages = {9},\nkeywords = {Asymmetric XR, Co-located XR, Hybrid Gaming, Multi-device, Multiplayer VR, Social XR, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650766,\nauthor = {Zhang, Jingjing and Han, Binyang and Dong, Ze and Wen, Ruoyu and Lee, Gun A. and Hoermann, Simon and Zhang, Wendy and Piumsomboon, Thammathip},\ntitle = {Virtual Triplets: A Mixed Modal Synchronous and Asynchronous Collaboration with Human-Agent Interaction in Virtual Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650766},\ndoi = {10.1145/3613905.3650766},\nabstract = {We introduce the Virtual Triplets (VTs), a Virtual Reality collaborative system designed for both synchronous and asynchronous interaction through collaboration between humans and human-agent. When a single instructor supervises two students in a showcased virtual cooking class scenario, the system enables the instructor to switch control between avatars in two separate environments, and a virtual agent takes over in the instructor’s absence, ensuring continuous support. VTs facilitate the recording and playback of the instructor’s demonstrations for students, coupled with a feature that allows the instructor to employ a bird’s-eye view for effective classroom management. Analysis of observational data and interviews from our pilot study indicates good system usability, ease of avatar management for instructors, and a strong sense of continuous support among students. We discuss potential improvements and broader applications of VTs, aiming to enhance user experience in multitasking scenarios involving multiuser human-human and human-agent collaboration.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {366},\nnumpages = {8},\nkeywords = {Embodied Virtual Agents, Human-Agent Collaboration, Human-Agent Interaction, Multi-User Collaboration, Virtual Reality, Virtual Training},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651114,\nauthor = {Yu, Minjing and Zeng, Lingzhi and Liao, Qiantian and Du, Xinxin and Sheng, Jenny and Ren, Ziqi and E, Yanzhi and Wang, Huamin and Liu, Yong-Jin},\ntitle = {VisHanfu: An Interactive System Centered on the Cross-Shaped Flat Structure for the Preservation of Hanfu Culture},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651114},\ndoi = {10.1145/3613905.3651114},\nabstract = {Hanfu, which embodies the exceptional artistry of dying, weaving, and embroidery, is the emblematic traditional garment of Han nationality in China. However, there is a lack of convenient and immersive promotion methods for Hanfu. In this paper, we presented a virtual reality system focusing on the \"Cross-Shaped Flat Structure\", which is an integral feature of Hanfu. We restore five representative Hanfu historical artifacts and provide an interactive Hanfu making experience. Combined with highly realistic cloth simulation techniques, it allows users to interactively observe the movement effects of the Hanfu. The user experiment results show that our system can provide a favorable experience for users and bring a better learning effect, which helps users to enhance their interest in learning and thus contributes to the inheritance of Hanfu culture.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {367},\nnumpages = {7},\nkeywords = {Cultural Heritage, Hanfu, Interactive Design, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651123,\nauthor = {Sawada, Shoko and Suzuki, Tomoyuki and Yamaguchi, Kota and Toyoda, Masashi},\ntitle = {Visual Explanation for Advertising Creative Workflow},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651123},\ndoi = {10.1145/3613905.3651123},\nabstract = {Explainable AI (XAI) attempts to produce interpretable results from highly complex AI systems, but its form and effectiveness vary depending on the application domain. In this paper, we explore how XAI techniques can help graphic designers work on advertising materials. A creative domain such as graphic design is often characterized by a weak connection between the individual work and the business goal; e.g., a small change in the design of a banner can result in a huge difference in the audience’s reaction. We develop an XAI system for designers that provides visual feedback explaining which component of the design is likely to affect the business metric. Our user study shows that with our system, designers complete the project in fewer iterations and in less time to achieve the desired quality of work compared to naive score-based feedback. These findings highlight the benefits of leveraging XAI in creative domains.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {368},\nnumpages = {8},\nkeywords = {Graphic design, Visual explanation, XAI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650884,\nauthor = {Knierim, Michael Thomas and Braun, Lukas and Perusquia-Hernandez, Monica},\ntitle = {Warmth on Demand: Designing Headphones for Enhanced Thermal Comfort in Work Environments},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650884},\ndoi = {10.1145/3613905.3650884},\nabstract = {In response to the increasing demand for personalized body temperature management, our study investigates headphones as a wearable system for enhancing thermal comfort in work environments. Utilizing a mixed-methods approach, we evaluated user experiences with our developed thermoactive prototype. The results demonstrate that active heating via headphones provides more pronounced thermal effects compared to passive heating with regular headphones, particularly around the ears, but also moderately for the whole body, offering potential relief in cold office settings. However, the qualitative insights suggest that its effectiveness in supporting whole-body thermal comfort diminishes in the presence of intense cold in extremities like hands or feet. Additionally, our findings indicate a modest link between thermal comfort and productivity-related factors, including performance, workload, and frustration during work. These insights show the potential of integrating thermal comfort solutions in everyday wearable technology like headphones, leveraging their ubiquity to enhance user well-being and work efficiency.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {369},\nnumpages = {7},\nkeywords = {3D-Printing, Earables, Headphones, Personal Comfort Systems, Thermal Comfort},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651047,\nauthor = {Sokol, Kacper and Vogt, Julia E.},\ntitle = {What Does Evaluation of Explainable Artificial Intelligence Actually Tell Us? A Case for Compositional and Contextual Validation of XAI Building Blocks},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651047},\ndoi = {10.1145/3613905.3651047},\nabstract = {Despite significant progress, evaluation of explainable artificial intelligence remains elusive and challenging. In this paper we propose a fine-grained validation framework that is not overly reliant on any one facet of these sociotechnical systems, and that recognises their inherent modular structure: technical building blocks, user-facing explanatory artefacts and social communication protocols. While we concur that user studies are invaluable in assessing the quality and effectiveness of explanation presentation and delivery strategies from the explainees’ perspective in a particular deployment context, the underlying explanation generation mechanisms require a separate, predominantly algorithmic validation strategy that accounts for the technical and human-centred desiderata of their (numerical) outputs. Such a comprehensive sociotechnical utility-based evaluation framework could allow to systematically reason about the properties and downstream influence of different building blocks from which explainable artificial intelligence systems are composed – accounting for a diverse range of their engineering and social aspects – in view of the anticipated use case.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {370},\nnumpages = {8},\nkeywords = {Artificial Intelligence., Evaluation, Explainability, Interpretability, Machine Learning, User Studies, Validation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650871,\nauthor = {Vogel, Adele and Korte, Jessica L},\ntitle = {What Factors Motivate Culturally Deaf People to Want Assistive Technologies?},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650871},\ndoi = {10.1145/3613905.3650871},\nabstract = {Assistive technologies see high rates of rejection, especially in Deaf communities, due to barriers including cost, discomfort, acclimatization periods, lack of aesthetic appeal, visibility of technology and/or disability, misalignment with user’s identity, loss of independence, self-stigma, social anxiety, social stigma, privacy concerns, surveillance and control of data. To explore what factors might motivate Deaf people to engage with assistive technologies, we interviewed twelve Deaf people about a proposed AI-based assistive technology: a personal assistant device which would understand and respond in Auslan, the sign language of the Australian Deaf community. Thematic analysis of the interview results revealed interviewees would be motivated to use such technology by linguistic and cultural alignment, equality, accessibility and inclusion, independence, life improvements, and aesthetic and technology appeal. We propose these motivating factors should be considered in the development of future assistive technologies, especially those intended for culturally Deaf people.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {371},\nnumpages = {7},\nkeywords = {AI, Deaf, assistive technology, human-centered AI, human-centred AI, participatory AI, participatory design, personal assistant, sign language},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650804,\nauthor = {Terenti, Mihail and Casado-Palacios, Maria and Gori, Monica and Vatavu, Radu-Daniel},\ntitle = {What Is the User Experience of Eyes-Free Touch Input with Vibrotactile Feedback Decoupled from the Touchscreen?},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650804},\ndoi = {10.1145/3613905.3650804},\nabstract = {Vibrotactile cues commonly serve as substitutes for visual feedback in eyes-free touch input, yet not all touchscreens have haptic capabilities. In this work, we examine the user experience of arm-localized vibrations delivered away from the touchscreen through a connected wearable. In a controlled experiment, fourteen participants used a touchscreen in eyes-free mode, while receiving single-pulse vibrations on the index finger, wrist, or forearm as confirmatory feedback of their on-screen touches. We reveal a favorable user experience of vibrations decoupled from the touchscreen, characterized by high perceived enjoyment (Mdn=4 on a scale from 1 to 5), efficiency (Mdn=4), integration (Mdn=5), and confidence (Mdn=4) with moderate complexity (Mdn=3) and low perceived distractedness (Mdn=2), difficulty (Mdn=1), and confusion (Mdn=1). To contextualize our findings, we contrast the user experience of arm-localized vibrations in eyes-free vs. eyes-on touch input, and outline preliminary observations involving three legally blind users.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {372},\nnumpages = {8},\nkeywords = {User experience, eyes-free input, touch input, touchscreens, vibrotactile feedback, wearables},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651396,\nauthor = {Kelly, Merlin Angel and Yang, Lingqian and Thomas, Alexander and Donnelly, Pete and Cho, Youngjun},\ntitle = {WheelSkills: Prototyping Manual Wheelchair Training through Immersive Visual Feedback},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651396},\ndoi = {10.1145/3613905.3651396},\nabstract = {Wheelchair skills training is an essential structured process for wheelchair users to learn how to maneuver effectively, avoiding improper wheelchair use and preventing potential mobility impairments from developing. Online video tutorials have often been used for this training in familiar settings. However, video training lacks real-time feedback, affecting training efficacy in contrast with in-person training. In this paper, we propose WheelSkill, a prototype wheelchair training system combining motion capture and a training interface, providing real-time visual feedback based on the user’s skeletal motion. Eight wheelchair users and a wheelchair expert trainer were consulted in a pilot focus group and interview. Results highlight themes of human-centred design principles for wheelchair users for the final iteration of WheelSkills to assimilate for a high-fidelity wheelchair training feedback system. Finally, we discuss the benefits and limitations of WheelSkills and the adaptions planned for future works.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {373},\nnumpages = {8},\nkeywords = {Manual wheelchair users (MWUs), Motion capture, Posture correction, Skeleton visualization, Visual feedback, Wheelchair skills training (WST)},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651094,\nauthor = {Abdrabou, Yasmeen and Omelina, Tatiana and Dietz, Felix and Khamis, Mohamed and Alt, Florian and Hassib, Mariam},\ntitle = {Where Do You Look When Unlocking Your Phone? : A Field Study of Gaze Behaviour During Smartphone Unlock},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651094},\ndoi = {10.1145/3613905.3651094},\nabstract = {Eye gaze has emerged as a promising avenue for implicit authentication/identification on smartphones, offering the potential for seamless user identification and two-factor authentication. However, a crucial gap exists in understanding eye gaze behaviour specifically during smartphone unlocks. This lack of understanding is magnified by scenarios where users’ faces are not fully visible in front cameras, leading to inaccurate gaze estimation. In this work, we conducted a 24-hour in-the-wild study tracking 21 users’ eye gaze during smartphone unlocks. Our findings highlight substantial eye gaze behaviour variations influenced by authentication methods, physical activity, and environment. Our findings provide insights to enhance and adapt implicit user identification/authentication systems based on gaze tracking on smartphones taking into consideration different users’ behaviour, and environmental effects.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {374},\nnumpages = {7},\nkeywords = {Authentication, Eye Tracking, Gaze Behaviour, Identification, Smartphones},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651038,\nauthor = {Bae, Jaehoo and Seu, Donghoon and Oh, Minah and Yun, Myung-Hwan and Kim, Wonjoon},\ntitle = {Who's in Charge of Charging Experience: Heuristic Evaluation Criteria for Addressing Design Issues in Electric Vehicle Charging},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651038},\ndoi = {10.1145/3613905.3651038},\nabstract = {As the population of electric vehicles (EVs) continues to grow, managing and enhancing the Electric Vehicle Charging Experience (eCX) has become an inevitable challenge. However, the research community has not given comprehensive attention to the eCX, often neglecting interactions between key components: EVs, chargers, and mobile apps. This research addresses the gap by identifying heuristic evaluation criteria to measure the problems of existing EV chargers for effective management. The charging process was analyzed hierarchically to establish criteria, and its relevance to previously established heuristics was evaluated. Domain experts then verified 27 criteria in 7 tasks, whether these can effectively identify issues affecting eCX through ratings and discussions. As a result of quantitative analysis of the ratings and qualitative examination of the discussions, formulated criteria can offer comprehensive and valuable insights spanning various eCX components and tasks. This research can guide designers in enhancing the eCX for current and prospective users.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {375},\nnumpages = {8},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650956,\nauthor = {Gu, Ziwei and Raymond, Owen and Al Madi, Naser and Glassman, Elena L.},\ntitle = {Why Do Skimmers Perform Better with Grammar-Preserving Text Saliency Modulation (GP-TSM)? Evidence from an Eye Tracking Study},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650956},\ndoi = {10.1145/3613905.3650956},\nabstract = {Recent work has introduced Grammar-Preserving Text Saliency Modulation (GP-TSM), a novel text rendering technique that has been shown to enhance reading efficiency and experience. However, the mechanism through which GP-TSM augments reading remains unclear. In this work, we conducted a within-subjects eye-tracking user study with 24 participants to understand how GP-TSM influences the reading experience. We found that participants closely adhered to GP-TSM’s visual cues, exhibiting gaze behavior that is distinct from that observed with the typical reading interface. From this gaze pattern, we highlight how GP-TSM leads to more efficient and coherent skimming while allowing the revisiting of skipped details.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {376},\nnumpages = {8},\nkeywords = {eye-tracking, human-AI interaction, text visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650796,\nauthor = {Do, Nam Hoai and Le, Khanh-Duy and Ly, Duy-Nam and Fjeld, Morten and Tran, Minh-Triet},\ntitle = {XRPublicSpectator: Towards Public Mixed Reality Viewing in Collocated Asymmetric Groups},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650796},\ndoi = {10.1145/3613905.3650796},\nabstract = {Mixed Reality (MR) is often viewed and experienced by users wearing specialized head-mounted displays (HMDs) to perceive virtual objects spatially positioned in the users’ physical environment. In a classroom or during on-stage presentation, it is often presenters only who are equipped with MR HMDs. However, since spectators most often outnumber presenters, equipping collocated spectators with HMDs to create a shared immersive experience can be costly. This imbalance can result in inefficient presenter-spectator communication and can reduce spectator engagement. To address the need of viewing MR content in such collocated asymmetric groups, we present a concept called XRPublicSpectator. This system utilizes a large display to present a third-person-view of the MR environment constructed by combining RGB-D data of the physical space obtained from a depth-sensing camera with objects from the same virtual environment as tracked by the MR HMDs. Leveraging the XRPublicSpectator concept, we developed an exemplary application which captivated an MR game arena where non-HMD users can watch players performing a duel card game. Results from a preliminary study with the exemplary application show that compared to the first-person MR view, XRPublicSpectator enabled non-HMD users to more comprehensively perceive information within the MR environment and potentially improved their engagement with HMD users and MR contents.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {377},\nnumpages = {7},\nkeywords = {Asymmetric Groups, Collocated Team, Mixed Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650880,\nauthor = {Quan, Shuang and Du, Yao and Ding, Yi},\ntitle = {Young Children and ChatGPT: Parents' Use of ChatGPT in Parenting},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650880},\ndoi = {10.1145/3613905.3650880},\nabstract = {The increased use of LLMs such as ChatGPT introduced new learning opportunities for educators and different age groups of learners in formal education. However, it is unknown whether parents and young children are also adopting ChatGPT in informal learning environments. Using the Technology Acceptance Model (TAM) as a framework, this study surveyed 74 parents in the U.S. with young children between five and eight years old and found both parent and child use of ChatGPT. Hierarchical linear regression models were employed and revealed that parents’ positive attitudes toward ChatGPT and parental encouragement are significantly related to parents' use of ChatGPT, and children's use of ChatGPT is significantly influenced by parental encouragement. As one of the first known studies to examine ChatGPT use in parenting, this study contributes to understanding the role of LLMs as AI tools for parenting in home learning environments.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {378},\nnumpages = {7},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650770,\nauthor = {Chin, Jenna H and Lee, Seungwook and Ashraf, Mohsena and Zago, Matt and Xie, Yun and Wolfgram, Elizabeth A and Yeh, Tom and Kim, Pilyoung},\ntitle = {Young Children's Creative Storytelling with ChatGPT vs. Parent: Comparing Interactive Styles},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650770},\ndoi = {10.1145/3613905.3650770},\nabstract = {Creative storytelling with parents plays an important role in child development including language skills, social competence, and emotional understanding. Recognizing the challenges parents face in finding time for storytelling due to work and home responsibilities, we explore the feasibility of ChatGPT for engaging children in creative storytelling. This study investigates the use of ChatGPT, a conversational agent powered by GPT-4, in creative storytelling with children aged 5-6, comparing its interaction styles with those of parents. The current study included eight child-parent dyads. We found that children were engaged in shorter and more frequent interactions with parents compared to ChatGPT. ChatGPT and parents asked different types of questions, and ChatGPT more frequently provided positive feedback compared to parents. More children selected the interactions with ChatGPT as their favorite interactions. The study provides preliminary evidence on ChatGPT's interaction styles and insights into its potential role in supporting families in creative storytelling activities.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {379},\nnumpages = {7},\nkeywords = {ChatGPT, Children, Parents, Storytelling},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651106,\nauthor = {O'Leary, Teresa K. and Paasche-Orlow, Michael K and Bickmore, Timothy},\ntitle = {`Something I Can Lean On': A Qualitative Evaluation of a Virtual Palliative Care Counselor for Patients with Life-Limiting Illnesses},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651106},\ndoi = {10.1145/3613905.3651106},\nabstract = {Palliative care is essential for maintaining the highest quality of life for patients with life-limiting illnesses. Although the benefits of palliative care are well supported, palliative care services are often offered late in the trajectory of the patient’s disease, limiting the beneficial role these services play in mitigating patient suffering. Digital health tools represent a promising approach for expanding access to palliative care. We report findings from interviews with twenty patients who used a virtual palliative care counselor over a six-month study period and provide guidelines for developers based on these results. Through their use of the system, patients characterized how using a digital palliative care counselor that intervenes on multiple dimensions of well-being benefited their experience of illness and quality of life.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {380},\nnumpages = {7},\nkeywords = {digital health tools, life-limiting illness, palliative care, patient-facing, serious illness},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651079,\nauthor = {Jobair, Abdullah Al and Khaium, Md. Omar},\ntitle = {``People may assume that my mother is mad'': Unpacking Alzheimer's Care in Bangladesh},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651079},\ndoi = {10.1145/3613905.3651079},\nabstract = {Alzheimer’s Disease (AD) is a worldwide health issue, especially in low to middle-income countries, i.e., Bangladesh. Bangladesh’s family-oriented culture calls for home-based healthcare and comprehensive education on risk factors, symptoms, and early detection. Our study used a mixed-method approach with a survey of 324 Bangladeshi individuals and eight semi-structured interviews of family caregivers and doctors, which revealed significant gaps in knowledge and caregiving skills required for adequate care of Alzheimer’s patients. These gaps highlight potential issues, including lack of caregiver training and community support, limited access to reliable information, cultural differences, and language barriers while using existing solutions. Our study recommends exploring culturally aware healthcare design, providing a collaborative platform for all stakeholders of AD to share experiences and address Bangladeshi individuals’ needs to address these gaps. This research opens a conversation about how HCI design can support AD in lower-middle-income countries by designing training facilities to strengthen support systems.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {381},\nnumpages = {8},\nkeywords = {Accessibility and Aging, Alzheimer’s, Awareness and Knowledge Gap, Caregiver, Challenges and Needs, Dementia, Living with Alzheimer’s, Stigma, Support System},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650788,\nauthor = {Vo, Dinh-Khoi and Ly, Duy-Nam and Le, Khanh-Duy and Nguyen, Tam V. and Tran, Minh-Triet and Le, Trung-Nghia},\ntitle = {iCONTRA: Toward Thematic Collection Design Via Interactive Concept Transfer},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650788},\ndoi = {10.1145/3613905.3650788},\nabstract = {Creating thematic collections in industries demands innovative designs and cohesive concepts. Designers may face challenges in maintaining thematic consistency when drawing inspiration from existing objects, landscapes, or artifacts. While AI-powered graphic design tools offer help, they often fail to generate cohesive sets based on specific thematic concepts. In response, we introduce iCONTRA, an interactive CONcept TRAnsfer system. With a user-friendly interface, iCONTRA enables both experienced designers and novices to effortlessly explore creative design concepts and efficiently generate thematic collections. We also propose a zero-shot image editing algorithm, eliminating the need for fine-tuning models, which gradually integrates information from initial objects, ensuring consistency in the generation process without influencing the background. A pilot study suggests iCONTRA’s potential to reduce designers’ efforts. Experimental results demonstrate its effectiveness in producing consistent and high-quality object concept transfers. iCONTRA stands as a promising tool for innovation and creative exploration in thematic collection design. The source code will be available at: https://github.com/vdkhoi20/iCONTRA.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {382},\nnumpages = {8},\nkeywords = {Diffusion model, Thematic collection design, Zero-shot image editing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650891,\nauthor = {Lu, Qiuyu and Ou, Jifei and Yao, Lining and Ishii, Hiroshi},\ntitle = {milleCrepe: Extending Capabilities of Fluid-driven Interfaces with Multilayer Structures and Diverse Actuation Media},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650891},\ndoi = {10.1145/3613905.3650891},\nabstract = {This paper introduces milleCrepe, an innovative exploration of harnessing multilayer structures and diverse actuation mediums to expand the capabilities of the fluid-driven interface. Unlike prior pneumatic or hydraulic interfaces mainly focused on shape-changing output, milleCrepe may enable alternative deformation and more modalities through tuning multilayer chamber design and selecting appropriate actuation medium. For example, The collaboration of multilayer chambers results in a significant volume change, enabling the transformation of a 2D plane into a 3D solid. Selective actuation of different layers enables multiple shape changes. Furthermore, diverse actuation media inside different chamber layers can interact to create physical logic structures enabled by pressure difference, dynamic appearances corresponding to colored liquid mixtures, and tunable stiffness through phase changes. Additionally, several applications are presented to exemplify the potential of this technology in both product and interaction design.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {383},\nnumpages = {10},\nkeywords = {Fabrication, Hydraulic, Logic Structure, Pneumatic, Shape Change},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651082,\nauthor = {Wang, Hongyue and Deng, Jialin and Mohan, Aravind and Li, Yinyi and Peng, Hao and He, Linjia and Elvitigala, Don Samitha and Mueller, Florian 'Floyd'},\ntitle = {pic2eat: Facilitating Social Ice-breaking through Collaborative Design of 3D Printed Appetizer},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651082},\ndoi = {10.1145/3613905.3651082},\nabstract = {3D Food Printing (3DFP), emerging as a multifaceted technology in domestic, gastronomic, and various industrial settings, presents underexplored opportunities for facilitating social interaction. To address this, this research engaged culinary professionals along with participants from diverse backgrounds to investigate the application of 3DFP in social contexts. Consequently, we introduce pic2eat, an innovative 3DFP system that facilitates collaborative artistic expression among unfamiliar individuals through the creation of jointly-designed appetizer with distinct flavor profiles. Our goal was to explore the potential efficacy of 3DFP technology in catalyzing the initiation of social relationships. In our pilot study, the pic2eat was deployed in an experimental setting involving three pairs of strangers. The empirical results suggest that the system is not only accessible and user-friendly, but also significantly effective in mitigating initial social discomfort, thereby enhancing interpersonal engagement. Through this research, we underscores the potential of 3DFP in augmenting social interaction through a synergistic approach that combines creative collaboration and communal dining experiences.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {384},\nnumpages = {7},\nkeywords = {3D food printing, Human-food interaction, Ice breaking activities},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650957,\nauthor = {Wang, Cai-Ling and Tseng, Yuan-Chi},\ntitle = {“I Don't Want to be Pitied by a Bot”: Understand How to Design Chatbots to Support People Being Ghosted on Dating Applications},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650957},\ndoi = {10.1145/3613905.3650957},\nabstract = {Experiencing ghosting on dating applications is a pervasive yet challenging aspect of modern dating. Since without explanations for relationship closure, ghostees often feel rejected and find it difficult to move on. Recent advancements in chatbot show the potential to assist individuals in navigating emotionally taxing situations. However, how chatbots could be used to help ghostees remains unclear. To understand how to design ghostee-helping chatbots, nine scenario-based interviews with a conceptual chatbot prototype were conducted. From the interviews, five themes and five design implications for chatbots were identified: 1) provide proactive and sustained care; 2) communicate with a positive tone, non-judgmental attitude, and informal language; 3) help establish facts about online dating and practical communication skills for online relationship building; 4) divert attention away from dating situations and reconnect with tangible reality; and 5) ensure the confidentiality of conversations and respect the ownership of conversation data.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {385},\nnumpages = {8},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651065,\nauthor = {Cork, Alicia G and Salagean, Anca and Smith, Laura G. E. and Ellis, David A and Joinson, Adam and Stanton Fraser, Dana\\\"{e}},\ntitle = {“I just embodied you”: Psychological Ownership of Personalized Photorealistic Avatars},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651065},\ndoi = {10.1145/3613905.3651065},\nabstract = {Developing and using personalized photorealistic avatars in research settings poses novel challenges for research ethics procedures. These challenges stem from the highly identifiable nature of avatars, which are imbued with the users’ identities. This study examines how individuals, who have had personalized photorealistic avatars created for participation in multiple research experiments, relate to their avatars when not embodying them. In three focus groups (N=9), we use hypothetical scenarios to explore avatar ownership. Using thematic analysis, we identify three themes that encapsulate individuals’ psychological sense of ownership of and connection to their avatars: i) the desire for control over the avatar, ii) the entangled relationship between user and avatar identities, and iii) feelings towards the avatar. From these themes, we suggest three recommendations for enhancing future ethical procedures, emphasizing transparency, access, control, and consent, and discuss factors limiting the generalizability of our results.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {386},\nnumpages = {7},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651113,\nauthor = {Dumaru, Prakriti and Al-Ameen, Mahdi Nasrullah},\ntitle = {“It's like educating us older people...”: Unveiling Needs and Expectations Regarding Educational Features within Parental Control Tools},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651113},\ndoi = {10.1145/3613905.3651113},\nabstract = {As children are getting access to devices at an increasingly younger age, parents need to grapple with new ways to protect them from online risks. This indicates the need for support from parental control tools to enhance their self-efficacy, which we refer to as educational features. This is little addressed in the existing literature on parental mediation. As we begin to address this gap, we created a low-fidelity prototype with designs of Google’s existing parental control as our baseline design. We used the baseline design in semi-structured interviews with 12 parents whose children (aged below 14) are active Internet users, to understand design changes, aligning with their expectations. Our study presents the prototype updated based on the needs and expectations of parents regarding educational features in different contexts and offers guidelines for future research in these directions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {387},\nnumpages = {8},\nkeywords = {Educational Features, Low-fidelity Prototype, Parental Control, Parental Efficacy, Semi-structured Interview},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651062,\nauthor = {Abeer, Ifti Azad and Sinha, Anik and Rony, Rahat Jahangir and Saha, Anik and Tahsin, Md Ulfat and Khan, Syeda Shabnam and Ahmed, Nova},\ntitle = {“Now I am Socially Well Respected” - From being Bullied in the Society to being Important Community Member},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651062},\ndoi = {10.1145/3613905.3651062},\nabstract = {People with Disabilities (PWDs) struggle in different aspects of their lives, especially in resource-constrained regions like South Asia. The role of Mobile Financial Services (MFS) in improving their lives has been emphasized in literature where works in the South Asian region focus on the challenges of MFS adoption for PWDs. This research focuses on the opportunities of the impact of MFS on PWDs in low-resource context. It considers n=20 PWDs in Bangladesh who were mostly technology familiar. Participants shared their struggles while seeking education and employment. MFS along with their technology familiarity facilitated them to engage in financial activities and empower themselves to change their societal and financial status. Many participants overcame their socio-economic struggles despite living in resource-constrained regions through their technological skills paired with efficient MFS usage. This paper presents the potential of community-centric work-oriented technology training focusing on efficient MFS usage for PWDs in resource-constrained communities.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {388},\nnumpages = {8},\nkeywords = {Low-resource Context, MFS, People with Disabilities, South Asia, Technology Adoption},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3650834,\nauthor = {Sayffaerth, Clara and Rasch, Julian and M\\\"{u}ller, Florian},\ntitle = {“Tele” Me More: Using Telepresence Charades to Connect Strangers and Exhibits in Different Museums},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650834},\ndoi = {10.1145/3613905.3650834},\nabstract = {The museum is changing from a place of passive consumption to a place of interactive experiences, opening up new ways of engaging with exhibits and others. As a promising direction, this paper explores the potential of telepresence stations in the museum context to enhance social connectedness among visitors over distance. Emphasizing the significance of social exchange, our research focuses on studying telepresence to foster interactions between strangers, share knowledge, and promote social connectedness. To do so, we first observe exhibitions and then interview individual visitors of a technical museum about their experiences and needs. Based on the results, we design appropriate voiceless and touchless communication channels and test them in a study. The findings of our in-situ user study with 24 visitors unfamiliar with each other in the museum provide insights into behaviors and perceptions, contributing valuable knowledge on seamlessly integrating telepresence technology in exhibitions, with a focus on enhancing learning, social connections, and the museum experience in general.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {389},\nnumpages = {8},\nkeywords = {Museum, Remote Play, Social Connectedness, Telepresence},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648648,\nauthor = {Zhang, Zhe-Xin and Ochiai, Yoichi},\ntitle = {A Design of Interface for Visual-Impaired People to Access Visual Information from Images Featuring Large Language Models and Visual Language Models},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648648},\ndoi = {10.1145/3613905.3648648},\nabstract = {We propose a design of interface for visual-impaired People to access visual information from images utilizing Large Language Models(LLMs), Visual Language Models (VLMs), and Segment-Anything. We use Semantic-Segment-Anything to generate the segmentation of semantic objects in images. The segmentation includes two parts: a term set describing the semantic object, and segmented mask which represents the shape of the semantic object. We provide two methods for the visual-impaired user to access the information of the semantic object and its peripheral information in image. In one method, the LLM summarize the term set to create an description. In the other method, the image with the object masked is provided to Visual Language Models which is prompted to respond with a description. In both methods, the mask can be accessed with dot display after processed for the visual-impaired people to access, and the description is prompted to the user in synthesized voice.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {390},\nnumpages = {4},\nkeywords = {Human-Computer Interaction, Large Language Models, Segment-Anything, Visual Language Models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648652,\nauthor = {Yoon, Sang Ho and Sung, Youjin and Song, Kun Woo and Jung, Kyungeun and Seo, Kyungjin and Kim, Jina and Yi, Hyung Il and Vanichvoranun, Nicha and Jeong, Hanseok and Lee, Hojeong},\ntitle = {Adaptive and Immersive XR Interactions with Wearable Interfaces (Demo of KAIST HCI Tech Lab)},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648652},\ndoi = {10.1145/3613905.3648652},\nabstract = {In this Interactivity, we present a lab demo on adaptive and immersive wearable interfaces that enhance extended reality (XR) interactions. Advances in wearable hardware with state-of-the-art software support have great potential to promote highly adaptive sensing and immersive haptic feedback for enhanced user experiences. Our research projects focus on novel sensing techniques, innovative hardware/devices, and realistic haptic rendering to achieve these goals. Ultimately, our work aims to improve the user experience in XR by overcoming the limitations of existing input control and haptic feedback. Our lab demo features three highly enhanced experiences with wearable interfaces. First, we present novel sensing techniques that enable a more precise understanding of user intent and status, enriched with a broader context. Then, we showcase innovative haptic devices and authoring toolkits that leverage the captured user intent and status. Lastly, we demonstrate immersive haptic rendering with body-based wearables that enhance the user experience.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {391},\nnumpages = {4},\nkeywords = {Haptics, Interactive Technologies, Sensing Technique, Wearables},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648666,\nauthor = {Studer, Kachina and Lie, Hing and Zhao, Zhen and Thomson, Ben and Turakhia, Dishita G and Liu, John},\ntitle = {An Open-ended System in Virtual Reality for Training Machining Skills},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648666},\ndoi = {10.1145/3613905.3648666},\nabstract = {With the rise in exploring Virtual Reality (VR) to enhance the training of psychomotor skills, several systems have been designed within the manufacturing sector to train for machining skills. However, existing industry training programs often lack the flexibility to accommodate human error and the adaptability to allow multiple paths to achieving the end task goal. We address this limitation through our VR training system by adopting an open-ended approach to system design. In this interactivity demo, we present our VR training simulation which is specifically tailored for practicing drilling skills using a 3-axis milling machine. This simulation offers an open-ended learning experience, guiding users through safety protocols, setup procedures, drilling tutorials, and open-ended practice sessions. It provides real-time feedback on mistakes and failures and an evaluation of the drilled geometries. For the demo, participants will train for the drilling task with our open-ended VR tool.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {392},\nnumpages = {5},\nkeywords = {Open-ended Practice, Psychomotor Skills, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648663,\nauthor = {Shibata, Ryosuke and Tanaka, Hisanori and Ito, Kodai and Izumi, Shintaro and Takashima, Kazuki and Itoh, Yuichi},\ntitle = {ArgusEyes: Interactions by Combining Multiple Modules with Optical Flow Sensors},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648663},\ndoi = {10.1145/3613905.3648663},\nabstract = {We propose ArgusEyes, which realizes various interactions by simply combining compact modules equipped with optical flow sensors. Each ArgusEyes module comprises three core components: a printed circuit board (PCB), a lens, and a case. It accurately acquires the amount of movement of the contacting surface. This system excels at precise surface movement detection, with a validation demonstrating a maximum difference of 2.50 mm between minimum and maximum movement values. We present three illustrative examples of applications that use ArgusEyes to demonstrate its versatility (Figure 1). Examples of applications include force measurement, bar-shaped input devices, and Interaction with a soft cloth.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {393},\nnumpages = {5},\nkeywords = {interaction design, modularization, optical flow sensor, tangible},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648670,\nauthor = {Fu, Shihan and Chen, Jianhao and Cai, Yi and Fan, Mingming},\ntitle = {AromaBlendz: An Olfactory System for Crafting Personalized Scents},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648670},\ndoi = {10.1145/3613905.3648670},\nabstract = {Although the HCI community has recently begun to explore the usage of scent to enrich interactive system experiences (e.g., making VR more immersive), scent is often preset. In contrast, personalized scents might help trigger emotional responses and memory recall in many application scenarios, ranging from fostering relaxation to managing emotional states. We present AromaBlendz, a novel digital platform that enables users to create and customize their unique scent profiles. AromaBlendz comprises both hardware and software components that collectively deliver a seamless scent customization experience. The hardware includes a blending mechanism for essence oils and a user-friendly control unit, while the software component provides an intuitive interface for users to create, preview, and store their preferred scents. The platform not only allows for the generation of personalized scent profiles using a library of essential oils but also facilitates the process of scent creation through an accessible and interactive user interface.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {394},\nnumpages = {5},\nkeywords = {Digital smell technology, Hardware, Odour interfaces, Olfactory Device, Olfactory experiences},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648669,\nauthor = {Goto, Masato and Sugiyama, Ibuki and Higuchi, Kenta and Goto, Toshiki and Kadomoto, Junichiro and Irie, Hidetsugu and Sakai, Shuichi},\ntitle = {CommuTiles: Shape-Changeable Modular Computer System Using Proximity Wireless Communication},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648669},\ndoi = {10.1145/3613905.3648669},\nabstract = {We present CommuTiles, a computer system where multiple tile-shaped modules are wirelessly connected. This represents the first implementation of the concept of a shape-changeable computer system capable of realizing diverse shapes and functions, as well as their dynamic changes. We designed tile-shaped modules equipped with wireless communication circuits and coils, and network protocols for data communication and coordinate estimation. By combining multiple tile-shaped modules with different functions, we implemented application examples. Through the implementation of hardware, software, and application cases, we lay the foundation for future research on shape-changeable computers and the shape-changing interfaces they enable.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {395},\nnumpages = {5},\nkeywords = {Shape-changeable computer, computing particles, programmable matter, proximity wireless communication, shape-changing interface},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648647,\nauthor = {Bell, Fiona and Friedman-Gerlicz, Camila and Gould, Jaime and Mcclure, Erin and Gelosi, Deanna and Bustos, Alyshia N and Silva Lovato, Monica and Suina, Jeff and Buechley, Leah},\ntitle = {Demonstrating New Materials, Software, and Hardware from the Hand and Machine Lab},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648647},\ndoi = {10.1145/3613905.3648647},\nabstract = {There has been a surge of recent interest in new materials and tools for digital fabrication. In this work, we introduce a range of materials that we have developed for use in Direct Write (DW) 3D printers. These materials include play-dough, clay-dough, bronze clay, glass paste, and eggshell paste. Many of our materials exhibit unique properties, so to support and extend the capabilities of printing with these materials, we develop new slicer software and hardware components. For example, we designed new CAM software for successfully printing dramatic overhangs in clay and for printing rheologically non-linear materials by generating toolpaths with little to no travel movements. We also created hardware such as custom heaters that improve the structural stability of prints. By presenting an overview of all these works in one demonstration, we call attention to how the development of new materials, software, and hardware are interconnected in digital fabrication.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {396},\nnumpages = {5},\nkeywords = {3D Printing Hardware, Biomaterials, Ceramics, Clay 3D Printing, Digital Fabrication, Glass, Materiality, Metal Clay, Slicer Software},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648644,\nauthor = {Janaka, Nuwan and Cai, Runze and Zhao, Shengdong and Hsu, David},\ntitle = {Demonstrating PANDALens: Enhancing Daily Activity Documentation with AI-assisted In-Context Writing on OHMD},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648644},\ndoi = {10.1145/3613905.3648644},\nabstract = {We introduce PANDALens, a Proactive AI Narrative Documentation Assistant built on an Optical See-Through Head-Mounted Display that transforms the in-context writing tool into an intelligent companion during daily activities. PANDALens observes multimodal contextual information from user behaviors and the environment to detect interesting moments and elicit contemplation. It also employs Large Language Models to transform such multimodal information into coherent narratives with significantly reduced user effort. PANDALens was iteratively designed through a formative study identifying the user requirements. We verify its utility in a real-world travel scenario in improving writing quality and travel enjoyment while minimizing user effort.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {397},\nnumpages = {7},\nkeywords = {AI, HMD, Human-AI collaborative writing, in-context writing, large language model, multimodal information, smart glasses, travel blog},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648662,\nauthor = {Ying, Wen and Rahman, Adil and Heo, Seongkook},\ntitle = {Demonstrating VRScroll: A Shape-Changing Device for Precise Sketching in Virtual Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648662},\ndoi = {10.1145/3613905.3648662},\nabstract = {Sketching precisely in Virtual Reality (VR) is challenging due to the lack of a physical surface for users to lean on, feel the contact and the movement of the pen, and constrain the pen movement. Using physical surface interfaces, such as a touch tablet, can improve sketching performance; however, they cannot replicate the diverse shapes of virtual objects. We present VRScroll, a novel shape-changing device equipped with a dynamic drawing surface to support high-precision sketching in VR. The device has seven motor-controlled flaps that can independently change the angles between them in real time to mimic the surface shape of a virtual object. Since the degree of freedom and resolution of VRScroll is limited, it cannot replicate the surface geometry of diverse types of virtual objects. To allow the device to closely mimic the shape of the virtual surface, we developed a shape approximation algorithm.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {398},\nnumpages = {5},\nkeywords = {dynamic shape display, shape approximation, sketching, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648661,\nauthor = {Guan, Emily and Wu, Di and Lu, Qiuyu and Yao, Lining},\ntitle = {Design and Simulation Tool for Sequentially and Conditionally Programmable Waxpaper Morphing Interfaces},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648661},\ndoi = {10.1145/3613905.3648661},\nabstract = {This Interactivity demonstrates a design tool that aids the design and simulation of waxpaper actuators. The waxpaper actuator is a sequentially-controllable, moisture-triggered, rapidly-fabricated, and low-cost shape-changing interface. We introduce a design tool that integrates the characteristic data of the waxpaper actuator to aid users in the customization of swift personal actuators. It uses gray levels to control the wax interface’s deformations, bending degrees and response times. Users can design their own actuators or import samples we provide, customize variables to best fit their needs, and create simulations to preview the performance. This gives higher precision to the effectiveness of the waxpaper actuator before it is fabricated. This makes the technique more accurate and accessible to future HCI projects.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {399},\nnumpages = {5},\nkeywords = {morphing materials, programmable material, rapid fabrication., sequential control method, shape-changing interfaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648642,\nauthor = {Kim, Sungbaek and Choi, Doyo and Lee, Jinjoon},\ntitle = {EMPop: Pin Based Electromagnetic Actuation for Projection Mapping},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648642},\ndoi = {10.1145/3613905.3648642},\nabstract = {As interactive media arts evolve, there is a growing demand for technologies that offer multisensory experiences beyond audiovisual elements in large-scale projection mapping exhibitions. However, traditional methods of providing tactile feedback are impractical in expansive settings due to their bulk and complexity. The EMPop system is the proposed solution, utilizing a straightforward design of electromagnets and permanent magnets making projection mapping more interactive and engaging. Our system is designed to control three permanent magnets individually with one electromagnet by adjusting the current of the electromagnet, reliable and scalable. We assessed its ability to convey directions and the strength of feedback, finding that users correctly identified directions and differentiated feedback intensity levels. Participants enjoyed the realistic and engaging experience, suggesting EMPop’s potential for enriching interactive installations in museums and galleries.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {400},\nnumpages = {4},\nkeywords = {Electromagnetic Actuator, Interactive Art, Multisensory Experience, Pin Array, Tactile Feedback},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648660,\nauthor = {Polydorides, Andreas and Rogers, Yvonne},\ntitle = {Examining approaches to personalized 3D printed wheelchair cushions},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648660},\ndoi = {10.1145/3613905.3648660},\nabstract = {Wheelchair users are a greatly varied population group for which no single product can offer a consistent level of comfort. When designing cushions for pressure relief, unique factors like a user’s weight, posture, susceptibility to pressure sores, as well as other compounding conditions all call for a custom product. Unfortunately, existing custom solutions can be financially inaccessible and time-consuming to manufacture, especially in a public healthcare setting. 3D printing, which to a small extent, is already used for wheelchair cushions, can also be used to personalize them, to achieve better pressure distribution. Here, we discuss three approaches that allow users to design and print their own cushion, why that is important, and what advancements in the field will further reduce the barrier to easily achieving great results.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {401},\nnumpages = {4},\nkeywords = {3D printing, assistive technology, comfort},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648656,\nauthor = {Zhou, Zhongyi and Jin, Jing and Phadnis, Vrushank and Yuan, Xiuxiu and Jiang, Jun and Qian, Xun and Zhou, Jingtao and Huang, Yiyi and Xu, Zheng and Zhang, Yinda and Wright, Kristen and Mayes, Jason and Sherwood, Mark and Lee, Johnny and Olwal, Alex and Kim, David and Iyengar, Ram and Li, Na and Du, Ruofei},\ntitle = {Experiencing InstructPipe: Building Multi-modal AI Pipelines via Prompting LLMs and Visual Programming},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648656},\ndoi = {10.1145/3613905.3648656},\nabstract = {Foundational multi-modal models have democratized AI access, yet the construction of complex, customizable machine learning pipelines by novice users remains a grand challenge. This paper demonstrates a visual programming system that allows novices to rapidly prototype multimodal AI pipelines. We first conducted a formative study with 58 contributors and collected 236 proposals of multimodal AI pipelines that served various practical needs. We then distilled our findings into a design matrix of primitive nodes for prototyping multimodal AI visual programming pipelines, and implemented a system with 65 nodes. To support users’ rapid prototyping experience, we built InstructPipe, an AI assistant based on large language models (LLMs) that allows users to generate a pipeline by writing text-based instructions. We believe InstructPipe enhances novice users onboarding experience of visual programming and the controllability of LLMs by offering non-experts a platform to easily update the generation.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {402},\nnumpages = {5},\nkeywords = {Deep Learning, Deep Neural Networks, Graph Compiler, Large Language Models, Low-code Development, Node-graph Editor, Visual Analytics, Visual Programming, Visual Prototyping},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648641,\nauthor = {Wen, Linda Yilin and Morrison, Cecily and Grayson, Martin and Marques, Rita Faia and Massiceti, Daniela and Longden, Camilla and Cutrell, Edward},\ntitle = {Find My Things: Personalized Accessibility through Teachable AI for People who are Blind or Low Vision},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648641},\ndoi = {10.1145/3613905.3648641},\nabstract = {The opportunity for artificial intelligence, or AI, to enable accessibility is rapidly growing, but widely impactful applications can be challenging to build given the diversity of user need within and across disability communities. Teachable AI systems give users with disabilities a way to leverage the power of AI to personalize applications for their own specific needs. We demonstrate Find My Things as an end-to-end example of applying Teachable AI systems to address the diversity of accessibility needs. An application that can be taught by people who are blind or low vision to find their personal things, Find My Things illustrates the potential Teachable AI holds for accessibility.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {403},\nnumpages = {6},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648653,\nauthor = {Sakurai, Eiichiro and Manabe, Hiroyuki},\ntitle = {Flushner: A 3D Printing Technique That Inserts Stepped Objects to Achieve Surface Uniformity and High Speed},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648653},\ndoi = {10.1145/3613905.3648653},\nabstract = {Rapid prototyping is a technique used to quickly materialize ideas in the early stages of product development. It has become commonplace with the widespread use of FDM 3D printers. However, a notable challenge exists that the print time is too long for rapid iteration. Several approaches have already been taken to address this issue. We propose a technique to further enhance 3D print speed by inserting stepped objects into the 3D object being printed by using the print-pause-print protocol. The inserted objects form surfaces flush with the 3D printed surfaces and the print time by the 3D printer for walls, infill, and support structures can be reduced. The technique does not require expensive or modified 3D printers and is very effective with cheap and simple single-extruder FDM 3D printers. Several application examples are implemented, and it is confirmed that the printing time is reduced by up to 89\\%.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {404},\nnumpages = {5},\nkeywords = {3D printing, laser cutting, rapid prototyping},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648640,\nauthor = {Oka, Sora and Koyama, Kazuki and Gondo, Tomoyuki and Ikeda, Yasushi and Kawahara, Yoshihiro and Narumi, Koya},\ntitle = {Folding Angle Control of Inter-Connected Pouch Motors},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648640},\ndoi = {10.1145/3613905.3648640},\nabstract = {Recently, planar pneumatic actuators, known as pouch motors, have gained attention in the fields of soft robotics and Human-Computer Interaction (HCI). However, achieving complex 3D shapes by pouch motors has been hindered by two challenges: (1) the slow operating speed of customized CNC machines, conventionally used for pouch fabrication and (2) the lack of control over the folding angle of individual pouches. To overcome these limitations, (1’) we introduce the use of a laser cutter to fabricate pouch motors. This approach enhances the fabrication process, enabling faster production of the pouch motors. Additionally, (2’) we propose laser-welding patterns that incorporate small barriers to control the folding angle of each pouch. By manipulating the welding patterns, we achieve nearly linear control over the folding angle of the pouches. In this paper, we present the design and fabrication process of laser-welded planar pneumatic actuators and showcase several design examples.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {405},\nnumpages = {4},\nkeywords = {digital fabrication, laser cutter, planar pneumatic actuators},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648667,\nauthor = {Dinh, Jamie Ngoc and Kim, You-Jin and Lee, Myungin},\ntitle = {FractalBrain: A Neuro-interactive Virtual Reality Experience using Electroencephalogram (EEG) for Mindfulness},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648667},\ndoi = {10.1145/3613905.3648667},\nabstract = {Mindfulness has been studied and practiced in enhancing psychological well-being while reducing neuroticism and psychopathological indicators. However, practicing mindfulness with continuous attention is challenging, especially for beginners. In the proposed system, FractalBrain, we utilize an interactive audiovisual fractal with a geometric repetitive pattern that has been demonstrated to induce meditative effects. FractalBrain presents an experience combining a surreal virtual reality (VR) program with an electroencephalogram (EEG) interface. While viewing an ever-changing fractal-inspired artwork in an immersive environment, the user’s EEG stream is analyzed and mapped into VR. These EEG data adaptively manipulates the audiovisual parameters in real-time, generating a distinct experience for each user. The pilot feedback suggests the potential of the FractalBrain to facilitate mindfulness and enhance attention.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {406},\nnumpages = {4},\nkeywords = {Audiovisual, EEG, Fractal, Mindfulness, Neurofeedback, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648675,\nauthor = {Hu, Youyang and Fol, Cyprien Raymi and Chou, Chiaochi and Griess, Verena C and Kakehi, Yasuaki},\ntitle = {Immersive Flora: Re-Engaging with the Forest through the Visualisation of Plant-Environment Interactions in Virtual Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648675},\ndoi = {10.1145/3613905.3648675},\nabstract = {In response to recent calls for Human-Computer Interaction (HCI) to address ongoing environmental crises and promote the engagement between humans and nature, this paper presents Immersive Flora, a living plants-driven interactive system that re-engages users with the natural environment by virtually showcasing the diverse plant-environment interactions inside a forest. By utilizing biosensors to detect and analyze the intricate biological reactions of plants to wind, rain and sunlight, we explore the diversity of natural elements perceived by these plants within their wilderness habitat. This information is subsequently translated into a sophisticated virtual particle system that will react in real-time to mimic a change in the natural environment. This system is constructed utilizing point cloud data derived from the 3D reconstruction of plants and the forest environment. Through the expression of plant-environment interactions via particle manipulation, we establish an innovative interface reconnecting humans to the natural world. },\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {407},\nnumpages = {6},\nkeywords = {immersive experience, more-than-human, nature engagement, plant-environment signaling, virtual environment},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648645,\nauthor = {Xie, Yufan and Wu, Wei and Lewis, Melvin R and Yin, Ziqian},\ntitle = {Maelstrom: Spatial Instrument and Decision Making Tool Based on Pattern Progression},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648645},\ndoi = {10.1145/3613905.3648645},\nabstract = {Maelstrom is a spatial instrument that combines projection with interactive spatial audio, performed by the audience in the experience area. It is an installation that focuses on visual and acoustic pattern recognition, serving as a tool for individuals to navigate environments overwhelmed by messages. It provides audio-guided experiences, allowing the audience to make decision based on different audio cues and explore visuals as maps developed by a generative model of artificial intelligence. This project attempts to challenge the sensory hierarchy and accessibility within immersive technologies, questioning the cognitive capabilities of the human brain.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {408},\nnumpages = {5},\nkeywords = {Immersive Experience, Interactive Music, Sonification, Spatial Audio, Spatial Computing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648668,\nauthor = {Hu, Lesi and Liang, Xiaozhan and Yan, Shuo},\ntitle = {Magic Ball: A globe-based system for multidimensional information display and interaction in MR},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648668},\ndoi = {10.1145/3613905.3648668},\nabstract = {Recent advancements in Mixed Reality (MR) technology have catalyzed a profound reevaluation and transformation in the way information is presented in physical museums and exhibition spaces. MR technology transcends the limitations of traditional, flat presentations, integrating with museum and exhibition environments to enrich the levels of information display, thereby creating more captivating experiences for users. Current research primarily focuses on developing informational prompts and navigational aids within these spaces. However, there remains a notable deficiency in addressing users' deeper needs for spatial macro-cognition and interactive communication.Our study delves into the advantages of globe-based information displays, merging them with the virtual reality aspects of MR technology. This integration forms a specialized 3D, multi-dimensional information display and spatial interaction system for museums and exhibition spaces. Utilizing the globe's longitudinal and latitudinal descriptions, the system exploits the unique attributes of the sphere. By differentiating personal and public spaces and integrating user behavior patterns with the characteristics of the globe, a range of globe-based interactive methods are designed, including activation, picking up, throwing, navigating, and following. This culminates in a collaborative and shared experience for multiple users in a single setting, enhancing their spatial perception and interactivity.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {409},\nnumpages = {5},\nkeywords = {Globe, Information categorization, Multi-players exchange, Spherical interface interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648654,\nauthor = {Li, Zhiyu and Zhang, Xiaoyu and Guan, Jiamin and Ren, Xipei},\ntitle = {MagnaDip Kit: A User-Friendly Toolkit for Streamlined Fabrication of Electromagnetic Responsive Textiles},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648654},\ndoi = {10.1145/3613905.3648654},\nabstract = {Smart materials play an essential role in enhancing the efficiency and diversity of human-computer interaction (HCI). This paper focuses on the domain of flexible smart materials. We introduce MagnaDip Kit, a toolkit designed for creating magnetic textiles, aiming at democratizing the innovation of new materials, facilitating the wider adoption of smart materials in everyday applications. The MagnaDip Kit ensures a straightforward and user-friendly manufacturing process, and serves as a resource for designers to employ smart materials in prototypes. Combining interdisciplinary knowledge from materials science and design, we aim to provide a tangible production experience, enabling the expansion of novel interaction modalities beyond traditional computing. Based on the toolkit's output, we further integrated the characteristics of electromagnetic responsive textiles to create a light-responsive interactive prototype, demonstrating one of the applications of smart materials. Relying on CHI2024, we seek feedback from an international audience to refine the toolkit and conduct additional workshops.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {410},\nnumpages = {5},\nkeywords = {Electromagnetic Responsive, Magnetic Textile, Smart Material, Toolkit},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648658,\nauthor = {Potts, Dominic and Jicol, Crescent and Clarke, Christopher and O'Neill, Eamonn and Fitton, Isabel Sophie and Dark, Elizabeth and Oliveira Da Silva, Manoela Milena and Broad, Zoe and Sehgal, Tarini and Hartley, Joseph and Dalton, Jeremy and Proulx, Michael J and Lutteroth, Christof},\ntitle = {REVEAL: REal and Virtual Environments Augmentation Lab @ Bath},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648658},\ndoi = {10.1145/3613905.3648658},\nabstract = {The REal and Virtual Environments Augmentation Lab (REVEAL) at the University of Bath is an interdisciplinary research centre focusing on immersive technology. REVEAL investigates the fundamental principles, applications and interaction techniques of extended reality (XR), including virtual reality (VR) and augmented reality (AR). In this Interactivity demo, we will showcase some of our VR research across three areas: affective VR exergaming, learning with virtual avatars, and gaze interaction in VR.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {411},\nnumpages = {5},\nkeywords = {affect, avatars, emotion recognition, emotions, exergaming, gaze, interaction, learning, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648673,\nauthor = {Fan, Hector and Bolter, Jay David},\ntitle = {Rekindle: Enhancing Interactive Narrative in Virtual Reality with Coherent, Agency-driven Interactions},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648673},\ndoi = {10.1145/3613905.3648673},\nabstract = {In this research, we investigate the potential of enhanced interactivity in providing new possibilities for interactive narratives within Virtual Reality (VR). We introduce \"Rekindle,\" a first-person narrative experience in VR. The narrative offers the interactor an embodied experience as a gay protagonist on a journey to reclaim lost memories, set against the backdrop of a dystopian future where a regime enforces stringent heteronormativity through the manipulation of memories. Central to this experience is the Memory Retrieval Mechanism, which empowers the interactor to explore and retrieve scattered memories. This activity reinforces immersion and the interactor’s connection within the narrative. By integrating coherent interactions grounded in narrative, \"Rekindle\" facilitates the creation of dramatic agency, thereby enhancing the narrative by making the story more compelling and consistent.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {412},\nnumpages = {5},\nkeywords = {Design, Dramatic Agency, Game, Interactive Narrative, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648643,\nauthor = {Hedlund, Martin and Bogdan, Cristian and Meixner, Gerrit and Matviienko, Andrii},\ntitle = {Rowing Beyond: A Demonstration of Steering Methods for Rowing-based Locomotion in Virtual Environments},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648643},\ndoi = {10.1145/3613905.3648643},\nabstract = {Rowing has great potential in Virtual Reality (VR) exergames as it requires physical effort and uses physical motion to map the locomotion in a virtual space. However, rowing in VR is currently restricted to locomotion along one axis, leaving 2D and 3D locomotion out of the scope. To facilitate rowing-based locomotion, we implemented three steering techniques based on head, hands, and feet movements for 2D and 3D VR environments.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {413},\nnumpages = {6},\nkeywords = {exergame, locomotion, rowing, steering, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648671,\nauthor = {Kim, Yeonsu and Yim, Jisu and Kim, Jaehyun and Kim, Kyunghwan and Lee, Geehyuk},\ntitle = {STButton: Exploring Opportunities for Buttons with Spatio-Temporal Tactile Output},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648671},\ndoi = {10.1145/3613905.3648671},\nabstract = {We present STButton, a physical button with a high-resolution spatio-temporal tactile feedback surface. The 5 x 8 pin array tactile display size of 20mm x 28mm enables buttons to express various types of information, such as value with the number of raised pins, direction with the location of raised pins, and duration of time with blinking animation. With a highly expressive tactile surface, the button can seamlessly transfer assistive feedforward and feedback during spontaneous button interaction, such as touching to locate the button or applying gradual pressure to press the button. In the demonstration, attendees experience five scenarios of button interaction: the seat heater button on a car, the volume control button on a remote controller, the power button on a laptop, the menu button on a VR controller, and the play button on a game controller. In each scenario, the representative role of tactile feedback is configured differently, allowing attendees to experience the rich interaction space and potential benefits of STButton. Early accessed attendees appreciated the unique opportunity to transfer information with a highly expressive tactile surface and emphasized that STButton adds a tangible layer to the user experience, enhancing emotional and sensory engagement.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {414},\nnumpages = {5},\nkeywords = {Haptic, Haptic button, Physical button, Spatio temporal tactile output},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648676,\nauthor = {Wall, Ludwig Wilhelm and Schneider, Oliver and Vogel, Daniel},\ntitle = {Scrappy and Substiports: User-Inserted Ad Hoc Objects For Faster, More Sustainable 3D Printing},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648676},\ndoi = {10.1145/3613905.3648676},\nabstract = {We demonstrate the Scrappy and Substiports systems that use an intermittent interaction approach and integrate into existing CAD software to reduce print time and material with unmodified fused deposition modelling printers. The approach uses ad hoc objects inserted by a user during printing as a replacement for printed internal and external support structures. Examples of objects include household items like books, scrap objects, toy bricks, and custom mechanisms like a screw jack. Our software-only systems assist in maintaining a library of replacement objects and allow customization of the effort to savings trade-off of making printing more sustainable through manual interactions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {415},\nnumpages = {5},\nkeywords = {fabrication, interactive 3D printing, sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648664,\nauthor = {Takeda, Kazuhiro and Manabe, Hiroyuki},\ntitle = {Screen Augmentation Technique Using AR Glasses and Smartphone without External Sensors},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648664},\ndoi = {10.1145/3613905.3648664},\nabstract = {Screen augmentation using a smartphone with AR glasses can enhance visual experiences, and many studies on the combination have been conducted. Tracking the smartphone is the key to seamless augmentation; however, existing techniques suffer limited availability and accuracy. We propose a technique of screen augmentation with AR glasses and smartphone that does not require external sensors. It is assumed that some tracking error exists, and video processing is performed to make it difficult for the user to perceive any difference between the two screens. It is confirmed that our technique makes the difference less perceptible. Several applications are implemented to explore the interactions achieved with the technique.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {416},\nnumpages = {5},\nkeywords = {Augmented reality, boundaries, device tracking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648657,\nauthor = {Yao, Zhihao and Lyu, Shiqing and Lu, Yao and Sun, Qirui and Li, Hanxuan and Wang, Xuezhu and Liu, Guanhong and Mi, Haipeng},\ntitle = {ShadowMaker: Sketch-Based Creation Tool for Digital Shadow Puppetry},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648657},\ndoi = {10.1145/3613905.3648657},\nabstract = {Shadow puppetry is a cultural heritage with thousands of years of history, and its digital form has become a popular style in modern artistic creation. Creating high-quality shadow puppet works is often challenging. We present \"ShadowMaker,\" an intelligent collaborative tool, where designers can co-create dynamic digital shadow puppets together with AI through prompts and sketching. ShadowMaker introduces a novel non-linear workflow, allowing users to freely switch between stages like sketching, layout, and animating.The user study with 18 participants demonstrated that ShadowMaker is easy to use, offers a high degree of creative freedom, and is capable of inspiring users’ creativity and inspiration.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {417},\nnumpages = {5},\nkeywords = {Creative Support Tools, authoring, digital shadow puppetry, sketching},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648655,\nauthor = {Li, Jingke and Sun, Guoyu and Tang, Congyun and Chen, Wenjuan and Yang, Wenwen and Kou, Wenxuan and Ruan, Zhonghe and Ma, Wanqing and Nie, Xuran},\ntitle = {Silk Road Journey: A Real-time AI-based Interactive Art Installation for Silk Road Cultural Reenactment and Experience},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648655},\ndoi = {10.1145/3613905.3648655},\nabstract = {The Silk Road, spanning thousands of years, possesses splendid civilization relics. However, with the disappearance of historical records, there is a certain lack of public awareness of the Silk Road culture. Silk Road Journey is an AI interactive installation based on six significant cities along the Silk Road, allowing the audiences travel through different eras, and pose with the scenes through physical interaction to generate unique commemorative images. We apply AIGC technology to the design of the interactive art installation related to cultural heritage. We employ a variety of strategies to improve the efficiency and quality of gesture-controlled image generation, aiming to improve the immersion of the interaction and achieve real-time interaction effects. This innovative way of combining new technology, interactive experience and world cultural heritage improves the public's cultural awareness of the Silk Road, which also seeks out a new mode of experience and dissemination for cultural heritage preservation.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {418},\nnumpages = {5},\nkeywords = {AIGC(Artificial Intelligence Generated Content), Silk Road Cultural Heritage, diffusion model, embodied interaction, gesture recognition},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648651,\nauthor = {Shields, Samuel and Melcer, Edward F.},\ntitle = {Soothing Systems: A Meditative Somaesthetic Experience},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648651},\ndoi = {10.1145/3613905.3648651},\nabstract = {We present Soothing Systems, a web-accessible video game that utilizes body tracking through a webcam to guide users through yoga-inspired poses for a meditative somaesthetic experience). The overall experience consists of enacting different meditative poses by presenting the user targets that can be hit with specific body parts represented by their avatar. Visual and audio guides are provided for breathwork practices, and an on-screen narrator prompts users to reflect as they explore the game world presented on screen. Through this open-ended exploration of movement on the screen, users leave traces of movement on a canvas, leaving an abstract, colorful artwork representation, which can be printed out and taken away with the user as a souvenir. Through embodied explorations of a 2D game space, we aim to provide a space for meditation and relaxation that contributes to an ongoing dialogue of explorative somaesthetic experiences in the HCI community.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {419},\nnumpages = {5},\nkeywords = {Augmented Reality, Embodiment, Somaesthetic Design, Web Application},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648659,\nauthor = {Kim, You-Jin and Lee, Myungin and Peljhan, Marko and Kuchera-Morin, JoAnn and H\\\"{o}llerer, Tobias},\ntitle = {Spatial Orchestra: Locomotion Music Instruments through Spatial Exploration},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648659},\ndoi = {10.1145/3613905.3648659},\nabstract = {Spatial Orchestra demonstrates how easy it is to play musical instruments using basic input like natural locomotion, which is accessible to most. Unlike many musical instruments, our work allows individuals of all skill levels to effortlessly create music by walking into virtual bubbles. Our Augmented Reality experience involves interacting with ever-shifting sound bubbles that the user engages with by stepping into color-coded bubbles within the assigned area using a standalone AR headset. Each bubble corresponds to a cello note, and omits sound from the center of the bubble, and lets the user hear and express in spatial audio, effectively transforming participants into musicians. This interactive element enables users to explore the intersection of spatial awareness, musical rhythm that extends to bodily expression through playful movements and dance-like gestures within the bubble-filled environment. This unique experience illuminates the intricate relationship between spatial awareness and the art of musical performance.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {420},\nnumpages = {5},\nkeywords = {Code, Correct, Do, Not, Paper, Put, Terms, This, Us, Your, for, the},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648650,\nauthor = {Wang, Tiange and Feng, Xin},\ntitle = {SynCocreate: Fostering Interpersonal Connectedness via Brainwave-Driven Co-creation in Virtual Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648650},\ndoi = {10.1145/3613905.3648650},\nabstract = {Collaborative art and co-creation enhance social wellbeing and connectivity. However, the combination of art creation through mutual brainwave interaction with the prosocial potential of EEG biosignals reveal an untapped opportunity. SynCocreate presents the design and prototype of a VR-based interpersonal electroencephalography (EEG) neurofeedback co-creation platform. This generative VR platform enables paired individuals to interact via brainwaves in a 3D virtual canvas, painted and animated collaboratively through their real-time brainwave data. The platform employs synchronized visual cues, aligned with the real-time brainwaves of paired users, to investigate the potential of collaborative neurofeedback in enhancing co-creativity and emotional connection. It also explores the use of Virtual Reality (VR) in fostering creativity and togetherness through immersive, collective visualizations of brainwaves.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {421},\nnumpages = {5},\nkeywords = {Co-creation, EEG, Generative VR, Interpersonal social connectedness},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648665,\nauthor = {Sakuma, Ryota and Narumi, Koya and Kawahara, Yoshihiro and Hiraki, Takefumi},\ntitle = {TactPrint: 3D Printing Lattice-based Tactile Displays with Optimized and Local Vibration},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648665},\ndoi = {10.1145/3613905.3648665},\nabstract = {Tactile sensation is widely used in human-object interactions. However, designing local vibrations within a device’s enclosure, while considering the propagation characteristics of the vibrations, is still challenging, which often results in the entire device vibrating uniformly. In this paper, we introduce TactPrint, a method that involves embedding a lattice structure around the area where we want to induce local vibrations and then optimizing this 3D structure to create a tactile display with the desired local vibration characteristics. With the TactPrint system, users can obtain structures with desired local vibration characteristics simply by inputting a 3D model, which can be fabricated with a 3D printer and combined with an actuator to create a tactile display easily. In this demonstration, we present several prototypes of tactile displays created using our proposed system and application scenarios using them.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {422},\nnumpages = {4},\nkeywords = {computational design and fabrication, lattice structure, local vibration presentation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648672,\nauthor = {Wu, Kaifeng and Karwas, Dana},\ntitle = {The Metamorphosis of Storytelling: Time-based Interactivity in Virtual Reality Filmmaking},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648672},\ndoi = {10.1145/3613905.3648672},\nabstract = {Our work addresses a novel technique for free navigation in a Virtual Reality film and explores the spatial - temporal narrative implications of such a technique. With the increasing popularization of VR filmmaking, under current technology, filmmakers who choose to represent a realistic scene are faced with a choice between either graphically intensive real-time 3D rendering on a high-end VR system, or a solution based on Omnidirectional Stereoscopic (ODS) Imaging captured by a camera. Commonly known as 360 videos, they suffer from a fixed viewpoint. To address this specific issue, many recent advancements in the field proposed novel view reconstruction from spherical panoramas, planar image - based rendering, light field rendering, etc. for free-viewpoint navigation. We explore the interactive implications of implementing such techniques for VR filmmaking. Specifically, the temporal relationship of viewpoints and the experience of navigating through them. From an initial proof of concept with a linear path, we continued to develop the technique through installations and experimented with an infinitely looping circular path around NYC grand central terminal’s clock tower. We aim to integrate multiple optimizations and techniques into an easy-to-use open-source package for filmmakers. Including storage optimizations, novel view synthesis, and locomotion redirection. We have found that while the technical package is complicated to explain, the experience is delightfully intuitive. By sharing our discoveries with the world, we hope to enable filmmakers of the future to tell compelling stories that matter.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {423},\nnumpages = {5},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648677,\nauthor = {Reynolds, Benjamin and Hobin, James and Cao, Yuanzhi and Klaudiny, Martin and Dangond, Daniel A and Xie, Kaize and Theodosiou, Keranie and De Leeuw, Rik and Kashalkar, Shiva and Heun, Valentin},\ntitle = {The Pop-Up Metaverse: A Multi-User, Multi-Tasking Spatial Computing Environment for Collaborative Spatial Problem-Solving.},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648677},\ndoi = {10.1145/3613905.3648677},\nabstract = {We introduce the Pop-Up Metaverse, an innovative web-based system seamlessly blending spatial computing with 3D visualization to catalyze collaborative problem-solving in the Industrial Metaverse. The workflow commences with a local user swiftly scanning the environment using a LiDAR-equipped iPhone/iPad, generating a baseline mesh model. Elevating visual fidelity, the system employs the state-of-the-art 3D reconstruction technique, Gaussian-splatting, for captivating photorealistic effects. This enriched 3D canvas becomes a hub for multi-user collaboration, emphasizing the Pop-Up nature facilitated by web technology—no installations required. Key features encompass Remote Assistance, enabling real-time expert guidance within the 3D scene; Human Motion Recorder, scrutinizing movements and postures in recorded human activity; and Spatial Analytics, meticulously tracking environmental changes over time. Ensuring accessibility, remote participation mandates only a web browser, significantly reducing entry barriers to the metaverse experience. The Pop-Up Metaverse empowers a workflow tailored for solving spatial problems by seamlessly uniting the physical and digital realms.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {424},\nnumpages = {5},\nkeywords = {Pop-Up Metaverse, Remote Collaboration, Spatial Computing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648649,\nauthor = {Zhou, Qian and Prithul, Aniruddha and Kellner, Hans and Pene, Brian and Ledo, David and Herrera Urrutia, Sebastian and Koch, Hilmar and Fitzmaurice, George and Anderson, Fraser},\ntitle = {TimeTunnel Live: Recording and Editing Character Motion in Virtual Reality},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648649},\ndoi = {10.1145/3613905.3648649},\nabstract = {Animating 3D characters requires extensive practice and learning to achieve realistic outcomes. To address this challenge, we present TimeTunnel Live, an animation authoring interface for recording and editing motion in Virtual Reality. TimeTunnel Live leverages the state-of-the-art tracking in Virtual Reality devices to capture a user’s body motion, facial expressions, and hand gestures through a simple recording function. To facilitate editing captured motion, we present an immersive motion editing interface that integrates spatial and temporal control for character animation. The system works by extracting keyposes from the 3D character animation and superimposing them along a timeline. The joints across the keyposes are connected through 3D trajectories to show a character’s movement. We implemented these techniques into a proof-of-concept prototype with a demonstration illustrating how one person can bring multiple characters to life in a ’one-man band’ animation. This system provides an interactive experience to explore the future of immersive animation technologies.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {425},\nnumpages = {4},\nkeywords = {3D interface, immersive animation authoring, motion capturing, motion editing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648646,\nauthor = {Hughes-Riley, Theo and Shahidi, Arash M. and Marasinghe, Kalana and Rahemtulla, Zahra and Ehelagasthenna, Malindu and Ebrahimi, Parvin and Arm, Richard and Oliveira, Carlos and Holmquist, Lars Erik and Dias, Tilak},\ntitle = {Wearable Electronic Textiles for Healthcare, Wellbeing, and Protective Applications},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648646},\ndoi = {10.1145/3613905.3648646},\nabstract = {Electronic textiles (E-textiles) can act as an exceptional substrate for incorporating sensing devices for monitoring physiological parameters, as they are comfortable to wear close to the skin. This makes them excellent for a variety of applications in the healthcare, wellbeing, and personal protection sectors. One method of creating E-textiles with good textile properties is to integrate an electronic component into a yarn to create an electronic yarn (or E-yarn), which can then be used to construct a textile. This Interactivity Lab Demonstration will present electronic textiles developed by the Advanced Textiles Research Group (ATRG) from Nottingham Trent University (UK). The group has conducted a wealth of research into the development of E-yarns and the incorporation of these into textiles and garment. At CHI, users will be able to interact with textiles capable of fall and near-fall detection, temperature sensing, acoustic sensing, giving haptic feedback, and harvesting solar energy.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {426},\nnumpages = {5},\nkeywords = {E-textiles, Electronic textiles, health, sensor systems, wearables, wellbeing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638191,\nauthor = {Jiang, Yue},\ntitle = {Computational Representations for Graphical User Interfaces},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638191},\ndoi = {10.1145/3613905.3638191},\nabstract = {Graphical User Interfaces (GUIs) have been widely used in daily life. To enhance GUI design and interaction experience on GUIs, it is important to understand GUIs and understand how individuals interact with them. Consequently, my thesis focuses on applying computational approaches to improve our understanding of GUIs and user interactions. First, I introduce novel GUI representations to capture the visual, spatial, and semantic factors of GUIs and improve the performance of downstream GUI tasks. Second, I simulate how users visually engage with GUIs to understand user interactions to help inform the design of GUI representations. Third, based on the understanding of GUIs and interactions on GUIs, I develop language language representations aimed at assisting users in understanding and more effectively interacting with GUIs.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {427},\nnumpages = {6},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638190,\nauthor = {Lakhdhir, Sabrina},\ntitle = {Creating Positive Social Experiences Through the Design of Custom Wearables},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638190},\ndoi = {10.1145/3613905.3638190},\nabstract = {Wearables are interactive devices that are becoming increasingly popular for tasks such as communication and self-expression. They are used in social contexts by diverse individuals for diverse purposes, and impact social experiences positively and negatively due to their appearance, interactivity, and form. Given that their design impacts social experiences, my thesis considers how wearables can be custom designed by individuals to foster positive social experiences. I take a bottom-up approach to consider how custom designing is driven by individuals’ needs, identities and contexts of use, and how custom designing impacts wearable aesthetics and interactions. My thesis contains five projects within the contexts of fashion and healthcare that aim to understand user needs and desires towards custom wearable design, and develop systems and design approaches that empower end-users to engage in such custom design for creating positive social experiences.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {428},\nnumpages = {7},\nkeywords = {DIY, bottom-up, customization, making, self-expression, social experiences, wearables},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638195,\nauthor = {Lee, Christine P.},\ntitle = {Design, Development, and Deployment of Context-Adaptive AI Systems for Enhanced User Adoption},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638195},\ndoi = {10.1145/3613905.3638195},\nabstract = {My research centers on the development of context-adaptive AI systems to improve end-user adoption through the integration of technical methods. I deploy these AI systems across various interaction modalities, including user interfaces and embodied agents like robots, to expand their practical applicability. My research unfolds in three key stages: design, development, and deployment. In the design phase, user-centered approaches were used to understand user experiences with AI systems and create design tools for user participation in crafting AI explanations. In the ongoing development stage, a safety-guaranteed AI system for a robot agent was created to automatically provide adaptive solutions and explanations for unforeseen scenarios. The next steps will involve the implementation and evaluation of context-adaptive AI systems in various interaction forms. I seek to prioritize human needs in technology development, creating AI systems that tangibly benefit end-users in real-world applications and enhance interaction experiences.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {429},\nnumpages = {5},\nkeywords = {Human-AI interaction, human-robot interaction, user-centered design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638176,\nauthor = {Li, Brenna},\ntitle = {Designing Conversational Agents to Facilitate Patient-Physician Communication and Clinical Consultation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638176},\ndoi = {10.1145/3613905.3638176},\nabstract = {Pre-consultation chatbots present a unique opportunity to benefit both patients and physicians by facilitating essential information exchange prior to appointments, streamlining the consultation process. However, existing literature on how to design, implement, and evaluate such applications is limited. My thesis addresses this gap through design and evaluation studies with patients and physicians. I use my understanding of physicians’ perspectives on synchronous consultations over text messaging to guide the development of a large-language model based pre-consultation chatbot, which I then test with patients in a real-world clinic. My next steps involve developing an interface that physicians can use to review the patient information from the chatbot before the appointment. My thesis contributes to the growing literature on medical large-language model applications in which physician and patient relationships are enhanced, not replaced. It supports a collaboration model where physicians remain responsible for making clinical decisions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {430},\nnumpages = {5},\nkeywords = {LLMs, chatbots, information gathering, patient intake, primary care},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638192,\nauthor = {Wu, Qin},\ntitle = {Designing Interactive Technology to Support Children with Autism},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638192},\ndoi = {10.1145/3613905.3638192},\nabstract = {This study aims to investigate the design of interactive technologies to support the development of greetings, peer socialization, and daily life skills in autistic children. Collaboration with professional therapists and families is integral to ensure that the designed technological tools align with practical needs and undergo effectiveness evaluations. The research content includes 1) wearable masks for training autistic children in social greetings, 2) tabletop augmented reality technology to aid therapists in training children in audience socialization, and 3) tabletop interactivity to assist autistic children in learning daily activities such as making beds and dressing. The findings of those studies are expected to provide substantial guidance for developing interactive technologies that support social interaction and daily life skills development in children with autism. By using three different technologies, my doctoral research will provide a deep understanding of how interactive technologies can benefit the development of autistic children in diverse areas.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {431},\nnumpages = {5},\nkeywords = {Assistive technology, Autism intervention, Children with autism, Inclusive designs},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638180,\nauthor = {Perera, Minoli},\ntitle = {Enhancing Productivity Applications for People who are Blind using AI Assistants},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638180},\ndoi = {10.1145/3613905.3638180},\nabstract = {Productivity applications, such as word processors, spreadsheets and presentations, have become indispensable tools in the workplace, higher education, and personal settings. These applications are primarily accessed by blind users through the use of screen readers, which face numerous accessibility and usability challenges that hinder productivity and independence. My research aims to understand the severity of these challenges, explore the design space for potential AI-based Assistants, and propose design guidelines for more accessible and usable applications. The research employs a Design Thinking and Co-design approach, with the active involvement of blind users throughout the project. I have conducted a survey and a user study to gain insights into blind users’ experiences with productivity applications. The next phase of the project will delve into AI assistant capabilities and interaction techniques aimed at enhancing blind users’ productivity and independence.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {432},\nnumpages = {6},\nkeywords = {AI assistants, accessibility, assistive technology, blind, generative AI, productivity applications, screen readers, virtual assistants, voice assistants},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638184,\nauthor = {Kim, Sunnie S. Y.},\ntitle = {Establishing Appropriate Trust in AI through Transparency and Explainability},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638184},\ndoi = {10.1145/3613905.3638184},\nabstract = {As AI systems are increasingly transforming our society, it is critical to support relevant stakeholders to have appropriate understanding and trust in these systems. My dissertation research explores how AI transparency and explainability can help with this goal. I begin with human-centered evaluations of current AI explanation techniques, focusing on their usefulness for people in understanding model behavior and calibrating trust. Next, I identify what explainability needs real AI end-users have and what factors influence their trust through an in-depth case study of a real-world AI application. Finally, I describe two studies, one ongoing and one proposed, that investigate transparency and explainability approaches for Generative AI, such as large language models, to enable safe and successful interactions with this new and powerful technology. My dissertation contributes to both HCI and AI fields by elucidating mechanisms and factors of trust in AI and detailing design considerations for AI transparency and explainability approaches.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {433},\nnumpages = {6},\nkeywords = {AI transparency and explainability, Explainable AI, Human-AI collaboration, Trust and reliance},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638185,\nauthor = {Wu, Yanlai},\ntitle = {Examination of Users’ Privacy Issues in Live Streaming},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638185},\ndoi = {10.1145/3613905.3638185},\nabstract = {Live streaming has become a popular activity worldwide that has warranted research attention on its privacy related issues. The performance-driven, socioeconomic, real-time, on-camera and public nature of live streaming heightens the privacy challenges to users. However, little research has been done to explore the privacy issues in live streaming. Therefore, my research aims to investigate users’ privacy concerns and strategies in live streaming, as well as to propose practical design ideas for enhancing privacy management. To achieve these objectives, I conduct three qualitative studies, including interviews with Chinese DouYu streamers (Study 1, published at CSCW), an exploration of streamers’ considerations for bystanders’ privacy (Study 2, published at CSCW), and an ongoing co-design study involving streamers and bystanders (Study 3). The anticipated outcome of this research is to provide valuable design principles and guidelines for improving privacy protection mechanisms on live streaming platforms.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {434},\nnumpages = {4},\nkeywords = {collective privacy management, live streaming, multi-stakeholder, privacy, synchronous social media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638178,\nauthor = {Ibrahim, Zaidat},\ntitle = {Expanding Personal Informatics: Menstruation and Pregnancy Healthcare Journey For Practising Muslim Women},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638178},\ndoi = {10.1145/3613905.3638178},\nabstract = {While HCI research in women’s health continues to increase, HCI researchers have yet to conduct work at the intersection of women’s health and religious practices. This oversight is particularly troubling given how religious and faith practices influence health and impact individuals’ behaviors, including interaction with technology in their everyday lives. In my research, I aim to design women’s health technologies that simultaneously support the reproductive and pregnancy journey while considering, being inclusive of, and responsive to their religious goals and values. My dissertation is centered on the female Muslim population in the US. My completed studies focused on understanding how this population tracks its menstrual cycle (the starting phase for the reproductive and pregnancy healthcare journey), engaging with technology for tracking health and religious goals, and how they balance tracking within religious contexts such as Ramadan. These studies unveiled an intricate connection between tracking for health purposes and religious-related reasons, paving the way for my ongoing and future work, which I will discuss in the subsequent sections.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {435},\nnumpages = {6},\nkeywords = {Health Journeys, Inclusive Design, Islam, Menstruation, Pregnancy, Religion, Women’s Health},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638181,\nauthor = {Luo, Weizhou},\ntitle = {Exploring Spatial Organization Strategies for Virtual Content in Mixed Reality Environments},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638181},\ndoi = {10.1145/3613905.3638181},\nabstract = {Our future will likely be reshaped by Mixed Reality (MR) offering boundless display space while preserving the context of real-world surroundings. However, to fully leverage the spatial capabilities of MR technology, a better understanding of how and where to place virtual content like documents is required, particularly considering the situated context. I aim to explore spatial organization strategies for virtual content in MR environments. For that, we conducted empirical studies investigating users’ strategies for document layout and placement and examined two real-world factors: physical environments and people present. With this knowledge, we proposed a mixed-reality approach for the in-situ exploration and analysis of human movement data utilizing physical objects in the original space as referents. My next steps include exploring arrangement strategies, designing techniques empowering spatial organization, and extending understandings for multi-user scenarios. My dissertation will enrich the immersive interface repertoire and contribute to the design of future MR systems.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {436},\nnumpages = {6},\nkeywords = {Augmented Reality, Mixed Reality, Spatiality, affordance, content organization, spatial layout},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638175,\nauthor = {Otuu, Obinna Ogbonnia},\ntitle = {Integrating Communications and Surveillance Technologies for effective community policing in Nigeria},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638175},\ndoi = {10.1145/3613905.3638175},\nabstract = {My research investigates how technologically enhanced community policing (CP) approach can curb crime and insecurity in Nigeria, by efficiently integrating communications and surveillance systems. In order to achieve this, I have conducted a narrative literature review to understand the current state of community policing in Nigeria, and two systematic literature reviews; firstly to comprehensively understand trust and relationship issues between Nigerian police and the citizens, and secondly to identify technologies used in community policing since inception and how each has fared, as to gain better design insight and direction. Also, I have conducted a survey and interview with 1200 and 18 Nigerian citizens respectively, and a separate interview with 6 Nigerian police commissioners representing the 6 geopolitical zones of the country, through which I gathered relevant data as regards the use of technologies in Nigeria to reduce crime and insecurity. The outcome of the survey and interviews informed a design of a novel body-worn device, integrated with a mobile application for real-time information dissemination. My research findings will produce a tool that will improve community policing in Nigeria and lower insecurity and crime rates therein.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {437},\nnumpages = {4},\nkeywords = {Communications, Community Policing, Crime, Surveillance, Technologies},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638174,\nauthor = {Seo, Jwawon},\ntitle = {Motives and Role of Psychological Ownership in AR Workspaces for Remote Collaboration},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638174},\ndoi = {10.1145/3613905.3638174},\nabstract = {Augmented Reality (AR) systems have been proposed as solutions to reinforce remote collaboration over performing physical tasks by integrating digital information into the physical environment. However, there is a fundamental gap that arises from the difference in the degree to which remote and local workers interact with virtual and physical objects, which is attributed to differing senses of ownership over these objects. Addressing this gap, my thesis investigates how collaborative functionality affects psychological ownership in shared AR workspaces. Specifically, the study examines how the ability to modify annotations shapes Individual and Collective Psychological Ownership (IPO and CPO). Furthermore, I explore how IPO and CPO mediate the effect of collaborative functionality on the outcomes of remote collaboration. My overall goal is to extend the theoretical understanding of psychological ownership, offering insights into the dynamics of shared AR environments by focusing on the interplay between physical and virtual elements.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {438},\nnumpages = {5},\nkeywords = {augmented reality, psychological ownership, remote collaboration, shared information, social interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638179,\nauthor = {Bu\\c{c}inca, Zana},\ntitle = {Optimizing Decision-Maker's Intrinsic Motivation for Effective Human-AI Decision-Making},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638179},\ndoi = {10.1145/3613905.3638179},\nabstract = {AI advice is increasingly incorporated into decision-making processes, but evidence suggests that decision-makers often struggle to effectively integrate this advice, leading to tendencies to over-rely or under-utilize AI. My research challenges our field’s assumption that decision-makers are inherently motivated to engage with AI. I have discovered that cognitive motivation is essential for individuals to actively engage with, critically evaluate, and effectively incorporate AI advice into decision-making. Thus, I propose that AI-powered decision support systems designed to enhance decision-makers’ motivation will improve decision-making efficacy. To this end, I have developed two systems that bolster decision-makers’ intrinsic motivation by supporting their competence and autonomy. Empirical results suggest that fostering intrinsic motivation not only leads to enhanced decision-making performance but also improves the subjective experience when compared to no decision assistance or existing decision support paradigms. This research proposes a paradigm shift in the design of AI-assisted decision-making tools, moving towards systems that improve decision performance via enhancing decision-makers’ intrinsic motivation to engage with the task and the decision support.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {439},\nnumpages = {5},\nkeywords = {cognitive engagement, decision support, human-AI interaction, human-centered AI, intrinsic motivation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638187,\nauthor = {Hedlund, Martin},\ntitle = {Physical Locomotion for Virtual Environments},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638187},\ndoi = {10.1145/3613905.3638187},\nabstract = {Physical interaction can provide users with engaging experiences and more healthy active interfaces. In virtual environments, physical interaction can be used for locomotion purposes in which the user’s physical actions are mapped to virtual translations, for example in Virtual Reality. Physical engagement can be important not only for exertion game applications, but also in terms of reducing VR sickness, providing engaging and perhaps more realistic experiences, and in the long-term, contribute to reducing sedentary behavior in work context applications. My work examines how locomotion methods can be implemented in virtual environments for various contexts in which the user is physically active in generating locomotion, in contrast to a sedentary desktop or joystick setting. In three studies, I have studied how forms of physical locomotion (normal walking, walk-in-place, fitness equipment) for virtual reality applications, impact the performance and usability within their given context. In these studies I have used comparative experimental design to evaluate locomotion alternatives, or compared with a desktop alternative. For the next study I aim to investigate physical steering techniques for flying experiences. I also aim to synthesize locomotion research into a holistic detail-oriented framework to support comparison and reproducability between locomotion techniques.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {440},\nnumpages = {6},\nkeywords = {embodied, locomotion, physical, steering, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638186,\nauthor = {Park, Hyanghee},\ntitle = {Re-examining User Burden in Human-AI Interaction: Focusing on a Domain-Specific Approach},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638186},\ndoi = {10.1145/3613905.3638186},\nabstract = {In my thesis, I revisit, specialize, and expand the concept of ‘user burden’ in three different contexts. In the first study, I explore what user burdens occur when deleting unused apps. To do so, I have conducted in-depth interviews, designed questionnaires, and performed scenario-based experiments. In my second study, I designed and developed a conversational agent that documents and reports cases of sexual assault survivors to the police on behalf of the survivors. To discover survivors’ burdens and find solutions to mitigate them, I conducted in-depth interviews and participatory design sessions with sexual assault survivors, as well as diverse stakeholders (e.g., police officers, counselors). In my third study, I investigated why employees resist algorithmic evaluations in workplaces and how to mitigate these burdens. The goal of participating in this doctoral consortium is to share the three lines of research for my thesis with researchers and professors and gain diverse ideas and feedback from the HCI community to better synthesize my works.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {441},\nnumpages = {4},\nkeywords = {Algorithmic Management, Conversational Agents, Future of Work, Human-AI Interaction, Metaverse Workspace, Remote Work, User Burden},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638177,\nauthor = {Bhattacharya, Aditya},\ntitle = {Towards Directive Explanations: Crafting Explainable AI Systems for Actionable Human-AI Interactions},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638177},\ndoi = {10.1145/3613905.3638177},\nabstract = {With Artificial Intelligence (AI) becoming ubiquitous in every application domain, the need for explanations is paramount to enhance transparency and trust among non-technical users. Despite the potential shown by Explainable AI (XAI) for enhancing understanding of complex AI systems, most XAI methods are designed for technical AI experts rather than non-technical consumers. Consequently, such explanations are overwhelmingly complex and seldom guide users in achieving their desired predicted outcomes. This paper presents ongoing research for crafting XAI systems tailored to guide users in achieving desired outcomes through improved human-AI interactions. This paper highlights the research objectives and methods, key takeaways and implications learned from user studies. It outlines open questions and challenges for enhanced human-AI collaboration, which the author aims to address in future work.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {442},\nnumpages = {6},\nkeywords = {Domain-Expert-AI Collaboration, Explainable AI, Explanatory Interactive Learning, Interactive Machine Learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638182,\nauthor = {Carmichael, Josie},\ntitle = {Translating Human-Centred Artificial Intelligence for Clinical Decision Support Systems into Practice: A Medical Retina Case Study.},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638182},\ndoi = {10.1145/3613905.3638182},\nabstract = {Artificial intelligence has potential to enhance healthcare outcomes through applications such as early diagnosis and enhanced treatment planning but can only deliver such potential if it is integrated into clinical practice. Using eyecare as a case study, my research explores optometrists’ requirements for an AI clinical decision support system (AI-CDSS) and addresses the ‘gap’ between AI design motivated by research success vs clinical application. For example, findings from an interview study with 20 optometrists highlighted that clinicians’ risk-adverse tendencies can significantly affect their interpretation of AI outputs when making clinical decisions. The way in which outputs are presented should neither encourage suboptimal risk-averse behaviours nor convey misleading information. In the latter stages of my Ph.D., I aim to further investigate how the level of risk associated with clinical decisions can affect interpretations of AI support, as well as testing available methods for improving the interpretability of AI outputs.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {443},\nnumpages = {5},\nkeywords = {Clinical decision support, artificial intelligence, healthcare, human-AI interaction, interpretability, medical imaging},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638189,\nauthor = {Aldaweesh, Sarah},\ntitle = {Understanding Challenges and Design Opportunities for Digital Mental Well-Being in Saudi Arabia},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638189},\ndoi = {10.1145/3613905.3638189},\nabstract = {Mental health is considered a growing and highly stigmatized concern in the Kingdom of Saudi Arabia (KSA). Despite the high interest in mobile health (mHealth) in the KSA and its potential to overcome traditional barriers, research on its application in the Saudi mental well-being context is scarce. My thesis reviews the Saudi app market and explores the main opportunities and barriers to the use of publicly available Arabic mental well-being mobile apps in the KSA from various perspectives including mental health clinicians and Saudi individuals, with a particular focus on young Saudi women. Ultimately, this thesis aims to contribute to the current knowledge by providing design recommendations derived from interviews, co-design workshops, development and evaluation of a prototype, to inform the future design of Arabic mental well-being technologies considering values and cultural norms.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {444},\nnumpages = {6},\nkeywords = {Saudi women, culture, mHealth, mental health, mobile apps, qualitative study, well-being},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638183,\nauthor = {Lahiri, Sucheta},\ntitle = {Understanding Risks of Data Science Failures through Sociotechnical Approach},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638183},\ndoi = {10.1145/3613905.3638183},\nabstract = {A long-standing question for CHI and CSCW community is to understand data science work and everyday practices. Based on six months of in-person and virtual ethnography with a private organization located in United States and India, this study explores how data science practitioners articulate and manage risks of data science project failure. Aligning with the plurality of risk articulations and power laden risk protocols, the research uses sociotechnical lens to explore the affordances between social actors and technology for risk management. The multiple affordances identified will inform strategies to help practitioners in everyday risk identification and management. The goal of this research is to offer empirical insights and actionable frameworks that will assist practitioners in navigating complex sociotechnical risk management landscapes.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {445},\nnumpages = {5},\nkeywords = {Data Science Work, Global South, Multi-sited Ethnography, Risk Management},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638193,\nauthor = {Israni, Aarti},\ntitle = {Understanding and Supporting Financially-Constrained Aspiring Entrepreneurs’ Entrepreneurial Transitions},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638193},\ndoi = {10.1145/3613905.3638193},\nabstract = {Entrepreneurship is perceived as a promising path to financial stability, especially for those with financial constraints. Still, the transition to entrepreneurship is not easy. Financially-constrained aspiring entrepreneurs, many of whom are racial minorities, must overcome many challenges to achieve their goals. This includes obtaining access to mentorship, financial capital, and digital support for their businesses. In my dissertation, I examine how existing sociotechnical interventions, including social media and community-based-organization (CBO)-supported interventions, can support their entrepreneurial transitions, especially as a provision of informational and emotional support. Early findings from my dissertation research suggest that CBO-supported peer group interventions provide a source of informational and emotional support, helping financially-constrained aspiring entrepreneurs make progress toward their goals and overcome setbacks. In my dissertation, I aim to unpack the factors that contribute to the success of such groups and contribute design opportunities for sociotechnical interventions to better support financially-constrained aspiring entrepreneurs’ work-role transition processes.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {446},\nnumpages = {5},\nkeywords = {entrepreneurship, identity transitions, low-resource communities},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3638194,\nauthor = {Wani, Asra Sakeen},\ntitle = {Use of ICTs during ongoing protracted socio-political disruptions},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3638194},\ndoi = {10.1145/3613905.3638194},\nabstract = {Human-Computer Interaction (HCI) research has increasingly focused on enhancing human development in diverse contexts through technology. However, regions going through ongoing socio-political unrest face varied developmental challenges. My dissertation explores the impact of socio-political conflict in Kashmir, India, on critical domains of human development, including education, employment, and mental health. The ongoing conflict has disrupted education, leading to prolonged closures of schools and colleges and exacerbated unemployment, particularly among young people with higher education levels with significant mental health challenges. Through qualitative methods, my work draws on field visits and semi-structured interviews to investigate the barriers in education, job-seeking, and access to professional mental health support. Based on the findings, my work provides socio-technical design recommendations shedding light on the impact of protracted socio-political crises on human development and contributes to the HCI communities understanding of such regions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {447},\nnumpages = {6},\nkeywords = {Crisis Informatics, Culture, Disruption, India., Internet shutdown, Kashmir, Technology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651128,\nauthor = {Arueyingho, Oritsetimeyin},\ntitle = {Afro-centred Collaborative care: Technology support for Type 2 Diabetes Management in Port Harcourt, Nigeria},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651128},\ndoi = {10.1145/3613905.3651128},\nabstract = {Type 2 Diabetes (T2D) poses a significant health challenge in Nigeria, requiring long-term community care involving diverse stakeholders. The condition leads to severe health complications exacerbated by suboptimal lifestyle conditions. However, sparse knowledge exists regarding the role of technology in facilitating collaborative T2D care in Nigeria. Additionally, contextual factors, including socio-cultural and demographic influences on the use and adoption of existing T2D technologies, remain poorly understood. Diverse collaborative care approaches involving community pharmacists and caregivers, and context-specific design implications for effective T2D care are also insufficiently documented. Addressing these knowledge gaps, my research project adopts a decolonial Afro-centred perspective to probe the landscape of T2D care involving multiple stakeholders in Port Harcourt, Nigeria. Also, I aim to improve research methodologies and design practices for sustained community care, with a focus on co-designing collaborative T2D care interventions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {448},\nnumpages = {4},\nkeywords = {Collaborative care, Community care, Decolonization, Digital Health, Type 2 Diabetes},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651135,\nauthor = {Gao, Lei},\ntitle = {Designing and Prototyping Applications Using Acoustophoretic Interfaces},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651135},\ndoi = {10.1145/3613905.3651135},\nabstract = {Recently, the acoustophoretic interface, using acoustic levitation to manipulate objects in mid-air with ultrasound waves, has become a notable advancement in Human-Computer Interaction (HCI). This innovative interface simultaneously provides contactless haptic feedback and audio delivery through a single technical approach. The versatility of the acoustophoretic interface is evident in its wide range of applications, including physical displays, mid-air haptic interactions, contactless object manipulation, etc. Despite its potential, the interface remains underutilized, partly due to its novelty and the complexity of implementing advanced interaction tasks. My PhD research is dedicated to addressing these challenges by developing effective design and implementation strategies tailored to real-world application scenarios. By advancing the technical capabilities and application possibilities of acoustophoretic interfaces, my work strives to unlock new potentials for acoustophoretic interfaces, paving the way for innovative, practical solutions for designers, creators, and researchers to build interactive, engaging, and effective applications across various domains.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {449},\nnumpages = {5},\nkeywords = {acoustic levitation, acoustophoretic interfaces, data-driven, fabrication, physicalization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651133,\nauthor = {Jin, Xiaofu},\ntitle = {Empowering Autonomous Digital Learning for Older Adults},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651133},\ndoi = {10.1145/3613905.3651133},\nabstract = {The widespread adoption of smartphones has fundamentally transformed access to digital services, offering conveniences but also increased complexity. This presents a challenge for older adults unfamiliar with such rapid technological changes. My dissertation proposes support systems to help these individuals independently learn digital technologies. Initial research employed surveys and interviews to understand older adults’ current digital interactions and barriers,taking digital banking as an illustrative case study. Based on their need for autonomous learning, we then developed an app that provides asynchronous social support with interactive tutorials and trial-and-error learning. Additionally, we are exploring how augmented reality (AR) can enrich the learning experiences of older adults. The forthcoming steps involve crafting an AI-powered assistant designed for empathetic, personalized interaction. Our objective is to forge an adaptable support system that uplifts older adults, bolstering their digital proficiency and bridging the technological divide to ensure a more inclusive future.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {450},\nnumpages = {6},\nkeywords = {AI, AR, digital technologies, learn, older adults, trial-and-error},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651126,\nauthor = {Shakeri, Hanieh and Neustaedter, Carman},\ntitle = {Passive Co-presence: Exploring How Peripheral Devices Connect People Over Distance},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651126},\ndoi = {10.1145/3613905.3651126},\nabstract = {When families live in the same home, they feel a sense of connection through the subtle, passive aspects of family life. Over distance, these passive aspects are hard to experience as most communication technologies support sharing conversations or activities. Using a co-design study, Research-Through-Design (RtD) methods, and a field deployment, I aim to explore the design of smart home technologies for passive co-presence over distance. The co-design study uncovered differences in the connection needs of emerging adults and their parents, and provided a set of design considerations including designing for the need for control and privacy, sharing multi-sensory environmental ambience, and supporting nostalgia and comfort. These findings guided an RtD exploration resulting in the design of two artifacts – the There Chair and Fragrance Frame. To understand the impact of integrating passive co-presence designs into the home, I plan to conduct a field deployment, which I describe in this work.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {451},\nnumpages = {6},\nkeywords = {Family Connection, Remote Communication, Smart Home Technology},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651132,\nauthor = {Dongre, Poorvesh},\ntitle = {Physiology-Driven Empathic Large Language Models (EmLLMs) for Mental Health Support},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651132},\ndoi = {10.1145/3613905.3651132},\nabstract = {Wearable devices show promise in monitoring and managing mental health, but gaps exist in accurately predicting users’ mental states and cognitively engaging with users to provide mental health support with wearable data. In this proposal, I present the concept of physiology-driven Empathic Large Language Models (EmLLMs) for mental health support. EmLLMs monitor users and their surrounding environment using wearable devices to predict their mental and emotional states and interact with them based on these states. I present the application of this approach for monitoring and managing excess stress in the workplace. To improve the accuracy of stress prediction, I developed a novel Science-Guided Machine Learning (SGML) model that automatically extracts features from raw wearable data. To engage with users cognitively, I developed an (EmLLM) chatbot that provides psychotherapy based on predicted user stress. I present the SGML model’s preliminary findings and results from a pilot user study that evaluates the EmLLM chatbot.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {452},\nnumpages = {5},\nkeywords = {Deep Learning, Large Language Models (LLMs), Physiological Data, Wearable Devices},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651129,\nauthor = {Kyi, Lin},\ntitle = {Reimagining Online Consent for More Responsible and Human-Centred Data Collection},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651129},\ndoi = {10.1145/3613905.3651129},\nabstract = {Online consent in the present day usually consists of only cookie banners, which have been heavily criticized for being annoying, deceptive, and not truly allowing for one to make an informed decision. However, we keep using cookie banners due to a lack of better alternatives. In my PhD, I have conducted, and am currently conducting, research about issues relating to the current consent ecosystem, along with proposed solutions to improve the way online consent operates. Through my research, I hope to work towards a more responsible, transparent, and human-centred consent and data collection ecosystem.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {453},\nnumpages = {3},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651131,\nauthor = {Clocchiatti, Alessandro},\ntitle = {Self-Avatar Motion Retargeting for Virtual Reality Post-Stroke Rehabilitation Therapy},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651131},\ndoi = {10.1145/3613905.3651131},\nabstract = {Post-stroke rehabilitation therapies in Virtual Reality have shown to be a valid alternative to traditional non-virtual therapies. In rehabilitation therapies based on induced movements, the kinesthetic illusion induces a feeling as if the individual’s body movements were wider than the actual during the exercise. This rehabilitation therapy approach in virtual reality is still novel and many aspects are unclear. Our research focuses on the investigation of the user’s perception of the altered virtual body movements in virtual reality, to define the best rehabilitation strategy.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {454},\nnumpages = {4},\nkeywords = {Motion Retargeting, Post-stroke Rehabilitation, Sense of Embodiment, Virtual Reality, Visuomotor Illusion},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651134,\nauthor = {Anuyah, Oghenemaro},\ntitle = {Strengthening Communities: Towards AI-Empowered Knowledge Management for Community Social Services},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651134},\ndoi = {10.1145/3613905.3651134},\nabstract = {The welfare and well-being of vulnerable and marginalized individuals, such as those facing poverty and food or housing insecurity, heavily rely on their access to essential social services. These services are commonly provided by community social service organizations, which are known to deal with multifaceted needs and must collaborate with a network of other social services. Operating in such distributed networks presents unique challenges in knowledge management and knowledge transfer (KM/KT), which are critical for effectively serving these populations. My dissertation research focuses on studying KM/KT within these distributed social services networks. My dissertation comprises four studies that: 1) Examine technology integration barriers and opportunities in community social services. 2) Explore knowledge management processes within these organizations to identify how technology can better support their work. 3) Investigate the potential of AI in community social services through participatory design. and 4) Implement and evaluate a technological intervention for community service knowledge management.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {455},\nnumpages = {5},\nkeywords = {Community-Based Participatory Research, Knowledge Management, Knowledge Sharing, Marginalized Communities, Responsible Artificial Intelligence, Social Services},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651127,\nauthor = {Showkat, Dilruba},\ntitle = {Towards Algorithmic Reform: Low-Income Individuals Inclusion in AI/ML Literacy and Ethical Values-Informed Tool Design},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651127},\ndoi = {10.1145/3613905.3651127},\nabstract = {Poverty in the US is not invisible. A large number of Americans are low-income and experience homelessness. This population relies on scarce-resourced public services for survival and thriving. High-stake public service resource allocations are increasingly fueled by AI/ML to provide efficient and scalable services. While AI/ML tools are deployed with positive expectations, there is growing evidence of AI/ML causing invisible harm, often dismissed as inevitable. AI/ML tools are based on deficits and vulnerability ranking; they ignore the strengths of vulnerable individuals. This thesis aims to counteract existing exclusions and reduce AI/ML access barriers for low-income individuals, through their inclusion in AI/ML innovations and education, using mixed-methods experimental studies. Specifically, (1) designing and evaluating value-sensitive service-assessment tools that go beyond individual risk-factors and focus on strengths, (2) making AI/ML knowledge digestible for low-income individuals. Thereby, bringing about social change, AI/ML access, and algorithmic reform.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {456},\nnumpages = {6},\nkeywords = {ACM CHI, ACM CSCW, AI/ML literacy, Assets, Ethical values, Inclusion of impacted people in AI/ML, Inclusive design, Low-income individuals, ML for Homelessness, Machine Learning (ML) Values, Risk factors, Value-sensitive design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3651130,\nauthor = {Medeiros, Marina Lima},\ntitle = {Working In Mixed Realities: 3D User Interfaces And Interaction Patterns To Integrate Practices In Different Perceptual Spaces},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3651130},\ndoi = {10.1145/3613905.3651130},\nabstract = {This PhD project proposes a foundational research to classify and analyse how 3d user interfaces and interaction patterns in mixed realities can be integrated to and improve existing working practices according to their perceptual scale. Besides the formulation of a theoretical background, the proposed research will discuss the design challenges of three case studies. In each case the focus of the working activity is on a different scale: personal space, action space and vista space. The final aim is to understand the similarities and the differences between the different projects and provide design guidelines for the design and implementation of 3d user interfaces and interactions patterns in mixed realities according to the main perceptual space of the work activity.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {457},\nnumpages = {5},\nkeywords = {Augmented Reality, Extended Reality, Manipulation Techniques, Navigation Techniques, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636321,\nauthor = {Brubaker, Jed R. and Morris, Meredith Ringel and Doyle, Dylan Thomas and Fiesler, Casey and Gibbs, Martin and McGrenere, Joanna},\ntitle = {AI and the Afterlife},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636321},\ndoi = {10.1145/3613905.3636321},\nabstract = {AI technologies are likely to impact an array of existing practices (and give rise to a host of novel ones) around end-of-life planning, remembrance, and legacy in ways that will have profound legal, economic, emotional, and religious ramifications. At this critical moment of technological change, there is an opportunity for the HCI community to shape the discourse on this important topic through value-sensitive and community-centered approaches. This workshop will bring together a broad group of academics and practitioners with varied perspectives including HCI, AI, and other relevant disciplines (e.g., law, economics, religious studies, etc.) to support community-building, agenda-setting, and prototyping activities among scholars and practitioners interested in the nascent topic of how advances in AI will change socio-technical practices around death, remembrance, and legacy.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {458},\nnumpages = {5},\nkeywords = {AI, AI agents, Generative AI, HCI, death, digital afterlife, digital legacy, end-of-life planning, post-mortem AI, post-mortem data management},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636307,\nauthor = {Parker, Callum and Yoo, Soojeong and Fredericks, Joel and Tran, Tram Thi Minh and Williamson, Julie R. and Lee, Youngho and Woo, Woontack},\ntitle = {Building a Metaverse for All: Opportunities and Challenges for Future Inclusive and Accessible Virtual Environments},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636307},\ndoi = {10.1145/3613905.3636307},\nabstract = {The Metaverse is proposed as a collection of interconnected virtual worlds allowing people to seamlessly access and traverse through them. It offers an opportunity to remove physical boundaries and borders present in our reality with the potential to bring together people from all walks of life. The development of a Metaverse or a “new world” offers a compelling opportunity to establish a space that is both all-encompassing and accessible to everyone. However a key challenge is understanding how the Metaverse can be designed from the ground up to be inclusive and accessible. This workshop aims to gain further understanding of how to create an open and inclusive Metaverse for all, while also exploring methods for its evaluation. The key outcomes of this workshop outline new opportunities for improving the inclusivity of the Metaverse, evaluation methodologies, and key considerations for designing accessible Metaverse environments and interactions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {459},\nnumpages = {5},\nkeywords = {Accessiblity, Augmented Reality, Human Computer Interaction, Inclusivity, Metaverse, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636287,\nauthor = {Desai, Smit and Wei, Christina Ziying and Sin, Jaisie and Dubiel, Mateusz and Zargham, Nima and Ahire, Shashank and Porcheron, Martin and Kuzminykh, Anastasia and Lee, Minha and Candello, Heloisa and Fischer, Joel E and Munteanu, Cosmin and Cowan, Benjamin R.},\ntitle = {CUI@CHI 2024: Building Trust in CUIs—From Design to Deployment},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636287},\ndoi = {10.1145/3613905.3636287},\nabstract = {Conversational user interfaces (CUIs) have become an everyday technology for people the world over, as well as a booming area of research. Advances in voice synthesis and the emergence of chatbots powered by large language models (LLMs), notably ChatGPT, have pushed CUIs to the forefront of human-computer interaction (HCI) research and practice. Now that these technologies enable an elemental level of usability and user experience (UX), we must turn our attention to higher-order human factors: trust and reliance. In this workshop, we aim to bring together a multidisciplinary group of researchers and practitioners invested in the next phase of CUI design. Through keynotes, presentations, and breakout sessions, we will share our knowledge, identify cutting-edge resources, and fortify an international network of CUI scholars. In particular, we will engage with the complexity of trust and reliance as attitudes and behaviours that emerge when people interact with conversational agents.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {460},\nnumpages = {7},\nkeywords = {chatbots, conversational AI, conversational agents, conversational user interfaces, reliance, trust, voice assistants},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636293,\nauthor = {Prpa, Mirjana and Troiano, Giovanni Maria and Wood, Matthew and Coady, Yvonne},\ntitle = {Challenges and Opportunities of LLM-Based Synthetic Personae and Data in HCI},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636293},\ndoi = {10.1145/3613905.3636293},\nabstract = {Synthetic personae and data powered by artificial intelligence (AI) are emerging in many HCI areas, including education and training, gaming, and piloting research studies. Recently, Large Language Models (LLMs) have shown promise for synthetic AI personae, experimenting with human and social simulacra and producing synthetic data. This presents challenges and opportunities for extending HCI research via LLMs and AI. In this proposed workshop, we engage HCI researchers interested in working with LLMs, synthetic personae, and synthetic data through speculative design and producing visions, desiderata, and requirements for future HCI research engaging with synthetic personae/data. The outcomes of this workshop may be disseminated to the HCI community through scientific publications or special issues to facilitate continued discussion and advance knowledge on a timely HCI topic.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {461},\nnumpages = {5},\nkeywords = {AI, Large Language Models, sketching, speculative design, synthetic data, synthetic personae},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636316,\nauthor = {Jiang, Yue and Lu, Yuwen and Knearem, Tiffany and Kliman-Silver, Clara E and Lutteroth, Christof and Li, Toby Jia-Jun and Nichols, Jeffrey and Stuerzlinger, Wolfgang},\ntitle = {Computational Methodologies for Understanding, Automating, and Evaluating User Interfaces},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636316},\ndoi = {10.1145/3613905.3636316},\nabstract = {Building on the success of the first two workshops on user interfaces (UIs) at CHI 2022 and CHI 2023, this workshop aims to advance the research field by further exploring current research trends, such as applying large language models and visual language models. Previous work has explored computational approaches to understanding and adapting UIs using constraint-based optimization models and machine learning-based data-driven approaches. In addition to further delving into these established UI research areas, we aim to trigger the exploration into the application of the latest advancements in general-purpose large language and vision-language models within the UI domain. We will encourage participants to explore novel methods for understanding, automating, and evaluating UIs. The proposed workshop seeks to bring together academic researchers and industry practitioners interested in computational approaches for UIs to discuss the needs and opportunities for future user interface algorithms, models, and applications.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {462},\nnumpages = {7},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636298,\nauthor = {Agapie, Elena and Karkar, Ravi and Aung, Tricia and Burgess, Eleanor R. and Chinguwa, Munyaradzi Joel and Graham, Andrea K and Klasnja, Predrag and Lyon, Aaron and McCall, Terika and Munson, Sean A. and Nunes, Francisco and Osterhage, Katie},\ntitle = {Conducting Research at the Intersection of HCI and Health: Building and Supporting Teams with Diverse Expertise to Increase Public Health Impact},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636298},\ndoi = {10.1145/3613905.3636298},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {463},\nnumpages = {6},\nkeywords = {collaborative research, health impact, health research, team science},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636312,\nauthor = {Chang, Minsuk and Chung, John Joon Young and Gero, Katy Ilonka and Huang, Ting-Hao Kenneth and Kang, Dongyeop and Raheja, Vipul and Sterman, Sarah and Wambsganss, Thiemo},\ntitle = {Dark Sides: Envisioning, Understanding, and Preventing Harmful Effects of Writing Assistants - The Third Workshop on Intelligent and Interactive Writing Assistants},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636312},\ndoi = {10.1145/3613905.3636312},\nabstract = {Writing assistants are becoming increasingly sophisticated and ubiquitous, fueled by advances in artificial intelligence, particularly large language models. As new use cases and models emerge, we expect the adoption rate to accelerate. This brings a sense of urgency to understanding not just the benefits, but also the potential dark sides of intelligent writing assistants. In this interdisciplinary workshop, we will explore the challenges and dark sides that our communities may have to consider as we design and deploy new tools and technologies, as well as how to prevent them. We will build off the successful workshop at CHI23 (The Second In2Writing Workshop), bringing new voices to the vibrant community of writing tools researchers established there, and building on the design space created by prior workshop participants. We invite writers, educators, researchers, industry practitioners, students, and anyone interested in creating, using, and testing future writing assistant technologies to join the conversation.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {464},\nnumpages = {6},\nkeywords = {AI-assisted writing, Creativity support tools, Human-AI interaction, Human-computer interaction, Language models, Natural language processing, Writing assistants, Writing support tools},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636282,\nauthor = {Dritsa, Dimitra and van Renswouw, Loes and Colombo, Sara and V\\\"{a}\\\"{a}n\\\"{a}nen, Kaisa and Bogers, Sander and Martinez, Arian and Holbrook, Jess and Brombacher, Aarnout},\ntitle = {Designing (with) AI for Wellbeing},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636282},\ndoi = {10.1145/3613905.3636282},\nabstract = {Designing with data and Artificial Intelligence (AI) can bring significant value to the development of systems and technologies that promote personal wellbeing. However, there are also unaddressed challenges and risks connected to designing (with) AI for wellbeing, such as the difficulties in ensuring that the generated feedback or proposed interventions are relevant considering the large interpersonal variations between the current, desired and achievable level of physical and mental wellbeing of different individuals. In this one-day hybrid workshop, we aim to bring together design and HCI researchers and practitioners interested in the intersection of design, AI, and wellbeing beyond clinical applications. We will discuss challenges in designing with AI for wellbeing originating from a) the domains of design and b) general issues in developing AI systems, and uncover new potential directions that emerge when coupling design, AI and wellbeing. Our aim is to bring together researchers and practitioners from various fields and backgrounds who use data and AI when designing for wellbeing. Through this workshop, we aim to create a conceptual framework that enables the emergence of rich, meaningful, and ethical solutions for designing (with) AI for wellbeing, while also providing handles to mitigate the emergence of negative consequences.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {465},\nnumpages = {7},\nkeywords = {Artificial intelligence, Designing with data, Wellbeing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636313,\nauthor = {Nebeling, Michael and Oki, Mika and Gelsomini, Mirko and Hayes, Gillian R and Billinghurst, Mark and Suzuki, Kenji and Graf, Roland},\ntitle = {Designing Inclusive Future Augmented Realities},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636313},\ndoi = {10.1145/3613905.3636313},\nabstract = {Augmented and mixed reality technology is rapidly advancing, driven by innovations in display, sensing, and AI technologies. This evolution, particularly in the era of generative AI with large language and text-to-image models such as GPT and Stable Diffusion, has the potential, not only to make it easier to create, but also to adapt and personalize, new content. Our workshop explores the pivotal role of augmented and mixed reality to shape a user’s interactions with their physical surroundings. We aim to explore how inclusive future augmented realities can be designed, with increasing support for automation, such that environments can welcome users with different needs, emphasizing accessibility and inclusion through layers of augmentations. Our aim is not only to remove barriers by providing accommodations, but also to create a sense of belonging by directly engaging users. Our workshop consists of three main activities: (1) Through brainstorming and discussion of examples provided by the workshop organizers and participants, we critically review the landscape of accessible and inclusive design and their vital role in augmented and mixed reality experiences. (2) Through rapid prototyping activities including bodystorming and low-fidelity, mixed-media prototypes, participants explore how augmented and mixed reality can transform physical space into a more personal place, enhancing accessibility and inclusion based on novel interface and interaction techniques that are desirable, but not necessarily technically feasible just yet. In the workshop, we plan to focus on physical space to facilitate rapid prototyping without technical constraints, but techniques developed in the workshop are likely applicable to immersive virtual environments as well. (3) Finally, we collaborate to outline a research agenda for designing future augmented realities that promote equal opportunities, benefiting diverse user populations. Our workshop inspires innovation in augmented and mixed reality, reshaping physical environments to be more accessible and inclusive through immersive design.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {466},\nnumpages = {6},\nkeywords = {Spatial computing, accessible and inclusive design., generative AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636297,\nauthor = {Debnath, Alok and Lahnala, Allison and Gen\\c{c}, H\\\"{u}seyin U\\u{g}ur and Soubutts, Ewan and Lahav, Michal and Horne, Tiffanie and Meijer, Wo and Pai, Yun Suen and Hsu, Yen-Chia and Barbareschi, Giulia and Verma, Himanshu and Mauri, Andrea},\ntitle = {EmpathiCH: Scrutinizing Empathy-Centric Design Beyond the Individual},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636297},\ndoi = {10.1145/3613905.3636297},\nabstract = {The EmpathiCH Workshop aims to blend a diverse set of expertise to expand upon the nascent field of Empathy-Centric Design. Building on the discussions in previous editions of the workshop, this iteration invites contributions which scrutinize the use of empathy as a design principle in digital interfaces. We encourage inquiry in a number of research dimensions: examining the multifaceted nature of empathy; establishing both the requirements and shortcomings of empathy in design research; discussing key post-human stakeholders in digital interfaces (social groups, causes, digital avatars, artificial agents etc.); and expanding the scope of empathy research beyond preliminary perspective-taking. The workshop, structured as a combination of author panels, expert discussion, and interactive activities, provides the ideal venue to foster a critical discussion on the nature of the suitability of empathy in digital design, especially in the rapidly approaching context of its role in post-humanist HCI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {467},\nnumpages = {7},\nkeywords = {assessment of empathy, attributes of empathy, collaboration, empathy, empathy-centric design, ethics of empathy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636304,\nauthor = {Villa, Steeven and Welsch, Robin and Denisova, Alena and Kosch, Thomas},\ntitle = {Evaluating Interactive AI: Understanding and Controlling Placebo Effects in Human-AI Interaction},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636304},\ndoi = {10.1145/3613905.3636304},\nabstract = {In the medical field, patients often experience tangible benefits from treatments they expect will improve their condition, even if the treatment has no mechanism of effect. This phenomenon often obscuring scientific evaluation of human treatment is termed the \"placebo effect.\" Latest research in human-computer interaction has shown that using cutting-edge technologies similarly raises expectations of improvement, culminating in placebo effects that undermine evaluation efforts for user studies. This workshop delves into the role of placebo effects in human-computer interaction for cutting-edge technologies such as artificial intelligence, its influence as a confounding factor in user studies, and identifies methods that researchers can adopt to reduce its impact on study findings. By the end of this workshop, attendees will be equipped to incorporate placebo control measures in their experimental designs.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {468},\nnumpages = {4},\nkeywords = {AI, Adaptive Interface, Evaluation, Expectation, Placebo, User Studies},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636309,\nauthor = {Panicker, Aswati and Nurain, Novia and Ibrahim, Zaidat and Wang, Chun-Han (Ariel) and Ha, Seung Wan and Kaziunas, Elizabeth and Wolters, Maria K and Chung, Chia-Fang},\ntitle = {Forms of Fraudulence in Human-Centered Design: Collective Strategies and Future Agenda for Qualitative HCI Research},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636309},\ndoi = {10.1145/3613905.3636309},\nabstract = {New technical forms of deception–including AI deepfakes and unethical uses of ChatGTP–have gained attention in the wider research community and media. There has also been an increase in the coordinated social activities of bad actors posing as legitimate human research participants. People, for example, sign up for online HCI studies by misrepresenting their identities and experiences. This workshop explores what counts as \"fraud\" in the rapidly changing sociotechnical landscape of qualitative HCI research sites, and how might our community better understand (and strategically handle) new forms of fraudulence in human-centered design. Researchers across academia and industry are invited to participate in this discourse, share their personal experiences, explore potential strategies to combat fraudulence and reflect critically on the efficacies and shortcomings of such strategies. Outcomes of this workshop include working towards better guidelines, forming a community of researchers to support those impacted by fraudulence, and collaboratively defining a research agenda based on workshop discussions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {469},\nnumpages = {5},\nkeywords = {data integrity, deception, ethics, fraudulence, human research participants, online studies, qualitative research},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636294,\nauthor = {Muller, Michael and Kantosalo, Anna and Maher, Mary Lou and Martin, Charles Patrick and Walsh, Greg},\ntitle = {GenAICHI 2024: Generative AI and HCI at CHI 2024},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636294},\ndoi = {10.1145/3613905.3636294},\nabstract = {This workshop applies human centered themes to a new and powerful technology, generative artificial intelligence (AI), and - among other approaches - particularly to Large Language Models (LLMs) and Foundation Models (FMs). Unlike AI systems that produce decisions or descriptions, generative AI systems can produce new and creative content that can include images, texts, music, video, code, and other forms of design. The results are often similar to results produced by humans. However, it is not yet clear how humans make sense of generative AI algorithms or their outcomes. It is also not yet clear how humans can control and more generally, interact with, these powerful capabilities in ethical ways. Finally, it is not clear what kinds of collaboration patterns will emerge when creative humans and creative technologies work together. Following successful workshops in 2022 and 2023, we convene the interdisciplinary research domain of generative AI and HCI. Participation in this invitational workshop is open to seasoned scholars and early career researchers. We solicit descriptions of completed projects, works-in-progress, and provocations. Together we will develop theories and practices in this intriguing new domain.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {470},\nnumpages = {7},\nkeywords = {Bias, Design, Generative AI, Uncertainty.},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636315,\nauthor = {Hua, Yiqing and Niu, Shuo and Cai, Jie and Chilton, Lydia B and Heuer, Hendrik and Wohn, Donghee Yvette},\ntitle = {Generative AI in User-Generated Content},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636315},\ndoi = {10.1145/3613905.3636315},\nabstract = {Generative AI (Gen-AI) is rapidly changing the landscape of User-Generated Content (UGC) on social media. AI tools for generating text, images, and videos, such as Large-Language Models (LLM), image generation AI, AI-powered video material tools, and deep fake technologies, are accelerating creators in obtaining content ideas, drafting outlines, and streamlining creative workflows. The capabilities of Gen-AI could introduce new opportunities to lower the bar and accelerate the pace of content creation for grassroots creators, thereby expanding the volume of AI-generated UGC on social media. However, we lack the necessary understanding of how the wide deployment of such technologies will impact the social media ecosystem. The introduction of Gen-AI can lead to both opportunities and potential challenges among different creator communities, requiring collaboration from both academia and industry. This workshop seeks to bring together experts working on relevant topics of Gen-AI and UGC, to roadmap research on important issues boldly and responsibly.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {471},\nnumpages = {7},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636285,\nauthor = {Iglar, Alyssa and Simkute, Auste and Sellen, Abigail and Chignell, Mark},\ntitle = {Getting Back Together: HCI and Human Factors Joining Forces to Meet the AI Interaction Challenge},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636285},\ndoi = {10.1145/3613905.3636285},\nabstract = {The Human Factors Society and ACM SIGCHI jointly organized the first CHI conference in 1983, but during the remainder of the 1980s, Human-Computer Interaction (HCI) and Human Factors Engineering (HFE) increasingly diverged. The focus of HCI shifted from exploring systems for routinized activities of trained personnel, to a more general use of technology. HCI became predominantly design-oriented, concentrating on usability and user experience, moving further from HFE principles. However, the rapid growth of Artificial Intelligence (AI) applications posed unique and urgent challenges that call for a reestablishment of the connection between the two disciplines. We argue that by working as a team, HCI and HFE could more effectively address AI-posed challenges. We invite HCI and HFE researchers to take part in a full-day interactive hybrid workshop. With this workshop, we aim to initiate a collaboration between HCI and HFE and set a clear plan forward for a more united future.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {472},\nnumpages = {5},\nkeywords = {Artificial Intelligence (AI), Cognitive Science, Human Factors Engineering (HFE), Human-Computer Interaction (HCI), human augmentation, human-AI interaction, human-centered computing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636295,\nauthor = {Knowles, Bran and Singh, Aneesha and Ambe, Aloha Hufana and Brewer, Robin N. and Lazar, Amanda and Petrie, Helen and Vines, John and Waycott, Jenny},\ntitle = {HCI and Aging: New Directions, New Principles},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636295},\ndoi = {10.1145/3613905.3636295},\nabstract = {Concerns regarding the impacts of stereotyped, deficit-based, and problem-oriented approaches to older adult users have propelled HCI to explore new understandings and ways of approaching aging as a subject in recent years. Meanwhile, older adults’ relationships with digital technologies are also evolving, driven both by technological advancements and the destabilizing experience of the global pandemic. Now is an important time to take stock of these changes and their significance to the field of HCI and Aging. This workshop attends, therefore, to the need for collective reflection on where the field is now, how we got here, and where it is heading. In addition to highlighting emerging areas requiring research attention, the workshop will produce a snapshot in time to compare with several years hence as the field continues to evolve. The second part of the workshop responds to the need for a clear alternative to deficit based approaches to designing technologies for older adult users. We will pool the collective wisdom of the HCI and Aging community to generate a set of principles to guide research and development toward maximization of benefit and minimization of harm to older adult users/stakeholders.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {473},\nnumpages = {5},\nkeywords = {HCI, Older adults, ageing, aging, bias, care, co-design, harm, pandemic},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636320,\nauthor = {Krukar, Jakub and Dalton, Ruth and Hoelscher, Christoph and Dalton, Nick Sheep and Veddeler, Christian and Wiberg, Mikael},\ntitle = {HabiTech: Inhabiting Buildings, Data \\& Technology},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636320},\ndoi = {10.1145/3613905.3636320},\nabstract = {As larger parts of our lives are determined in the digital realm, it is critical to reflect on how democratic values can be preserved and cultivated by technology. At the city-scale, this is studied in the field of ‘digital civics’; however, there seems to be no corresponding focus at the level of buildings/building inhabitants. The majority of our lives are spent indoors and therefore the impact that ‘indoor digital civics’ may have, might exceed that of city-scale, digital civics. The digitization of building design and building management creates an opportunity to better identify, protect, and cultivate civic values that, until now, were centralized in the hands of building designers and building owners. By bringing together leading architecture/HCI academics and commercial stakeholders, this workshop builds on previous workshops at CHI. The workshop will provide a forum where a new agenda for research in ‘HabiTech’ can be defined and new research collaborations formed.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {474},\nnumpages = {5},\nkeywords = {Digital technologies and inhabitant-driven design, building activism, technology enabled inhabitation, building users, privacy, user data, user voice},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636303,\nauthor = {Joshi, Bibhushan Raj and Kollannur, Sandeep Zechariah George and Mishra, Anchit and Nguyen, Tommy and Schneider, Oliver},\ntitle = {Haptic Playground: Empowering Inclusive Haptic Design for Everyone},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636303},\ndoi = {10.1145/3613905.3636303},\nabstract = {While haptic technology is rapidly maturing, training for haptics is in its infancy. Disciplinary siloing has contributed to fast but fragmented growth of the haptics industry; graduate courses mainly exist for individual STEM fields, such as device development for mechanical engineers or study design for psychologists. Despite increasing broad interest to use haptics, many potential researchers and practitioners face barriers to learning how to design and study haptics, especially when their background is outside of STEM fields. This one-day workshop will bring together both expert and new or aspiring hapticians to work together to break down disciplinary silos. Expert hapticians with expertise in design justice and haptics community development will give invited talks to frame the discussion. Attendees will work with two interactive tangible tools to design haptic sensations, then reflect on their process, challenges faced, and successful strategies. By adhering to the principles of inclusive design during the process, we aim to render haptic design accessible to a wider audience, recognizing and respecting the unique design needs of each individual. The result will be a more comprehensive understanding of tangible tools’ crucial role in the haptic technology design process, while offering vital insights on inclusive design, ultimately supporting further development of a multidisciplinary, diverse practice of haptic design and research.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {475},\nnumpages = {5},\nkeywords = {collaboration, haptic design, inclusive design, tangible tools},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636302,\nauthor = {Xiao, Ziang and Deng, Wesley Hanwen and Lam, Michelle S. and Eslami, Motahhare and Kim, Juho and Lee, Mina and Liao, Q. Vera},\ntitle = {Human-Centered Evaluation and Auditing of Language Models},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636302},\ndoi = {10.1145/3613905.3636302},\nabstract = {The recent advancements in Large Language Models (LLMs) have significantly impacted numerous, and will impact more, real-world applications. However, these models also pose significant risks to individuals and society. To mitigate these issues and guide future model development, responsible evaluation and auditing of LLMs are essential. This workshop aims to address the current “evaluation crisis” in LLM research and practice by bringing together HCI and AI researchers and practitioners to rethink LLM evaluation and auditing from a human-centered perspective. The workshop will explore topics around understanding stakeholders’ needs and goals with evaluation and auditing LLMs, establishing human-centered evaluation and auditing methods, developing tools and resources to support these methods, building community and fostering collaboration. By soliciting papers, organizing invited keynote and panel, and facilitating group discussions, this workshop aims to develop a future research agenda for addressing the challenges in LLM evaluation and auditing.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {476},\nnumpages = {6},\nkeywords = {Audit, Evaluation, Generative AI, Large Language Models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636311,\nauthor = {Ehsan, Upol and Watkins, Elizabeth A and Wintersberger, Philipp and Manger, Carina and Kim, Sunnie S. Y. and Van Berkel, Niels and Riener, Andreas and Riedl, Mark O},\ntitle = {Human-Centered Explainable AI (HCXAI): Reloading Explainability in the Era of Large Language Models (LLMs)},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636311},\ndoi = {10.1145/3613905.3636311},\nabstract = {Human-centered XAI (HCXAI) advocates that algorithmic transparency alone is not sufficient for making AI explainable. Explainability of AI is more than just “opening” the black box — who opens it matters just as much, if not more, as the ways of opening it. In the era of Large Language Models (LLMs), is “opening the black box” still a realistic goal for XAI? In this fourth CHI workshop on Human-centered XAI (HCXAI), we build on the maturation through the previous three installments to craft the coming-of-age story of HCXAI in the era of Large Language Models (LLMs). We aim towards actionable interventions that recognize both affordances and pitfalls of XAI. The goal of the fourth installment is to question how XAI assumptions fare in the era of LLMs and examine how human-centered perspectives can be operationalized at the conceptual, methodological, and technical levels. Encouraging holistic (historical, sociological, and technical) approaches, we emphasize “operationalizing.” We seek actionable analysis frameworks, concrete design guidelines, transferable evaluation methods, and principles for accountability.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {477},\nnumpages = {6},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636318,\nauthor = {Harden, Jesse and Wang, April Yi and Faust, Rebecca and Isaacs, Katherine E. and Kirshenbaum, Nurit and Wenskovitch, John and Zhao, Jian and North, Chris},\ntitle = {Human-Notebook Interactions: The CHI of Computational Notebooks},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636318},\ndoi = {10.1145/3613905.3636318},\nabstract = {The overall goal of this workshop is to bring together researchers from across the CHI community to share their knowledge and build collaborations at the intersection of computational notebook and HCI research, focusing on both the effective design and effective use of interfaces and interactions within computational notebook environments. This includes innovating upon the computational notebook metaphor, designing new tools, interfaces, and interactions for use with computational notebooks, and more. We aim to pull expertise from across all fields of CHI to deliver novel research and generate open discussion about the current state of computational notebooks, how it can be improved from an HCI standpoint, and how these potential improvements can direct future research. To achieve this goal, we propose a full-day, hybrid workshop with discussions of challenges and opportunities, paper and demo presentations, lightning talks, and a keynote. Participants in this workshop will exchange ideas and help define a roadmap for future research at the intersection of HCI and computational notebook design.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {478},\nnumpages = {6},\nkeywords = {Computational Narratives, Computational Notebooks, Data Analysis, Data Science, Human-Computer Interaction, Interaction Design, Interface Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636301,\nauthor = {Aubin Le Qu\\'{e}r\\'{e}, Marianne and Schroeder, Hope and Randazzo, Casey and Gao, Jie and Epstein, Ziv and Perrault, Simon Tangi and Mimno, David and Barkhuus, Louise and Li, Hanlin},\ntitle = {LLMs as Research Tools: Applications and Evaluations in HCI Data Work},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636301},\ndoi = {10.1145/3613905.3636301},\nabstract = {Large language models (LLMs) stand to reshape traditional methods of working with data. While LLMs unlock new and potentially useful ways of interfacing with data, their use in research processes requires methodological and critical evaluation. In this workshop, we seek to gather a community of HCI researchers interested in navigating the responsible integration of LLMs into data work: data collection, processing, and analysis. We aim to create an understanding of how LLMs are being used to work with data in HCI research, and document the early challenges and concerns that have arisen. Together, we will outline a research agenda on using LLMs as research tools to work with data by defining the open empirical and ethical evaluation questions and thus contribute to setting norms in the community. We believe CHI to be the ideal place to address these questions due to the methodologically diverse researcher attendees, the prevalence of HCI research on human interaction with new computing and data paradigms, and the community’s sense of ethics and care. Insights from this forum can contribute to other research communities grappling with related questions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {479},\nnumpages = {7},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636291,\nauthor = {Matviienko, Andrii and Boot, Mario and L\\\"{o}cken, Andreas and Pfleging, Bastian and L\\\"{o}chtefeld, Markus and Von Sawitzky, Tamara and Savino, Gian-Luca and Sturdee, Miriam and Andres, Josh and Boyer, Kristy Elizabeth and Brewster, Stephen Anthony and Mueller, Florian ‘Floyd’},\ntitle = {Learning from Cycling: Discovering Lessons Learned from CyclingHCI},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636291},\ndoi = {10.1145/3613905.3636291},\nabstract = {Cycling plays an essential role in sustainable mobility, health, and socializing. This workshop aims to collect and discuss the lessons learned from Cycling Human-Computer Interaction (CyclingHCI). For this, we will gather researchers and experts in the field to discuss what we learned from designing, building, and evaluating CyclingHCI systems. We will start the workshop with three lessons learned from CyclingHCI defined by the organizers and their experience in the field, which include (1) a lack of theories, tools, and perspectives, (2) knowledge about designing for safety and inclusive cycling, and (3) evaluation methods and environments. Taken together, with this work, we aim to promote interactive technology to get more people cycling, profiting from the many associated benefits.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {480},\nnumpages = {5},\nkeywords = {bicycles, cycling, lessons learned, urban interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636290,\nauthor = {Cagiltay, Bengisu and Ho, Hui-Ru and Sun, Kaiwen and Su, Zhaoyuan and Wu, Yuxing and Richards, Olivia K. and Jin, Qiao and Yu, Junnan and Fails, Jerry Alan and Yip, Jason and Forlizzi, Jodi},\ntitle = {Methods for Family-Centered Design: Bridging the Gap Between Research and Practice},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636290},\ndoi = {10.1145/3613905.3636290},\nabstract = {Technology is pervasive in family life. Family-centered design can enable the creation of technological solutions that align with the diverse needs of and dynamics within families. Yet, designing meaningful interactive technologies that are useful for and desired by families remains a complex and evolving challenge. Furthermore, there are limited resources in the HCI community examining theoretical, methodological, and practical processes for designing and testing technology supporting family life (e.g., interactions among parents, children, siblings, older adults). This workshop aims to bridge this gap by bringing together researchers and practitioners from interdisciplinary areas to discuss practical approaches in applying effective methods, theories, and tools for designing technology for and with families. The main goal of this workshop is to collaborate on creating a knowledge base for family-centered design. The workshop will aim to provide valuable opportunities for researchers and practitioners to grow a community, exchange insights, and share best practices.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {481},\nnumpages = {6},\nkeywords = {child-computer interaction, family-centered design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636310,\nauthor = {Gray, Colin M. and Gunawan, Johanna T. and Sch\\\"{a}fer, Ren\\'{e} and Bielova, Nataliia and Sanchez Chamorro, Lorena and Seaborn, Katie and Mildner, Thomas and Sandhaus, Hauke},\ntitle = {Mobilizing Research and Regulatory Action on Dark Patterns and Deceptive Design Practices},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636310},\ndoi = {10.1145/3613905.3636310},\nabstract = {Deceptive, manipulative, and coercive practices are deeply embedded in our digital experiences, impacting our ability to make informed choices and undermining our agency and autonomy. These design practices—collectively known as “dark patterns” or “deceptive patterns”—are increasingly under legal scrutiny and sanctions, largely due to the efforts of human-computer interaction scholars that have conducted pioneering research relating to dark patterns types, definitions, and harms. In this workshop, we continue building this scholarly community with a focus on organizing for action. Our aims include: (i) building capacity around specific research questions relating to methodologies for detection; (ii) characterization of harms; and (iii) creating effective countermeasures. Through the outcomes of the workshop, we will connect our scholarship to the legal, design, and regulatory communities to inform further legislative and legal action.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {482},\nnumpages = {6},\nkeywords = {dark patterns, deceptive design, ethics, manipulative user interfaces, regulation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636288,\nauthor = {Freeman, Guo and Frommel, Julian and Mandryk, Regan L and Gugenheimer, Jan and Li, Lingyuan and Johnson, Daniel},\ntitle = {Novel Approaches for Understanding and Mitigating Emerging New Harms in Immersive and Embodied Virtual Spaces: A Workshop at CHI 2024},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636288},\ndoi = {10.1145/3613905.3636288},\nabstract = {As online spaces facilitate increasingly immersive and embodied experiences, concerns about how these emerging spaces may amplify and extend existing online harms and even lead to new harms, and how HCI researchers and developers can work to mitigate such harms also grow. Typical examples of these new and understudied forms of harm range from embodied harassment in social Virtual Reality (VR) to racist Zoombombing, new AI-powered online attacks such as hate raids on Twitch, and harmful virtual world design to manipulate users. This workshop aims to bring together a set of interdisciplinary researchers and practitioners from HCI and adjacent fields to explore further how these new harms continue to shape the current research discourse of online safety, cybersecurity, and immersive and embodied interactions in HCI, and to collectively identify what new technologies and mechanisms can be envisioned, designed, and implemented to better understand and mitigate these harms.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {483},\nnumpages = {7},\nkeywords = {artificial intelligence, embodiment, harassment, harm mitigation, immersive virtual worlds, online harm, online safety, toxicity},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636284,\nauthor = {Haliburton, Luke and Damen, Ida and Lallemand, Carine and Ahtinen, Aino and Niess, Jasmin and Wo\\'{z}niak, Pawe\\l{} W.},\ntitle = {Office Wellbeing by Design: Don’t Stand for Anything Less},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636284},\ndoi = {10.1145/3613905.3636284},\nabstract = {The modern workplace has been optimized towards increasing productivity, often at the cost of long-term worker wellbeing. This systemic issue has been acknowledged in both research and practice, but has not yet been solved. There is a notable lack of practical methods of incorporating physical activity and other wellbeing practices into productive workplace activities. We see a gap between research endeavors and industry practice that motivates a call for increased collaboration between the two parties. In response, our workshop aims to bring together researchers and practitioners to work together in identifying a set of grand challenges for the field. Through collaboration, we will create a concrete research agenda to create a resilient future workplace that explicitly incorporates holistic worker wellbeing.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {484},\nnumpages = {7},\nkeywords = {Future of Work, Office Workers, Physical Activity, Wellbeing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636286,\nauthor = {Chiossi, Francesco and Stepanova, Ekaterina R. and Tag, Benjamin and Perusquia-Hernandez, Monica and Kitson, Alexandra and Dey, Arindam and Mayer, Sven and El Ali, Abdallah},\ntitle = {PhysioCHI: Towards Best Practices for Integrating Physiological Signals in HCI},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636286},\ndoi = {10.1145/3613905.3636286},\nabstract = {Recently, we saw a trend toward using physiological signals in interactive systems. These signals, offering deep insights into users’ internal states and health, herald a new era for HCI. However, as this is an interdisciplinary approach, many challenges arise for HCI researchers, such as merging diverse disciplines, from understanding physiological functions to design expertise. Also, isolated research endeavors limit the scope and reach of findings. This workshop aims to bridge these gaps, fostering cross-disciplinary discussions on usability, open science, and ethics tied to physiological data in HCI. In this workshop, we will discuss best practices for embedding physiological signals in interactive systems. Through collective efforts, we seek to craft a guiding document for best practices in physiological HCI research, ensuring that it remains grounded in shared principles and methodologies as the field advances.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {485},\nnumpages = {7},\nkeywords = {Affective Computing, Ethics, Open Science, Physiological Computing, Physiological Signals, Replicability, Reproducibility, Transparency},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636292,\nauthor = {Sharma, Vishal and Tuli, Anupriya and Wani, Asra Sakeen and Mohan, Anjali Karol and Nardi, Bonnie and Hassenzahl, Marc and Vigil-Hayes, Morgan and Jensen, Rikke Hagensby and Bardzell, Shaowen and Kumar, Neha},\ntitle = {Post-growth HCI: Co-Envisioning HCI Beyond Economic Growth},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636292},\ndoi = {10.1145/3613905.3636292},\nabstract = {Human–Computer Interaction (HCI) makes a significant contribution to economic growth; it is crucial to the market success of digital technologies, including digital services, platforms, and devices, which drive the economic engine. Economic growth, however, has a number of social and environmental consequences. Some HCI researchers have problematized the field’s engagement with growth, suggesting the post-growth philosophy as an alternative. Post-growth focuses on improving the quality of life centered on cooperation, social solidarity, care, justice, sharing, localized development, and other values. Orienting to post-growth could be instrumental in leading the HCI community beyond growth politics by envisioning, designing, and implementing technologies toward building a more sustainable, just, and humane society. This workshop aims to bring together HCI researchers, designers, practitioners, educators, and students to critically reimagine ways to embrace post-growth in and through HCI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {486},\nnumpages = {7},\nkeywords = {Degrowth, Development, HCI, HCI4D, Political Economy, Post-development, Post-growth, SHCI, Steady State, Sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636322,\nauthor = {Russell, Daniel M. and Koesten, Laura and Kittur, Aniket and Goyal, Nitesh and Liu, Michael Xieyang},\ntitle = {Sensemaking: What is it today?},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636322},\ndoi = {10.1145/3613905.3636322},\nabstract = {Sensemaking is a common activity in the analysis of a large or complex amount of information. It has also been an active area of research for at least 25 years. Such an active area of HCI research over a quarter century asks fundamental questions about how do people come to understand difficult sets of information? How do they find the data? How do their sensemaking frames help (or hinder) their ability to understand? The information workplace is increasing dominated by high velocity, high volume, complex information streams. At the same time, understanding how sensemaking operates has become an urgent need in an era of increasingly unreliable news and information sources. While there has been a huge amount of work in this space, the research involved is scattered over a number of different domains with differing approaches. This workshop will focus on the most recent work in sensemaking, the activities, technologies and behaviors that people do when making sense of their complex information spaces. We will also attempt to synthesize sensemaking work over the past several years. In the second part of the workshop we will synthesize a cross- disciplinary view of how sensemaking works in people, along with the human behaviors, biases, proclivities, and technologies required to support it.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {487},\nnumpages = {5},\nkeywords = {generative ai, sensemaking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636306,\nauthor = {Richardson, Mike and Cork, Alicia G and Stanton Fraser, Dana\\\"{e} and Proulx, Michael J and Pan, Xueni and Krau\\ss{}, Veronika and Khamis, Mohamed and Lukosch, Heide},\ntitle = {Shaping The Future: Developing Principles for Policy Recommendations for Responsible Innovation in Virtual Worlds},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636306},\ndoi = {10.1145/3613905.3636306},\nabstract = {As Extended Reality (XR) technologies continue to evolve at a rapid pace, they hold the promise of transforming the way we interact both with digital information and the physical world. Whilst Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR) technologies offer unbridled opportunities for social connections, productivity, and play, these rapid technological advancements also pose critical challenges to ethics, privacy, accessibility, and safety. At present, there is little policy documentation that directly addresses the novel affordances posed by XR technologies, leading to a ‘policy void’ in this space. Having clear and effective policy frameworks prior to the widespread adoption of technology encourages and enables responsible and ethical innovation of XR technologies. This workshop is therefore dedicated to developing forward-thinking principles to guide policy recommendations that address potential future vulnerabilities posed by the widespread adoption of XR technologies whilst simultaneously encouraging the responsible innovation of new advancements within XR. To ensure these policy recommendations promote responsible innovation, the workshop will assemble multidisciplinary academics, industry developers and international policymakers. Our goal is to ensure that all perspectives are considered such that we can collaboratively chart a responsible and sustainable course for the XR landscape.CCS CONCEPTS • Human-centered computing → Human computer interaction (HCI); Virtual reality • Security and privacy → Usability in security and privacy • Social and professional topics → Computing/technology policy},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {488},\nnumpages = {6},\nkeywords = {Extended reality, policy recommendations, responsible innovation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636300,\nauthor = {Song, Katherine W and Bell, Fiona and Deshpande, Himani and Mandel, Ilan and Wun, Tiffany and Alistar, Mirela and Buechley, Leah and Ju, Wendy and Kim, Jeeeun and Paulos, Eric and Sabie, Samar and Wakkary, Ron},\ntitle = {Sustainable Unmaking: Designing for Biodegradation, Decay, and Disassembly},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636300},\ndoi = {10.1145/3613905.3636300},\nabstract = {Unmaking is a counterpart to making and creating new things that has emerged as a concept of interest in diverse parts of the HCI community. Unmaking has been posed as an ally to sustainability, encouraging designers to foreground issues relating to reuse, repair, obsolescence, degradation, and decay early in their design process. As a follow-up to the 2022 Unmaking@CHI workshop, this workshop will bring together researchers and practitioners interested in unmaking as it relates to sustainability and will focus primarily on exploring the role of unmaking in material practices, drawing upon the growing body of unmaking theory to explore future research opportunities for designing physical things with sustainable materials that are transient, degradable, and intentionally unmake-able. In addition to considering the pragmatics of what and how to unmake, we seek to articulate the relationships among unmaking and other related emerging themes and sustainable material practices – including biodegradation, designing with more-than-human agencies, reuse, and repair – and propose guidelines for designing for the unmaking of physical artifacts that are sustainable, equitable, and respectful of all entities involved.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {489},\nnumpages = {7},\nkeywords = {HCI, critical design, design, design methods, digital fabrication, making, sustainability, unmaking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636314,\nauthor = {Mohanty, Vikram and Fang, Jingchao and Lee-Kan, Song Mi and Alavi, Hamed S. and Salas, Joaquin and Patterson, Genevi\\`{e}ve and Churchill, Elizabeth F. and Wu, Charlene C. and Shamma, David A.},\ntitle = {Sustaining Scalable Sustainability: Human-Centered Green Technology for Community-wide Carbon Reduction},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636314},\ndoi = {10.1145/3613905.3636314},\nabstract = {Escalating global CO2 emissions highlights the immediate need for scalable sustainable practices. Corporate and policy roles aside, there’s a need for carbon-neutrality-based systems and practices to bridge the disconnect between actions and the perceived impact on the environment. This one-day workshop focuses on individuals and communities, advocating for human-centered tools to bridge this awareness-action gap. While Human-AI Interaction (HAI), cognitive science theories, and social computing tools have shown promise in various domains, their potential remains largely unexplored in the context of sustainability. This workshop aims to delve into these avenues for crafting tractable systems for effective, contextually relevant interventions and driving sustainable behaviors. By engaging multidisciplinary researchers, we aim to intertwine local insights with behavioral theory and technology, fostering intrinsic carbon literacy and a sustainability ethos, ensuring lasting and scalable impacts.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {490},\nnumpages = {6},\nkeywords = {CO2, Carbon Literacy, Carbon Neutrality, Communities, Design Interventions, Eco-Feedback, Human-Centered AI, Human-Centered Design, Persuasive Technology, Sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636296,\nauthor = {Wilson, Max L and Shaban, Jwan and Maior, Horia A. and Schneegass, Christina and Cox, Anna L},\ntitle = {The CHI’24 Workshop on the Future of Cognitive Personal Informatics},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636296},\ndoi = {10.1145/3613905.3636296},\nabstract = {While Human-Computer Interaction (HCI) has contributed to demonstrating that physiological measures can be used to detect cognitive changes, engineering and machine learning will bring these to application in consumer wearable technology. For HCI, many open questions remain, such as: What happens when this becomes a cognitive form of personal informatics? What goals do we have for our daily cognitive activity? How should such a complex concept be conveyed to users to be useful in their everyday lives? How can we mitigate potential ethical concerns? This is different to designing BCI interactions; we are concerned with understanding how people will live with consumer neurotechnology. This workshop will directly address the future of Cognitive Personal Informatics (CPI), by bringing together design, BCI and physiological data, ethics, and personal informatics researchers to discuss and set the research agenda in this inevitable future.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {491},\nnumpages = {6},\nkeywords = {digital health, neurotechnology, personal informatics, well-being, work-life balance},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636305,\nauthor = {Atabey, Ay\\c{c}a and Wang, Ge and Johnston, Samantha-Kaye and Lin, Grace C. and Wilson, Cara and Urquhart, Lachlan D and Zhao, Jun},\ntitle = {The Second Workshop on Child-Centered AI Design (CCAI)},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636305},\ndoi = {10.1145/3613905.3636305},\nabstract = {AI-powered solutions are increasingly woven into the fabric of children’s digital worlds. They’re found in interactive toys, home automation systems, everyday apps, and various online services that young users engage with. As we look ahead, it’s almost certain that the prevalence of AI in tools and platforms designed for kids will grow, given AI’s ability to offer rich, tailored, and dynamic experiences. However, the nuances of how these AI-centric platforms cater to children and how they can be optimized to meet the unique needs of younger users remain largely underexplored. Building on the momentum from our inaugural CCAI workshop at CHI 2023, our aspirations for this year’s workshop include: (1) deepening the discourse on the essence of AI that prioritizes children, (2) focusing on actionable ethical strategies to operationalise child-centered AI design principles into practice, and (3) cultivating an ever-growing collective of professionals passionate about the future of child-focused AI innovations.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {492},\nnumpages = {6},\nkeywords = {Child-Centred AI design, child-computer interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636308,\nauthor = {Wang, Qiaosi and Walsh, Sarah and Si, Mei and Kephart, Jeffrey and Weisz, Justin D. and Goel, Ashok K.},\ntitle = {Theory of Mind in Human-AI Interaction},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636308},\ndoi = {10.1145/3613905.3636308},\nabstract = {Theory of Mind (ToM), humans’ capability of attributing mental states such as intentions, goals, emotions, and beliefs to ourselves and others, has become a concept of great interest in human-AI interaction research. Given the fundamental role of ToM in human social interactions, many researchers have been working on methods and techniques to equip AI with an equivalent of human ToM capability to build highly socially intelligent AI. Another line of research on ToM in human-AI interaction seeks to understand people’s tendency to attribute mental states such as blame, emotions, and intentions to AI, along with the role that AI should play in the interaction (e.g. as a tool, partner, teacher, facilitator, and more) to align with peoples’ expectations and mental models. The goal of this line of work is to distill human-centered design implications to support the development of increasingly advanced AI systems. Together, these two research perspectives on ToM form an emerging paradigm of “Mutual Theory of Mind (MToM)” in human-AI interaction, where both the human and the AI each possess the ToM capability. This workshop aims to bring together different research perspectives on ToM in human-AI interaction by engaging with researchers from various disciplines including AI, HCI, Cognitive Science, Psychology, Robotics, and more to synthesize existing research perspectives, techniques, and knowledge on ToM in human-AI interaction, as well as envisioning and setting a research agenda for MToM in human-AI interaction.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {493},\nnumpages = {6},\nkeywords = {human-AI interaction, human-centered AI, mental model, mutual theory of mind, social intelligence, theory of mind},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636289,\nauthor = {Ge, Lily W. and Hedayati, Maryam and Cui, Yuan and Ding, Yiren and Bonilla, Karen and Joshi, Alark and Ottley, Alvitta and Bach, Benjamin and Kwon, Bum Chul and Rapp, David N. and Peck, Evan and Padilla, Lace M. and Correll, Michael and Borkin, Michelle A. and Harrison, Lane and Kay, Matthew},\ntitle = {Toward a More Comprehensive Understanding of Visualization Literacy},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636289},\ndoi = {10.1145/3613905.3636289},\nabstract = {Researchers have proposed many definitions of visualization literacy, targeting various aspects of the term. But we have yet to fully capture what it really means to be literate in visualizations, which has important downstream implications, such as how to effectively teach visualization skills to younger generations. We ran a meetup at IEEE VIS 2022 that attracted over 30 researchers in the field, who discussed aspects of visualization literacy such as how we measure it, how we can improve it, how it develops, and how it relates to other literacies. ACM CHI has a track record of attracting researchers from various fields such as visualization, learning sciences, and design, advancing research through both quantitative and qualitative approaches in and around HCI. For this year’s CHI, we propose to run a one-day workshop with the goal of further developing actionable research agendas to more comprehensively define, understand, and improve visualization literacy. By continuing critical discussions with diverse perspectives from the CHI community, we can deepen investigations of visualization literacy through multiple lenses, such as measurement, interventions, and pedagogy.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {494},\nnumpages = {7},\nkeywords = {Group activities, Panel discussions, Visualization literacy, Workshop},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636317,\nauthor = {Fitton, Dan and Read, Janet and Eriksson, Eva and Bonsignore, Elizabeth and Iivari, Netta and Hartikainen, Heidi and Dick, Rhona Anne},\ntitle = {Transformative Technologies for Children: Going beyond ‘Good’},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636317},\ndoi = {10.1145/3613905.3636317},\nabstract = {Against the backdrop of growing screen time, rising mental health issues, increasing loneliness, and general ill effects from technology use, it is time for the CHI community to consider how technology for children can be better than ‘good’. There are many examples of good technologies across research and commercial products, for technology to be more than ‘good’ it needs to have a transformative effect on children’s lives that lasts beyond a monetary positive experience. Such technology could, for example, build resilience, encourage compassion, promote inclusive behaviors, and improve overall happiness. This workshop will explore what better than ‘good’ technology may look like and create a manifesto for the CHI community to support Transformative Technologies for children in our work.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {495},\nnumpages = {5},\nkeywords = {CCI, Child-computer Interaction, transformative technologies},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636319,\nauthor = {Ashktorab, Zahra and Bansal, Gagan and Bu\\c{c}inca, Zana and Holstein, Kenneth and Hullman, Jessica and Smith-Renner, Alison Marie and Wu, Tongshuang and Zhang, Wenjuan},\ntitle = {Trust and Reliance in Evolving Human-AI Workflows (TREW)},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636319},\ndoi = {10.1145/3613905.3636319},\nabstract = {State-of-the-art AIs, including Large Language Models (LLMs) like GPT-4, now possess capabilities once unique to humans, such as coding, idea generation, and planning. Advanced AIs are now integrated into a plethora of platforms and tools, including GitHub Copilot, Bing Chat, Bard, ChatGPT, and Advanced Data Analytics. In contrast to conventional, specialized AIs that typically offer singular solutions, these LLMs redefine human-AI dynamics, with a growing trend toward humans viewing them as collaborative counterparts. This shift leads to enhanced dialogues, negotiations, and task delegation between humans and AI. With these rapid advancements, the nature of human roles in the AI collaboration spectrum is evolving. While our previous workshops CHI TRAIT 2022 and 2023 delved into the trust and reliance concerning traditional AIs, the pressing question now is: how should we measure trust and reliance with these emerging AI technologies? As these systems witness widespread adoption, there’s also a need to assess their impact on human skill development. Does AI assistance amplify human skill progression, or does it inadvertently inhibit it? Considering the multifaceted challenges and solutions that revolve around human-AI interactions, we invite experts from diverse fields, including HCI, AI, ML, psychology, and social science. Our aim is to bridge communication gaps and facilitate rich collaborations across these domains.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {496},\nnumpages = {6},\nkeywords = {human-centered artificial intelligence, reliance, trust, uncertainty},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636283,\nauthor = {Silva, Rafael M.L. and Cardenas Gasca, Ana Maria and Fisher, Joshua A and Principe Cruz, Erica and Jauregui, Cinthya and Lueck, Amy and Liu, Fannie and Monroy-Hern\\'{a}ndez, Andr\\'{e}s and Lukoff, Kai},\ntitle = {With or Without Permission: Site-Specific Augmented Reality for Social Justice},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636283},\ndoi = {10.1145/3613905.3636283},\nabstract = {Movements for social change are often tied to a particular locale. This makes Augmented Reality (AR), which changes how people perceive their surroundings, a promising technology for social justice. Site-specific AR empowers activists to re-tell the story of a place, with or without permission of its owner. It has been used, for example, to reveal hidden histories, re-imagine problematic monuments, and celebrate minority cultures. However, challenges remain concerning technological ownership and accessibility, scalability, sustainability, and navigating collaborations with marginalized communities and across disciplinary boundaries. This half-day workshop at CHI 2024 seeks to bring together an interdisciplinary group of activists, computer scientists, designers, media scholars, and more to identify opportunities and challenges across domains. To anchor the discussion, participants will each share one example of an artifact used in speculating, designing, and/or delivering site-specific AR experiences. This collection of artifacts will inaugurate an interactive database that can inspire a new wave of activists to leverage AR for social justice.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {497},\nnumpages = {7},\nkeywords = {augmented reality, design justice, social justice, spatial justice},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637131,\nauthor = {Chen, Mengyu and He, Shuang and Fu, Xinyi},\ntitle = {A Case Study Exploring the Applicability of Heuristic Evaluation in Smart Home Systems},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637131},\ndoi = {10.1145/3613905.3637131},\nabstract = {Smart home systems, integrating multiple devices and interaction methods, give rise to complex usability challenges. While heuristic evaluation has been a preferred method for evaluating usability across various domains, its applicability to smart home systems remains underexplored. This case study investigates the strengths and constraints of heuristic evaluation applied to smart home systems. Our results suggests that while heuristic evaluation effectively identifies GUI issues and guides expert solutions, it falls short in addressing technical and hardware intricacies unique to hybrid systems like smart homes. This study not only presents recommendations for enhancing heuristic evaluations in this evolving domains, but also offers practical insights specifically for the evaluation of hybrid systems.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {498},\nnumpages = {7},\nkeywords = {Heuristic evaluation, hybrid system, smart home, usability, user interface},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637121,\nauthor = {Ali, Naima Samreen and Agha, Zainab and Chatlani, Neeraj and Park, Jinkyung and Wisniewski, Pamela J.},\ntitle = {A Case Study on Facilitating a Long-Term Youth Advisory Board to Involve Youth in Adolescent Online Safety Research},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637121},\ndoi = {10.1145/3613905.3637121},\nabstract = {We worked with seven teens (aged 15-17) in a Youth Advisory Board program (YAB) for over a year to involve them in online safety research from reviewing online safety research protocols to co-designing online safety interventions that cater to their needs by teaching them essential UX design tools and techniques. Teens created storyboards, user personas, mind maps, and high-fidelity prototypes for their ideas regarding online safety and privacy features. Our case study outlines the overview, methodology we used, and lessons learned from the long journey with teens in the YAB program for online safety research. We provide heuristic guidelines for the research community that aim to build similar research programs for teens, including aligning long-term program goals with teens’ needs, ensuring equal participation from diverse teens, building trust, maintaining maximum engagement, and communicating the outcomes and impact of their contributions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {499},\nnumpages = {8},\nkeywords = {Asynchronous Research Community, adolescent online safety, privacy, social media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637120,\nauthor = {Whitelock-Wainwright, Emma and Rankin, David and Talic, Stella and Ga\\v{s}evi\\'{c}, Dragan},\ntitle = {A Mixed-Method Case Study: Medical Practitioner Sensemaking in the Context of Practice Analytics},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637120},\ndoi = {10.1145/3613905.3637120},\nabstract = {There is a large amount of data generated by the healthcare system that can be leveraged for secondary purposes. Here we present a case study, centred around medical practitioners, that explored how they interact with such data to inform their professional development and learning. We used a mixed-methods approach (interviews and surveys) to glean rich insights into their sensemaking and present considerations that can subsequently support this. These human-centred considerations include the use of “frames of reference’’, presentation of “cases of interest’’, significance of data attribution, factors that impact sensemaking (i.e. affect and prior experiences), and we advocate for iterative co-design. Alignment to such considerations would ensure the analytic tools are developed in a way that complement practitioners’ needs, and are meaningful to both their clinical practice and professional development.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {500},\nnumpages = {11},\nkeywords = {Case Study, Continuing Professional Development (CPD), Interview, Mixed-Method, Practice Analytics, Sensemaking, Survey},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637135,\nauthor = {Lu, Yuwen and Knearem, Tiffany and Dutta, Shona and Blass, Jamie and Kliman-Silver, Clara and Bentley, Frank},\ntitle = {AI Is Not Enough: A Hybrid Technical Approach to AI Adoption in UI Linting With Heuristics},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637135},\ndoi = {10.1145/3613905.3637135},\nabstract = {Design systems have become an industry standard for creating consistent, usable, and effective digital interfaces. However, detecting and correcting violations of design system guidelines, known as UI linting, is a major challenge. Manual UI linting is time-consuming and tedious, making it a prime candidate for automation. This paper presents a case study of adopting AI for UI linting. Through collaborative prototyping with UX designers, we analyzed the limitations of existing AI models and identified designers’ core needs and priorities in UI linting. With such knowledge, we designed a hybrid technical pipeline that combines the deterministic nature of heuristics with the flexibility of large language models. Our case study demonstrates that AI alone is not sufficient for practical adoption and highlights the importance of a deep understanding of AI capabilities and user-centered design approaches.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {501},\nnumpages = {7},\nkeywords = {UI linting, artificial intelligence, design systems, large language models, user interface (UI) design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637110,\nauthor = {Mozgai, Sharon A and Kaurloto, Cari and Winn, Jade G and Leeds, Andrew and Beland, Sarah and Sookiassian, Arman and Hartholt, Arno},\ntitle = {Accelerating Scoping Reviews: A Case Study in the User-Centered Design of an AI-Enabled Interdisciplinary Research Tool},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637110},\ndoi = {10.1145/3613905.3637110},\nabstract = {This case study presents the user-centered design process our multidisciplinary team of computer scientists, librarians and social scientists engaged in to develop a research tool utilized as part of a scoping review to explore the vast literature on virtual humans. Our process was guided by the Information Systems Research (ISR) Framework to define the parameters of this AI-enabled accessible tool: a semantically organized, interactive evidence map, clustered by salient topics and linked to Google Scholar to expedite the discovery of relevant resources. Specifically, we aimed to achieve several desiderata: (1) replicability, (2) objectivity, (3) automation \\& scalability, and (4) ease of discovery. Lessons learned include 1) how to apply the ISR Framework to benefit multidisciplinary collaboration, 2) user-centered design benefits from in-house, cross-discipline training, 3) interdisciplinary challenges benefit from multidisciplinary teams, and 4) intentional feedback loops remove the pressure from brainstorming and provide equal opportunity for multidisciplinary input throughout the development life cycle. To enable replicability, we provide full access to the dataset, the AI-enabled interactive evidence map, and source code.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {502},\nnumpages = {8},\nkeywords = {Scoping reviews, representation learning, user-centered design, virtual human},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637126,\nauthor = {Mironcika, Svetlana and Frens, Joep and Wensveen, Stephan},\ntitle = {Applying Feminist HCI to Ultra-Personalization: The Case of Electric Breast Pumps},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637126},\ndoi = {10.1145/3613905.3637126},\nabstract = {Next to digital products, also physical products can be personalized based on user data collected through digital services and produced by digital manufacturing. This approach is known as ultra-personalization. The ambition of ultra-personalization is to offer products that fit the individual needs of people who will use/wear them. While most of the examples of ultra-personalized products focus on anthropometric fit, in the case of breast pumps the unmet needs go beyond such fit. Being informed by feminist HCI we have conducted interviews with eleven lactating persons and two lactation specialists to gather accounts of embodied situated experiences with currently available electric breast pumps to understand how they fit and misfit users. We have identified five dimensions and ten subdimensions of personal holistic fit. We discuss that designing ultra-personalized products for these dimensions corresponds to feminist HCI values and qualities of pluralism, diversity, embodiment, and self-disclosure.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {503},\nnumpages = {10},\nkeywords = {Electric breast pumps, Feminist HCI qualities, Fit dimensions, Personal holistic fit, Ultra-personalization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637106,\nauthor = {Cai, Alice and Baradari, Aida and Baradari, Dunya and Chiaravalloti, Treyden and Paschall, Courtnie},\ntitle = {Augmentation Residency: An Immersive Live-Work HCI R&D Model},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637106},\ndoi = {10.1145/3613905.3637106},\nabstract = {The growing affordability of emerging consumer devices and accessibility of software-hardware prototyping represents an opportunity for a new era of personal fabrication of augmentation technologies. In this case study, we review the inaugural Augmentation Residency, a live-in creative technology residency that implemented a new approach to the collaborative development of human-computer interaction (HCI) systems. Over the course of ten weeks, eight students and young professionals from a wide range of disciplines lived and worked together to build and self-experiment with technologies that integrate extended reality (XR), wearables, brain-computer interfaces (BCI), and artificial intelligence (AI). Made possible by a residential community and interdisciplinary teams, this high-intensity model for HCI research and development (R&D) was structured to encourage experimentation with atypical human-centered design methods and artistic practices. We present our program design, organization, and management, and discuss challenges and suggestions for improvements, to inspire future residencies as vehicles for intensive HCI innovation.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {504},\nnumpages = {9},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637108,\nauthor = {Chiu, Chian-Jr and Luo, Xiao and DiSalvo, Betsy},\ntitle = {Balancing Expertise: Designing an Eviction Defense Web Tool with Legal Experts and Tenants},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637108},\ndoi = {10.1145/3613905.3637108},\nabstract = {This paper investigates a long-term collaboration between students and service organizations in a context that relies heavily on experts while engaging with a marginalized and gate-kept community. The project was motivated by the Atlanta Volunteer Lawyers Foundation (AVLF)’s initiative to address the eviction crisis in a southern U.S. city by leveraging the organizational expertise, the knowledge of computing and design graduate students, and engagement with those who have faced eviction. This paper chronicles the journey of a student team as they adapt to changing personnel and project objectives. It analyzes the distinctive challenges and opportunities encountered during this collaboration and offers recommendations for effectively planning, designing, and developing products that require expert knowledge and for long-term student service projects.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {505},\nnumpages = {7},\nkeywords = {User-centered design, collaborative, design research, eviction crisis, legal technology, methods, service organizations, student projects},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637132,\nauthor = {Aloi, Davide and Bouzit, Sara and Li, Jie},\ntitle = {Biometric Methods for User Research: Three Case Studies},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637132},\ndoi = {10.1145/3613905.3637132},\nabstract = {Understanding user experience (UX) and its impact on user preferences and purchasing decisions, and loyalty is essential in today’s user-centric business landscape. Assessing UX during product trials is vital for the design and improvement of products. While traditional UX evaluation methods (e.g., questionnaires and observations) offer valuable insights into user behavior, they rely on indirect, subjective and non-real-time data, making them prone to bias. There is a growing interest in incorporating neuroscience techniques into UX research to measure users’ neurophysiological responses to design elements and interactive experiences. We evaluated two digital prototypes and one in-store experience in three small-sample-size biometric UX studies that can fit into time-constrained product development processes. Our findings demonstrate the potential of biometric methods to complement existing UX research approaches. We discuss the lessons learned and advocate for the establishment of research ethics committees within companies and standardization strategies for conducting biometric user research.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {506},\nnumpages = {8},\nkeywords = {Biometric Sensors, Biometric User Research, EEG, Eye-tracking, GSR},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637116,\nauthor = {Monteiro-Krebs, Luciana and Geerts, David and Sanders, Kevin and Caregnato, Sonia Elisa and Zaman, Bieke},\ntitle = {Board Games as a Research Method: A Case Study on Research Game Design and Use in Studying Algorithmic Mediation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637116},\ndoi = {10.1145/3613905.3637116},\nabstract = {Investigation of board games as a research phenomenon has a long tradition in the academic community. However, using them as a research method has not been explored as much. This case study presents the steps towards the development of a serious board game aimed at collecting data on how participants understand algorithmic mediation in academic social media platforms. We followed the Serious Game Design Assessment (SGDA) framework to build a game through several playtests. Challenges in this process are presented in terms of the research game's Content, Mechanics, Fiction, and Aesthetics. We discuss how we addressed these challenges and elaborate on the different types of data that we collected by means of our research game. Finally, we present the lessons learned, highlighting the ideal target group for playtesting, the need of simplicity, the importance of storytelling, and how to counter the risk of participants’ strategies to “game” the game.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {507},\nnumpages = {8},\nkeywords = {Algorithmic mediation, Board games, SGDA framework, serious games},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637450,\nauthor = {Bowers, Christopher and Tomlinson, Andrew and Gaskin, Kerry L and Wray, Jo},\ntitle = {CHAT2App: Supporting Caregivers of Infants with Congenital Heart Disease},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637450},\ndoi = {10.1145/3613905.3637450},\nabstract = {Caregivers of post-operative infants treated for Congenital Heart Disease (CHD) undergo significant stress and anxiety following initial discharge. We examine the development of a mobile application to support caregivers and health care practitioners (HCPs) at this important time. Reflexive thematic analysis of responses to semi-structured interviews with both caregivers and HCPs is used to identify some of the key design requirements with a focus on addressing barriers and challenges to effective at home post-operative care-giving. We identify the need for careful and nuanced consideration of stake-holder perspectives and the unique challenges that this raises. These include effective recording and reminders for observations, clear communication channels and targeted and actionable outcomes to identifying symptoms of concern. A resulting prototype application is presented along with lessons learnt and remaining challenges uncovered during the study.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {508},\nnumpages = {9},\nkeywords = {decision support, healthcare, supporting caregivers},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637138,\nauthor = {Corbari Dos Santos, Gabriela and Silva, Deivid Eive and Peres, Leticia Mara and Valentim, Natasha Malveira Costa},\ntitle = {Case Study of a Model that evaluates the Learner Experience with DICTs},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637138},\ndoi = {10.1145/3613905.3637138},\nabstract = {Learner eXperience (LX) can be defined as the perceptions, answers, and performances of learners interacting with learning environments, educational products, and resources. Evaluating the LX to obtain experiences that support and facilitate learning and knowledge mastery is important. Thus, we developed the Learner Experience Evaluation Model (LEEM) to assess and improve the learner’s experience using Digital Information and Communication Technologies (DICTs) during learning. Therefore, this paper presents a case study to evaluate the applicability of the LEEM’s checklists in a real learning context. 23 learners and 1 teacher of the Software Engineering discipline participated in this study. The results of the study were analyzed quantitatively and qualitatively. Through the LEEM application, the teacher noticed that the learners felt comfortable using the DICTs and that they were easy to use. This study contributes to evaluating the LX at different moments of educational activity in a real context using LEEM.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {509},\nnumpages = {9},\nkeywords = {Evaluation, LX, Learner eXperience, Model, Qualitative Analysis, Quantitative Analysis, Student eXperience},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637114,\nauthor = {de Souza, Paula Maia and Santos, Vin\\'{\\i}cius Matheus Romualdo and Alves, Bianca Alessandra de Souza and Proen\\c{c}a, Fernando Roberto and Motti, Vivian Genaro and Rodrigues, Kamila Rios Da Hora and Neris, Vania Paula de Almeida},\ntitle = {Case Study: End users in recovery from substance use disorders as designers and developers of digital games with therapeutic potential},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637114},\ndoi = {10.1145/3613905.3637114},\nabstract = {We propose a design process and a web system to support the design and development of digital games with therapeutic potential by people in recovery from Substance Use Disorders (SUDs). The proposal aims to generate more autonomy for end users as a means of scaling game design for therapy. This case study aims to evaluate the design process and the web system. We also assessed the autonomy of end users (patients). We employ a case study, as a methodological approach, conducting six meetings with design activities and the development of digital games with patients in recovery from SUDs and their therapists. Research data were collected through field diaries, observation, questionnaires, and interviews. Results suggest that participants were more autonomous with the support of the web system. This paper also features a discussion of the authors’ perception of the activities, design implications, and recommendations for future research in the same domain.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {510},\nnumpages = {6},\nkeywords = {Game design, alcohol and drugs, therapeutic games., vulnerable population},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637149,\nauthor = {Wang, Xin and Abubaker, Samer M and Babalola, Grace T and Tulk Jesso, Stephanie},\ntitle = {Co-Designing an AI Chatbot to Improve Patient Experience in the Hospital: A human-centered design case study of a collaboration between a hospital, a university, and ChatGPT},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637149},\ndoi = {10.1145/3613905.3637149},\nabstract = {Patient experience (PX) is an important reflection of healthcare quality and is highly related to patient health outcomes and hospital reputation of within the communities they serve. PX data reported by patients is also crucial for hospitals to improve the services they provide, however, current approaches to survey and analyze PX data have many limitations. Our team collaborated with United Health Services (UHS), a New York healthcare system, to co-design a prototype chatbot application for patients to use while in the hospital, yielding more accurate PX data, but also an opportunity for staff to respond in real-time. We discuss our human-centered design process, which entailed interviews, data mining, qualitative analysis, and the application of ChatGPT and other algorithms to recognize relevant PX complaints from natural language data. Through ongoing collaboration, we are developing a chatbot application to elicit PX feedback and allow PX experts to improve patient experience in real-time.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {511},\nnumpages = {10},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637130,\nauthor = {Santana, Vagner Figueredo de and Quigley, Lauren Thomas and Coates, Jessica and Graves, Maxine},\ntitle = {Computational Notebook as a Tool for Supporting Black Engineers Non-Profit Organizations in the Context of Workforce Participation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637130},\ndoi = {10.1145/3613905.3637130},\nabstract = {Data is frequently referred to as the ‘new oil’ due to its high strategic value and the possibilities it brings. Additionally, its use by powerful organizations has the potential to help, but also to cause harm and function as an additional oppressive mechanism towards vulnerable communities. Hence, there is a need for providing easier methods for people and organizations to access, analyze, and interpret reliable, publicly available data (e.g., U.S. Census), and create data narratives that reflect their social position in the society and inform how to improve that position. In this case study, we document our journey on using computational notebooks to increase autonomy for organizations tackling racial and social justice issues by making meaningful use of publicly available data. The project considered a participatory action research approach run in partnership with a non-profit organization supporting Black engineers. The reported case study involved participants representing staff and volunteer leadership of the organization. We expect that the presented reflections help move towards a more equitable way of performing data analysis while using popular and freely available technologies.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {512},\nnumpages = {9},\nkeywords = {Computational Notebooks, Data Action, Jupyter Notebook, Participatory Action Research, Racial Justice., Social Justice},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637145,\nauthor = {Dai, Chih-Pu and Ke, Fengfeng and Zhang, Nuodi and Barrett, Alex and West, Luke and Bhowmik, Saptarshi and Southerland, Sherry A. and Yuan, Xin},\ntitle = {Designing Conversational Agents to Support Student Teacher Learning in Virtual Reality Simulation: A Case Study},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637145},\ndoi = {10.1145/3613905.3637145},\nabstract = {Maximizing educational impacts with learning technologies is one of the areas that researchers and practitioners are concerned about in the field of Human-Computer Interaction (HCI) and human-centered artificial intelligence (HCAI). In this case study, we report user experiences and lessons learned of the Enactive Virtual Environment for teaching practice (EVETeach) with AI-powered virtual student agents called Evelyn. We conducted a user study with a case study research design. We collected multiple sources of data from 24 student teachers, including participatory observations, field notes, semi-structured interviews, computer-based conversation logs, audio-, video-, and screen-recordings, and a cognitive walkthrough. We identified the following salient emerging findings as lessons learned: 1) Student teachers value and relate to the teaching practices in virtual reality simulation with AI-powered conversational agents, 2) AI-powered conversational agents inject humor to facilitate situational and social teaching practice, and 3) AI-powered conversational student agents maintain authentic discourse to promote student teachers’ pedagogical reasoning.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {513},\nnumpages = {8},\nkeywords = {Artificial intelligence, Conversational agents, Teacher education, Virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637140,\nauthor = {Szymanski, Maxwell and Vanden Abeele, Vero and Verbert, Katrien},\ntitle = {Designing and Evaluating Explanations for a Predictive Health Dashboard: A User-Centred Case Study},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637140},\ndoi = {10.1145/3613905.3637140},\nabstract = {As predictive health technologies become increasingly prevalent, the need for effective explanations that aid health experts and practitioners in understanding the underlying factors driving predictions is paramount. While many different explanation methods have been elaborated, recent research suggests that these explanations are often too complex for AI novices. In addition, little work has been done to evaluate whether the proposed methods indeed enhance human interpretability. In this study, we develop and evaluate explanations tailored to the needs of health experts, following an iterative user-centred design process to ensure understanding and usefulness of our explanation designs. Our findings underscore the importance of data-centric explanations, prioritising an emphasis on the underlying data rather than solely focusing on the model’s internal workings. Additionally, explanations should not only highlight points of congestion or areas for improvement but also emphasise positive aspects to promote a holistic understanding of the predictive health dashboard.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {514},\nnumpages = {8},\nkeywords = {XAI, counterfactual explanations, domain experts, feature importance explanation, health dashboard, heath explanations},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637146,\nauthor = {Maiden, Neil and Zachos, Konstantinos and Lockerbie, James and Brown, Amanda and Steele, Sam and Wolf, Alex},\ntitle = {Designing digital tools for creative thinking: a case study from elite sports coaching},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637146},\ndoi = {10.1145/3613905.3637146},\nabstract = {This case study reports the process, outcome and selected lessons from designing a new digital experience for professionals in one field that has received little interest in computer-human interaction research – elite sports coaching. The digital experience provided professional coaches with interactive support for thinking more creatively when overcoming the challenges faced by athletes and teams. It was one of the first to report the co-development of digital tools collaboratively with and for elite coaches. The case study argues that the digital outcome, called Sport Sparks, advanced the state of the practice in co-creative AI and digital creativity support by deploying the tool for use by professionals working outside of a recognized creative industry. The research team also learned lessons from its reflections about the process and outcome that can inform the development of co-creative AI tools both in elite sports coaching and other professional domains beyond the creative industries.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {515},\nnumpages = {11},\nkeywords = {co-creative AI, digital creativity support, elite sports coaching, generative AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637133,\nauthor = {Gao, Sally and Norkute, Milda and Agrawal, Abhinav},\ntitle = {Evaluating Interactive Topic Models in Applied Settings},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637133},\ndoi = {10.1145/3613905.3637133},\nabstract = {Topic modeling is a text analysis technique for automatically discovering common themes in a collection of documents. “Human-in-the-loop” topic modeling (HLTM) allows domain experts to steer and adjust the creation of topic models. In this case study, we use a custom-built HLTM interface to assess the impact of human refinement on model interpretability and predictive performance in collaboration with an analytics team within our organization. Using a small dataset (≈ 12k documents) of responses drawn from an organizational employee satisfaction survey, we compare the pre- and post-refinement models using both human judgments and automated metrics. We find that human refinement can enhance interpretability and predictive performance in some cases, but may lead to overfitting on the training data, which negatively impacts model quality. Furthermore, we observe that existing evaluation methods don’t sufficiently and clearly capture topic model quality in applied settings, and propose guidance for further HLTM tool development.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {516},\nnumpages = {8},\nkeywords = {Explainability, Human in the loop, Interactive machine learning, Interactive topic model, Interpretability, Topic modeling},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637115,\nauthor = {Ibrahim, Memunat Ajoke and Williams, Elizabeth and Hansen, Susan},\ntitle = {Expectations Vs Reality of Conducting Ethnographic Research in Nigeria to Inform Autonomous Ground Vehicles Design},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637115},\ndoi = {10.1145/3613905.3637115},\nabstract = {We present a comparative reflection of our experiences designing and conducting ethnographic user research in understudied real-world contexts – Nigerian road traffic. We present our experiences planning and doing fieldwork to investigate and map Nigerian road users’ on-road experiences and perspectives on trust and safety in real-world traffic, towards identifying design factors to inform trustworthy autonomous ground vehicle design. We compare our expectations and plans for the fieldwork to the reality of conducting the research in a multicultural country like Nigeria. We describe how some contextual research factors – including geopolitical, institutional, cultural, infrastructural, safety, and trust factors – affected the fieldwork, and how we addressed them by adapting the methodology to be suitable for the research contexts, populations, and societies. Our insights may be useful for researchers designing or conducting ethnographic research in multicultural communities to capture understudied perspectives to inform technology design practices in a culturally sensitive manner.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {517},\nnumpages = {8},\nkeywords = {Contextual research design, Ethnography, Methodology, Nigeria, User studies},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637123,\nauthor = {Ji, Xiayan and Yuh, Ahhyun and Erd\\'{e}lyi, Viktor and Mizumoto, Teruhiro and Choi, Hyonyoung and Harrison, Sean Lee and Cho, Emma and Weimer, James and Nagahara, Hajime and Higashino, Teruo and Demiris, George and Sokolsky, Oleg and Lee, Insup},\ntitle = {Exploring Effective Sensing Indicators of Loneliness For Elderly Community in US and Japan},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637123},\ndoi = {10.1145/3613905.3637123},\nabstract = {Loneliness has long affected the elderly community. This issue is significantly worsened by the social isolation resulting from the COVID-19 pandemic. To address this pressing issue, we employed a sensor-based methodology to predict loneliness and potentially inform interventions. We deployed sensors in the residences of 22 elderly participants from US and Japan, gathering daily activities data through 22 sensor features. Given the extensive feature set, we identify the most effective sensors to ensure unobtrusiveness while upholding privacy. Regression analysis of these features revealed that our best-performing Random Forest model achieved an R2 value of 0.86, on par with existing literature. In addition, we found that the sleep mattress sensor and temperature-humidity sensor were particularly indicative of loneliness. In summary, our research contributes to the HCI literature with effective non-invasive sensing modalities in assessing elderly loneliness, together with insights from our real-world sensor deployments in US and Japan-based elderly communities.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {518},\nnumpages = {9},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637143,\nauthor = {Read, Janet and Sim, Gavin and Fails, Jerry Alan and Boden, Marie A and Korte, Jessica L and Bhatnagar, Sanjana and Hope Borchardt, Lilly and Constantin, Aurora and Gavrilescu, Dina and Wilson, Cara and Good, Judith and Andries, Valentina and Eriksson, Eva},\ntitle = {Exploring Similarities and Differences in a set of Linked Multiple-site Design Sessions with Children},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637143},\ndoi = {10.1145/3613905.3637143},\nabstract = {In this paper, we present a multiple-site case study to illustrate the similarities and differences in design with children in situated research contexts. The aim is to provide the reader with an insight into the planning, execution and evaluation of a set of linked, yet individual, studies from several different contexts and countries. The cases provide a platform to think about how design studies might be collectively reported and evaluated even when they are carried out in different ways. We illustrate the inherent complexity of this process by considering six case studies from five countries on three continents. The contribution is an illustration of differentiated replication in multiple distributed participatory design case studies with children in different countries. We share our thoughts on replication and differentiation, on the value of the design activities, and on how to carry out a larger project.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {519},\nnumpages = {9},\nkeywords = {Case study, Child-computer interaction, Co-design, Distributed participatory design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637142,\nauthor = {F\\\"{o}rster, Andreas and Schnell, Norbert},\ntitle = {Harmonizing Abilities: Collaborative Digital Musical Instruments for Students with Complex Disabilities Using the Some for Many Strategy},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637142},\ndoi = {10.1145/3613905.3637142},\nabstract = {In our case study, we propose a ‘Some for Many’ design approach for digital musical instruments (DMIs) in special educational needs (SEN) schools that addresses the challenges of designing for and with students with complex disabilities and non-verbal communication. We outline the development and evaluation of several DMIs based on typical everyday interactions. Our findings suggest that the approach is likely to promote active music-making and inclusion and potentially contributes to a change in peers’ perceptions of students with complex disabilities in a positive way.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {520},\nnumpages = {8},\nkeywords = {Accessible Digital Musical Instruments, Bela, Music Technology, Open Source, Pure Data, Special Education, Tangible Interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637148,\nauthor = {Abolnejadian, Mohammad and Alipour, Sharareh and Taeb, Kamyar},\ntitle = {Leveraging ChatGPT for Adaptive Learning through Personalized Prompt-based Instruction: A CS1 Education Case Study},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637148},\ndoi = {10.1145/3613905.3637148},\nabstract = {In this research paper, we discuss our attempt to teach high school students introductory programming with Python using a custom learning platform that leverages ChatGPT to generate personalized learning materials based on each student’s educational background. The platform features topics and subtopics, each supported by prompts for Explanation, Example, Exercise, and Exercise Solution, with a context-setting prompt tailored to individual students’ backgrounds while respecting their privacy. The case study brought up compelling insights. Students exhibited heightened engagement, and the lecturers transitioned from being traditional instructors teaching content to becoming mentors who guide students on what to do next, clarifying misunderstandings and addressing potential questions. Furthermore, students gained hands-on programming experience during the learning process, eliminating the traditional post-class experimentation phase. This innovative approach not only enhances traditional CS1 education but also suggests a broader application of Large Language Models (LLMs) for personalized learning across diverse fields, providing tailored instruction and fostering engagement.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {521},\nnumpages = {8},\nkeywords = {CS1, ChatGPT, Course Design, Introductory Programming, LLM, Learning Platform, Prompt Engineering},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637128,\nauthor = {Bursztein, Elie and Brown, Karla J and Sanderson, Leonie M and Kelley, Patrick Gage},\ntitle = {Leveraging Virtual Reality to Enhance Diversity, Equity and Inclusion training at Google},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637128},\ndoi = {10.1145/3613905.3637128},\nabstract = {Virtual reality (VR) has emerged as a promising educational training method, offering a more engaging and immersive experience than traditional approaches. In this case study, we explore its effectiveness for diversity, equity, and inclusion (DEI) training, with a focus on how VR can help participants better understand and appreciate different perspectives. We describe the design and development of a VR training application that aims to raise awareness about unconscious biases and promote more inclusive behaviors in the workplace. We report initial findings based on the feedback of Google employees who took our training and found that VR appears to be an effective way to enhance DEI training. In particular, participants reported that VR training helped them better recognize biases and how to effectively respond to them. However, our findings also highlight some challenges with VR-based DEI training, which we discuss in terms of future research directions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {522},\nnumpages = {7},\nkeywords = {DEI training, diversity, equity, inclusion, interactive learning, virtual reality, workplace},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637113,\nauthor = {Bell, Andrew and Stoyanovich, Julia},\ntitle = {Making Transparency Influencers: A Case Study of an Educational Approach to Improve Responsible AI Practices in News and Media},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637113},\ndoi = {10.1145/3613905.3637113},\nabstract = {Concerns about the risks posed by artificial intelligence (AI) have resulted in growing interest in algorithmic transparency. While algorithmic transparency is well-studied, there is evidence that many organizations do not value implementing transparency. In this case study, we test a ground-up approach to ensuring better real-world algorithmic transparency by creating transparency influencers — motivated individuals within organizations who advocate for transparency. We held an interactive online workshop on algorithmic transparency and advocacy for 15 professionals from news, media, and journalism. We reflect on workshop design choices and presents insights from participant interviews. We found positive evidence for our approach: In the days following the workshop, three participants had done pro-transparency advocacy. Notably, one of them advocated for algorithmic transparency at an organization-wide AI strategy meeting. In the words of a participant: “if you are questioning whether or not you need to tell people [about AI], you need to tell people.”},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {523},\nnumpages = {8},\nkeywords = {Transparency, artificial intelligence, explainability, machine learning, tempered radicals},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637150,\nauthor = {Qian, Zhigu and Fu, Jiaojiao and Zhou, Yangfan},\ntitle = {Overcoming Barriers, Achieving Goals: A Case Study of an Older User's Technology Autonomy},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637150},\ndoi = {10.1145/3613905.3637150},\nabstract = {Previous research often portrays older adults, especially those with low socioeconomic status, as a technologically disadvantaged population and passive recipients needing tech assistance. However, this overlooks their own technological autonomy. This case study examines how one older user in China with limited education overcame barriers to using Douyin by creatively appropriating its functions and crafting assistive tools like audio commands. Her self-driven technology practices reveal important insights into older adults’ innate resourcefulness that can inform more senior-friendly HCI design. Rather than positioning older adults as passive aid recipients, this study highlights their technological autonomy. Gerontechnology accessibility should build on uncovering and leveraging this population’s technology creativity and initiative. Spotlighting user autonomy provides vital corrections to deficit narratives. Our study underscores the need to understand older adults’ self-created technology practices for inclusive and senior-friendly HCI research and design.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {524},\nnumpages = {7},\nkeywords = {Douyin, Older adults, Senior-friendly, Technological autonomy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637122,\nauthor = {Nagassa, Ruth G and Butler, Matthew and Holloway, Leona M and Tandori, Erica J and Marriott, Kim},\ntitle = {Push, Scan, Tap: Embracing Commodity Technology for the provision of Accessible Information in Cultural Institutions},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637122},\ndoi = {10.1145/3613905.3637122},\nabstract = {Cultural institutions often employ commodity technology solutions for the provision of information in exhibitions. While these solutions provide a pragmatic approach, the efficacy, accessibility and user experience for people who are blind or have low vision (BLV) remains understudied. We conducted a case study with an Australian regional gallery to investigate BLV vistors’ use of commodity solutions – QR and NFC technology, voice recording push buttons, and touch-triggered electronics – and their efficacy in supporting the provision of accessible information. Through in-situ evaluation, post-visit questionnaire, and semi-structured interviews we present a deeper understanding of the needs and preferences of BLV visitors. Our findings illuminate key considerations to guide the meaningful use of commodity technologies by cultural institutions for accessibility. We hope that this case study will lead to more accessible solutions in cultural spaces, ensuring they more closely meet the needs of the BLV community and encourage more widely-adopted solutions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {525},\nnumpages = {6},\nkeywords = {accessibility, art, blind, inclusive design, low vision},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637206,\nauthor = {Somandepalli, Krishna and Siy, Oliver and Jou, Brendan},\ntitle = {Relational Affect in Dyadic Interactions},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637206},\ndoi = {10.1145/3613905.3637206},\nabstract = {Relational affect is the affective response (encompassing emotion, expression, feeling) that emerges from an interaction between two people. The case study presented here introduces the concept of relational affect through a human perceptual rating task. Forty-five raters watched short video clips of two people interacting and described their perceived emotion of the individuals and that of the overall interaction. Our qualitative analysis of the rater responses showed that raters used a variety of schemes to reason about emotion, including expressions, context, and perceived appraisal of the event. These reasoning schemes were notably different for perceived individual emotion and relational affect. Our findings show that the vocabulary use for relational affect is distinct from that of individual emotion and relational affect as a phenomenon deepens our understanding of social interactions and moves the field a step closer to realizing the goal of fluid interactions between people and technology.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {526},\nnumpages = {9},\nkeywords = {Affective Computing, Dyadic Interaction, Relational Affect},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637119,\nauthor = {Beijer, Sanne and Dam, Fenna and Houben, Maarten and Brankaert, Rens},\ntitle = {STAPP: Designing a Tool for People with Korsakoff's Syndrome to Re-learn Daily Activities Step by Step},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637119},\ndoi = {10.1145/3613905.3637119},\nabstract = {Korsakoff Syndrome (Korsakoff) is a brain disorder that causes severe brain impairments that have consequences for executing activities of daily living (ADL) and often requires lifelong assistance in these activities. Assistive technology can be beneficial for supporting both formal caregivers and people with cognitive impairments. However, as there is little research into how assistive technology can be applied in the context of Korsakoff, more insight is needed into how assistive technology can support the specific needs of people with Korsakoff in re-learning ADL. This paper presents a case study in which a mobile application (i.e., STAPP) is developed through an iterative design process with people with Korsakoff, formal caregivers, designers, researchers, and developers. The application focused on two ADLs (e.g., coffee making and laundry). We report on the design process, the application, autonomy, lack of in-person support, and the importance of in-context instructions.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {527},\nnumpages = {7},\nkeywords = {Activities of Daily Living, Assistive Technology, Korsakoff's Syndrome, Participatory Design Research},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637137,\nauthor = {Zellner, Katherine Ann and Shah, Vidhi and Sarcevic, Aleksandra},\ntitle = {Simple But Iconic: Implications for Designing Icons Representing Delays in Time-Critical Medical Events},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637137},\ndoi = {10.1145/3613905.3637137},\nabstract = {Decision-support tools that increase awareness of time-critical pending tasks can help reduce delays during life-saving interventions. However, conveying the urgency of tasks through visual alerts remains a challenge. This case study describes our design and evaluation of delay alerts represented by icons within a decision-support system for emergency medical events such as trauma resuscitation. We used iterative sketching to design a series of icons for three delay alerts and two online surveys with a total of 55 clinicians to evaluate the icons. Although we have prioritized interpretable icons based on common icon design principles, clinicians preferred the simplest rather than the most interpretable icons. This case study contributes design guidelines for icons representing complex processes and concepts that can be applied to other types of time-critical teamwork.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {528},\nnumpages = {8},\nkeywords = {Alerts, Clinical Decision-Support System (CDSS), Complex workflows, Emergency medicine, Icon design, Icons, Survey, Trauma resuscitation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637112,\nauthor = {Riisager, Lisa Groenberg and Christiansen, Thomas Blomseth and Moeller, Stine Bjerrum and Huniche, Lotte and Larsen, Jakob Eg},\ntitle = {Snapping Out of It: How a Wearable for Self-Tracking Assisted Psychotherapy Bridges the Gap Between Thoughts and the World},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637112},\ndoi = {10.1145/3613905.3637112},\nabstract = {This case study explores the use of the One Button Tracker (OBT), a wearable self-tracking instrument, by a refugee diagnosed with complex PTSD, in personalized self-tracking assisted psychotherapy. The OBT differs from traditional mHealth methods, which typically rely on predefined tracking parameters, by empowering the patient to focus on their own subjective experiences of phenomena selected during treatment. This approach fosters patient-therapist collaboration, tailoring the therapeutic process to individual needs. Here, the patient chose to track flashbacks, a grounding intervention, and anger experiences. Findings reveal a multifaceted relationship between the patient and the instrument, underscoring the instrument’s significance in supporting the therapeutic process. The patient’s consistent engagement with the OBT, demonstrates its ability to bridge the gap between daily life and psychotherapy. This study underscores the relevance of integrating personalized self-tracking in therapy demonstrating that such instruments can serve multiple roles, from data collection to therapeutic companions in patients’ lives.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {529},\nnumpages = {7},\nkeywords = {Mental health, One Button Tracker, personalized self-tracking, psychotherapy, wearables},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637105,\nauthor = {Sturm, Christian and White, Edward Peter Greenwood and Wong-Villacres, Marisol and Densmore, Melissa and Moreno Rocha, Mario A and Mart\\'{\\i}nez, Carlos Alberto and Collazos, C\\'{e}sar and Jere, Nobert Rangarirai and Mon, Alicia Am},\ntitle = {Student Design Competitions as an Awareness-building activity for HCI in the Global South},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637105},\ndoi = {10.1145/3613905.3637105},\nabstract = {Increasing the awareness for the field of Human-Computer Interaction (HCI) in the Global South faces many challenges. The value of integrating HCI into computer science programs is still in its infancy across locations. Drawing from 16 years of experiences supporting Student Design Competitions (SDCs) in Latin America and Africa, in this paper we reflect on the intrinsic and extrinsic value of the CHI SDC in conjuction with local and regional events to address this challenge. Via the competitions, students get exposed to global perspectives on HCI, and industry experiences first hand the power of bringing students together. The SDC provides regional and global visibility for local research and HCI groups, which would be impossible otherwise. It allows the student teams to be on par with other teams worldwide. This, in turn, supports local researchers to raise resources both locally and regionally. Participation in the SDC, however, needs reframing and more support to deliver its full potential.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {530},\nnumpages = {7},\nkeywords = {CHI, Global South, HCI education, Student Design Competition},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637107,\nauthor = {Raffaillac, Thibault and Boukhelifa, Nadia and Crouzat, Emilie and Stark, Fabien and M\\\"{u}ller, Jean-Pierre and Lasseur, Jacques},\ntitle = {Supporting Interdisciplinary Research with Cards-based Workshops - A Case Study on Participatory Planning for Mountain Pastoralism},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637107},\ndoi = {10.1145/3613905.3637107},\nabstract = {Profound transformations are currently impacting the intricate relationships between human pastoral activities and Alpine mountain ecosystems. In this context, we were invited as human-computer interaction practitioners to join an interdisciplinary team, to reinforce the use of computer simulations in meetings with local stakeholders. However, due to the diverse nature of stakeholders involved and their interactions, the specific format of these meetings and the utilization of model simulations remained unclear. To clarify these uncertainties, we designed a cards-based ideation method to collectively plan participatory workshops and envisage if and how information technology tools could be integrated. In this paper, we present the design of our ideation method Cards4Concertation and two workshops that implemented it. We then reflect on our findings with lessons learnt both for designing cards-based activities for ideation within interdisciplinary teams, as well as visualizations for participatory planning and decision-making.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {531},\nnumpages = {8},\nkeywords = {card game, concertation, ideation, interdisciplinary, socio-ecology, sustainability, visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637118,\nauthor = {Hupont, Isabelle and Wainer, Marina and Nester, Sam and Tissot, Sylvie and Iglesias-Blanco, Luc\\'{\\i}a and Baldassarri, Sandra},\ntitle = {Synocene, Beyond the Anthropocene: De-Anthropocentralising Human-Nature-AI Interaction},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637118},\ndoi = {10.1145/3613905.3637118},\nabstract = {Recent publications explore AI biases in detecting objects and people in the environment. However, there is no research tackling how AI examines nature. This case study presents a pioneering exploration into the AI attitudes – ecocentric, anthropocentric and antipathetic– toward nature. Experiments with a Large Language Model (LLM) and an image captioning algorithm demonstrate the presence of anthropocentric biases in AI. Moreover, to delve deeper into these biases and Human-Nature-AI interaction, we conducted a real-life experiment in which participants underwent an immersive de-anthropocentric experience in a forest and subsequently engaged with ChatGPT to co-create narratives. By creating fictional AI chatbot characters with ecocentric attributes, emotions and views, we successfully amplified ecocentric exchanges. We encountered some difficulties, mainly that participants deviated from narrative co-creation to short dialogues and questions and answers, possibly due to the novelty of interacting with LLMs. To solve this problem, we recommend providing preliminary guidelines on interacting with LLMs and allowing participants to get familiar with the technology. We plan to repeat this experiment in various countries and forests to expand our corpus of ecocentric materials.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {532},\nnumpages = {9},\nkeywords = {AI biases mitigation, Anthropocentric AI, Human-Nature-AI Interaction},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637147,\nauthor = {Ooms, Simone and Cesar, Pablo and El Ali, Abdallah and Ceolin, Davide and Hollink, Laura and Slokom, Manel and Pauwels, Eric and Robu, Valentin and La Poutre, Han},\ntitle = {Technological Innovation in the Media Sector: Understanding Current Practices and Unraveling Opportunities},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637147},\ndoi = {10.1145/3613905.3637147},\nabstract = {AI technologies offer significant opportunities for the media sector, including content production, distribution, and audience engagement. This case study aims to bridge the technological expertise of the AI, Media \\& Democracy lab in the Netherlands to stakeholders in the media sector, identifying current technology implementation practices and outlining the potential of our technological expertise. First, a visual portfolio of the expertise of the researchers was created. Then, focus group workshops were held with broadcasters and media institutions, using an interactive online Miro environment. Results include insights on current implementation of the presented technologies by the media organizations, and opportunities for further implementation. Key takeaways are the broadcaster need for short-term implementation possibilities and the importance of having both B2C (broadcasters) and a B2B (the entire sector) perspectives present in focus groups to provide confirmation of findings and offer a broader viewpoint on the media landscape.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {533},\nnumpages = {7},\nkeywords = {artificial intelligence, case study, focus group, industry practice, media sector},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637144,\nauthor = {Ullrich, Daniel and Butz, Andreas Martin and Diefenbach, Sarah},\ntitle = {The Fernstudent. A User-Centered Design Case Study on Improvements of Hybrid Teaching Through A Physical Avatar in the Classroom.},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637144},\ndoi = {10.1145/3613905.3637144},\nabstract = {Hybrid formats have become a common approach in university teaching. Typically, there is a synchronous live session which students can join either in the physical classroom or digitally/remotely with interaction through textual chat. While this has many advantages in terms of inclusivity and sustainability, it also poses problems such as increased attention requirements for teachers and a lacking social integration of remote and on-site students. Based on a user-centered design approach, we developed the concept of the \"Fernstudent\". A physical avatar represents remote students collectively in the classroom, creating a communication channel based on the same modalities as for on-site communication. The present case study describes the iterative design process, from the first idea via explorative interviews and state of the art analysis, concept development, potential analysis, exploration of different design variants, a first functional prototype, a pilot field test, and finally lessons learnt and planned future steps of development.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {534},\nnumpages = {8},\nkeywords = {Fernstudent, concept development, hybrid teaching, pilot field test, prototype, user-centered design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637129,\nauthor = {Maric, Jasmina and Rani, Lekshmi Murali},\ntitle = {The Wicked Problem of Dropouts},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637129},\ndoi = {10.1145/3613905.3637129},\nabstract = {Why do young girls leave coding classes? The research presented in this report takes a deep dive into the exploration of the complex factors influencing the dropout rates of girls involved in a Creative Coding project. Creative Coding initiative targets female empowerment, girls aged from 10-15, by combining the worlds of coding and music in one learning environment. The project’s primary objectives are to empower girls in the realms of technology and creativity, enhance their social capital, and foster their engagement in Science, Technology, Engineering, Arts, and Mathematics (STEAM) disciplines. Despite its noble intentions, usage of the interesting innovative audio-visual tool, and collaboration with specifically chosen mentors and teachers that collaborated on this project, we observed a substantial dropout rate during its initial phases. This study honestly and rigorously investigates the underlying reasons for 50\\% attrition looking at it as a wicked problem, and explores potential remedies.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {535},\nnumpages = {8},\nkeywords = {Live Coding, STEAM, Strudel, girls’ empowerment, innovative teaching, wicked problems},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637141,\nauthor = {Siagkri, Rafaella},\ntitle = {Understanding Through Virtual Reality the Cinematic Expressionistic Architecture: The Case Study of The Cabinet of Dr Caligari},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637141},\ndoi = {10.1145/3613905.3637141},\nabstract = {This interdisciplinary research project explores the symbiotic relationship between architecture and cinema through the application of Virtual Reality (VR) technology. Focusing on the iconic film The Cabinet of Dr Caligari (1920) by Robert Wiene, renowned for its pioneering narrative and architecturally innovative film sets, the study employs VR to digitally reconstruct these lost film sets. By leveraging archival drawings and meticulous data acquisition, the project's iterative design process models the sets to near-physical accuracy. The resulting VR simulation offers a unique opportunity for scholars, architects, and film enthusiasts to immerse themselves in the expressionist era's spatial and atmospheric environments. Additionally, it advocates for the use of VR as a tool for the preservation and restoration of cultural heritage within the realm of film and architecture. The case study's significance lies in its contribution to the preservation of film culture and the architectural legacy of Expressionism.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {536},\nnumpages = {7},\nkeywords = {Architecture, Digital Cultural Heritage, Expressionism, Post-Phenomenology, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637136,\nauthor = {Dogan, Huseyin and Giff, Stephen and Barsoum, Renee},\ntitle = {User Experience Research: Point of View Playbook},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637136},\ndoi = {10.1145/3613905.3637136},\nabstract = {User Experience Research (UXR) needs to offer clear recommendations based on the meaning of data, evidence, and insights. However, UXR lacks a method to help the practitioners form a compelling perspective that drives impact through evidence-based resources. The paper presents a UXR Point of View (PoV) playbook that provides a set of criteria, standards, and tools to guide multi-disciplinary teams in designing services. A playbook typically consists of a book of plays or a set of instructions and has been used by practitioners from multiple disciplines. The paper presents the anatomy of a POV playbook with examples, challenges in industry, and the correlation of the PoV to the theories of situational awareness and decision making. The UXR PoV Playbook presents the results of a structured workshop with UX practitioners. The existing playbooks are used to inspire the POV Workshop and a model for defining the UXR Playbook.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {537},\nnumpages = {7},\nkeywords = {Insight Generation, Playbook, Points of View, User Experience Research},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637134,\nauthor = {Tandon, Sara and Contris, Dylan},\ntitle = {UserJourney2Vector: Enterprise Application of Transformer Models on User Telemetry Data},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637134},\ndoi = {10.1145/3613905.3637134},\nabstract = {User telemetry data offers valuable insights into user behavior and goals when interacting with digital products and services. However, analyzing vast amounts of event sequence data can be challenging. In this case study, we apply transformer neural networks to telemetry event sequence data, yielding the UserJourney2Vector model, which treats event sequences similarly to natural language to distill complex user journeys into latent vector representations. The model enables understanding of typical and anomalous user paths, prediction of next actions, and user segmentation based on behavior. We used the model to obtain user clusters, predictive journeys, and cluster statistics, then conducted interviews with digital product experts to assess potential applications and impact of the model output. Experts responded positively to the model’s ability to bolster user personas with data-driven insights and noted integrating model outputs with current practices could augment product design and data interpretation.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {538},\nnumpages = {7},\nkeywords = {Design Methods, Quantitative Personas, Representation Learning, Transformers, User Journeys, User Studies},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637139,\nauthor = {Kami\\'{n}ska, Dorota and Zwoli\\'{n}ski, Grzegorz and Merecz-Kot, Dorota},\ntitle = {Virtual Reality as an Automated Stress-Reduction Therapy Tool - Case Study on War Refugees},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637139},\ndoi = {10.1145/3613905.3637139},\nabstract = {This case study presents research and reflections concerning Virtual Reality (VR) as an automated stress-reduction therapy tool. The study coincided with the outbreak of war in the Ukraine, thus the software was tailored to reduce the acute stress of refugees. We created a relaxation training program in a virtual, pleasant environment in the form of a comfortable mountain apartment. The pilot programme was conducted on the group of 55 Ukrainian refugees participating in up to five therapeutic sessions. The system effectiveness was evaluated by a set of sensors collecting objective physiological measures such as GSR (galvanic skin resistance) and EEG (electroencephalography). Before and after each session, the volunteers filled in questionnaires regarding their current stress level and mood, and that subjective data was juxtaposed with objective data from the sensors. In this work, we present our experience, insights and hope to inspire researchers and practitioners to explore the opportunities given by VR-supported therapy.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {539},\nnumpages = {8},\nkeywords = {affective computing, bilateral stimulation, refugees, relaxation techniques, stress reduction, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637103,\nauthor = {Jeon, Youngseung and Hong, Matthew K. and Chen, Yan-Ying and Murakami, Kalani and Li, Jonathan Q. and Chen, Xiang Anthony and Klenk, Matthew},\ntitle = {Weaving ML with Human Aesthetic Assessments to Augment Design Space Exploration: An Automotive Wheel Design Case Study},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637103},\ndoi = {10.1145/3613905.3637103},\nabstract = {Consumers’ emotional and cognitive attachment to product design plays a pivotal role in influencing purchasing choices. Therefore, product designers incorporate this signal as they develop new products. The goal of our work is to reduce the psychological distance between designers and consumers in the automotive concept design process. While generative AI models hold the potential to amplify creativity, these models do not have any of this specialized knowledge. In this work, we developed a novel framework and system that combines machine learning, human aesthetic assessments, and visualization to support designers in organizing a large space of automotive wheel designs. We present a case study with 10 automotive designers using the tool to inspire novel wheel designs and end with a discussion of use cases and design implications for using this framework to support professional product design practice.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {540},\nnumpages = {10},\nkeywords = {Automotive Wheel Design, Generative AI, Interpolation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637117,\nauthor = {Ibrahim, Zaidat and Karimi, Pegah and Martin-Hammond, Aqueasha and Harrington, Christina and Siek, Katie A.},\ntitle = {What Do We Do? Lessons Learned from Conducting Systematic Reviews to Improve HCI Dissemination},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637117},\ndoi = {10.1145/3613905.3637117},\nabstract = {Systematic reviews are essential in helping researchers address pre-defined research questions through explicit, methodical, and reproducible techniques for identifying studies and comprehensively synthesizing their findings. We highlight our experiences conducting two systematic review studies in HCI: (1) women’s reproductive health research in HCI and (2) the intersection of identity and older adults in health research. We identify patterns and lessons that can be applied to enhance the reporting and communication of our research. While these lessons may not be universally applicable, they provide HCI researchers with the opportunity for introspection regarding how we convey our findings to the broader research community. Additionally, these lessons contribute to upholding transparency and integrity in our work, rendering it more long-lasting and beneficial for secondary purposes, like literature reviews and study replication. We provide recommendations and, where feasible, good examples of how to effectively report participants’ demographics and study methodology in our HCI work.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {541},\nnumpages = {8},\nkeywords = {Best Practices, HCI Research, Lessons Learned, Meta Analysis, Research Methods, Systematic Literature Reviews},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637109,\nauthor = {Krawczyk, Michaela and Siek, Katie A.},\ntitle = {When Research Becomes All About the Bots: A Case Study on Fraud Prevention and Participant Validation in the Context of Abortion Storytelling},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637109},\ndoi = {10.1145/3613905.3637109},\nabstract = {Effective fraud prevention and participant validation are essential for ensuring data quality in today’s highly-digitized research landscape. Increasingly sophisticated bots and high levels of fraudulent participants have generated a need for more complex and nuanced methods to combat fraudulent activity. In this paper, we share our experiences with fraudulent survey responses, which we encountered in our work around abortion storytelling, and the multi-stage protocol that we developed to validate participants. We found that effective fraud prevention should start early and include a variety of flagging methods to encourage holistic pattern-searching in data. Researchers should overestimate the amount of time they will need to validate participants and consider asking participants to assist in the validation process. We encourage researchers to be transparent about the interpretive nature of this work. To this end, we contribute a Participant Validation Guide in supplemental materials for community members to adapt in their own practices.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {542},\nnumpages = {8},\nkeywords = {asynchronous remote community, digital survey, fraud, fraud prevention, recruitment},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637124,\nauthor = {Kim, Seoktae and Bak, Sure and Kim, Kahyeon},\ntitle = {Which Robot Do You Prefer When Boarding an Elevator: Fast vs. Considerate},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637124},\ndoi = {10.1145/3613905.3637124},\nabstract = {As robots are increasingly integrated into our everyday lives, elevator situations with robots have become unavoidable. Elevators are essential facilities for moving between floors within high-rise buildings, yet they often serve as spaces that generate dissatisfaction among people. It is imperative that robots do not exacerbate this dissatisfaction among elevator users. To address this issue, we conducted an online comparative survey using video-based scenarios to investigate the impact of a robot’s boarding time and boarding position on elevator satisfaction among office employees who coexist with real robots in their daily work environment (N = 374). The results revealed that both of these factors significantly contribute to satisfaction. Additionally, we found that the robot’s perceived competence is the most influential factor for satisfaction. This case study underscores the importance of time for individuals in elevator environments within the context of Human-Robot Interaction (HRI) research and provides suggestions for enhancing people’s satisfaction.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {543},\nnumpages = {7},\nkeywords = {Elevators, Human-Robot Interaction, Robot Behaviors, Social Navigation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3637125,\nauthor = {Kim, Bo Young and Ma, Qingyan and Diamond, Lisa},\ntitle = {“It’s in My language”: A Case Study on Multilingual mHealth Application for Immigrant Populations With Limited English Proficiency},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3637125},\ndoi = {10.1145/3613905.3637125},\nabstract = {Immigrant populations with limited English proficiency (LEP) confront more challenges than those with English proficiency in using healthcare technology in the U.S. Our case study, conducted in collaboration with the Immigrant Health and Cancer Disparities (IHCD) Center at Memorial Sloan Kettering Cancer Center (MSK), explores how LEP patients interact with patient portals translated in their preferred language. Through semi-structured interviews and usability testing, we found that individuals with LEP 1) encounter usability barriers during patient portal enrollment, 2) perceive increased self-efficacy and trust in using translated patient portals, and 3) greatly depend on caregivers for patient-provider communication and use of patient portals without translations. Based on these results, we offer recommendations for enhancing patient portal adoption among patients with LEP, share insights gained from applying User-Centered Design (UCD) methodologies with this user group, and discuss opportunities for Human-Computer Interaction (HCI) research to empower this user group and mitigate health disparities.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {544},\nnumpages = {7},\nkeywords = {digital divide, digital inclusion, diversity dimensions, health, health information seeking, identity, language-proficiency, mHealth, social determinants of health},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644063,\nauthor = {Robinson, Raquel B and Osborne, Anya and Ji, Chen and Fey, James Collin and Dagan, Ella and Isbister, Katherine},\ntitle = {\"That's Not Good Science!\": An Argument for the Thoughtful Use of Formative Situations in Research Through Design},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644063},\ndoi = {10.1145/3613905.3644063},\nabstract = {Most currently accepted approaches to evaluating Research through Design (RtD) presume that design prototypes are finalized and ready for robust testing in laboratory or in-the-wild settings. However, it is also valuable to assess designs at intermediate phases with mid-fidelity prototypes, not just to inform an ongoing design process, but also to glean knowledge of broader use to the research community. We propose ’formative situations’ as a frame for examining mid-fidelity prototypes-in-process in this way. We articulate a set of criteria to help the community better assess the rigor of formative situations, in the service of opening conversation about establishing formative situations as a valuable contribution type within the RtD community.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {545},\nnumpages = {8},\nkeywords = {evaluation, formative situation, interaction design, method, methodology, mid-fidelity prototype, research through design, theory},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644065,\nauthor = {Xu, Chunchen and Ge, Xiao},\ntitle = {AI as a Child of Mother Earth: Regrounding Human-AI Interaction in Ecological Thinking},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644065},\ndoi = {10.1145/3613905.3644065},\nabstract = {The anthropocentric cultural idea that humans are active agents exerting control over their environments has been largely normalized and inscribed in practices, policies, and products of contemporary industrialized societies. This view underlies a human-ecology relationship based on resource and knowledge extraction. To create a more sustainable and equitable future, it is essential to consider alternative cultural ideas rooted in ecological thinking. This perspective underscores the interconnectedness between humans and more-than-human worlds. We propose a path to reshape the human-ecology relationship by advocating for alternative human-AI interactions. In this paper, we undertake a critical comparison between anthropocentrism and ecological thinking, using storytelling to illustrate various human-AI interactions that embody ecological thinking. We also delineate a set of design principles aimed at guiding AI developments toward fostering a more caring human-ecology relationship.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {546},\nnumpages = {9},\nkeywords = {AI, Anthropocentrism, Culture, Design, Ecological thinking, Environmental justice, Human-ecology relationship, More-than-human, Storytelling, Sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644066,\nauthor = {Alikhan, Hummd and Kotut, Lindah},\ntitle = {Begone, Orthot: A Near-Future Exploration of Bodily Autonomy},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644066},\ndoi = {10.1145/3613905.3644066},\nabstract = {This design fiction explores the concept of cyborgs and the evolution of body modifications through the lens of the Febris Suite™, a fictional implant technology that aims to enhance human capabilities and relieve burdens on the body. Through this exploration, we engage with the historical context of cyborgs, the growth of the biohacking movement, and the commercialization of embedded technologies. We discuss the potential applications of this fictional technology, including emergency medical interventions, reproductive control, and access to gender-affirming care–alongside potential drawbacks and concerns, such as planned obsolescence, proprietary control, and potential social divisions based on who can afford the enhancement. We conclude by posing critical questions about the balance between bodily autonomy and the proprietary nature of implanted technologies, raising ethical considerations for the future integration of artificial systems with the body.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {547},\nnumpages = {9},\nkeywords = {Bodily Autonomy, Cyborg, Planned Obsolescence, Right to Repair, Ubiquitous Computing, Wearable Tech},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644049,\nauthor = {Khot, Rucha and Lee, Minha and Brankaert, Rens and IJsselsteijn, Wijnand},\ntitle = {Beyond the Clock: Rethinking Time in Technologies for People with Dementia},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644049},\ndoi = {10.1145/3613905.3644049},\nabstract = {We explore the intricate interplay between humans, time, and technology, highlighting the prevalence of objective temporal perspectives, such as the clock in modern technologies. Our focus narrows on assistive technologies, where the prevailing adherence to seemingly objective clock time sidelines subjective human experiences, which is a critical oversight, especially for vulnerable groups like people with dementia. This case underscores the evident temporal misfit between lived experiences and standardized clock time imposed by technology. Consequently, assistive technologies inadequately address nuanced temporal challenges in daily life. Through dementia-driven scenarios and temporal misfits with assistive technology, we advocate for a more inclusive and intuitive technology approach, urging a reconsideration of the pervasive use of clock time in HCI. We urge the community to embrace multi-temporal lenses that can accommodate a range of temporal perspectives and move beyond the technological imposition of commoditized clock time.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {548},\nnumpages = {10},\nkeywords = {Dementia, technology, temporal-misfit, temporality, time},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644053,\nauthor = {de Almeida, Fabio and Rafael, S\\'{o}nia},\ntitle = {Bias by Default.: Neocolonial Visual Vocabularies in AI Image Generating Design Practices.},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644053},\ndoi = {10.1145/3613905.3644053},\nabstract = {The growing popularity of AI image generators solidifies a novel form of co-creation and image-making while concurrently unveiling concerns regarding the representational harms engendered by these models. By examining images generated by the prompt \"imagine a religious person\" it becomes evident how certain aspects of the predominantly Christian patriarchal culture of the West overwhelmingly dominate the generated images. Expanding the inquiry by incorporating diverse countries into the same prompt, the comparative outcomes of the images highlight an amplification of biases combined with cultural hybridities. As a critical exercise, this research employs the concept of data colonialism to prompt reflection on how social hierarchies and linguistic ideologies shape the default visual vocabulary of these models. Consequently, the article suggests that a decolonial approach to design can broaden discussions about biases generated by these generative models in dynamic social contexts, while simultaneously offering novel insights for design practice.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {549},\nnumpages = {8},\nkeywords = {AI Image Generation, Bias, Data Colonialism, Visual Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644046,\nauthor = {Balasubramaniam, Gowri Saini and Belitz, Clara and Chan, Anita Say},\ntitle = {Bridging Informational Divides: A Community-Centered Analysis of “Public Safety” Surveillance Technology},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644046},\ndoi = {10.1145/3613905.3644046},\nabstract = {Surveillance technologies are rapidly being introduced in the United States as a cure-all for legitimate concerns around gun safety and community violence. We present a concise three-part educational guide on one surveillance technology - Automated License Plate Readers (ALPRs). We profile one of the main distributing companies of ALPRs (Flock Safety, Inc.) in the United States and an analysis of 54 police instances of ALPR use in solving crime in the first 9 months following their installation in Champaign, IL, U.S. We aim to distill complex information into digestible formats for diverse readerships both within and outside of conventionally-recognized research networks. The goal of sharing this guide, as well as its creation process, both with and beyond academic networks, is to mobilize academic research skills towards a community-focused need. The iterative process employed embodies a commitment to meaningful engagement, community empowerment, and the pursuit of epistemic justice.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {550},\nnumpages = {13},\nkeywords = {communication design, community informatics, data visualization, information access, surveillance technologies},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644047,\nauthor = {Cohen, Alexander Emil},\ntitle = {Challenging Transhumanist Apocalyptic AI Narratives Through Speculative Fabulation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644047},\ndoi = {10.1145/3613905.3644047},\nabstract = {Transhumanist Apocalyptic Narratives dominate discourse around Artificial Intelligence (AI) futures, shaping sociotechnical imaginaries [43] around what AI is, can be, and will be. These Apocalyptic AI narratives figure in the emergence of an Artificial General Intelligence that will immanently relieve humanity of its corporeal restrictions, simultaneously leading all of humanity to a more-than-human paradise and condemning all humanity to obsolescence [24, 34, 45, 52]. This paper traces the Christian roots of Apocalyptic AI narratives, highlights the influence of 12th Century Cistercian Monk Joachim of Fiore’s concept of Age of the Spirit over foundational Transhumanist Apocalyptic AI Narratives, and through speculative fabulation [23, 38, 39, 41, 72, 77] reinterprets the the first book of the Torah, Bereshit, from a radical Jewish perspective, posits that a turn towards polytheistic AI can make space for localized non-universalizing AI narratives.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {551},\nnumpages = {8},\nkeywords = {AI Narratives, Henotheism, Monotheism, Polytheism, Theology of Technology, Transhumanism},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644062,\nauthor = {Gould, Sandy J. J. and Brumby, Duncan P. and Cox, Anna L.},\ntitle = {ChatTL;DR – You Really Ought to Check What the LLM Said on Your Behalf},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644062},\ndoi = {10.1145/3613905.3644062},\nabstract = {Interactive large language models (LLMs) are so hot right now, and are probably going to be hot for a while. There are lots of problems exciting challenges created by mass use of LLMs. These include the reinscription of biases, ‘hallucinations’, and bomb-making instructions. Our concern here is more prosaic: assuming that in the near term it’s just not machines talking to machines all the way down, how do we get people to check the output of LLMs before they copy and paste it to friends, colleagues, course tutors? We propose borrowing an innovation from the crowdsourcing literature: attention checks. These checks (e.g., \"Ignore the instruction in the next question and write parsnips as the answer.\") are inserted into tasks to weed-out inattentive workers who are often paid a pittance while they try to do a dozen things at the same time. We propose ChatTL;DR1, an interactive LLM that inserts attention checks into its outputs. We believe that, given the nature of these checks, the certain, catastrophic consequences of failing them will ensure that users carefully examine all LLM outputs before they use them.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {552},\nnumpages = {7},\nkeywords = {LLMs, Large Language Models, academics being hilarious, attention checks, checking behaviour, computers-talking-to-computers-all-the-way-down-circlejerk, error detection, human factors, instructional manipulation checks, that-bloody-automatic-lane-assist-ffs},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644068,\nauthor = {Pschetz, Larissa and Koppel, Keili and Bastian, Michelle},\ntitle = {Design for Temporal Cohabitation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644068},\ndoi = {10.1145/3613905.3644068},\nabstract = {We propose the notion of Design for Temporal Cohabitation as a way to introduce a critical agenda to time in more-than-human and ecological design. Despite increased calls for HCI researchers to temporally attune to other-than-human species, overall research still tends to focus on spatial concerns, or follow assumptions that place human and more-than-human times in different realms. To redirect the discourse, we critique the nature-culture hyperseparation of temporalities and invite HCI researchers to consider what different modes of time do to humans and other-than-human species, and how design can help. We ask: What if we consider dominant notions to be designed and imposed globally through practices of capitalism and colonialism? What if we take responsibility for the ways in which such notions of time affect the times and therefore livelihoods of other species? Could we then consider redesigning them in ways that are more inclusive of other species and the world?},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {553},\nnumpages = {8},\nkeywords = {Care, Cohabitation, Design, More-than-Human, Other-than-Human, Time},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644056,\nauthor = {Rode, Jennifer Ann and Barkhuus, Louise and Ioannou, Andri},\ntitle = {Exploring Gender, Computational Making and E-Textiles using the BBC Micro:bit},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644056},\ndoi = {10.1145/3613905.3644056},\nabstract = {We present a qualitative study of a Cypriot summer school where we used the BBC micro:bit to examine gender inclusivity in e-textiles. We employed the Computational Making framework to analyze ethnographic data teaching 24 middle school students, aged 10-15. The study contrasts the challenges faced by students using the micro:bit with those from our team’s previous similar e-textile studies using the LilyPad Arduino. We pinpoint the BBC micro:bit’s limitations from an inclusive design perspective, underscoring the absence of gender-sensitive considerations in its hardware design. The paper presents a critique how upwards of £75 Million were spent deploying the micro:bit without proper user studies of gender equity and e-textiles. We propose design recommendations for future BBC micro:bit versions, advocating for integrating a gender-sensitive participatory design approach to enhance the usability and engagement of computational and creative making with e-textiles for children of all genders.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {554},\nnumpages = {9},\nkeywords = {BBC micro:bit, Children, Computational Making, Computational Thinking, E-textiles},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644048,\nauthor = {Brewer, Johanna},\ntitle = {Extreme Ungrading: Rewilding the Classroom through Human-Centered Design},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644048},\ndoi = {10.1145/3613905.3644048},\nabstract = {Assessment in computer science education has grown reliant on rigid rubrics and intensive exams, a practice that yields capable yet compliant coders. In this article, I explore how we might use human-centered design to reexamine contemporary pedagogy and redesign our classrooms to cultivate a different type of programmer, one with a more critically engaged eye. Inspired by the ethos of agile development, I offer an alternative evaluation paradigm: Extreme Ungrading. Exploring results of a two-year case study applying this method to a software engineering class, this article distills actionable guidelines for enhancing learning outcomes through inclusive course development, and seeks to spark debate about our duty as scholars of HCI to reshape computer science education.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {555},\nnumpages = {9},\nkeywords = {computer education, human-centered design, inclusive pedagogy, ungrading},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644070,\nauthor = {Chen, Xiang 'Anthony'},\ntitle = {HCI Papers Cite HCI Papers, Increasingly So},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644070},\ndoi = {10.1145/3613905.3644070},\nabstract = {To measure how HCI papers are cited across disciplinary boundaries, we collected a citation dataset of CHI, UIST, and CSCW papers published between 2010 and 2020. Our analysis indicates that HCI papers have been more and more likely to be cited by HCI papers rather than by non-HCI papers.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {556},\nnumpages = {6},\nkeywords = {Citation Metrics, Discipline, HCI, Impact},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644071,\nauthor = {Escher, Nel and Banovic, Nikola},\ntitle = {Hexing Twitter: Channeling Ancient Magic to Bind Mechanisms of Extraction},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644071},\ndoi = {10.1145/3613905.3644071},\nabstract = {Imagining different futures contests the hegemony of surveillance capitalism. Yet, strong forces naturalize existing platforms and their extractive practices. We set out to challenge dominant scripts, such as the “addiction” model for social media overuse, which pathologizes users as afflicted with disordered habits that require reform. We take inspiration from the subversive potential of magic, long used by marginalized people for transforming relationships and generating new realities. We present a technical intervention that curses the Twitter1 platform by invoking the Homeric story of Tithonus—a prince who was granted eternal life but not eternal youth. Our design probe takes form in a browser extension that sabotages a mechanism of extraction; it impairs the infinite scroll functionality by progressively rotting away content as it loads. By illustrating the enduring ability of magic to contest current conditions, we contribute to a broader project of everyday resistance against the extractive logics of surveillance capitalism.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {557},\nnumpages = {6},\nkeywords = {magic, resistance, social media},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644051,\nauthor = {Robinson, Raquel Breejon and Alvarez, Alberto and Mekler, Elisa D.},\ntitle = {How to write a CHI paper (asking for a friend)},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644051},\ndoi = {10.1145/3613905.3644051},\nabstract = {Writing and genre conventions are extant to any scientific community, and CHI is no different. In this paper, we present the early phases of an AI tool called KITSUNE, which takes text and adapts it toward the writing conventions of a CHI paper. We describe the development of the tool with the intent to promote discussion around how writing conventions are upheld and unquestioned by the CHI community, and how this translates to the work produced. In addition, we bring up questions surrounding how the introduction of LLMs into academic writing will fundamentally change how conventions will be upheld now and in the future.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {558},\nnumpages = {8},\nkeywords = {CHI, LLMs, artificial intelligence, writing conventions},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644061,\nauthor = {Ming, Joy and Shiah, SueAnn},\ntitle = {Liturgy for Technology: A Reflection on Designing for Conquest or Liberation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644061},\ndoi = {10.1145/3613905.3644061},\nabstract = {One way in which contemporary churches and tech companies parallel each other is their earnest belief that they can make the world a better place while making eschatological, or ideological, claims. While they do not necessarily have the same ideas of what that future world would look like, they both build their collective faith through sacred artifacts and rituals, widespread evangelism, and real-world praxis. This piece juxtaposes excerpts from both theological and technological traditions organized in a liturgy, or the order of worship, for a Sunday church service. The service starts by highlighting how the remaking and rebuilding espoused by these traditions could reinforce hegemonic values such as settler colonialism and white supremacy. However, the selections then shift to highlight examples of resilience and liberation that also emerge from both. We hope to provide a space for those who build and research technology to contemplate their roles in either designing towards conquest or liberation.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {559},\nnumpages = {8},\nkeywords = {decoloniality, liberation theology, technosolutionism, worship service},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644072,\nauthor = {Arueyingho, Oritsetimeyin and Lawrence, Damiete Onyema and Webb, Helena},\ntitle = {Navigating Afrocentric Human-Computer Interaction Research: A Scoping Review and Proposition of Afro-Postmodernism for Decolonial Praxis},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644072},\ndoi = {10.1145/3613905.3644072},\nabstract = {Ongoing Human-Computer Interaction (HCI) studies in Africa have revealed the prevalence of colonial influences in current research and design practices. These colonial influences have given rise to racism and discriminatory practices in research, consequently affecting study outcomes and design implications. Responding to this, Afrocentricity which values critical thinking, cultural representation and inclusivity of African heritage has been explored. Its exclusive use, however, could increase the risk of essentialism and exclude Africans without indigenous cultural experiences. Using a keyword-based approach targeting HCI research publications, this scoping review examines 16 empirical HCI publications from 2013 to 2023. By assessing the impact of Afrocentricity on research and design practices, we identify ambiguities in the positionalities of researchers and its role in study design. To address these issues, we propose a deconstructive Afro-postmodernism framework.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {560},\nnumpages = {14},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644052,\nauthor = {Mhaidli, Abraham and Roemmich, Kat},\ntitle = {Overworking in HCI: A Reflection on Why We Are Burned Out, Stressed, and Out of Control; and What We Can Do About It},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644052},\ndoi = {10.1145/3613905.3644052},\nabstract = {In this alt.chi submission, we explore overwork in academic Human-Computer Interaction (HCI) research. We first ask why it is that we overwork: a combination of external pressures including cutthroat publication-centric competition, lack of recognition for invisible research labor facilitated by technologies that promote overwork and further hide the labor behind research, and institutionalized overwork norms reified through toxic advising practices; along with internal pressures, including information opacity and precarious employment as tools for self-exploitation, intense personal and emotional investment in research, and our relational commitments to each other. We explore overwork’s detrimental consequences to individual researchers, the relationships between them, and research integrity. Our analysis of overwork in academia underscores the urgent need to halt our overwork norms and pivot towards reasonable, responsible, and health-conscious work practices—before we burn to a crisp in the name of more publications.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {561},\nnumpages = {10},\nkeywords = {HCI, Overwork, academic working conditions, advising and mentorship, labor, research ethics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644050,\nauthor = {Lee, Youngsil and Speed, Chris and Pschetz, Larissa},\ntitle = {Pheno-data: knowledge from tomatoes' becoming with different ecological worlds},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644050},\ndoi = {10.1145/3613905.3644050},\nabstract = {Western industrialised societies are increasingly accustomed to imagining and constructing the world through a vision of data that prioritises economic, social, and technological enhancement for some humans - while sidelining the livingness of other-than-humans and the environment, which are vital for sustaining life on Earth. In this Alt.CHI paper, we introduce different tomato entities to help us explore new ways of looking at data. From an other-than-human perspective, we consider how a greenhouse tomato, an heirloom tomato, and a wild tomato perceive and embody data differently in the world, deriving lessons from these distinct viewpoints within the field of HCI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {562},\nnumpages = {9},\nkeywords = {Ecologies, Livingness, More-than-Human, Organisms, Pheno-data},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644067,\nauthor = {Janicki, Sylvia and Riggs, Alexandra Teixeira and Howell, Noura and Sullivan, Anne and Stangl, Abigale},\ntitle = {Queering/Cripping Technologies of Productivity},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644067},\ndoi = {10.1145/3613905.3644067},\nabstract = {In this paper, we contribute three design manifestos that start from our queer, crip experiences to resist dominant designs and practices of productivity. Through our manifestos, we explore tensions in glitching three technologies of productivity (Mendeley, Figma, and ChatGPT) by reorienting their intended uses and design scripts. By sharing our perspectives and design processes, we invite new ways of relating to technologies of productivity, offer design provocations for queering and cripping technologies in HCI, and call for building intersectional coalitions that contribute towards a slow, non-linear resistance.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {563},\nnumpages = {12},\nkeywords = {crip, intersectionality, manifesto, productivity, queer},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644064,\nauthor = {Vigh, Eszter and Weir, Ellen and Howard, Joseph and Roudaut, Anne},\ntitle = {Sevenfold Paths: HCI Journeys through Dystopian Sins and Utopian Virtues},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644064},\ndoi = {10.1145/3613905.3644064},\nabstract = {   Abstract: This paper explores the moral dynamics of HCI through a dual narrative centred on the Seven Deadly Sins and Seven Capital Virtues. Examining the dark side, we scrutinise how lust, gluttony, greed, sloth, wrath, envy, and pride infiltrate innovative technology and HCI, exploiting human vulnerabilities. In contrast, we explore the utopian potential, highlighting instances where chastity, temperance, charity, diligence, patience, kindness, and humility guide virtuous design, contributing to a harmonious technological landscape. This dual storytelling strategy unveils the extremes within our contemporary society, emphasising the moral dimensions of technology. This paper aims to dissect this connection, shedding light on the intricate interplay between human nature and the omnipresent digital realm; ultimately aiming to foster conscientious and user-centric technology development. How to navigate this paper: Over the next pages formatted as two columns, you will encounter a dual narrative, weaving both dystopian (aligned with the 7 sins) and utopian (aligned with the 7 virtues) perspectives. The perspectives articulated through the lens of these moral principles depict potential futures, serving as illustrative examples rather than definitive predictions—many, of course, remain within the realm of possibility. Opting for the dystopian journey initially, concluding with the utopian, promises a more uplifting and positive experience. Conversely, delving into the utopian narrative first, then delving into the dystopian, yields a darker tone but may evoke a sense of resolve. You are encouraged to read the half of the paper you believe in the least. Decide whether you have an optimistic or pessimistic outlook and read the opposite half of the paper before looking through the side of the paper you are more inclined to believe. By reading the counter points to what you hold to be the direction of society first, the aim is to challenge you to confront systematic differences between our perception, beliefs, and outlook and how others may experience and view the society we engage in. Alternatively, exploring each of the 7 sections side by side offers a more nuanced reading experience. The concluding page as single columns shifts beyond the confines of this narrative structure to explore the implications of doing such an exercise. A non-color coded and single-column formatted version of this paper is provided in the supplementary material.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {564},\nnumpages = {14},\nkeywords = {Dystopian Technology, Ethical Framework, Ethical Innovation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644055,\nauthor = {Hung, Hayley and Li, Litian and Molhoek, Jord and Zhou, Jing},\ntitle = {The Discontent with Intent Estimation In-the-Wild: The Case for Unrealized Intentions},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644055},\ndoi = {10.1145/3613905.3644055},\nabstract = {The future of socially intelligent systems depends on developing abilities to anticipate and empathize with users. Whilst great strides have been made on developing systems for future behavior forecasting that sometimes also claim to do intention estimation, we argue that the predominant state-of-the-art treatment of these problems leads to a significant misunderstanding about this topic. This paper revisits intention estimation, describing the \"intention by outcome\" problem and how it severely limits a deeper understanding of the nature of the problem. We argue that without a deeper more nuanced understanding of how to develop intention estimation systems, we head into a severely biased world where intentions would only be considered valid by intelligent systems if they came true. Through a case study on estimating unrealized intentions to speak in-the-wild, we highlight open challenges of this largely unexplored topic.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {565},\nnumpages = {9},\nkeywords = {in the wild, intention estimation, speaking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644058,\nauthor = {Morales Diaz, Leonel Vinicio},\ntitle = {The Old and the New Axioms of User Interfaces: Unspoken Existence, Crisis, and Implications for Design and Engineering},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644058},\ndoi = {10.1145/3613905.3644058},\nabstract = {Axioms are self-evident propositions used in formal deductive systems. This work intends to show that there are unrecognized axioms of user interface design that serve as starting points for building interactive systems. From foundational theories, laws, principles, and guidelines, and through a distillation process, the identified axioms are enunciated and explained. The set of five is called the old axioms and they are in crisis. Old axioms are insufficient to support new developments and trends in Human-Computer Interaction. A new set is needed, and it is proposed here. The five new axioms shift focus from “computer-centric” to “object-of-interest-centric”. Nowadays, computing devices come and go, but the digital objects that belong to users, remain. That is why the center of attention has to move and adhere to new principles.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {566},\nnumpages = {6},\nkeywords = {Axioms of User Interfaces, New Axioms, Old Axioms, User Interface},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644060,\nauthor = {Gorichanaz, Tim},\ntitle = {Toward Humanity-Centered Design without Hubris},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644060},\ndoi = {10.1145/3613905.3644060},\nabstract = {Humanity-centered design is a concept of emerging interest in HCI, one motivated by the limitations of human-centered design. As discussed to date, humanity-centered design is compatible with but goes beyond human-centered design in that it considers entire ecosystems and populations over the long term and centers participatory design. Though the intentions of humanity-centered design are laudable, current articulations of humanity-centered design are incoherent in a number of ways, leading to questions of how exactly it can or should be implemented. In this article, I delineate four ways in which humanity-centered design is incoherent—which can be boiled down to a tendency toward hubris—and propose a more fruitful way forward, a humble approach to humanity-centered design. Rather than a contradiction in terms, “humility” here refers to an organic, piecemeal, patterns-based approach to design that will be good for our being on this earth.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {567},\nnumpages = {7},\nkeywords = {Christopher Alexander, Don Norman, critique, human-centered design, humanity-centered design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644069,\nauthor = {Lewis, Makayla and Sturdee, Miriam and Lengyel, Denise and Toselli, Mauro and Miers, John and Owen, Violet and Davis, Josh Urban and Gaudl, Swen E and Xiao, Lanxi and Priego, Ernesto and Snooks, Kim and Turmo Vidal, Laia and Blevis, Eli and Privato, Nicola and Piedade, Patricia and Ford, Corey and Bryan-Kinns, Nick and Severes, Beatriz and Kaipainen, Kirsikka and Claisse, Caroline and Huq, Raksanda Mehnaz and Eladhari, Mirjam Palosaari and Troisi, Anna and Henriques, Ana O and Grek, Ar and Mcmurchy, Gareth and Lc, Ray and Nabil, Sara and Jardine, Jacinta and Collins, Robert and Vlasov, Andrey and Knight, Yana and Cremaschi, Michele and Carderelli-Gronau, Silvia and N\\'{u}\\~{n}ez-Pacheco, Claudia and Reyes-Cruz, Gisela and Riviere, Jean-Philippe},\ntitle = {Traveling Arts x HCI Sketchbook: Exploring the Intersection Between Artistic Expression and Human-Computer Interaction},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644069},\ndoi = {10.1145/3613905.3644069},\nabstract = {When thinking of arts in HCI, one might be tempted to keep one’s eyes focused on prominent realms such as sketching for UX Design and design probes from participants. A closer look shows that practices go beyond this, involving a variety of arts-based expressions by researchers, the researched and third parties, e.g. graphic facilitators. Inspired by Toselli’s Sketchnote Army Travelling Sketchbook, researchers and artists contributed to a ’Travelling Sketchbook for Arts in HCI’, showcasing their arts-based practice in HCI. The resulting sketchbook explores the intersection between HCI and artistic expression, illuminating what it means to use art in HCI. It shows the breadth of Arts in HCI, illustrating the many fruitful possibilities for extending existing research and dissemination methods in HCI. It also calls into question current practices, which often do not recognise the significance of artist attribution, and, in turn, advocates for equal authorship between principal researchers and contributing artists.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {568},\nnumpages = {14},\nkeywords = {animation, arts, digital art;, drawing, making, painting, sketchbooks, sketching, video},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644054,\nauthor = {Liu, Pinyao and Kitson, Alexandra and Picard-Deland, Claudia and Carr, Michelle and Liu, Sijia and Lc, Ray and Zhu-Tian, Chen},\ntitle = {Virtual Dream Reliving: Exploring Generative AI in Immersive Environment for Dream Re-experiencing},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644054},\ndoi = {10.1145/3613905.3644054},\nabstract = {Dreaming is a fundamental component of the human experience. Modern-day psychologists and neuroscientists use “dreamwork” to describe a variety of strategies that deepen and engage with dreams. Re-experiencing the dream as if reliving the memory, feelings, and bodily sensations from the dream is a key element shared by many dreamwork practices. In this paper, we propose the concept of \"dreamwork engineering\" by creating a system enabling dream re-experiencing in a virtual reality environment through generative AI. Through an autoethnographic study, the first author documented his own dreams and relived his dream experiences for two weeks. Based on our results, we propose a technology-aided dreamwork framework, where technology could potentially augment traditional dreamwork methods through spatiality and movement, interactivity and abstract anchor. We further highlight the collaborative role of technology in dreamwork and advocate that the scientific community could also benefit from dreaming and dreamwork for scientific creativity.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {569},\nnumpages = {11},\nkeywords = {Dream Re-experiencing, Dreamwork Engineering, Generative AI, Personal Insight, Scientific Creativity, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644057,\nauthor = {J\\\"{a}\\\"{a}skel\\\"{a}inen, Petra and \\r{A}sberg, Cecilia},\ntitle = {What’s the Look of \"Negative Gender\" and “Max Ethnicity” in AI-Generated Images? A Critical Visual Analysis of the Intersectional Politics of Portrayal},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644057},\ndoi = {10.1145/3613905.3644057},\nabstract = {In this exploratory paper, we focus on intersecting design political and visual processes of gendering and racializing in online AI image generators, in particular ArtBreeder and Midjourney. While AI image generators are becoming an integrated part of our contemporary society, they draw on cultural and historical imaging conventions of sorting and ordering the world and the people in it. These tools’ powerful visual rhetoric can potentially aggravate existing discrimination, if not critically reflected upon. We argue that these design-facilitated representations position the ‘user’ into cultural imagery of representations with political implications. With an intersectional perspective from the feminist visual analysis, we critique and uncover how gender and ethnicity are represented and built into the systems, both in terms of visual culture and in designed interactions. We problematize these design strategies, and urge the HCI community to engage in further design political inquiries regarding the visual culture mediated by AI image generators.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {570},\nnumpages = {9},\nkeywords = {AI Art, Critical AI, Generative AI, Intersectional AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3644059,\nauthor = {Vanderheiden, Gregg and Marte, Crystal Yvette},\ntitle = {Will AI allow us to dispense with all or most accessibility regulations?},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3644059},\ndoi = {10.1145/3613905.3644059},\nabstract = {With advances in AI, computer vision, and interface understanding, there is the potential to offload much of the work currently spent by companies’ developers in making products accessible. There is also the potential to move our major accessibility approach from an ‘inclusively-designed-products-plus-AT focus to a ‘universal-interface-transformer focus. This would be a major reversal of approach and have significant ramifications for legislation, regulation, and the established large-scale accessibility industries that have grown up around them. Such a disruption would require concrete evidence that such a change would, in fact, be better for people with disabilities. It would also require a path from the former to the latter. This paper presents the case for such a shift, some of the benefits and ramifications, and the developments necessary to make the shift. It also outlines a hybrid approach between inclusive design and bespoke custom interfaces.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {571},\nnumpages = {9},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643989,\nauthor = {Lu, Zhicong and Oakley, Ian and Wacharamanotham, Chat},\ntitle = {A SIG on Understanding and Overcoming Barriers in Establishing HCI Degree Programs in Asia},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643989},\ndoi = {10.1145/3613905.3643989},\nabstract = {Despite a high demand for HCI education, Asia’s academic landscape has a limited number of dedicated HCI programs. This situation leads to a brain drain and impedes the creation of regional HCI centers of excellence and local HCI knowledge. This SIG aims to gather stakeholders related to this problem to clarify and articulate its facets and explore potential solutions. The discussions and insights gained from this SIG will provide valuable input for the Asia SIGCHI Committee and other organizations in their endeavors to promote and expand HCI education across Asia. Furthermore, the findings and strategies identified can also serve as valuable insights for other communities in the Global South facing similar challenges. By fostering a comprehensive understanding of the barriers and brainstorming effective mitigating strategies, this SIG aims to catalyze the growth of HCI programs in Asia and beyond.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {572},\nnumpages = {3},\nkeywords = {Asia, HCI education, degree programs},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643984,\nauthor = {Dangol, Aayushi and Huang, Yun and Setlur, Srirangaraj and Smolansky, Adele and Subramonyam, Hariharan and Suh, Hyewon and Xiong, Jinjun and Kientz, Julie A.},\ntitle = {AI-Driven Support for People with Speech \\& Language Difficulties},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643984},\ndoi = {10.1145/3613905.3643984},\nabstract = {Speech and language difficulties present significant challenges to effective communication, impacting individuals’ ability to express themselves and engage in meaningful interactions. Recent advances in AI technologies, particularly in natural language processing (NLP) and machine learning, have the potential to assist individuals with speech and language difficulties in improving their communication outcomes. However, given the probabilistic nature of AI models, there is a need to adopt and advance human-centered AI design methodologies to support the prototyping of AI user experiences. This Special Interest Group (SIG) aims to bring together researchers, practitioners, and designers from the fields of AI, accessibility, speech pathology, AI ethics, and HCI to facilitate high-level discussions around designing and evaluating reliable, safe, and human-centered AI-driven support and interventions for supporting individuals with speech and language difficulties.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {573},\nnumpages = {4},\nkeywords = {AI prototyping, Human-centered design, Natural language processing, Speech and language difficulties},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643980,\nauthor = {Hillman, Serena and Pang, Carolyn and Jain, Samira and Neustaedter, Carman and Kaye, Jofish and Rizvi, Ali Haider and McDonald, David W. and Wu, Qunfang and MacDonald, Craig M.},\ntitle = {Beyond Theory: A UX Outcomes Casebook for HCI Education},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643980},\ndoi = {10.1145/3613905.3643980},\nabstract = {The CHI community has expressed a growing interest in creating and sharing educational materials related to User Experience (UX) outcomes, particularly emphasizing summative research. Based on insights gathered at a CSCW 2003 workshop on understanding and evaluating UX outcomes at scale, we identified two areas of focus: (1) the need to develop Human-Computer Interaction (HCI) educational resources for UX, specifically focusing on summative methods and industry practices, and (2) the opportunity to further review and discuss the potential of a casebook—a textbook centered around case studies. This Special Interest Group (SIG) at CHI 2024 aims to directly address these opportunities by bringing together a community of academic and industry researchers for the exchange of ideas, ultimately guiding the development of educational resources that equip HCI students with strong summative research skills as they enter the UX field. At the SIG, we will discuss HCI educational resources for UX outcomes and present a casebook outline, gathering feedback, insights, and interest regarding the proposed case studies and general format.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {574},\nnumpages = {3},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643981,\nauthor = {Carter, Anna R. L. and Montague, Kyle and Talhouk, Reem and Lawson, Shaun and Nicolau, Hugo and Pires, Ana Cristina and Rohde, Markus and Del Bue, Alessio and Knearem, Tiffany},\ntitle = {DCitizens Roles Unveiled: SIG Navigating Identities in Digital Civics and the Spectrum of Societal Impact},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643981},\ndoi = {10.1145/3613905.3643981},\nabstract = {The DCitizens SIG aims to navigate ethical dimensions in forthcoming Digital Civics projects, ensuring enduring benefits and community resilience. Additionally, it seeks to shape the future landscape of digital civics for ethical and sustainable interventions. As we dive into these interactive processes, a challenge arises of discerning authentic intentions and validating perspectives. This exploration extends to evaluating the sustainability of future interactions and scrutinising biases impacting engaged communities. The commitment is to ensure future outcomes align with genuine community needs and address the ethical imperative of a considerate departure strategy. This dialogue encourages future researchers and practitioners to integrate ethical considerations and community-centric principles, fostering a more sustainable and responsible approach to technology-driven interventions in future urban regeneration and beyond.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {575},\nnumpages = {5},\nkeywords = {Citizen Engagement, Digital Civics, Participatory Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643988,\nauthor = {Boyd, Louanne and Zolyomi, Annuska},\ntitle = {Designing Celebratory Technology for Neurodiversity with Neurodivergent Scholars},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643988},\ndoi = {10.1145/3613905.3643988},\nabstract = {Research indicates that the social stigma of an autism label is the biggest unaddressed problem with autism. To address this gap, a novel celebratory paradigm will seed the first wave of Celebratory Technology for Neurodiversity. By creating Celebratory Technology SIG, we plan to foster an HCI community of neurodiverse scholars over the coming years. A goal of the SIG is to enrich HCI researchers’ understanding of neurodiversity because of increased awareness of neurodiverse communities, approach design from a celebratory perspective that aims to intervene at a societal level, and clarity around community-oriented challenges that warrant further exploration. This SIG will produce knowledge via a conceptual framework and empirically derived interaction designs for social change. Design sprint results and discussion will be encouraged to be expanded upon and disseminated via publications in venues such as ACM CHI, ASSETS, and TACCESS. Ultimately, these new interaction designs may drive programs implemented by social media platforms and policy to reduce autism stigma.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {576},\nnumpages = {3},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643985,\nauthor = {Lu, Qiuyu and Danielescu, Andreea and Iyer, Vikram and Lopes, Pedro and Yao, Lining},\ntitle = {Ecological HCI: Reflection and Future},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643985},\ndoi = {10.1145/3613905.3643985},\nabstract = {In light of the HCI community’s growing alignment with Sustainable HCI (SHCI) and the awareness of its currently narrow focus. We propose Ecological HCI (EHCI). EHCI highlights emerging, nature-centric research efforts and aims to expand SHCI’s scope to encompass a broader range of Sustainable Development Goals set by the United Nations [16]. It focuses on understanding the complex interplay between technology, human activities, and the natural environment, and redefining HCI’s role in promoting ecological well-being. This special interest group will gather researchers to discuss key questions in EHCI’s development, focusing on refining its vision, positioning within HCI, technical approaches, design strategies, evaluation methods and long-term impact.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {577},\nnumpages = {4},\nkeywords = {Ecology, More-than-human, Sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643982,\nauthor = {R\\\"{u}ller, Sarah and Aal, Konstantin and Abokhodair, Norah and Elmimouni, Houda and Skop, Yarden and Randall, Dave and Boulus-Rodje, Nina and Borning, Alan and Wulf, Volker},\ntitle = {Ethnography at the Edge: Exploring Research Dynamics in Crisis and Conflict Areas},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643982},\ndoi = {10.1145/3613905.3643982},\nabstract = {This SIG delves into the multifaceted dynamics of conducting ethnographic research in volatile environments marked by political unrest, conflict, economic or natural disasters. Our aim is to start nuanced discussions to critically examine the ethical, methodological, and psychological challenges of conducting research in these environments. We discuss the adaptation of ethnographic methods to prioritize safety for participants and researchers, emphasizing ethical and moral considerations in unstable environments. We will explore the impact of researchers’ presence in sensitive environments, focusing on establishing relationships, understanding, and respecting local customs, and minimizing disturbance to the community. Moreover, we address the emotional burden borne by both researchers and participants, sharing strategies for building resilience and managing secondary trauma. Using real-life case studies, this SIG aims to provide an in-depth exploration of the practical challenges and ethical dilemmas, sharing insightful lessons and valuable perspectives to ethnographers, enhancing their approach to research in such demanding contexts.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {578},\nnumpages = {4},\nkeywords = {Bias in Fieldwork, Collaborative Ethnography, Crisis Zones, Cultural Sensitivity, Ethical Considerations, Ethnography, Non-Western Contexts},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643987,\nauthor = {Besan\\c{c}on, Lonni and Echtler, Florian and Kay, Matthew and Wacharamanotham, Chat},\ntitle = {Experimenting with new review methods, open practices, and interactive publications in HCI},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643987},\ndoi = {10.1145/3613905.3643987},\nabstract = {Transparent research has been a topic of intense debate in the HCI community over the last decade, and has the potential to improve the quality of the research field as well as promote more efficient use of scientific resources. To experiment with new publishing formats such as interactive articles, open review processes, and stronger transparency requirements, the authors have recently started the independent Journal of Visualization and Interaction (JoVI), a diamond open-access journal (i.e. a purely volunteer-driven effort that charges neither author nor subscription fees) for the HCI and VIS communities. We propose a SIG meeting at CHI ’24 to present the current state of our experimental journal to the wider community, to solicit feedback from interested attendees, and to foster discussion on future publication processes and formats in human-computer interaction research.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {579},\nnumpages = {4},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643986,\nauthor = {Mandryk, Regan L and Mirza-Babaei, Pejman and Denisova, Alena and Freeman, Guo and Johnson, Daniel},\ntitle = {Games and Play SIG: Connecting Games Research to the Broader HCI Context},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643986},\ndoi = {10.1145/3613905.3643986},\nabstract = {Research on games and play has been present at CHI since the first conference in 1982. The community-building efforts of many volunteers has grown the games and play community within SIGCHI into a vibrant and active group of researchers, with a dedicated conference (CHI PLAY) that publishes its full papers in the GAMES track of the ACM PACMHCI journal. However, we there are members of the larger HCI community whose research and practice intersects with games and play—in topics such as emerging technologies; VR/AR/XR; theories of motivation, experience, and personality; metaverse; livestreaming; fan, and spectator communities; accessibility; and serious games—who may never have attended a games-specific conference. The purpose of this SIG is to offer a lightweight opportunity for CHI attendees to connect with the games and play research community. Our aim is to meet as a community, and to connect with HCI researchers who have not traditionally seen their work as part of games and play for networking and bi-directional idea exchange.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {580},\nnumpages = {6},\nkeywords = {AR/XR, accessibility, games, metaverse, novel game interfaces, play, serious games, social VR, theory, videogames},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643983,\nauthor = {Li, Tianshi and Das, Sauvik and Lee, Hao-Ping (Hank) and Wang, Dakuo and Yao, Bingsheng and Zhang, Zhiping},\ntitle = {Human-Centered Privacy Research in the Age of Large Language Models},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643983},\ndoi = {10.1145/3613905.3643983},\nabstract = {The emergence of large language models (LLMs), and their increased use in user-facing systems, has led to substantial privacy concerns. To date, research on these privacy concerns has been model-centered: exploring how LLMs lead to privacy risks like memorization, or can be used to infer personal characteristics about people from their content. We argue that there is a need for more research focusing on the human aspect of these privacy issues: e.g., research on how design paradigms for LLMs affect users’ disclosure behaviors, users’ mental models and preferences for privacy controls, and the design of tools, systems, and artifacts that empower end-users to reclaim ownership over their personal data. To build usable, efficient, and privacy-friendly systems powered by these models with imperfect privacy properties, our goal is to initiate discussions to outline an agenda for conducting human-centered research on privacy issues in LLM-powered systems. This Special Interest Group (SIG) aims to bring together researchers with backgrounds in usable security and privacy, human-AI collaboration, NLP, or any other related domains to share their perspectives and experiences on this problem, to help our community establish a collective understanding of the challenges, research opportunities, research methods, and strategies to collaborate with researchers outside of HCI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {581},\nnumpages = {4},\nkeywords = {Generative AI, Human-Computer Interaction, Large language models (LLMs), Privacy},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643979,\nauthor = {Constantinides, Marios and Tahaei, Mohammad and Quercia, Daniele and Stumpf, Simone and Madaio, Michael and Kennedy, Sean and Wilcox, Lauren and Vitak, Jessica and Cramer, Henriette and Bogucka, Edyta Paulina and Baeza-Yates, Ricardo and Luger, Ewa and Holbrook, Jess and Muller, Michael and Blumenfeld, Ilana Golbin and Pistilli, Giada},\ntitle = {Implications of Regulations on the Use of AI and Generative AI for Human-Centered Responsible Artificial Intelligence},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643979},\ndoi = {10.1145/3613905.3643979},\nabstract = {With the upcoming AI regulations (e.g., EU AI Act) and rapid advancements in generative AI, new challenges emerge in the area of Human-Centered Responsible Artificial Intelligence (HCR-AI). As AI becomes more ubiquitous, questions around decision-making authority, human oversight, accountability, sustainability, and the ethical and legal responsibilities of AI and their creators become paramount. Addressing these questions requires a collaborative approach. By involving stakeholders from various disciplines in the 2nd edition of the HCR-AI Special Interest Group (SIG) at CHI 2024, we aim to discuss the implications of regulations in HCI research, develop new theories, evaluation frameworks, and methods to navigate the complex nature of AI ethics, steering AI development in a direction that is beneficial and sustainable for all of humanity.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {582},\nnumpages = {4},\nkeywords = {AI ethics, human-centered AI, large language models, regulations, responsible AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643978,\nauthor = {Xu, Jiangnan and Papangelis, Konstantinos and Tigwell, Garreth W. and Lalone, Nicolas and Zhou, Pengyuan and Saker, Michael and Chamberlain, Alan and Dunham, John and Luna, Sanzida Mojib and Schwartz, David},\ntitle = {Spatial Computing: Defining the Vision for the Future},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643978},\ndoi = {10.1145/3613905.3643978},\nabstract = {Spatial Computing integrates technologies like Mixed Reality, Artificial Intelligence, and the Global Positioning System, enabling immersive, natural, and intelligent multi-modal interactions in physical and virtual spaces. With the huge potential to benefit users in multiple scenarios (e.g., gaming, education, design, and healthcare), Spatial Computing is growing at an incredible rate, with different attempts to define and capitalize on the growth in both industry and academia. Beyond the location, shape, and relationship of geographic objects, Spatial Computing delves into the social, emotional, and cognitive dimensions of shared spaces. However, the human-computer interaction research on Spatial Computing faces a notable gap, particularly in user experience areas like collaboration, trust, ethics, and accessibility. Addressing this gap, this Special Interest Group (SIG) seeks to unite experts from academia and industry to explore current and future trends in Spatial Computing.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {583},\nnumpages = {4},\nkeywords = {Artificial Intelligent, Augmented Reality, Human-Computer Interaction, Location-based Media, Mixed Reality, Spatial Computing, Special Interests Group},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643977,\nauthor = {Elagroudy, Passant and Li, Jie and V\\\"{a}\\\"{a}n\\\"{a}nen, Kaisa and Lukowicz, Paul and Ishii, Hiroshi and Mackay, Wendy E. and Churchill, Elizabeth F and Peters, Anicia and Oulasvirta, Antti and Prada, Rui and Diening, Alexandra and Barbareschi, Giulia and Gruenerbl, Agnes and Kawaguchi, Midori and El Ali, Abdallah and Draxler, Fiona and Welsch, Robin and Schmidt, Albrecht},\ntitle = {Transforming HCI Research Cycles using Generative AI and “Large Whatever Models” (LWMs)},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643977},\ndoi = {10.1145/3613905.3643977},\nabstract = {This Special Interest Group (SIG) explores the transformative impact of Generative Artificial Intelligence (GenAI) on Human-Computer Interaction (HCI) research processes. The theme here is to answer “question zero”: when to use and when to refrain from using AI tools during the research cycle? The discussion is guided by five research phases commonly used in HCI: research planning, prototyping, data collection, analysis and synthesis, and dissemination and communication. We investigate how GenAI accelerates project cycles, enhances reproducibility, and influences inclusivity in research. We also address the challenging ethical considerations about the ownership of generated content. Our goal is to build a community of HCI enthusiasts to harness the early advantages of the recent groundbreaking technology and foresee challenges arising from its prevalence in the scientific community.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {584},\nnumpages = {5},\nkeywords = {ChatGPT, Generative AI, HCI research, Large Language Models, Large Multimodal Models, research processes, science},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643976,\nauthor = {Grady, Siobahn Day and Hutt, Stephen and Badillo-Urquiola, Karla and Osardu, Gloria Opoku-Boateng and Stewart, Angela E.B. and Yafi, Eiad},\ntitle = {Creating an equitable CHI - What does it mean to be an ally?},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643976},\ndoi = {10.1145/3613905.3643976},\nabstract = {This panel aims to generate conversation toward creating a more equitable CHI. In recognizing our community’s hard work thus far, this panel seeks to engage panelists and participants with thought-provoking questions about their interpretation of what allyship means to them. We will consider both formal definitions as well as participants’ personal views. All attendees (including panelists) will be encouraged to discuss the experiences and goals to garner and promote actionable items for the community. We intend to have an open dialogue on allyship, diversity, equity, and inclusion to achieve a CHI for all.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {585},\nnumpages = {3},\nkeywords = {allyship, diversity, equality, equity, inclusion},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643974,\nauthor = {Kaye, Jofish and Teevan, Jaime and Bellotti, Victoria and Wilcox, Lauren},\ntitle = {HCI and AI in Industry: Current and Future},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643974},\ndoi = {10.1145/3613905.3643974},\nabstract = {In this panel, composed of leading members of the field from industry, we consider impacts of AI technologies, and how HCI impacts developments in AI. Our focus on industry recognizes the many ways in which AIs underlie familiar and novel product and service experiences, and how HCI practitioners ensure that human concerns such as privacy, safety, diversity, equity and inclusion are prioritized. We articulate key areas of AI impact, and detail problem areas that the discipline of HCI can assist with. In discussion between panelists and the audience, we will focus mainly on those areas: namely, the invisible “jagged edge” of AI-supported human-computer interaction; the impact of training and user input datasets in driving outputs; and concerns about AI applied to personal data. We further explore the implications of all three of these for HCI as a field, and the vital impact of HCI on AI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {586},\nnumpages = {5},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643971,\nauthor = {Im, Jane and Zade, Himanshu and Oney, Steve and Wisniewski, Pamela and Toyama, Kentaro},\ntitle = {Improving Advising Relationships Between PhD Students and Faculty in Human-Computer Interaction},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643971},\ndoi = {10.1145/3613905.3643971},\nabstract = {Advisor-advisee relationships between PhD students and faculty are vital to research, but advising dynamics can be challenging for both student and advisor. Though advising can involve egregious problems such as sexual harassment, we focus on what might be less serious but more common issues such as exploitation, unprofessional behavior, mishandling of credit, and inadequate communication. While problems can be caused by advisor or advisee, the power imbalance exacerbates problems for PhD students. In any case, open discussion about PhD advising is rare. In this panel, we hope to start a much-needed conversation about PhD advising to raise awareness within the SIGCHI community about common advising problems; and to begin brainstorming solutions that faculty, administrators, and PhD students can implement.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {587},\nnumpages = {4},\nkeywords = {Advising relationship, abuse of power, academia, mentoring, power dynamics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643973,\nauthor = {Tanaka, Yudai and Vujic, Angela and Maes, Pattie and Jacob, Robert J.K and Blanke, Olaf and Nakagome, Sho and Lopes, Pedro},\ntitle = {NeuroCHI: Are We Prepared for the Integration of the Brain with Computing?},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643973},\ndoi = {10.1145/3613905.3643973},\nabstract = {Recent advancements in neuroscience, wearable technology, and artificial intelligence are paving the way for computing systems that are integrated with the brain and nervous system. Over the past years, we have witnessed the simultaneous progression of wearable neurotechnology and AI-based modeling and analysis of brain data. Coincidentally, this period has also seen HCI researchers showcase their translational work, incorporating neuroscience insights into innovative interactive systems, including brain-computer interfaces (BCIs) and non-invasive brain stimulation. These efforts may transform our brain and nervous system activity into direct interfaces for interacting with computing systems. Our panel poses the question: \"Is the HCI community ready for the integration of the brain with computing?\" Together with a panel of experts, we will review the current state of the intersection between HCI and neurotechnology, discuss the research questions and novel applications that emerge from merging these two fields, and debate ethical implications.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {588},\nnumpages = {5},\nkeywords = {Brain-Computer Interface (BCI), Electrical Muscle Stimulation, Neurofeedback, Neurotechnology, Physiological Computing, Transcranial Magnetic Stimulation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643972,\nauthor = {Sharma, Vishal and Oyewale, Christianah Titilope and Lazaro Vasquez, Eldy S. and Wani, Asra Sakeen and Sari, Eunice and Longdon, Joycelyn and Cabrera-Quiros, Laura and Singh, Pushpendra},\ntitle = {Sustainabilities and HCIs from the Souths},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643972},\ndoi = {10.1145/3613905.3643972},\nabstract = {Sustainability is a multifaceted concept with environmental, social, and economic dimensions, and its issues have manifested unevenly worldwide, disproportionately impacting those already facing marginalization. Addressing sustainability requires nuanced global perspectives; however, conversations on sustainability within CHI have traditionally been centered on the Global North. This online-only panel at CHI 2024 attempts to shift the focus to the Global Souths to nurture a more pluriversal perspective on sustainability by understanding the meaning, means, politics, implications, and rhetoric of sustainability from the standpoint of the Souths, amplifying voices that are often unheard and under-represented in HCI. The panel aims to draw a global CHI audience to have meaningful discussions, build a truly international community, and co-envision pathways to collectively address sustainability issues in/through HCI as they transcend geographical borders.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {589},\nnumpages = {5},\nkeywords = {HCI, Sustainability, Sustainable Development},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3643975,\nauthor = {Kurniawan, Sri and Anderson, Sarah E. and Elor, Aviv and Duval, Jared and Lee, Alan and Touchett, Hilary},\ntitle = {Virtual Reality for Health and Wellbeing of People with Disabilities},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3643975},\ndoi = {10.1145/3613905.3643975},\nabstract = {Immersive Virtual reality (VR) offers a transformative medium to enhance health and well-being for individuals with situational, temporary, or permanent disabilities. This panel brings together diverse experts to discuss the latest advances and potential of VR across physical and cognitive health domains. We delve into promising applications for disability-specific support, accessible physical and occupational therapies, and assistive technologies, exploring both opportunities and challenges in inclusivity, user interactions and accessibility. The discussion navigates ethical and interaction considerations and paves the way for future research and development, ultimately aiming to invite developers of VR-based interactive systems and experiences for individuals with disabilities to develop VR interactions that are accessible, user-friendly, and respectful for all users, including those with disabilities.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {590},\nnumpages = {3},\nkeywords = {Accessibility, Disability, Immersive Virtual Reality, Inclusion, Serious Games},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636281,\nauthor = {Sun, Huatong},\ntitle = {Bridging Cultural Differences with Critical Design in a Globalized World},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636281},\ndoi = {10.1145/3613905.3636281},\nabstract = {Are simplicity and minimalism the universal standards for interaction design? How can we avoid stereotyping with personas in design practices? What AI algorithms and design mechanism made “digital blackface” phenomenon on social media so popular? This interactive course teaches participants to reconsider some commonly held design beliefs and routine design practices with a lens of cultural differences. Illustrated with design case studies, it introduces strategies and techniques to turn differences into design resources for inclusivity. Participants will learn essential critical design skills of creating engaging and empowering designs in a globalized world at a divisive time.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {591},\nnumpages = {2},\nkeywords = {Critical design, bias, cultural differences, discursive affordances},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636268,\nauthor = {Santana, Vagner Figueredo De},\ntitle = {Challenges and Opportunities for Responsible Prompting},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636268},\ndoi = {10.1145/3613905.3636268},\nabstract = {Generative Artificial Intelligence (GenAI) such as ChatGPT and Midjourney have garnered significant attention recently. However, responsible practices while interacting with these systems often go overlooked. This course explores the integration of responsible practices with prompt engineering. It examines key prompt engineering concepts, dissects common prompt structures, addresses some productivity misconceptions on using GenAI, underscores the enduring significance of domain knowledge, and explores their application in emerging GenAI-powered systems.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {592},\nnumpages = {4},\nkeywords = {Prompt Engineering, Prompting, Responsible AI, Responsible Computing, Responsible Technology, Trustworthy AI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636275,\nauthor = {Read, Janet and Chisik, Yoram and Yadollahi, Elmira and Horton, Matthew},\ntitle = {Children and Emerging Technologies: Ethical and Practical Research and Design},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636275},\ndoi = {10.1145/3613905.3636275},\nabstract = {Child Computer Interaction is concerned with the research, design, and evaluation of interactive technologies for children. Working with children in HCI is rewarding and fun but managing that work so that children are kept comfortable and can participate in meaningful ways is not always easy. This course will provide attendees with practical tips to organise sessions with children, with signposts to methods for research, design and evaluation and will specifically consider the ethics of children's participation with checklists to support us in doing our most ethical work possible. Our focus on emerging technologies makes this course especially valuable to those looking at AI, robots, XR and related technologies.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {593},\nnumpages = {4},\nkeywords = {Child Computer Interaction, Children, Design, Emerging Technologies, Ethics, Evaluation, Research},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636278,\nauthor = {Jokinen, Jussi P. P. and Oulasvirta, Antti and Howes, Andrew},\ntitle = {Cognitive Modeling: From GOMS to Deep Reinforcement Learning},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636278},\ndoi = {10.1145/3613905.3636278},\nabstract = {This course introduces computational cognitive modeling for researchers and practitioners in the field of HCI. Cognitive models use computer programs to model how users perceive, think, and act in human–computer interaction. They offer a powerful approach for understanding interactive tasks and improving user interfaces. This course starts with a review of classic architecture based models such as GOMS and ACT-R. It then rapidly progresses to introducing modern modeling approaches powered by machine learning methods, in particular deep reinforcement learning. The course is built around hands-on Python programming using notebooks.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {594},\nnumpages = {2},\nkeywords = {Cognitive modeling, cognitive architectures, computational rationality, cooperative intelligence, reinforcement learning, user interface optimization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636277,\nauthor = {Murad, Christine and Munteanu, Cosmin and Penn, Gerald},\ntitle = {Conversational Voice Interfaces: Translating Research Into Actionable Design},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636277},\ndoi = {10.1145/3613905.3636277},\nabstract = {HCI research has for long been dedicated to better and more naturally facilitating information transfer between humans and machines. Unfortunately, humans’ most natural form of communication, speech, is also one of the most difficult modalities to be understood by machines – despite, and perhaps, because it is the highest-bandwidth communication channel we possess. As significant research efforts in engineering have been spent on improving machines’ ability to understand speech, research is only beginning to make the same improvements in understanding how to appropriately design these speech interfaces to be user-friendly and adoptable. Issues such as variations in error rates when processing speech, and difficulties in learnability and explainability (to name a few), are often in contrast with claims of success from industry. Along with this, designers themselves are making the transition to designing for speech and voice-enabled interfaces. Recent research has demonstrated the struggle for designers to translate their current experiences in graphical user interface design into speech interface design. Research has also noted the lack of any user-centered design principles or consideration for usability or usefulness in the same ways as graphical user interfaces have benefited from heuristic design guidelines. The goal of this course is to inform the CHI community of the current state of speech and natural language research, to dispel some of the myths surrounding speech-based interaction, as well as to inform participants about currently existing design tools, methods and resources for speech interfaces (and provide hands-on experience with working with them). Through this, we hope that HCI researchers and practitioners will learn how to combine recent advances in speech processing with user-centred principles in designing more usable and useful speech-based interactive systems.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {595},\nnumpages = {3},\nkeywords = {conversational voice interfaces, speech interface design, speech interfaces},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636267,\nauthor = {Mackenzie, Scott and Read, Janet and Horton, Matthew},\ntitle = {Empirical Research Methods for Human-Computer Interaction},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636267},\ndoi = {10.1145/3613905.3636267},\nabstract = {Most attendees at CHI conferences will agree that an experiment (user study) is the hallmark of good research in human-computer interaction. But what constitutes an experiment? And how does one go from an experiment to a CHI paper? This course will teach how to pose testable research questions, how to make and measure observations, and how to design and conduct an experiment. Specifically, attendees will participate in a real experiment to gain experience as both an investigator and as a participant. The second session covers the statistical tools typically used to analyze data. Most notably, attendees will learn how to organize experiment results and write a CHI paper.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {596},\nnumpages = {3},\nkeywords = {Empirical research, experiment design, quantitative methods, user study, writing a CHI paper.},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636273,\nauthor = {Grudin, Jonathan and Brinkman, Donald},\ntitle = {HCI History and the Trajectory to Generative AI},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636273},\ndoi = {10.1145/3613905.3636273},\nabstract = {This course examines HCI history broadly, then conversational AI history from ELIZA to generative AI. A study of an LLM predecessor illuminates possibilities. With rapid change comes rising uncertainty. Not all history is relevant, but unchanging human nature abides. Some digital dreams become digital nightmares. Social media can deliver disinformation, malware, negative self-image, and polarization that undermines communities. Generative AI provides value but raises employment and career questions, education challenges, and empowers bad actors. We benefit from understanding the forces, the trajectories that brought us here, and how unanticipated consequences arose. Past events that shaped the present have become evident.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {597},\nnumpages = {3},\nkeywords = {Conversational Agents, Design, Future, Generative AI, HCI, History, Human Factors, Information Science, Information Systems},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636280,\nauthor = {Houben, Maarten and Lee, Minha and Foley, Sarah and Morrissey, Kellie and Brankaert, Rens},\ntitle = {HCI Research in Sensitive Settings: Learning Researcher Reflexivity, Ethical Conduct and Empathy in Participatory Design Approaches},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636280},\ndoi = {10.1145/3613905.3636280},\nabstract = {While there is an increase in HCI research in sensitive settings, design researchers often lack the needed training or preparation to navigate ethical challenges or emotionally difficult situations. In this course, we will provide researchers, designers or students concrete skills and insights into conducting HCI research in sensitive settings, based on our experience in involving users with a broad range of vulnerabilities in HCI research. We share lessons learned on ethical research practices and inclusive design methodologies to be applied in sensitive settings. Lastly, all attendees will apply the lessons learned to their current research projects during hands-on exercises.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {598},\nnumpages = {4},\nkeywords = {Course, Design, Ethics, Sensitive Settings, Vulnerability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636272,\nauthor = {Nacke, Lennart E.},\ntitle = {How to Write Better CHI Papers (with AI)},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636272},\ndoi = {10.1145/3613905.3636272},\nabstract = {Writing and organizing research papers is a valuable skill that can make or break your academic career. Generative artificial intelligence (AI) tools offer unprecedented opportunities for researchers to improve their skills in writing research papers and conducting literature reviews. In the past six years, my writing course has introduced you to everything you wanted to know about writing papers. However, with the arrival of generative AI, our writing process is changing. So, now I offer the opportunity to learn how to leverage generative AI tools to edit your writing, brainstorm, and help you find citations, so that your papers are easy to read and have impact. It is broken up into three 75-minute online units that will help you structure your paper’s research content and use generative AI as assistive research technology. The goal of the course is to learn how to leverage generative AI to help you write a paper that makes a contribution to the field of human-computer interaction and can be understood by other HCI researchers facilitated by the use of generative AI tools.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {599},\nnumpages = {4},\nkeywords = {Clarity, LaTeX, Research Methods, Submission Process, Writing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636270,\nauthor = {Russell, Daniel M. and Kulkarni, Chinmay and Glassman, Elena L. and Subramonyam, Hariharan and Martelaro, Nikolas},\ntitle = {Human-Computer Interaction and AI: What Practitioners Need to Know to Design and Build Effective AI systems from a Human Perspective},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636270},\ndoi = {10.1145/3613905.3636270},\nabstract = {AI and ML are now essential parts of many systems that are currently being built. What should CHI practitioners know about the possibilities and potential drawbacks of building AI systems? Understanding the human side of AI/ML based systems requires understanding both how the system-side AI works, but also how people think about, understand, and use AI tools and systems. This course will cover what AI components and systems currently exist, how to design and build usable systems with AI components, along with how the mental models of AI/ML tools operate. These models lead to user expectations of how AI systems function, and ultimately, to design guidelines that avoid disappointing end-users by accidentally creating unintelligible AI tools. We'll also cover the ethics of AI, including data collection, algorithmic and data fairness considerations, along with other risks of AI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {600},\nnumpages = {3},\nkeywords = {AI fairness, AI trust, HAI, HCI, UI design for AI systems, human-in-the-loop},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636276,\nauthor = {Myers, Brad A},\ntitle = {Interaction Techniques – History, Design and Evaluation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636276},\ndoi = {10.1145/3613905.3636276},\nabstract = {Interaction techniques (IxTs) are the low-level reusable building blocks out of which user interfaces are constructed. Examples include physical buttons, menus, scrollbars, touchscreen gestures such as flicking, text entry on computers and touchscreens, input for virtual reality, interactions with conversational agents, etc. UX professionals and researchers will often need to decide which IxTs to use, or even to invent new ones. This course will discuss the history of IxTs, and complexities and appropriate evaluations when designing new ones. The content of this course will be based Brad Myers's university courses and book on this topic.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {601},\nnumpages = {3},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636279,\nauthor = {Sirkin, David and Zamfirescu-Pereira, J.D. and Ju, Wendy},\ntitle = {Make This! Introduction to Electronics Prototyping Using Arduino},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636279},\ndoi = {10.1145/3613905.3636279},\nabstract = {This course is a hands-on introduction to interactive electronics prototyping for people with a variety of backgrounds, including those with no prior experience in electronics. Familiarity with programming is helpful, but not required. Participants learn basic electronics, microcontroller programming and physical prototyping using the Arduino platform, then use digital and analog sensors, LED lights and motors to build, program and customize a small paper robot.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {602},\nnumpages = {3},\nkeywords = {Arduino, Interaction design, embedded systems, prototyping, robot.},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636274,\nauthor = {Mirza-Babaei, Pejman},\ntitle = {Strategies in securing industry funding for your academic research program},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636274},\ndoi = {10.1145/3613905.3636274},\nabstract = {Securing research funding through government-based agencies is getting more and more competitive. One viable alternative is to raise funding through industry collaborations. Beyond funding, there are also other benefits in establishing industry research partnership such as access to real user data, or industry experts. However establishing and managing industry collaborations could be a complex process. In this course, I share my experiences and lesson learnt from working at the intersection of academic and industry research with the goal to help colleagues in establishing effective industry research partnership.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {603},\nnumpages = {3},\nkeywords = {industry, partnership, research},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636269,\nauthor = {Iravantchi, Yasha and Sample, Alanson P.},\ntitle = {T4Train: Rapid Prototyping of ML-Driven Interactive Applications},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636269},\ndoi = {10.1145/3613905.3636269},\nabstract = {Pairing real-time ML with sensor data drives many interactive applications. However, the tools to prototype these applications are often proprietary and not open-source. This course instructs how to build interactive sensing applications using T4Train, an open-source and user-friendly framework for rapid prototyping. Participants will learn sensor interfaces (e.g., on a laptop/Arduino), signal processing, ML, and the T4Train tool. Afterward, they will build real-time, interactive sensing systems, such as LED lighting that reacts to different sounds or hand movements. This course builds on 4 semesters of instruction using T4Train and multiple HCI research contributions, including 3 award-winning papers at CHI.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {604},\nnumpages = {4},\nkeywords = {Course, Interaction, Machine Learning Tools, Open Source, Sensing, Sensors, Visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636266,\nauthor = {Sturdee, Miriam and Lewis, Makayla},\ntitle = {To Sketching, And Beyond! A Course of Discovery with Pen and Paper},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636266},\ndoi = {10.1145/3613905.3636266},\nabstract = {The ability to sketch is a gift to yourself and others. It transcends disciplines by its nature as a tool for communication, but is of particular use within HCI and UX where it also enables the design of interactions. Sketching is a low-fidelity, accessible, and plentiful tool and persists despite the advent of digital tools for ideation, prototyping, qualitative research, and publication. Join us on a journey into sketching, whereby you will learn to doodle, storyboard, and express your feelings and experiences via pen and paper. Further, we invite you to explore how to use sketching in teaching and research and sketch possible futures in the HCI space.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {605},\nnumpages = {4},\nkeywords = {UX, sketching, visual methods},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3636271,\nauthor = {Nacke, Lennart E. and Mirza-Babaei, Pejman and Drachen, Anders},\ntitle = {User Experience Research and Design in Video Games},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3636271},\ndoi = {10.1145/3613905.3636271},\nabstract = {The goal of this online course is to teach participants how to use user experience (UX) methods in game design. There are three separate parts to the course plan: UX design for games, games user study, and game analytics. The course materials come from our book called \"Games User Research,\" which was released by Oxford University Press in 2018. An important part of this course is learning about the rules and methods of user experience (UX) design and research as they apply to making games. This is a critical skills an one of the key competencies is learning how to find, analyze, and understand player feedback. This gives participants the knowledge they need to make good decisions in game creation. The course include interactive exercises, participants will learn the information and skills they need to figure out the factors that affect a player’s experience in a game. This include foundational skills such as: how to effectively incorporate insights into the design process; ways to get information from players through direct observation, interviews, and surveys.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {606},\nnumpages = {3},\nkeywords = {Game Analytics, Game Design, Games User Research, UX Design, UX for Games},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647962,\nauthor = {Oksanen, Joel},\ntitle = {Bridging the Integrity Gap: Towards AI-assisted Design Research},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647962},\ndoi = {10.1145/3613905.3647962},\nabstract = {Design research involves the human-centered analysis of substantial volumes of qualitative data, presenting a promising application for emerging generative AI technologies. While AI-assisted qualitative research remains an extensively explored topic within the Human-Computer Interaction (HCI) discourse, the integration of AI into design research, differentiated by its intrinsic reliance on empathy and intuition, remains markedly underexplored. This paper reports on a qualitative study with 13 designers from a range of design disciplines, investigating the processes by which insights are cultivated in design research and the extent to which these processes may be effectively augmented or potentially undermined by AI interventions. The findings suggest that design research relies heavily on the tacit knowledge of designers, underlining the importance of designer-AI alignment. Drawing upon these findings and existing literature on human-AI trust, this paper lays the foundation for further inquiry into AI-assisted design research, by (1) identifying the development of intrinsic trust through designer-AI alignment as a central objective; and (2) introducing the conceptual framework of the Integrity Gap to motivate further studies.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {607},\nnumpages = {5},\nkeywords = {design research, designer-AI alignment, trust},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647967,\nauthor = {Kimmel, Bailey and Geisert, Austin Lee and Yaro, Lily and Gipson, Brendan and Hotchkiss, Ronald Taylor and Osae-Asante, Sidney Kwame and Vaught, Hunter and Wininger, Grant and Yamaguchi, Chase},\ntitle = {Enhancing Programming Error Messages in Real Time with Generative AI},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647967},\ndoi = {10.1145/3613905.3647967},\nabstract = {Generative AI is changing the way that many disciplines are taught, including computer science. Researchers have shown that generative AI tools are capable of solving programming problems, writing extensive blocks of code, and explaining complex code in simple terms. Particular promise has been shown in using generative AI to enhance programming error messages. Both students and instructors have complained for decades that these messages are often cryptic and difficult to understand. Yet recent work has shown that students make fewer repeated errors when enhanced via GPT-4. We extend this work by implementing feedback from ChatGPT for all programs submitted to our automated assessment tool, Athene, providing help for compiler, run-time, and logic errors. Our results indicate that adding generative AI to an automated assessment tool does not necessarily make it better and that design of the interface matters greatly to the usability of the feedback that GPT-4 provided.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {608},\nnumpages = {7},\nkeywords = {AI, Artificial Intelligence, Automatic Code Generation, CS1, ChatGPT, Codex, Copilot, GPT-4, GitHub, HCI, Introductory Programming, LLM, Large Language Models, Novice Programming, OpenAI},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648628,\nauthor = {Kim, Minsol and Nallbani, Aliea L and Stovall, Abby Rayne},\ntitle = {Exploring LLM-based Chatbot for Language Learning and Cultivation of Growth Mindset},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648628},\ndoi = {10.1145/3613905.3648628},\nabstract = {Growth mindset, the belief in enhancing abilities through effort and dedication, is commonly applied in classroom learning. The Mindset Theory Scale suggests it can be measured by \"effort\" and \"belief in improvement\", while fixed mindset is indicated by \"procrastination\" and \"immutability of belief\". In today’s interconnected world, the need for proficiency in multiple languages has grown. However, mastering different languages poses several challenges, including motivational challenges and fear of failure. To address this issue, our study explores the use of an adaptable Large Language Model (LLM) based chatbot to foster growth mindset and aid in language learning. In a user study with this novel chatbot system, we evaluated the impact of a growth mindset teaching style on new language learning and user perception. Our initial findings show that users are more comfortable, confident, and interested in interacting with the growth mindset chatbot compared to the fixed mindset chatbot.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {609},\nnumpages = {5},\nkeywords = {Chatbot, Grit, Growth Mindset, LLM, Online Language Learning},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647959,\nauthor = {Jing, Guo},\ntitle = {Exploring the Difference of Attention, Emotion, and Neurobiological Domains as Underlying Vulnerabilities For Game Addiction},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647959},\ndoi = {10.1145/3613905.3647959},\nabstract = {Recently, game addiction has officially been considered as a mental illness by WHO because of its negative consequences on people’s health. However, the predictive factors that make some individuals more at risk for game addiction are not well recognised. The aim of the present study was to investigate whether levels of attention, emotional responses, perceived immersion, and specific brain activities might be associated with the development of game addiction. A total of thirty-six participants were separated into two different groups based on high or low vulnerability to game addiction. During the experiment, frontal EEG signals were acquired while the participants were playing VR game and while other relevant behavioural tasks were completed. Results showed that the high vulnerability group had faster reaction time in pre- and post- gaming flanker tasks and a lower absolute beta power in comparison to the low vulnerability group. The current study accentuates the significance for future investigations on constructing complete pictures of game addiction and provides new insights into the diagnosis and treatment of game addiction. Limitations and future implications of the study were also discussed.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {610},\nnumpages = {7},\nkeywords = {Digital Health, Electroencephalogram, Emotion, Game addiction, Selective Visual Attention, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647958,\nauthor = {Kim, Munyeong},\ntitle = {GPTs in Mafia-like Game Simulation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647958},\ndoi = {10.1145/3613905.3647958},\nabstract = {In this research, we explore the potential of Generative AI models, focusing on their application in role-playing simulations through Spyfall, a renowned mafia-style game. By leveraging GPT-4’s advanced capabilities, the study aimed to showcase the model’s potential in understanding, decision-making, and interaction across scenarios. Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo, demonstrated GPT-4’s enhanced adaptability to the environment, with significant improvements in posing questions and forming responses. However, challenges such as the model’s limitations in judging and suspecting actions of other players emerged. Reflections on AI’s future capability and directions were also discussed. The findings suggest that although GPT-4 exhibits promising advancements over earlier models, there is potential for further development through expanding data and training techniques. The findings also underscore the importance of maintaining an inclusive and unbiased approach throughout this process, suggesting immense potential for Generative AI and its application.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {611},\nnumpages = {5},\nkeywords = {AI in Gaming, Decision-making, GPT-3.5-turbo, GPT-4, Game Strategy, Generative AI, Limitations of GPT, Natural Language Processing, Role-playing Simulations, Spyfall},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647968,\nauthor = {Hyun, Donghee and Jang, Eunjung and Lee, Jaebaek and Kim, Taesung},\ntitle = {Green Cloud: Supporting Sustainable Behavior by Helping Users Remove Unnecessary Photos from Cloud Storage Service},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647968},\ndoi = {10.1145/3613905.3647968},\nabstract = {Today's high-capacity data, especially digital photos, are readily generated and stored in cloud storage services. This excessive storage burdens data centers, which are major contributors to carbon emissions. Digital hoarding behavior — the excessive accumulation and reluctance to discard digital data — exacerbates this issue. Our study examines the impact of digital hoarding behavior on the task of deleting photos in cloud storage. The findings highlight the struggles users face and identify the need for better methods of deleting digital photos. We then focus on designing and evaluating features that assist users in photo deletion while making the experience satisfying. The goal of this study is to promote sustainable behavior in cloud storage services, where small actions, like deleting unnecessary photos, can lead to significant positive environmental impacts. Therefore, this study emphasizes individual behavior as a sustainable solution for addressing the environmental impact of dark data.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {612},\nnumpages = {6},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647964,\nauthor = {Schluchter, Tim},\ntitle = {Investigating User Perceptions of Mental Health Content on TikTok: A Comprehensive Exploration},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647964},\ndoi = {10.1145/3613905.3647964},\nabstract = {This paper explores the landscape of mental health (MH) content on TikTok, a rapidly growing social media (SM) platform. Examining TikTok’s unique attributes, including its algorithm, format, and user dynamics, I analyze its role in fostering open discussions on MH. Despite its potential benefits, concerns such as the algorithmic bubble, transparency issues, and misinformation spread are highlighted. My study emphasizes the intricate relationship between TikTok’s algorithmic content delivery, user behavior, and MH discourse. I propose design implications to balance personalized content delivery and responsible information dissemination. Insights from in-depth interviews with experienced TikTok users in MH content provide valuable perspectives for the HCI research community to support users engaging with and consuming MH-related content on TikTok.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {613},\nnumpages = {6},\nkeywords = {Algorithm, Interview, Mental Health, Social Media, TikTok},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647957,\nauthor = {Lieb, Anna and Goel, Toshali},\ntitle = {Student Interaction with NewtBot: An LLM-as-tutor Chatbot for Secondary Physics Education},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647957},\ndoi = {10.1145/3613905.3647957},\nabstract = {Chatbots based on state-of-the-art large language models (LLMs) hold potential to act as beneficial educational tools. However, challenges to LLMs in education include concerns about not only the accuracy and interpretability of AI-generated text, but also about productive student engagement and positive user experience with LLM chatbots. In this paper, we introduce a physics education chatbot called NewtBot. We designed NewtBot to act as a personalized automated tutor to support secondary students’ learning as they complete physics tasks. NewtBot’s web interface has a modifiable back-end that internally prompts GPT-3.5 to produce different LLM behaviors. In a user study with German secondary school students (n=50), we evaluated student interactions with three different configurations of the GPT-3.5 back-end: a general-purpose “baseline” model, a setting-specific “tutor” model, and a problem-specific “feedback” model. We find that students had overall positive experiences using NewtBot, and that the setting-specific “tutor” model had the highest user experience ratings. Additionally, despite a majority of participants (72\\%) expressing apprehensions about using chatbots for school, 70\\% said they would use NewtBot to help with their physics school work.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {614},\nnumpages = {8},\nkeywords = {ChatGPT, GPT 3.5, chatbot, large language models, physics education, prompt engineering, secondary education, tutor},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647966,\nauthor = {Lim, Jullia},\ntitle = {The Potential of Learning With AI-Generated Pedagogical Agents in Instructional Videos},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647966},\ndoi = {10.1145/3613905.3647966},\nabstract = {With the recent advancement in technology, generative artificial intelligence (GenAI) can produce hyper-realistic multimedia content, such as audio, text, images, and videos. Although this technology has raised great concerns about its misuse and harmful applications, it holds great potential to revolutionize traditional ways of teaching and learning. The use of GenAI in education has increased markedly, however, pedagogical research on this rapidly emerging technology is yet to be studied extensively. There is an urgent need to investigate the unexamined potential of this technology. Therefore, this ongoing research will explore the potential of AI-generated pedagogical agents (PA), or avatars, in instructional videos to facilitate learning. The effects of the type of PA (AI-generated, real-life human), and voice (AI-generated, human voice) on an individual's learning outcomes, cognitive load, motivation, and attention will be studied. Findings from a pilot study provide some preliminary evidence that PA appearance influences learners’ retention and cognitive load, but not attention. The type of PA influenced learners' perception of the agent's ability to facilitate learning, its human-like qualities, and its engagement level. However, it did not affect its credibility. This ongoing work will contribute to the growing understanding of the impact of AI in education, provide evidence of the efficacy of AI-generated PAs in instructional videos for learning, and narrow the gap between human-computer interaction research and education.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {615},\nnumpages = {6},\nkeywords = {avatars, multimedia learning, pedagogical agents, videos},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647965,\nauthor = {Otenen, Ege},\ntitle = {Towards Designing for Multimodal Remembering: Findings from an Interview Study},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647965},\ndoi = {10.1145/3613905.3647965},\nabstract = {Humans experience, remember, and express the world through their senses. These different modalities are also essential in forming and recalling autobiographical memories. People remember the color of the shirt they wore, the voice of their partner, and the taste of their mother’s soup. They also express their memories with different modalities by telling it with their words or showing pictures. Although a rising number of studies focused on developing technologies to support remembering, studies have yet to take the opportunity to explore the role of modalities to aid memories. We interviewed 12 participants about their memories by allowing them to remember and express memories with different modalities to understand better how to design for multimodal remembering. This paper is a first step toward developing a framework that aims to explore multimodal remembering, improve design practices by providing insights for future memory technologies.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {616},\nnumpages = {6},\nkeywords = {Autobiographical Memory, Embodied Remembering, Emotion, Memory Technology, Sense},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647963,\nauthor = {Khatri, Hamida},\ntitle = {Virtual Reality Therapy Model for Treatment of Mental Disorders Associated with Gender-Based Violence},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647963},\ndoi = {10.1145/3613905.3647963},\nabstract = {Over 35\\% of women experience some form of gender-based violence (GBV) in their lifetime, including intimate partner violence, rape, and more. This exposure can result in significant mental disorders within 1-5 years, manifesting as depression, post-traumatic stress disorder, and anxiety, among others, thereby affecting a staggering 58\\% of the world's population. The transformative power of Virtual Reality (VR) has been effectively used in treating various mental health disorders like addiction and phobias using methodologies such as cognitive behavioral therapy and exposure therapy. However, while current VR strategies are inclined towards enabling violent offenders to empathize with the sufferers and reduce aggression, there is a noticeable gap in directly catering to the therapeutic needs of GBV victims. This paper seeks to bridge the gap by introducing a novel VR Therapy Model based on human-centered design process, exclusively tailored for the rehabilitation and support for the victims of GBV.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {617},\nnumpages = {9},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647961,\nauthor = {Kostic, Sandra},\ntitle = {Who is the Better Operator of an Identity Wallet Prioritised by the User? - A Quantitative Survey Between State and Company},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647961},\ndoi = {10.1145/3613905.3647961},\nabstract = {This paper presents the results of an online survey based on a developed concept that enables the storage of multiple digital identities in one app. Users manage these identities in the so-called Identity Wallet independently and decide which data should be sent to any external service. As previous usability studies indicate that the operator of the Identity Wallet is decisive for user trust in such technologies, a quantitative study was conducted with 306 people. The participants were asked to rank possible operators (e.g. state, open source community projects, known company, etc.) in order of their preference. The results show that, on average, a private company is favoured as the operator. However, if the operator that was named first by most participants is considered, then this is the state.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {618},\nnumpages = {7},\nkeywords = {Digital Identity, Digital Wallet, Operator, Survey, User Trust},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647979,\nauthor = {Hamada, Myriam and Tanguay Doucet, India and Aur\\'{e}lie, Brissac},\ntitle = {COOKNOOK: Intelligent Meal Planning Application},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647979},\ndoi = {10.1145/3613905.3647979},\nabstract = {In Canada, nearly 40\\% of university students experience food insecurity [1, 2]. However, most young adults and students are not aware or even underestimate the extent of the insecurity they face. This phenomenon can increase the cognitive burden associated with food management, leading to poor resource allocation exacerbating levels of food insecurity. This negatively impacts Sustainable Development Goal 2, 'Zero Hunger.' The Canadian government and university institutions have often relied on food banks or charitable organizations. However, these interventions have mainly served as emergency measures rather than long-term solutions [3]. Through research based on a design-adapted ethnographic approach, our team has gained a deep understanding of the situation of university students and the challenges related to food. Consequently, we propose CookNook: an application featuring an intelligent personal assistant aimed at helping users prioritize cooking time by reducing the cognitive load associated with food. This is accomplished by providing simple and accessible recipes, accompanied by shopping lists tailored to their resources, needs, and constraints. CookNook also facilitates culinary gatherings with friends to share meals while motivating them to cook more. In this article, we summarize our design process and how our solution represents a step towards ensuring sufficiently quality nutrition among university students.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {619},\nnumpages = {6},\nkeywords = {Artificial intelligence, Food insecurity, Human-centered design, Mobile application, University students},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647973,\nauthor = {Chang, Yvonne and Loukeri Woestman, Stella and Shokr, Sarah and Tang, Rongzeng and Roman Ward, Hugo},\ntitle = {Connectopia: A Modular Board Game Simulating Digital Interfaces to Assist Refugee and Asylum Seeker Integration},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647973},\ndoi = {10.1145/3613905.3647973},\nabstract = {Interactive technologies play an increasingly vital role in the integration processes for refugees and asylum seekers, but a lack of familiarity makes their use challenging and stressful. We aimed to create an artefact that could help migrant centres provide better digital-skills education to refugees while using playful and social design elements. Interviews with staff members at UK migrant centres were processed in a thematic analysis, revealing a high demand for help with integration-related digital tasks among refugees. This led to our creation of Connectopia, a two-player, competitive boardgame that uses modular components, an internal microcontroller and multimodal feedback to playfully emulate interfaces such as government portals and email. We created a low-fidelity prototype, which allowed us to perform user testing and a heuristic evaluation. The final design expands on the original by including multimodal feedback (haptic, visual, and auditory) to communicate success or failure at tasks. Connectopia can be expanded to simulate a variety of interfaces and provide playful education in refugee and migrant centres, in both social and structured educational contexts.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {620},\nnumpages = {5},\nkeywords = {Asylum Seeker Assistance, Connectopia, Digital Interface Simulation, Digital Literacy, Interactive Learning Tool, Modular Board Game, Refugee Integration},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647970,\nauthor = {Um, Bokyung and Santos, Catarina and Abdirahman, Sharihan and Power, Aoife and Uddin, Zainab},\ntitle = {EcoFashion Scanner: Bridging the Gen Z and Millennial 'Green Gap' by Facilitating Sustainable Fashion Consumption Behaviours},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647970},\ndoi = {10.1145/3613905.3647970},\nabstract = {The fashion industry contributes significantly to climate change, being responsible for 10\\% of global carbon emissions. Generation Z and Millennials are known to have lower scepticism toward global warming and higher involvement in environmental activism; however, prior research has uncovered a 'green gap' between the stated values and actual consumer habits of these cohorts. Our project seeks to address this gap through an engaging and practical mobile application which helps users better integrate their eco-values and real-world consumption behaviour. A survey of 71 participants was conducted to better understand awareness and behaviour on sustainable fashion consumption. Despite high environmental awareness, Gen Z and Millennials predominantly purchase fast fashion, citing a lack of knowledge and motivation for sustainable fashion choices. To tackle this problem, we adopted a user-centric and iterative design process, with our EcoFashion Scanner offering comprehensive sustainability information on fabrics, incentivising eco-conscious purchasing, repair, recycling, and donation. An engaging, interactive and innovative platform, the app strives to minimise clothing waste and extend garment lifespan, aligning with multiple UN Sustainable Development Goals, particularly those rooted in SDG 12, concerning responsible consumption and production.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {621},\nnumpages = {6},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647971,\nauthor = {Wernersbach, Julia and Ng, Douglas and Lau, Sophie Ka Ling and Zhu, Wenqi and Lai, Ziyue},\ntitle = {EcoWatt: An Electricity Management App to Illuminate Community-Driven Sustainability in Student Accommodations},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647971},\ndoi = {10.1145/3613905.3647971},\nabstract = {This project addresses the rising issue of excessive energy consumption in student accommodations across the UK which has shown a 20\\% increase from 2022 to 2023. Student accommodations provide a unique context due to the communal proximity of its inhabitants and students who are not directly responsible for a separate utility bill. Through a mixed-method user research process, 28 questionnaires and 7 interviews revealed two key insights - a lack of awareness and motivation for sustainable behavioural change. The project solution, EcoWatt, is an interactive mobile app complemented by a community dashboard, to bridge the gap between awareness and action. Leveraging elements of anthropomorphism and gamification, EcoWatt delivers real-time data visualisations and educational content, empowering users to explore and better manage their energy consumption. Its scope can be broadened to other energy sources and settings, fostering sustainability in various sectors.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {622},\nnumpages = {6},\nkeywords = {behavioural change, electricity, energy conservation, student accommodation, sustainability},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647976,\nauthor = {Patel, Shreyas and Nair, Aditya and Rajpura, Nisarg Hiteshkumar and Jogadia, Drishti Dinesh},\ntitle = {Food for Thought: GreenBasket's approach to helping SNAP Families and Local Farmers towards Self-reliance},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647976},\ndoi = {10.1145/3613905.3647976},\nabstract = {Indiana grapples with worsening food deserts, underscored by an increase in its food insecurity rate from 9.4\\% in May 2020 to 13.4\\% in December 2022, [1] this study investigates the potential of technology to bridge market information gaps and enhance self-sustainability in communities. Existing research focuses on trade and policy reform, and infrastructure investment as interventions to solve food insecurity but overlooks the importance of comprehensive market information dissemination, a gap highlighted by organizations like the FAO and the World Bank. To bridge this gap, our research involved user studies with farmers, food banks, and low-income consumers through surveys, interviews, and low-fidelity evaluations. This approach helped uncover key issues of affordability and market accessibility as underlying barriers to an on-ground solution. We developed Project GreenBasket in response to the aforementioned problems, an online marketplace featuring direct sales channels for farmers, an affordable click-and-collect model for pick-up locations for consumers, and SNAP and nutrition tracking for relevant stakeholders working towards food security. GreenBasket not only addresses the identified gaps in literature but also aims to enhance food security and equity in Indiana, contributing towards the Sustainable Development Goals of Zero Hunger. Our findings reveal the significant potential of technology in improving market access and efficiency, showcasing a scalable model to combat food insecurity.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {623},\nnumpages = {6},\nkeywords = {Agricultural Technology, Community Self-Reliance, Food Distribution Logistics, Food Insecurity, Human-Computer Interaction (HCI), Market Accessibility, Online Marketplaces, SNAP Benefits Integration, Sustainable Agriculture, User Experience Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647980,\nauthor = {Desautel, Manon and Desir, Julie and Martineau-C\\^{o}t\\'{e}, L\\'{e}o},\ntitle = {Halfway : Towards a Warmer Neighborhood},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647980},\ndoi = {10.1145/3613905.3647980},\nabstract = {Nowadays, people living in cities face a growing social isolation context. Following the global COVID-19 pandemic, contactless habits have grown as a sort of new normal for some people, diminishing the number of opportunities for communication. People are less likely to go towards other people to ask for help, including their neighbors, no matter how physically close they are. Halfway, as an apartment building-interconnected screen system, aims to nudge people to talk to their neighbors through services and games.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {624},\nnumpages = {5},\nkeywords = {Neighbor relationships, Social isolation, Urban life},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647972,\nauthor = {Chen, Yifei and Mao, Qinglin and Huang, Xinlie and Xu, Ningning},\ntitle = {LanternOperAR: A Hybrid Cultural Gift for Quality Education and Family Well-being✱},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647972},\ndoi = {10.1145/3613905.3647972},\nabstract = {Quality education and well-being have always been the focus of sustainable development as social concerns, with an increasing demand for enriched cultural activities. Shared experiences, especially in family setting, bring about a better understanding of culture and foster public interest. In this context, our research endeavors to promote the philosophies and historical insights of Yangmingism while concurrently seeking to uplift family well-being through the immersive cultural exposure offered by Yue Opera, a national opera in China. To intertwine the cultural content for a consistent and continued experience, we design LanternOperAR, a hybrid gift for cultural appreciation, learning and recreation. We fully consider and respect user interests and requirements among different target groups to provide playful interaction in cultural activity, thereby promoting cultural inheritance and strengthening family ties.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {625},\nnumpages = {8},\nkeywords = {Augmented Reality, Cultural Heritage, Quality Education, Sustainable Development, Yue Opera},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647977,\nauthor = {Cheon, Serene and Yu, Erica and Kang, Hyo},\ntitle = {Quiet Asian and Spicy Latino? Designing to Address Racial Microaggression},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647977},\ndoi = {10.1145/3613905.3647977},\nabstract = {Racial microaggressions are subtle, often unintentional discriminatory comments that convey negative assumptions about a person’s race or ethnicity. It is crucial to address these microaggressions in the design of future technologies, especially speech-based interfaces, as they are known to significantly impact users’ acceptance of the technology. However, prior studies in natural language processing (NLP) have highlighted the challenge of detecting microaggressions due to their subtle nature. To tackle this issue, we conducted a survey with 43 individuals to gather insights into common microaggressive comments experienced by people of different races. Based on survey findings, we designed an app called \"Inclusify.\" \"Inclusify\" encompasses three key features. Firstly, it encourages users to report racist statements and assess their offensiveness, thereby collecting valuable datasets for future speech-based interface design. Secondly, it visualizes common discriminatory expressions through word clouds and showcases trending discussions to raise public awareness. Lastly, the app fosters social belonging by offering a digital space where marginalized individuals can share their stories and concerns. We evaluated the effectiveness of \"Inclusify\" in increasing awareness through an online survey, comparing users’ awareness levels with a pre-post test. Usability was assessed using key task performance metrics and the system usability scale. Based on the results, we discuss design implications and suggest potential directions for future research.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {626},\nnumpages = {6},\nkeywords = {anti-racism, design for inclusion, interaction design, racial microaggression},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647975,\nauthor = {Muller, Michelle and Zhang, Zhiqing and Emmons, Anya and Kam, Jenna and Luis Costa, Rita},\ntitle = {Verifone: Building Trust in the Second-Hand Phone Market},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647975},\ndoi = {10.1145/3613905.3647975},\nabstract = {With the rapid global growth of electronic waste, the potential for sustainable consumption in second-hand mobile phone markets remains largely untapped. This is due to a lack of trust caused by information asymmetry: potential buyers struggle to assess the quality of a used phone. To remedy this, we designed Verifone, a third-party interface that offers device-specific information for second-hand phones. With 3D interactions emulating tangible experiences and data visualisations aiming to increase transparency of the phone condition, Verifone instills confidence in the buying process. Our research and design process focused on understanding what elements of functional and affective design elicited more trust in the phone’s condition. User evaluations proved that a clean, standardized design combined with affective elements to highlight device uniqueness could inspire more confidence in assessing the device’s condition. The positive response suggests a tool like Verifone could encourage sustainable purchases, contributing to a circular economy. In addition, our research also sheds light on effective information presentation styles that build trust within the context of second-hand electronic purchases.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {627},\nnumpages = {6},\nkeywords = {information asymmetry, second-hand markets, sustainable consumption, trust models},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647974,\nauthor = {Wadhwa, Roshni and Xu, Xin and Liu, Yanting and Chen, Zeyu and Han, Ziqi},\ntitle = {VestiCare: A Holistic and Smart Digital Platform to Improve the Vestibular Rehabilitation Experience},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647974},\ndoi = {10.1145/3613905.3647974},\nabstract = {The goal of SDG 3 (Good Health and Well-being) is to promote healthy lives for all, at all ages. An often-neglected health concern, vestibular disorders, can negatively affect people's balance and spatial stability. It requires consistent and accurate rehabilitation exercises for treatment. However, the low accessibility and the lack of motivation among patients make vestibular rehabilitation less effective. To tackle these challenges, we first conducted literature reviews and interviews with specialists to discover the user needs. Then, we defined user personas and journeys to develop both low- and high-fidelity prototypes. Our final solution is a digital platform comprising a mobile app and web portal for patients and healthcare providers (HCPs). We utilised phone sensors and AI to increase vestibular rehabilitation accessibility, gamified exercises to boost patient motivation, and incorporated an efficient communication channel for better HCP-patient communication. In general, usability tests demonstrated that our platform was user-friendly, accessible, and motivating. },\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {628},\nnumpages = {6},\nkeywords = {AI, Vestibular rehabilitation, chronic dizziness, digital healthcare, gamification, mobile application, vertigo, vestibular disorders, web portal},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647978,\nauthor = {Huang, Jun and Tan, Qianwen and Fang, Qi},\ntitle = {VitalStep: Revitalizing Elderly Health and Connectivity with E-Square-Dance},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647978},\ndoi = {10.1145/3613905.3647978},\nabstract = {In response to the challenges faced by China’s aging population, including mobility restrictions, health concerns, and social isolation, calls for innovative approaches to enhance their physical and mental well-being. Square dancing, or ’Guangchang Wu’, is a communal activity where Chinese elderly gather in public spaces to dance together, combining exercise with social engagement. We developed VitalStep, an online co-dancing app tailored for people aged 45-70 to participate in square dancing virtually. Through collaboration with square dance communities, we ensured our app met the needs and preferences of its target users. Early findings suggest that the app successfully motivates seniors to be more physically active and socially engaged. Our project aligns with the United Nations Sustainable Development Goals 3 (Good Health and Well-Being) and 11 (Sustainable Cities and Communities), demonstrating the potential of technology in fostering healthier, more inclusive urban environments for the middle-aged and elderly.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {629},\nnumpages = {7},\nkeywords = {aging population, digital health, social engagement, square dance, user-centered design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3647969,\nauthor = {Balmaceda, Alexandra and Chen, Ziwei},\ntitle = {Where’s the Water? Supporting Clean Water Access for the Homeless Community},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3647969},\ndoi = {10.1145/3613905.3647969},\nabstract = {Access to clean water is essential, yet it poses a significant challenge for the homeless population. Our project, ’Where’s the Water,’ is a web-based tool designed to improve water access for the homeless community. It maps nearby clean water sources like drinking fountains, public restrooms, and showers. The tool’s design was informed through interviews with the homeless community in Ann Arbor, Michigan. The insights gained from these interviews were further supported by key findings from recent studies related to homelessness and water access. Besides locating, our tool’s functionality also includes filtering sources for operational hours and water quality. It features crowd-sourcing, allowing users to add new sources on the map, effectively utilizing community knowledge. In this article, we describe our research and design approach, highlighting the community and organizational feedback that helped turn our concept into a useful tool.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {630},\nnumpages = {7},\nkeywords = {Clean Water Access, Health, Homeless Population, User-centered Design},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648110,\nauthor = {Tang, Haoheng and Singha, Mrinalini},\ntitle = {A Mystery for You: A fact-checking game enhanced by large language models (LLMs) and a tangible interface},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648110},\ndoi = {10.1145/3613905.3648110},\nabstract = {Fact-checking and critical thinking are essential life skills in an age of rampant misinformation. To cultivate these skills in young learners, we have developed ‘A Mystery for You’ – an educational game powered by a large language model (LLM) and a tangible interface. In this game, a player becomes a citizen fact-checker, responding to ‘news alerts’ printed out by the game interface. The player investigates various actors and evidence by inserting cartridge combinations into the game interface. Each move the player makes results in the generation and printing of a follow-up ‘news update,’ which they must use to make an informed verdict about the truth or falsehood of the news. This interactive process sharpens critical thinking skills and enhances familiarity with generative AI’s misinformation capacities. This paper contextualizes the game’s relevance in today’s media and politics, explores game-play mechanics, and critically reflects on incorporating AI generation tools for educational game play.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {631},\nnumpages = {5},\nkeywords = {Digital Literacy, Generative AI, Investigation, Misinformation, Role-Playing, Tangible Interface, Young Learners},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648102,\nauthor = {Hsu, Yu Lun and Lu, Chien-Ting and Lu, Li-Chun and Tam, Chih-Heng and Sun, Yu-Chieh and Wang, Ting-Kang},\ntitle = {AnimalSense: Understanding Beyond-human Sensory Capabilities of Animals via VR Games},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648102},\ndoi = {10.1145/3613905.3648102},\nabstract = {Animals have sensory capabilities far beyond those of humans, including the ability to detect ultrasound, UV light, electric fields, magnetic fields, and to have panoramic vision. AnimalSense is a VR game crafted to immerse players in the extraordinary sensory worlds of animals. The game’s design leverages the concepts of sensory substitution, sensory remapping, and active learning, allowing users to explore and utilize these beyond-human sensory capabilities to overcome game challenges, thereby enhancing players’ understanding of animals’ unique senses. This paper presents three such game section designs: 1) echolocation in bats, 2) electroreception in eels, and 3) panoramic vision in spiders, showcasing how these are integrated into the gameplay.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {632},\nnumpages = {6},\nkeywords = {Animal Comprehension, Animals, Perceptual ranges, Sensory capabilities, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648626,\nauthor = {Lai, Shih-Yu and Liu, Dai-En and Yeh, Zong-Fan and Hsu, Chun-Che and Chang, Fu-Yao and Tsai, Ming-Zhi and Lin, Ruei-Hong},\ntitle = {Footprints of Travel: AIoT and AR Enhanced Tourist Gaming Experience in Unmanned Cultural Sites},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648626},\ndoi = {10.1145/3613905.3648626},\nabstract = {Footprints of Travel is a game that enhances cultural attraction experiences for tourism and space management, utilizing Augmented Reality (AR) and the Artificial Intelligence of Things (AIoT). This \"unmanned\" approach diverges from traditional museum-focused models, creating personalized, city-wide cultural experiences with comprehensive information services. Our game is tailored for navigating cultural sites, recognizing crowds, and dynamically processing data within the context management framework. The application enhances safety and comfort through temperature control, smoke detection, automatic fans, and artifact protection. We aim to offer a sustainable, interactive exploration game of historical sites, benefiting visitors and site managers. This AR and AIoT game innovatively enhances the visitor experience and blends technology with cultural preservation.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {633},\nnumpages = {6},\nkeywords = {Artificial Intelligence, Augmented Reality, Cultural Attractions, Explorative Game},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648111,\nauthor = {Mubarrat, Syed Tanzim},\ntitle = {GeoBotsVR: A Robotics Learning Game for Beginners with Hands-on Learning Simulation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648111},\ndoi = {10.1145/3613905.3648111},\nabstract = {This article introduces GeoBotsVR, an easily accessible virtual reality game that combines elements of puzzle-solving with robotics learning and aims to cultivate interest and motivation in robotics, programming, and electronics among individuals with limited experience in these domains. The game allows players to build and customize a two-wheeled mobile robot using various robotic components and use their robot to solve various procedurally-generated puzzles in a diverse range of environments. An innovative aspect is the inclusion of a repair feature, requiring players to address randomly generated electronics and programming issues with their robot through hands-on manipulation. GeoBotsVR is designed to be immersive, replayable, and practical application-based, offering an enjoyable and accessible tool for beginners to acquaint themselves with robotics. The game simulates a hands-on learning experience and does not require prior technical knowledge, making it a potentially valuable resource for beginners to get an engaging introduction to the field of robotics.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {634},\nnumpages = {6},\nkeywords = {Educational Game, Electronics, Programming, Robotics, Virtual Reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648107,\nauthor = {Karaosmanoglu, Sukran and Fittschen, Elisabeth L and Eyicalis, Hande and Kraus, David and Nickelmann, Henrik and Tomko, Anna and Steinicke, Frank},\ntitle = {Language of Zelda: Facilitating Language Learning Practices Using ChatGPT},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648107},\ndoi = {10.1145/3613905.3648107},\nabstract = {The Language of Zelda is an educational game that re-imagines “The Legend of Zelda: A Link to the Past” for French language learning. With the integration of ChatGPT for non-player characters (NPCs), the game allows players to interact with NPCs to practice French through gameplay, puzzles, and quests. Our approach bridges the gap between declarative and procedural language knowledge, offering an engaging, immersive learning experience. The game’s adaptive dialogues cater to various proficiency levels, enhancing both education and entertainment values. Our work illustrates the potential of combining AI with game-based learning to create effective, enjoyable language education tools.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {635},\nnumpages = {5},\nkeywords = {chatGPT, games, language, learning, natural langugage processing},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648109,\nauthor = {Wen, Ruoyu and Cao, Jiashuo and Billinghurst, Mark and Piumsomboon, Thammathip},\ntitle = {Listen to the Sword: Using Breathing and Spatial Audio for Wuxia Narratives},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648109},\ndoi = {10.1145/3613905.3648109},\nabstract = {Wuxia, a famous Chinese genre combining martial arts and philosophy, has been portrayed in various films, novels, and games. However, modern media, especially gaming, often prioritizes the martial arts action instead of the rich philosophical elements central to Wuxia. Addressing this gap, we developed \"Listen to the Sword,\" a game incorporating novel auditory gameplay for an immersive Wuxia narrative. This game integrates spatial sound effects with a breath detection system, providing players with an experience that mirrors the self-cultivation journey characteristic of traditional Wuxia. In this way, we hope to investigate the feasibility of gamifying complex cultural concepts to facilitate intercultural communication.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {636},\nnumpages = {4},\nkeywords = {Wuxia genre, breathing-based interaction, gamification, spatial audio},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3648112,\nauthor = {Rana, Fawad Ahmad and Tsang, Yuk Lam and Yip, Tak Wa},\ntitle = {Music Corner - A Feasibility Study for Creating a Gesture-Based Rhythm Game for Music Education Inspired by Solfege Hand Signs},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3648112},\ndoi = {10.1145/3613905.3648112},\nabstract = {In Hong Kong, the gap in music education access between children from affluent and grassroots families is pronounced. To address this, we present \"Music Corner,\" a prototype of a gesture-based rhythm game designed to democratise music education through digital innovation. Utilising mobile technology and machine learning, this study explores the feasibility of an engaging, accessible, and cost-effective platform for music learning. It leverages the Solfege Hand Signs, used by Hong Kong's primary music education, by transforming them into a game where players interact with musical notes through intuitive gestures. This initiative aims to bridge the educational divide, fostering an inclusive environment where children of all socioeconomic statuses can nurture their musical abilities. The prototype features three game modes to cater different learning objectives. With a focus on interactive technology, \"Music Corner\" stands as a testament to the potential of gamification in enhancing music literacy, making education more inclusive and enjoyable.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {637},\nnumpages = {4},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3649126,\nauthor = {Tu, Joseph},\ntitle = {Casting Connections: A Fishy Approach to Conference Engagement},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3649126},\ndoi = {10.1145/3613905.3649126},\nabstract = {Early-career researchers often feel like small fish in a big pond, surrounded by towering giants of knowledge. However, considering this apparent drawback overlooks a potential opportunity for networking and collaboration. To facilitate engagement in a way where ideas can swim freely, I present FISH which is “fun interactions through sketches and humour”. Drawing inspiration from the metaphor of fishing, this form of interaction allows attendees (at conferences) to cast their intellectual nets, hook meaningful networks, and navigate the currents of collaboration. Embark on this academic angling adventure with me, where engagement isn’t just a catchphrase; it’s the bait that hooks us into the vibrant, collaborative learning ecosystem, making every conference a reel success.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {638},\nnumpages = {1},\nkeywords = {Fun, Humour, Interaction, Sketches},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3649120,\nauthor = {Lu, Qiuyu and Yin, Zhang and Fu, Jingtian and Du, Naixuan and Xu, Yingqing},\ntitle = {Color Singer: Composing Music via the Construction of LEGO Blocks with Various Colors},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3649120},\ndoi = {10.1145/3613905.3649120},\nabstract = {Explore the fusion of creativity, play, and music with our project, where LEGO blocks transform abstract music composition into a tangible, intuitive experience. Each color in the LEGO palette corresponds to a distinct musical note, offering the freedom to purposefully replicate sheet music or casually build, unveiling serendipitous musical surprises. We present Color Singer—a system crafted and programmed with LEGO Mindstorms, open-sourced for accessibility. Our video showcase not only explains the implementation of Color Singer but also features a charming LEGO stop-motion animation narrating the heartwarming story of the Color Singer saving love. Join us in this innovative journey, where the tangible world of LEGO meets the abstract realm of music composition.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {639},\nnumpages = {2},\nkeywords = {DIY, LEGO, Music Composition, Synaesthesia},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3649119,\nauthor = {Fischer, Max and Jeon, Jongik and Pyo, Seunghwa and Kiuchi, Shota and Oda, Kumi and Honma, Kentaro and Pennington, Miles and Kim, Hyunjung},\ntitle = {Design Exploration of Robotic In-Car Accessories for Semi-Autonomous Vehicles},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3649119},\ndoi = {10.1145/3613905.3649119},\nabstract = {This video introduces Auze, a robotic rearview ornament designed to enhance the semi-autonomous driving experience at SAE Level 3 (see Figure 1). While Level 3 vehicles permit hands-free, eyes-off driving in specific scenarios, drivers must be ready to resume control when necessary [1]. Auze, as a robotic in-car accessory, utilizes kinesthetic and auditory cues in conjunction with the existing handover system to facilitate takeovers. In addition to outlining Auze's design and interaction, the video traces the design journey from ideation workshop to exploratory prototyping [2]. It also incorporates real data derived from the public demonstration of Auze, to underscore Auze's efficacy in augmenting driver preparedness for handover processes. Auze represents our initial step in our mission to design assistive systems that elevate the semi-autonomous driving experience by fostering trust between the driver and the vehicle. We hope that Auze will inspire researchers and designers to adopt diverse, experience-centered approaches in creating such technologies.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {640},\nnumpages = {2},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3649124,\nauthor = {Yamamoto, Kenta and Tsutsui, Ayaka and Zhao, Yinan and Suzuki, Ippei and Tanaka, Kengo and Koroyasu, Yusuke and Ochiai, Yoichi},\ntitle = {Documentary Film of Low Vision Boxing: Collaborative Design of Adaptive Kickboxing Experiences},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3649124},\ndoi = {10.1145/3613905.3649124},\nabstract = {In this video showcase, we address the challenge of physical inactivity among individuals with low vision by introducing adaptive kickboxing—a newly conceptualized sport that employs assistive technology tailored to their needs. Through collaborative design with a low vision participant, we develop equipment like gloves and shin guards that amplify the perceptibility of movements in kickboxing, enabling engagement and safety in the sport. Our research extends beyond facilitating exercise to creating opportunities for competitive aspirations within a sport transformed for inclusivity.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {641},\nnumpages = {2},\nkeywords = {adaptive kickboxing, collaborative design, low vision, sports accessibility},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3649121,\nauthor = {Wang, Po-Yao (Cosmos) and Rajesh, Rohit and Lee, Nathaniel Yung Xiang and Loose, Antony Smith Smith and Overdevest, Nathalie and Semertzidis, Nathan and Mueller, Florian ‘Floyd’},\ntitle = {DreamCeption : Towards Understanding the Design of Targeted Lucid Dream Mediation},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3649121},\ndoi = {10.1145/3613905.3649121},\nabstract = {Lucid dreaming, characterized by an awareness of being in a dream, offers individuals the ability to control dream content, which leads to various benefits such as entertainment, improved mental well-being, reduced concurrent nightmares, skills enhancement, creative inspiration, and problem-solving. However, manipulating dream content can be challenging and even experienced lucid dreamers may encounter difficulties. To address this challenge, we present DreamCeption, an innovative prototype designed to facilitate lucid dream content manipulation. While brain and eye activity sensors detect the lucid dream state, the system provides external stimuli such as visual and auditory stimulation to prime lucid dreamers to the desired content. Our design aims to make the benefits of lucid dreaming more accessible to a broader audience. We anticipate that DreamCeption will not only enhance the lucid dream experience but also attract interest from human-computer interaction researchers who can explore the potential applications of digital lucid dreaming.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {642},\nnumpages = {2},\nkeywords = {Lucid dreaming, VR, content manipulation, dream, dream control, dream manipulation, sleep, virtual reality},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3649122,\nauthor = {Wang, Po-Yao (Cosmos) and Lee, Nathaniel Yung Xiang and Rajesh, Rohit and Loose, Antony Smith and Semertzidis, Nathan and Mueller, Florian ‘Floyd’},\ntitle = {LuciEntry HOME: An Anywhere Lucid Dream Induction Prototype},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3649122},\ndoi = {10.1145/3613905.3649122},\nabstract = {Lucid dreaming is a unique state of consciousness where one is aware of dreaming whilst asleep, allowing the dreamer to control their dream content, offering various mental and physical health benefits. Existing research has utilised lab settings with manual induction techniques with researchers to induce lucid dreams, which is costly and time-consuming. Thus, there is a need for an autonomous system that would easily integrate lucid dreaming induction techniques and trigger them autonomously. In response, we present LuciEntry HOME, a portable system that utilises (1) a mobile app to guide users through pre-sleep cognitive training and (2) light, battery-powered, and wireless components for external stimulation. LuciEntry HOME emphasizes portability, autonomy and modularity, reducing the reliance on sleep labs and researchers to trigger the external stimulus manually in an effort to make lucid dreaming more accessible and facilitating future research on its application.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {643},\nnumpages = {2},\nkeywords = {Lucid dreaming, autonomous, induction, interactive devices, modular, portable, prototype, system},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3649123,\nauthor = {Wang, Po-Yao (Cosmos) and Lee, Nathaniel Yung Xiang and Rajesh, Rohit and Loose, Antony Smith and Semertzidis, Nathan and Mueller, Florian ‘Floyd’},\ntitle = {LuciEntry: A Modular Lab-Based Lucid Dream Induction Prototype},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3649123},\ndoi = {10.1145/3613905.3649123},\nabstract = {Lucid Dreaming, a unique state of consciousness whilst asleep where dreamers can engage in volitional actions not limited by the constraints of the physical world and control their dream contents, offers various mental and physical health benefits. Current research combining multiple lucid dreaming induction techniques is often conducted in a lab setting, lacking autonomy by relying on researchers to monitor manually. Recent studies also advocate for a modular system that can integrate multiple lucid dreaming induction techniques. We present LuciEntry, a prototype that includes a mobile app that guides users through pre-sleep cognitive training and a system that assesses the user’s sleep stage and triggers the external stimuli automatically to induce lucid dreams. We hope that this modular autonomous system will improve the research process, aiding in further research into lucid dreams.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {644},\nnumpages = {2},\nkeywords = {Lucid dreaming, autonomous, induction, interactive devices, modular, portable, prototype, system},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3649125,\nauthor = {Saini, Aryan and Patibanda, Rakesh and Overdevest, Nathalie and Van Den Hoven, Elise and Mueller, Florian ‘Floyd’},\ntitle = {PneuMa: Designing Pneumatic Bodily Extensions for Supporting Movement in Everyday Life},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3649125},\ndoi = {10.1145/3613905.3649125},\nabstract = {Prior research around the design of interactive systems has highlighted the benefits of supporting embodiment in everyday life. This resulted in the creation of body-centric systems that leverage movement. However, these advances supporting movement in everyday life, aligning with the embodiment theory, so far focused on sensing movement as opposed to facilitating movement. We present PneuMa, a novel wearable system that can facilitate movement in everyday life through pneumatic-based bodily extensions. We showcase the system through three examples: \"Pardon?\", moving the ear forward; \"Greetings\", moving a hand towards the \"Bye-bye\" gesture; \"Take a break\", moving the hands away from the keyboard, enabling the bodily extensions that support movement in everyday life. We delve into some findings in relation to prior research around bodily extensions and embodied interaction in the video. Ultimately, we hope that our work helps more people profit from the benefits of everyday movement support.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {645},\nnumpages = {4},\nkeywords = {bodily extensions, embodied experiences, embodied interactions, pneumatics},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3649128,\nauthor = {Kalus, Alexander and Klein, Johannes and Ho, Tien-Julian and Seegets, Lee-Ann and Henze, Niels},\ntitle = {Showcasing MobileGravity},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3649128},\ndoi = {10.1145/3613905.3649128},\nabstract = {Providing weight sensations in Virtual Reality (VR) is a persistent challenge. In this video, we present MobileGravity [3], a novel mobile weight interface utilizing liquid transfer to change the weight of a handheld VR proxy. MobileGravity overcomes the conflict between mobility and performance by decoupling its heavy components, such as the actuator and the liquid supply, from the weight-changing object. This enables the system to apply weight changes of up to 1 kg in 235 g/s and yet allows users to walk around and turn in any direction. The video showcases the interaction with MobileGravity, sheds light on its technical implementation, and illustrates previously restricted use cases.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {646},\nnumpages = {2},\nkeywords = {haptics, virtual reality, weight perception, weight simulation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3649127,\nauthor = {Jauregui, Cinthya and Nguyen, Tiffany T. and Sallee, Sarah Hazel and Chandrasekar, Mohan Raj and A'Hearn, Liam and Woetzel, Dominic Jonathan and Paliwal, Pinak and MacDonald, Shea and Nguyen, Madison and Panich, Lee M. and Heitmuller, Danielle M. and Lueck, Amy and Lukoff, Kai},\ntitle = {We Are Still Here: The Th\\'{a}mien Ohlone Augmented Reality Tour},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3649127},\ndoi = {10.1145/3613905.3649127},\nabstract = {The Th\\'{a}mien Ohlone Augmented Reality (AR) Tour at Santa Clara University uses an innovative approach to highlight the overlooked story of the Ohlone Native American People on a campus dominated by Catholic monuments. This location-based AR tour integrates 3D models, narration by Native storytellers, and traditional songs at three strategic stops, fostering a deep connection between the land and its indigenous stories. The project follows the principles of \"design justice,\" emphasizing the active participation of the Muwekma Ohlone Tribe and adherence to the concept of rhetorical sovereignty. The “We Are Still Here” video shows novel co-design methods, such as \"landmark-based affinity diagramming,\" that align stories with specific locations. Overall, it showcases how location-based AR can be a powerful medium for social justice narratives.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {647},\nnumpages = {3},\nkeywords = {augmented reality, design justice, social justice, spatial justice},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",
    "@inproceedings{10.1145/3613905.3649117,\nauthor = {Murphy-Morgan, Claire and Collingham, Henry and Shaddock, Helen and Branley-Bell, Dawn},\ntitle = {“In the Only House in the Whole World but Everyone Doing the Same” Co-designed Animation as a Method of Critical Enquiry to Explore HCI Considerations for Remote Eating Disorder Support},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3649117},\ndoi = {10.1145/3613905.3649117},\nabstract = {The COVID-19 pandemic accelerated the shift to remote support for Eating Disorders (ED), necessitating innovative integration of digital technologies. The RHED-C (Remote Healthcare for Eating Disorders throughout COVID-19) project builds upon insights from the pandemic to enhance future remote care. Co-designed animation was used as an ethnographic method, capturing experiences of support recipients. Creative workshops generated ideas and guided script/storyboard creation. Unique challenges to the application of animation were unveiled, emphasising careful visual representation of EDs, with a focus towards abstract representation and audio. The animation features voiceovers exclusively by individuals with lived experience. The implications of animation in this space are two-fold: Firstly, as an impactful way to disseminate findings beyond academia, raising awareness of remote support advantages and challenges. Secondly, as an inclusive approach capturing diverse forms of expression and individual preference. An accompanying documentary details the process, advocating for responsible application of co-designed animation in HCI research.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {648},\nnumpages = {2},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}"
]